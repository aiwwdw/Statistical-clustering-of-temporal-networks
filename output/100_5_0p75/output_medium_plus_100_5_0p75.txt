nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [33:36<55:26:36, 2016.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|         | 2/100 [1:10:19<57:53:08, 2126.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|         | 3/100 [1:49:01<59:42:06, 2215.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|         | 4/100 [2:26:58<59:43:38, 2239.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|         | 5/100 [3:00:51<57:08:33, 2165.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|         | 6/100 [3:38:46<57:30:41, 2202.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|         | 7/100 [4:17:10<57:45:17, 2235.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|         | 8/100 [4:52:25<56:09:03, 2197.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|         | 9/100 [5:33:01<57:25:35, 2271.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|         | 10/100 [6:07:14<55:06:39, 2204.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|         | 11/100 [6:44:41<54:48:56, 2217.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-----------------------------------------------------------------------------------------
This iteration is 0
True Objective function: Loss = -11490.417433882372
Iteration 0: Loss = -27656.783493702078
Iteration 10: Loss = -12206.706108137025
Iteration 20: Loss = -12166.31116531481
Iteration 30: Loss = -11715.015159145365
Iteration 40: Loss = -11487.95355368318
Iteration 50: Loss = -11487.95355240856
Iteration 60: Loss = -11487.95355240856
1
Iteration 70: Loss = -11487.95355240856
2
Iteration 80: Loss = -11487.95355240856
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7324, 0.2676],
        [0.3064, 0.6936]], dtype=torch.float64)
alpha: tensor([0.5198, 0.4802])
beta: tensor([[[0.1941, 0.0980],
         [0.6659, 0.3924]],

        [[0.1146, 0.1015],
         [0.5003, 0.0651]],

        [[0.2895, 0.0990],
         [0.4270, 0.1493]],

        [[0.4485, 0.0981],
         [0.3588, 0.2043]],

        [[0.7091, 0.1055],
         [0.1554, 0.5794]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27524.406438115504
Iteration 100: Loss = -12099.930200043022
Iteration 200: Loss = -11768.445267987914
Iteration 300: Loss = -11583.380484443325
Iteration 400: Loss = -11488.287509045793
Iteration 500: Loss = -11487.482253859527
Iteration 600: Loss = -11487.146346902313
Iteration 700: Loss = -11486.95208985433
Iteration 800: Loss = -11486.826387716928
Iteration 900: Loss = -11486.738834844704
Iteration 1000: Loss = -11486.673447648827
Iteration 1100: Loss = -11486.622341332315
Iteration 1200: Loss = -11486.5829459347
Iteration 1300: Loss = -11486.552332060986
Iteration 1400: Loss = -11486.527840107896
Iteration 1500: Loss = -11486.507972326117
Iteration 1600: Loss = -11486.49154320481
Iteration 1700: Loss = -11486.477851114336
Iteration 1800: Loss = -11486.466248833804
Iteration 1900: Loss = -11486.45625428156
Iteration 2000: Loss = -11486.447676016493
Iteration 2100: Loss = -11486.440202933169
Iteration 2200: Loss = -11486.433567096248
Iteration 2300: Loss = -11486.427317003257
Iteration 2400: Loss = -11486.422105115287
Iteration 2500: Loss = -11486.417412117573
Iteration 2600: Loss = -11486.41329040794
Iteration 2700: Loss = -11486.409594240042
Iteration 2800: Loss = -11486.406287052732
Iteration 2900: Loss = -11486.403297368432
Iteration 3000: Loss = -11486.400595842002
Iteration 3100: Loss = -11486.398157726224
Iteration 3200: Loss = -11486.39595664859
Iteration 3300: Loss = -11486.393981311932
Iteration 3400: Loss = -11486.392113466218
Iteration 3500: Loss = -11486.390413804218
Iteration 3600: Loss = -11486.388905463176
Iteration 3700: Loss = -11486.387484256204
Iteration 3800: Loss = -11486.386117763395
Iteration 3900: Loss = -11486.384917009977
Iteration 4000: Loss = -11486.383783241006
Iteration 4100: Loss = -11486.382692010062
Iteration 4200: Loss = -11486.38177601267
Iteration 4300: Loss = -11486.380873256065
Iteration 4400: Loss = -11486.380030641465
Iteration 4500: Loss = -11486.37929841811
Iteration 4600: Loss = -11486.379537167959
1
Iteration 4700: Loss = -11486.377940368962
Iteration 4800: Loss = -11486.377262168155
Iteration 4900: Loss = -11486.3774001171
1
Iteration 5000: Loss = -11486.376162707907
Iteration 5100: Loss = -11486.375669590627
Iteration 5200: Loss = -11486.375194631084
Iteration 5300: Loss = -11486.37600269402
1
Iteration 5400: Loss = -11486.374329057055
Iteration 5500: Loss = -11486.373904927375
Iteration 5600: Loss = -11486.373805114614
Iteration 5700: Loss = -11486.373238672762
Iteration 5800: Loss = -11486.372899296086
Iteration 5900: Loss = -11486.372881603296
Iteration 6000: Loss = -11486.372314089676
Iteration 6100: Loss = -11486.372087819416
Iteration 6200: Loss = -11486.373625827364
1
Iteration 6300: Loss = -11486.371583671558
Iteration 6400: Loss = -11486.382539514714
1
Iteration 6500: Loss = -11486.378221052768
2
Iteration 6600: Loss = -11486.370961969533
Iteration 6700: Loss = -11486.370799134236
Iteration 6800: Loss = -11486.371140982756
1
Iteration 6900: Loss = -11486.376088580251
2
Iteration 7000: Loss = -11486.370296362284
Iteration 7100: Loss = -11486.37201155268
1
Iteration 7200: Loss = -11486.370103022415
Iteration 7300: Loss = -11486.369865013785
Iteration 7400: Loss = -11486.369877717347
1
Iteration 7500: Loss = -11486.374388180851
2
Iteration 7600: Loss = -11486.370606317945
3
Iteration 7700: Loss = -11486.369499577586
Iteration 7800: Loss = -11486.369560545747
1
Iteration 7900: Loss = -11486.369946286399
2
Iteration 8000: Loss = -11486.37471400311
3
Iteration 8100: Loss = -11486.36973401766
4
Iteration 8200: Loss = -11486.369037723718
Iteration 8300: Loss = -11486.372450792453
1
Iteration 8400: Loss = -11486.36885077186
Iteration 8500: Loss = -11486.376445678787
1
Iteration 8600: Loss = -11486.370239099193
2
Iteration 8700: Loss = -11486.36937910268
3
Iteration 8800: Loss = -11486.368731605131
Iteration 8900: Loss = -11486.372049015787
1
Iteration 9000: Loss = -11486.369661409311
2
Iteration 9100: Loss = -11486.385432326622
3
Iteration 9200: Loss = -11486.368517933519
Iteration 9300: Loss = -11486.372879945891
1
Iteration 9400: Loss = -11486.381851927728
2
Iteration 9500: Loss = -11486.376316732509
3
Iteration 9600: Loss = -11486.369843420394
4
Iteration 9700: Loss = -11486.378722092732
5
Iteration 9800: Loss = -11486.377764927043
6
Iteration 9900: Loss = -11486.383300815876
7
Iteration 10000: Loss = -11486.374800321897
8
Iteration 10100: Loss = -11486.378656482488
9
Iteration 10200: Loss = -11486.375034667772
10
Stopping early at iteration 10200 due to no improvement.
tensor([[  1.3095,  -5.9247],
        [ -9.9625,   5.3472],
        [-10.0231,   5.4078],
        [ -9.9217,   5.3065],
        [-10.4935,   5.8783],
        [  4.9023,  -9.5176],
        [  2.5875,  -7.2027],
        [  4.0418,  -8.6571],
        [  4.8312,  -9.4464],
        [  4.7074,  -9.3226],
        [  4.1583,  -8.7735],
        [  3.9406,  -8.5558],
        [  3.9593,  -8.5745],
        [  4.3957,  -9.0109],
        [ -9.2698,   4.6546],
        [  4.4431,  -9.0583],
        [  2.9600,  -7.5752],
        [ -3.9830,  -0.6322],
        [ -9.3875,   4.7722],
        [ -9.5775,   4.9623],
        [ -8.3707,   3.7555],
        [-10.1692,   5.5539],
        [  3.0464,  -7.6616],
        [ -8.9748,   4.3596],
        [-10.1493,   5.5340],
        [ -8.7826,   4.1674],
        [  4.7369,  -9.3521],
        [  3.3915,  -8.0068],
        [ -7.4923,   2.8770],
        [  4.5351,  -9.1503],
        [  4.3087,  -8.9240],
        [-10.3011,   5.6859],
        [ -9.6176,   5.0024],
        [  3.4086,  -8.0238],
        [ -9.0487,   4.4335],
        [ -9.5466,   4.9314],
        [ -6.7379,   2.1227],
        [-10.4716,   5.8564],
        [  4.2752,  -8.8904],
        [ -9.4113,   4.7961],
        [ -8.9617,   4.3464],
        [  4.7104,  -9.3256],
        [ -9.5029,   4.8877],
        [ -4.6040,  -0.0112],
        [  4.0192,  -8.6344],
        [ -9.9015,   5.2862],
        [ -9.1834,   4.5682],
        [ -8.7835,   4.1683],
        [ -7.5603,   2.9451],
        [ -9.2883,   4.6731],
        [ -8.6595,   4.0443],
        [  4.6744,  -9.2896],
        [  4.6613,  -9.2765],
        [  4.5112,  -9.1265],
        [  2.9614,  -7.5766],
        [  3.1312,  -7.7465],
        [ -9.3687,   4.7534],
        [ -9.2113,   4.5960],
        [  4.7718,  -9.3870],
        [ -8.2632,   3.6479],
        [  4.7476,  -9.3628],
        [ -7.5365,   2.9213],
        [  4.7190,  -9.3342],
        [ -5.9095,   1.2942],
        [  3.2139,  -7.8291],
        [ -9.5965,   4.9813],
        [  4.1519,  -8.7672],
        [  1.7287,  -6.3439],
        [  2.4125,  -7.0277],
        [  5.0079,  -9.6231],
        [  4.5999,  -9.2152],
        [-10.4103,   5.7950],
        [  4.3158,  -8.9310],
        [  3.9598,  -8.5750],
        [  4.1452,  -8.7605],
        [  4.8556,  -9.4708],
        [  4.5366,  -9.1518],
        [ -6.2299,   1.6147],
        [  4.6191,  -9.2343],
        [-10.1222,   5.5069],
        [ -9.9848,   5.3696],
        [  4.5557,  -9.1709],
        [  4.4901,  -9.1053],
        [  2.9727,  -7.5879],
        [ -9.2920,   4.6767],
        [ -9.9854,   5.3701],
        [ -9.6458,   5.0306],
        [  3.8606,  -8.4759],
        [ -9.4143,   4.7991],
        [ -9.1879,   4.5727],
        [ -9.5447,   4.9295],
        [ -9.7280,   5.1128],
        [ -9.1316,   4.5163],
        [  4.5463,  -9.1615],
        [  4.0111,  -8.6264],
        [  3.8241,  -8.4393],
        [-10.0371,   5.4219],
        [ -6.9633,   2.3481],
        [ -9.4820,   4.8668],
        [  3.8098,  -8.4251]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7347, 0.2653],
        [0.3054, 0.6946]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4905, 0.5095], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1978, 0.0977],
         [0.6659, 0.4002]],

        [[0.1146, 0.1015],
         [0.5003, 0.0651]],

        [[0.2895, 0.0990],
         [0.4270, 0.1493]],

        [[0.4485, 0.0981],
         [0.3588, 0.2043]],

        [[0.7091, 0.1054],
         [0.1554, 0.5794]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
Iteration 0: Loss = -17478.468577114694
Iteration 10: Loss = -11487.953562956844
Iteration 20: Loss = -11487.953545419
Iteration 30: Loss = -11487.953545419
1
Iteration 40: Loss = -11487.953545419
2
Iteration 50: Loss = -11487.953545419
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.6936, 0.3064],
        [0.2676, 0.7324]], dtype=torch.float64)
alpha: tensor([0.4802, 0.5198])
beta: tensor([[[0.3924, 0.0980],
         [0.3405, 0.1941]],

        [[0.0881, 0.1015],
         [0.6935, 0.6934]],

        [[0.5089, 0.0990],
         [0.3835, 0.6396]],

        [[0.2949, 0.0981],
         [0.1405, 0.8254]],

        [[0.3566, 0.1055],
         [0.3755, 0.6794]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17478.183599701857
Iteration 100: Loss = -11624.14954386627
Iteration 200: Loss = -11519.851055522025
Iteration 300: Loss = -11518.455879191966
Iteration 400: Loss = -11517.909659202312
Iteration 500: Loss = -11517.617560029483
Iteration 600: Loss = -11501.539445456923
Iteration 700: Loss = -11501.42539391158
Iteration 800: Loss = -11501.348057153482
Iteration 900: Loss = -11501.292421392836
Iteration 1000: Loss = -11501.250576146791
Iteration 1100: Loss = -11501.207532368346
Iteration 1200: Loss = -11491.684214265504
Iteration 1300: Loss = -11491.663336398684
Iteration 1400: Loss = -11491.646546631702
Iteration 1500: Loss = -11491.632776032071
Iteration 1600: Loss = -11491.621293631004
Iteration 1700: Loss = -11491.611617013092
Iteration 1800: Loss = -11491.603343017165
Iteration 1900: Loss = -11491.59624377644
Iteration 2000: Loss = -11491.590093424798
Iteration 2100: Loss = -11491.584755590195
Iteration 2200: Loss = -11491.580029184106
Iteration 2300: Loss = -11491.575955863747
Iteration 2400: Loss = -11491.57219743759
Iteration 2500: Loss = -11491.568927230792
Iteration 2600: Loss = -11491.56600678349
Iteration 2700: Loss = -11491.563359995427
Iteration 2800: Loss = -11491.56102513599
Iteration 2900: Loss = -11491.55886411282
Iteration 3000: Loss = -11491.556934715441
Iteration 3100: Loss = -11491.555199457867
Iteration 3200: Loss = -11491.553563398757
Iteration 3300: Loss = -11491.558187132945
1
Iteration 3400: Loss = -11491.550802161772
Iteration 3500: Loss = -11491.558929573734
1
Iteration 3600: Loss = -11491.548463418563
Iteration 3700: Loss = -11491.547441051967
Iteration 3800: Loss = -11491.546449490397
Iteration 3900: Loss = -11491.54559454828
Iteration 4000: Loss = -11491.544780960947
Iteration 4100: Loss = -11491.54403987273
Iteration 4200: Loss = -11491.54385278577
Iteration 4300: Loss = -11491.542662402078
Iteration 4400: Loss = -11491.54210436083
Iteration 4500: Loss = -11491.55289293612
1
Iteration 4600: Loss = -11491.540992637645
Iteration 4700: Loss = -11491.540505010353
Iteration 4800: Loss = -11491.540084303124
Iteration 4900: Loss = -11491.54044185903
1
Iteration 5000: Loss = -11491.539242739651
Iteration 5100: Loss = -11491.53890715787
Iteration 5200: Loss = -11491.539393465964
1
Iteration 5300: Loss = -11491.538196642012
Iteration 5400: Loss = -11491.537928932683
Iteration 5500: Loss = -11491.539904719772
1
Iteration 5600: Loss = -11491.537377788934
Iteration 5700: Loss = -11491.543036108238
1
Iteration 5800: Loss = -11491.536963246423
Iteration 5900: Loss = -11491.536708028414
Iteration 6000: Loss = -11491.537913705
1
Iteration 6100: Loss = -11491.547063066655
2
Iteration 6200: Loss = -11491.538326896327
3
Iteration 6300: Loss = -11491.536058500338
Iteration 6400: Loss = -11491.541383177046
1
Iteration 6500: Loss = -11491.552927977917
2
Iteration 6600: Loss = -11491.550359420735
3
Iteration 6700: Loss = -11491.535313839231
Iteration 6800: Loss = -11491.54022147981
1
Iteration 6900: Loss = -11491.674180671895
2
Iteration 7000: Loss = -11491.538245863032
3
Iteration 7100: Loss = -11491.543471493791
4
Iteration 7200: Loss = -11491.587281449392
5
Iteration 7300: Loss = -11491.538467512824
6
Iteration 7400: Loss = -11491.536826529189
7
Iteration 7500: Loss = -11491.535996008113
8
Iteration 7600: Loss = -11491.562684122828
9
Iteration 7700: Loss = -11491.549919661602
10
Stopping early at iteration 7700 due to no improvement.
tensor([[-4.2625,  2.8090],
        [ 6.3841, -7.7992],
        [ 6.6045, -8.9579],
        [ 5.9613, -7.6417],
        [ 6.9040, -9.0339],
        [-7.2533,  5.8667],
        [-5.7106,  3.9349],
        [-7.3445,  5.9566],
        [-7.9427,  6.5383],
        [-8.3729,  5.5731],
        [-7.2890,  5.3525],
        [-7.2731,  5.8201],
        [-6.9376,  5.3432],
        [-9.2998,  4.6845],
        [ 5.5989, -7.5756],
        [-7.3485,  5.3937],
        [-5.9685,  4.5274],
        [ 0.5911, -2.8612],
        [ 5.7783, -7.9744],
        [ 6.9292, -8.4099],
        [ 5.2397, -7.2478],
        [ 5.9906, -7.6995],
        [-6.0061,  4.6001],
        [ 5.5421, -7.1125],
        [ 6.3795, -7.7860],
        [ 5.8331, -7.4366],
        [-8.1912,  6.6158],
        [-6.4632,  4.9334],
        [ 4.5034, -5.8996],
        [-8.7357,  5.5304],
        [-8.3712,  5.3931],
        [ 6.2510, -7.7903],
        [ 7.0598, -8.5064],
        [-6.9899,  4.3901],
        [ 5.6383, -7.8229],
        [ 6.1989, -7.6977],
        [ 2.1190, -6.7342],
        [ 5.9943, -7.6437],
        [-7.1134,  5.7039],
        [ 6.0676, -8.8306],
        [ 5.9781, -7.3951],
        [-7.7051,  6.2752],
        [ 4.8272, -8.7672],
        [ 1.1522, -3.5959],
        [-7.7530,  5.2257],
        [ 6.5110, -7.9761],
        [ 5.7740, -7.3865],
        [ 4.7573, -8.3999],
        [ 4.1821, -6.3696],
        [ 6.2508, -9.1670],
        [ 5.9516, -7.3702],
        [-7.1830,  5.7741],
        [-8.5401,  5.5231],
        [-7.9602,  6.4398],
        [-6.0623,  4.4074],
        [-6.1463,  4.5726],
        [ 5.9335, -8.5479],
        [ 5.9839, -7.9068],
        [-7.3268,  5.8703],
        [ 3.8093, -8.4246],
        [-7.5074,  5.5282],
        [ 4.2535, -6.0054],
        [-8.7823,  5.0827],
        [ 2.9038, -4.2956],
        [-8.7766,  6.0977],
        [ 5.2115, -8.7118],
        [-8.0197,  5.4710],
        [-4.9043,  3.0811],
        [-5.3641,  3.9596],
        [-7.7218,  6.1032],
        [-8.9939,  5.8499],
        [ 6.7756, -9.1111],
        [-7.6777,  5.6710],
        [-7.5527,  5.1027],
        [-7.5031,  5.8331],
        [-7.8276,  6.3076],
        [-7.9869,  4.6635],
        [ 3.2403, -4.6302],
        [-8.8804,  5.6263],
        [ 6.5888, -8.0116],
        [ 6.1525, -7.7885],
        [-7.7746,  6.0307],
        [-8.0504,  6.6623],
        [-5.9482,  4.5611],
        [ 6.0030, -7.3908],
        [ 6.3784, -7.8182],
        [ 5.9726, -8.3118],
        [-6.8438,  5.4086],
        [ 5.7470, -7.5554],
        [ 6.1334, -7.7854],
        [ 5.4238, -8.2897],
        [ 5.7071, -7.4863],
        [ 6.0188, -7.4239],
        [-8.0585,  5.4828],
        [-7.9654,  4.8123],
        [-6.9178,  5.4910],
        [ 6.1893, -7.8210],
        [ 3.9953, -5.3901],
        [ 6.7191, -8.8092],
        [-6.9960,  4.9155]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7013, 0.2987],
        [0.2624, 0.7376]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5095, 0.4905], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3984, 0.0977],
         [0.3405, 0.1988]],

        [[0.0881, 0.1014],
         [0.6935, 0.6934]],

        [[0.5089, 0.0993],
         [0.3835, 0.6396]],

        [[0.2949, 0.0989],
         [0.1405, 0.8254]],

        [[0.3566, 0.1055],
         [0.3755, 0.6794]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320634059188
Average Adjusted Rand Index: 0.9839964884296097
Iteration 0: Loss = -33966.327594876064
Iteration 10: Loss = -12229.819079643486
Iteration 20: Loss = -12228.56899965856
Iteration 30: Loss = -12049.736303220803
Iteration 40: Loss = -11643.721676437637
Iteration 50: Loss = -11644.287690618898
1
Iteration 60: Loss = -11648.717505786564
2
Iteration 70: Loss = -11650.455240622709
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7114, 0.2886],
        [0.4666, 0.5334]], dtype=torch.float64)
alpha: tensor([0.5727, 0.4273])
beta: tensor([[[0.2108, 0.0979],
         [0.0194, 0.3839]],

        [[0.2208, 0.1014],
         [0.3902, 0.9139]],

        [[0.4855, 0.0988],
         [0.6922, 0.6257]],

        [[0.9101, 0.0980],
         [0.6772, 0.7360]],

        [[0.8163, 0.1053],
         [0.3570, 0.3810]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 33
Adjusted Rand Index: 0.10870370747399068
Global Adjusted Rand Index: 0.5290426865070819
Average Adjusted Rand Index: 0.813740083222088
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33954.626648821424
Iteration 100: Loss = -12246.701731783147
Iteration 200: Loss = -12230.058586083995
Iteration 300: Loss = -12222.110827589877
Iteration 400: Loss = -12215.565253702545
Iteration 500: Loss = -12202.166865983942
Iteration 600: Loss = -12168.831341845185
Iteration 700: Loss = -12076.619332898337
Iteration 800: Loss = -11921.836769717253
Iteration 900: Loss = -11795.198574057953
Iteration 1000: Loss = -11768.504460979346
Iteration 1100: Loss = -11763.690139998678
Iteration 1200: Loss = -11762.911879397394
Iteration 1300: Loss = -11762.554625139575
Iteration 1400: Loss = -11762.246208063134
Iteration 1500: Loss = -11762.078257501036
Iteration 1600: Loss = -11761.959621367703
Iteration 1700: Loss = -11761.868742250008
Iteration 1800: Loss = -11761.796722521769
Iteration 1900: Loss = -11761.738086284551
Iteration 2000: Loss = -11761.689259289205
Iteration 2100: Loss = -11761.647847746115
Iteration 2200: Loss = -11761.61198631172
Iteration 2300: Loss = -11761.579373056344
Iteration 2400: Loss = -11761.536570819537
Iteration 2500: Loss = -11760.7444854233
Iteration 2600: Loss = -11760.58027308515
Iteration 2700: Loss = -11760.502204022461
Iteration 2800: Loss = -11757.922298208476
Iteration 2900: Loss = -11757.809900927965
Iteration 3000: Loss = -11757.534261386014
Iteration 3100: Loss = -11757.494573723025
Iteration 3200: Loss = -11757.475365320328
Iteration 3300: Loss = -11757.44803569285
Iteration 3400: Loss = -11757.277390798597
Iteration 3500: Loss = -11757.265458466933
Iteration 3600: Loss = -11757.256389979213
Iteration 3700: Loss = -11757.245183449451
Iteration 3800: Loss = -11757.180810094897
Iteration 3900: Loss = -11757.157327754796
Iteration 4000: Loss = -11757.151801476148
Iteration 4100: Loss = -11757.146959198524
Iteration 4200: Loss = -11757.14246767645
Iteration 4300: Loss = -11757.138290000943
Iteration 4400: Loss = -11757.134412990847
Iteration 4500: Loss = -11757.133827155845
Iteration 4600: Loss = -11757.12716671156
Iteration 4700: Loss = -11757.12400804694
Iteration 4800: Loss = -11757.12102314921
Iteration 4900: Loss = -11757.118273651653
Iteration 5000: Loss = -11757.115471702353
Iteration 5100: Loss = -11757.112765209902
Iteration 5200: Loss = -11757.117702954185
1
Iteration 5300: Loss = -11757.106566887576
Iteration 5400: Loss = -11757.102567356143
Iteration 5500: Loss = -11757.098545544754
Iteration 5600: Loss = -11757.09595498995
Iteration 5700: Loss = -11757.093774302342
Iteration 5800: Loss = -11757.091346930429
Iteration 5900: Loss = -11757.0888378207
Iteration 6000: Loss = -11757.087468618514
Iteration 6100: Loss = -11757.086130304562
Iteration 6200: Loss = -11757.084893817022
Iteration 6300: Loss = -11757.08368916442
Iteration 6400: Loss = -11757.082576334853
Iteration 6500: Loss = -11757.081458467588
Iteration 6600: Loss = -11757.08517734594
1
Iteration 6700: Loss = -11757.078837093613
Iteration 6800: Loss = -11757.07702943854
Iteration 6900: Loss = -11757.053871534177
Iteration 7000: Loss = -11753.242144508862
Iteration 7100: Loss = -11753.240084434787
Iteration 7200: Loss = -11753.239025940575
Iteration 7300: Loss = -11753.238394147766
Iteration 7400: Loss = -11753.237754782574
Iteration 7500: Loss = -11753.237433657165
Iteration 7600: Loss = -11753.2371622814
Iteration 7700: Loss = -11753.237091926812
Iteration 7800: Loss = -11753.236372046576
Iteration 7900: Loss = -11753.236296980755
Iteration 8000: Loss = -11753.241239964444
1
Iteration 8100: Loss = -11753.236267392756
Iteration 8200: Loss = -11753.234700587958
Iteration 8300: Loss = -11753.236686690578
1
Iteration 8400: Loss = -11753.238994482337
2
Iteration 8500: Loss = -11753.23383824627
Iteration 8600: Loss = -11753.234037205695
1
Iteration 8700: Loss = -11753.232968211305
Iteration 8800: Loss = -11753.237659189568
1
Iteration 8900: Loss = -11751.952035971126
Iteration 9000: Loss = -11751.94214075157
Iteration 9100: Loss = -11751.944592595943
1
Iteration 9200: Loss = -11751.94695939513
2
Iteration 9300: Loss = -11751.942191563303
3
Iteration 9400: Loss = -11751.959005442264
4
Iteration 9500: Loss = -11751.941144589673
Iteration 9600: Loss = -11751.944893157748
1
Iteration 9700: Loss = -11751.888847912773
Iteration 9800: Loss = -11751.888482136364
Iteration 9900: Loss = -11751.956575468417
1
Iteration 10000: Loss = -11751.89588512864
2
Iteration 10100: Loss = -11751.891873359322
3
Iteration 10200: Loss = -11751.887435001587
Iteration 10300: Loss = -11751.884831834284
Iteration 10400: Loss = -11751.872034160046
Iteration 10500: Loss = -11751.885553980146
1
Iteration 10600: Loss = -11751.871550253536
Iteration 10700: Loss = -11751.876172066724
1
Iteration 10800: Loss = -11751.871405379332
Iteration 10900: Loss = -11751.887512948293
1
Iteration 11000: Loss = -11751.999580554399
2
Iteration 11100: Loss = -11751.871725979201
3
Iteration 11200: Loss = -11751.871253482945
Iteration 11300: Loss = -11751.874039857415
1
Iteration 11400: Loss = -11751.892099658091
2
Iteration 11500: Loss = -11751.908997669203
3
Iteration 11600: Loss = -11751.87025873164
Iteration 11700: Loss = -11751.869558947383
Iteration 11800: Loss = -11751.873142790799
1
Iteration 11900: Loss = -11751.955158402561
2
Iteration 12000: Loss = -11751.88321233721
3
Iteration 12100: Loss = -11751.872235375977
4
Iteration 12200: Loss = -11751.89157204233
5
Iteration 12300: Loss = -11752.007129399482
6
Iteration 12400: Loss = -11751.869769440658
7
Iteration 12500: Loss = -11751.869347077782
Iteration 12600: Loss = -11751.898370191675
1
Iteration 12700: Loss = -11751.869192255217
Iteration 12800: Loss = -11751.870901531389
1
Iteration 12900: Loss = -11751.869149494
Iteration 13000: Loss = -11751.869359100958
1
Iteration 13100: Loss = -11751.869360769932
2
Iteration 13200: Loss = -11747.129367302146
Iteration 13300: Loss = -11747.098388352239
Iteration 13400: Loss = -11747.106879056882
1
Iteration 13500: Loss = -11747.098066082335
Iteration 13600: Loss = -11747.098682741409
1
Iteration 13700: Loss = -11747.100358518912
2
Iteration 13800: Loss = -11747.098367788385
3
Iteration 13900: Loss = -11747.245216683912
4
Iteration 14000: Loss = -11747.099823487335
5
Iteration 14100: Loss = -11747.101903761004
6
Iteration 14200: Loss = -11747.1160657894
7
Iteration 14300: Loss = -11747.09791219917
Iteration 14400: Loss = -11747.098192627718
1
Iteration 14500: Loss = -11747.120342678036
2
Iteration 14600: Loss = -11747.098312263235
3
Iteration 14700: Loss = -11747.118666379549
4
Iteration 14800: Loss = -11747.10514666392
5
Iteration 14900: Loss = -11747.104636496051
6
Iteration 15000: Loss = -11747.100993985172
7
Iteration 15100: Loss = -11747.098016577977
8
Iteration 15200: Loss = -11747.10216877103
9
Iteration 15300: Loss = -11747.097795367361
Iteration 15400: Loss = -11747.105435615098
1
Iteration 15500: Loss = -11747.10059347681
2
Iteration 15600: Loss = -11747.097818555452
3
Iteration 15700: Loss = -11747.109342703838
4
Iteration 15800: Loss = -11747.098470220044
5
Iteration 15900: Loss = -11747.09791835983
6
Iteration 16000: Loss = -11747.096784139465
Iteration 16100: Loss = -11747.101752476414
1
Iteration 16200: Loss = -11747.105763778765
2
Iteration 16300: Loss = -11747.098593422968
3
Iteration 16400: Loss = -11747.093845492722
Iteration 16500: Loss = -11747.097966794801
1
Iteration 16600: Loss = -11747.093016918143
Iteration 16700: Loss = -11747.175084196228
1
Iteration 16800: Loss = -11747.092881663957
Iteration 16900: Loss = -11747.094766847911
1
Iteration 17000: Loss = -11747.11116559166
2
Iteration 17100: Loss = -11747.092900590316
3
Iteration 17200: Loss = -11747.116967215654
4
Iteration 17300: Loss = -11747.095600402648
5
Iteration 17400: Loss = -11746.788526576867
Iteration 17500: Loss = -11746.68780034883
Iteration 17600: Loss = -11746.616126067382
Iteration 17700: Loss = -11746.61063600763
Iteration 17800: Loss = -11746.610539948724
Iteration 17900: Loss = -11746.612227980891
1
Iteration 18000: Loss = -11746.614676162922
2
Iteration 18100: Loss = -11746.617546388072
3
Iteration 18200: Loss = -11746.616685097044
4
Iteration 18300: Loss = -11746.624241705218
5
Iteration 18400: Loss = -11746.611281042702
6
Iteration 18500: Loss = -11746.772879246964
7
Iteration 18600: Loss = -11746.611681795312
8
Iteration 18700: Loss = -11746.612380151048
9
Iteration 18800: Loss = -11746.611324841915
10
Stopping early at iteration 18800 due to no improvement.
tensor([[  2.7855,  -4.1747],
        [ -9.2179,   7.5684],
        [-10.0397,   8.5308],
        [ -9.8168,   8.4179],
        [-12.3174,   8.0487],
        [  7.0820,  -8.5024],
        [  4.1205,  -5.9032],
        [  7.6327, -10.3663],
        [  6.0759, -10.4759],
        [  6.8049,  -8.2043],
        [  6.9175, -11.4646],
        [  4.9919,  -6.3891],
        [  4.9530,  -6.4309],
        [  6.2447,  -7.6330],
        [ -8.7825,   7.3755],
        [  7.1398,  -8.5267],
        [  3.5823,  -5.0665],
        [ -2.5896,   0.4141],
        [ -9.6256,   7.9297],
        [ -9.4576,   8.0521],
        [-10.4616,   8.2627],
        [ -9.2232,   7.5650],
        [  3.3980,  -5.0469],
        [ -9.1553,   7.6591],
        [-10.0000,   7.4888],
        [ -7.7019,   5.2204],
        [  7.6602,  -9.0661],
        [  7.3054,  -8.9762],
        [ -7.0451,   5.5917],
        [  6.7143,  -8.8287],
        [  5.6500,  -7.0393],
        [-11.7000,   7.6936],
        [-10.2514,   7.8123],
        [  3.6880,  -5.7337],
        [ -8.0485,   6.4539],
        [ -9.5223,   7.2115],
        [ -6.1195,   4.3429],
        [-11.1643,   8.5247],
        [  5.7145,  -7.4397],
        [ -9.4253,   7.9626],
        [-10.5520,   5.9367],
        [  6.7875,  -8.6195],
        [-10.7949,   8.3163],
        [ -6.4787,   5.0698],
        [  7.7034, -10.4415],
        [-10.1191,   8.6053],
        [ -9.4904,   7.8944],
        [ -8.3994,   6.4383],
        [ -6.1028,   4.0993],
        [ -9.8899,   8.4822],
        [ -7.9222,   5.9483],
        [  7.3036,  -8.8635],
        [  6.8172,  -8.7087],
        [  6.5132,  -9.5482],
        [  3.5415,  -5.1172],
        [  4.4798,  -5.9637],
        [-10.8988,   8.0538],
        [-10.0398,   8.5492],
        [  5.6649,  -9.9057],
        [ -7.1620,   5.0140],
        [  8.1752,  -9.6482],
        [ -7.0013,   5.5020],
        [  7.0426, -10.2198],
        [ -5.0532,   3.4505],
        [  3.0901,  -4.9242],
        [-10.2486,   7.4736],
        [  6.6116,  -8.0352],
        [  2.1402,  -3.5845],
        [  4.5304,  -5.9168],
        [  7.5691,  -9.9238],
        [  6.8974,  -8.3203],
        [-10.5891,   8.5098],
        [  8.8217, -10.3136],
        [  4.4865,  -6.6760],
        [  4.8032,  -7.1326],
        [  7.4877,  -9.1006],
        [  6.8260,  -8.2348],
        [ -5.6695,   4.0283],
        [  7.0309,  -8.4176],
        [-10.7578,   8.1377],
        [-11.9087,   8.8163],
        [  7.2211,  -8.6089],
        [  6.2132,  -9.4292],
        [  2.7170,  -5.9412],
        [ -8.2393,   6.2888],
        [-10.2241,   8.4324],
        [ -8.9661,   6.8995],
        [  4.2927,  -5.8050],
        [-10.3737,   8.8531],
        [ -9.0538,   7.4810],
        [ -8.6228,   7.2159],
        [-10.2756,   8.1198],
        [-10.3335,   8.5931],
        [  6.2076,  -8.6569],
        [  4.3854,  -6.7765],
        [  4.2026,  -6.9605],
        [ -9.9716,   8.5285],
        [ -6.6718,   5.1947],
        [-10.5630,   8.0338],
        [  5.1355,  -7.2645]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5878, 0.4122],
        [0.6959, 0.3041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4918, 0.5082], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2235, 0.0979],
         [0.0194, 0.3815]],

        [[0.2208, 0.1016],
         [0.3902, 0.9139]],

        [[0.4855, 0.0993],
         [0.6922, 0.6257]],

        [[0.9101, 0.0979],
         [0.6772, 0.7360]],

        [[0.8163, 0.1001],
         [0.3570, 0.3810]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.04092630029067774
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 29
Adjusted Rand Index: 0.16949300045420165
Global Adjusted Rand Index: 0.2137256165729212
Average Adjusted Rand Index: 0.6340832018762659
Iteration 0: Loss = -24232.622982082303
Iteration 10: Loss = -11520.536228062803
Iteration 20: Loss = -11487.953552222478
Iteration 30: Loss = -11487.953551644934
Iteration 40: Loss = -11487.953551644934
1
Iteration 50: Loss = -11487.953551644934
2
Iteration 60: Loss = -11487.953551644934
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6936, 0.3064],
        [0.2676, 0.7324]], dtype=torch.float64)
alpha: tensor([0.4802, 0.5198])
beta: tensor([[[0.3924, 0.0980],
         [0.1788, 0.1941]],

        [[0.2633, 0.1015],
         [0.1857, 0.7160]],

        [[0.3411, 0.0990],
         [0.4820, 0.1399]],

        [[0.5103, 0.0981],
         [0.4515, 0.4485]],

        [[0.8726, 0.1055],
         [0.5652, 0.8270]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24169.965354476346
Iteration 100: Loss = -12226.879758349001
Iteration 200: Loss = -12195.044329157488
Iteration 300: Loss = -11821.034329460961
Iteration 400: Loss = -11804.420890877798
Iteration 500: Loss = -11802.966385947264
Iteration 600: Loss = -11802.387072759788
Iteration 700: Loss = -11788.592934236955
Iteration 800: Loss = -11785.679936848572
Iteration 900: Loss = -11785.354868110493
Iteration 1000: Loss = -11785.21000415213
Iteration 1100: Loss = -11780.047323900513
Iteration 1200: Loss = -11779.337617904379
Iteration 1300: Loss = -11779.19398403238
Iteration 1400: Loss = -11779.15988425651
Iteration 1500: Loss = -11779.137728271484
Iteration 1600: Loss = -11779.120532300678
Iteration 1700: Loss = -11779.106666723395
Iteration 1800: Loss = -11779.095196669663
Iteration 1900: Loss = -11779.085381428291
Iteration 2000: Loss = -11779.076868400698
Iteration 2100: Loss = -11779.069483279805
Iteration 2200: Loss = -11779.06295272595
Iteration 2300: Loss = -11779.056528628547
Iteration 2400: Loss = -11779.049691748723
Iteration 2500: Loss = -11779.04426786589
Iteration 2600: Loss = -11779.03751900301
Iteration 2700: Loss = -11779.017197806199
Iteration 2800: Loss = -11779.013426026702
Iteration 2900: Loss = -11779.010247318223
Iteration 3000: Loss = -11779.006968380565
Iteration 3100: Loss = -11779.000847658088
Iteration 3200: Loss = -11778.960320669645
Iteration 3300: Loss = -11776.366244607478
Iteration 3400: Loss = -11776.037210898076
Iteration 3500: Loss = -11776.030510999497
Iteration 3600: Loss = -11776.026149458676
Iteration 3700: Loss = -11776.021466029008
Iteration 3800: Loss = -11776.019255393194
Iteration 3900: Loss = -11776.01774712375
Iteration 4000: Loss = -11776.016240586814
Iteration 4100: Loss = -11776.014047604
Iteration 4200: Loss = -11776.007949538638
Iteration 4300: Loss = -11776.006216059792
Iteration 4400: Loss = -11776.005513501275
Iteration 4500: Loss = -11776.004849743298
Iteration 4600: Loss = -11776.004208331477
Iteration 4700: Loss = -11776.003699491863
Iteration 4800: Loss = -11776.003215232373
Iteration 4900: Loss = -11776.002694105688
Iteration 5000: Loss = -11776.002245714395
Iteration 5100: Loss = -11776.001859733065
Iteration 5200: Loss = -11776.001477201238
Iteration 5300: Loss = -11776.001118054275
Iteration 5400: Loss = -11776.000782275689
Iteration 5500: Loss = -11776.000451919968
Iteration 5600: Loss = -11776.000140844086
Iteration 5700: Loss = -11775.999896204898
Iteration 5800: Loss = -11775.999618373624
Iteration 5900: Loss = -11775.999374022405
Iteration 6000: Loss = -11775.999171664422
Iteration 6100: Loss = -11775.99896418596
Iteration 6200: Loss = -11775.99874968354
Iteration 6300: Loss = -11775.99855698284
Iteration 6400: Loss = -11775.998376844756
Iteration 6500: Loss = -11775.998286566057
Iteration 6600: Loss = -11775.998047920388
Iteration 6700: Loss = -11775.99798296195
Iteration 6800: Loss = -11775.997775632955
Iteration 6900: Loss = -11775.997623868609
Iteration 7000: Loss = -11775.997529285562
Iteration 7100: Loss = -11775.997406824388
Iteration 7200: Loss = -11776.000326552148
1
Iteration 7300: Loss = -11775.997190156455
Iteration 7400: Loss = -11775.997085658777
Iteration 7500: Loss = -11775.997214111041
1
Iteration 7600: Loss = -11775.996931703863
Iteration 7700: Loss = -11775.996815748455
Iteration 7800: Loss = -11775.997046624132
1
Iteration 7900: Loss = -11775.996664484526
Iteration 8000: Loss = -11775.996580231109
Iteration 8100: Loss = -11775.996559012436
Iteration 8200: Loss = -11775.996523110864
Iteration 8300: Loss = -11775.996778305718
1
Iteration 8400: Loss = -11775.996625492531
2
Iteration 8500: Loss = -11776.000686736892
3
Iteration 8600: Loss = -11775.99633866782
Iteration 8700: Loss = -11775.996454464479
1
Iteration 8800: Loss = -11775.997463525315
2
Iteration 8900: Loss = -11775.996222916177
Iteration 9000: Loss = -11775.996305056218
1
Iteration 9100: Loss = -11775.996106033283
Iteration 9200: Loss = -11776.082654977146
1
Iteration 9300: Loss = -11776.003135974574
2
Iteration 9400: Loss = -11776.005461847648
3
Iteration 9500: Loss = -11775.996021545337
Iteration 9600: Loss = -11776.000320243305
1
Iteration 9700: Loss = -11775.99587514929
Iteration 9800: Loss = -11775.997074011415
1
Iteration 9900: Loss = -11776.217709012073
2
Iteration 10000: Loss = -11775.995781937832
Iteration 10100: Loss = -11776.002705702538
1
Iteration 10200: Loss = -11775.99569044352
Iteration 10300: Loss = -11775.999208105955
1
Iteration 10400: Loss = -11775.995696620954
2
Iteration 10500: Loss = -11775.996895625105
3
Iteration 10600: Loss = -11776.111066265
4
Iteration 10700: Loss = -11775.996829978512
5
Iteration 10800: Loss = -11775.99619170558
6
Iteration 10900: Loss = -11776.003262090966
7
Iteration 11000: Loss = -11775.99565606003
Iteration 11100: Loss = -11776.001143331509
1
Iteration 11200: Loss = -11775.996457021365
2
Iteration 11300: Loss = -11776.008548904007
3
Iteration 11400: Loss = -11776.023219095103
4
Iteration 11500: Loss = -11775.99552781766
Iteration 11600: Loss = -11775.996157632067
1
Iteration 11700: Loss = -11775.995570648121
2
Iteration 11800: Loss = -11775.997352033106
3
Iteration 11900: Loss = -11776.0140471064
4
Iteration 12000: Loss = -11776.00298879387
5
Iteration 12100: Loss = -11775.995645850478
6
Iteration 12200: Loss = -11775.99560265292
7
Iteration 12300: Loss = -11775.995572154401
8
Iteration 12400: Loss = -11775.997433710532
9
Iteration 12500: Loss = -11776.007709800144
10
Stopping early at iteration 12500 due to no improvement.
tensor([[ -4.2602,   2.1185],
        [ -7.4532,   6.0643],
        [ -7.6727,   6.0160],
        [ -8.0209,   6.6210],
        [ -8.6193,   7.2330],
        [  2.7927,  -4.1946],
        [ -4.6522,   2.2406],
        [ -2.4272,   0.1374],
        [ -4.5820,   2.3084],
        [  2.4349,  -5.0605],
        [ -6.6691,   5.2362],
        [ -5.3862,   3.8897],
        [ -6.6118,   3.8557],
        [ -2.8773,  -0.3520],
        [ -6.4656,   4.0198],
        [ -4.3987,   2.8328],
        [ -2.2594,   0.8516],
        [ -7.4806,   5.6493],
        [ -8.0924,   6.1620],
        [ -8.5705,   7.0187],
        [ -8.4932,   6.4179],
        [ -8.5764,   6.3694],
        [ -4.1935,   2.7568],
        [ -7.6998,   5.8260],
        [ -7.2249,   5.8352],
        [ -5.8010,   4.3597],
        [ -5.8718,   3.7004],
        [  1.9080,  -3.5880],
        [ -8.8467,   5.7368],
        [ -3.7456,   2.2103],
        [  2.7072,  -4.2995],
        [ -8.2552,   6.6275],
        [ -7.7115,   6.3168],
        [  1.1056,  -2.5181],
        [ -7.7108,   6.3165],
        [ -7.6210,   5.5843],
        [ -7.4822,   6.0955],
        [ -8.3383,   6.7655],
        [ -5.3390,   1.8806],
        [ -8.1661,   6.6538],
        [ -9.7615,   6.7378],
        [ -2.9703,   1.4199],
        [ -7.5886,   5.7585],
        [ -4.9136,   3.5272],
        [ -3.6391,   2.2211],
        [ -8.4703,   6.2049],
        [ -8.2537,   6.8470],
        [ -9.4708,   4.8556],
        [ -8.1080,   3.4928],
        [ -8.2802,   6.8406],
        [ -8.7636,   7.2419],
        [ -5.6778,   4.1411],
        [  1.9194,  -4.0016],
        [ -2.7871,   0.8449],
        [ -6.5112,   1.8960],
        [ -0.3262,  -4.2890],
        [ -8.4969,   6.9584],
        [ -8.8989,   7.3247],
        [ -4.4437,   2.2309],
        [ -8.0584,   6.4494],
        [ -4.9120,   3.4596],
        [ -7.2613,   4.9961],
        [  2.8348,  -4.4075],
        [ -8.7949,   7.3686],
        [ -2.2687,   0.3892],
        [ -8.3667,   6.8117],
        [  2.0313,  -3.8970],
        [ -5.2982,   2.9561],
        [ -6.7540,   4.7481],
        [  2.0874,  -3.4899],
        [ -0.6513,  -0.8320],
        [-10.1009,   7.0497],
        [  1.9434,  -4.9870],
        [ -3.3398,   1.7987],
        [ -5.0574,   3.6692],
        [ -1.5710,   0.1668],
        [ -4.0386,   2.3968],
        [ -3.4033,   0.8507],
        [ -3.8723,   1.8081],
        [ -8.4540,   6.1009],
        [ -7.5959,   5.8783],
        [ -3.4817,   2.0290],
        [ -1.6971,   0.2891],
        [ -4.9835,   0.3683],
        [ -8.0717,   6.5378],
        [ -8.4202,   6.5642],
        [ -8.8704,   5.4896],
        [ -4.2642,   2.6216],
        [ -8.1960,   6.8097],
        [ -8.2411,   6.7264],
        [ -7.5220,   6.1320],
        [ -7.8379,   6.3205],
        [ -7.8303,   6.4313],
        [  2.6256,  -4.1853],
        [  0.5540,  -2.4143],
        [  1.9955,  -4.1817],
        [ -9.1152,   6.9299],
        [ -7.3113,   5.6189],
        [ -9.0476,   7.2841],
        [  1.3582,  -2.7935]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3997, 0.6003],
        [0.4356, 0.5644]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1601, 0.8399], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3899, 0.0971],
         [0.1788, 0.2179]],

        [[0.2633, 0.1015],
         [0.1857, 0.7160]],

        [[0.3411, 0.1009],
         [0.4820, 0.1399]],

        [[0.5103, 0.0970],
         [0.4515, 0.4485]],

        [[0.8726, 0.1054],
         [0.5652, 0.8270]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 67
Adjusted Rand Index: 0.11060776050587955
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.031036965420772222
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.23272478047643252
Average Adjusted Rand Index: 0.6203282869126203
Iteration 0: Loss = -16853.319952341346
Iteration 10: Loss = -11487.954376519046
Iteration 20: Loss = -11487.95355164493
Iteration 30: Loss = -11487.953551644934
1
Iteration 40: Loss = -11487.953551644934
2
Iteration 50: Loss = -11487.953551644934
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.6936, 0.3064],
        [0.2676, 0.7324]], dtype=torch.float64)
alpha: tensor([0.4802, 0.5198])
beta: tensor([[[0.3924, 0.0980],
         [0.6522, 0.1941]],

        [[0.4273, 0.1015],
         [0.4905, 0.1984]],

        [[0.9604, 0.0990],
         [0.3950, 0.9496]],

        [[0.1518, 0.0981],
         [0.5209, 0.3417]],

        [[0.8552, 0.1055],
         [0.5566, 0.4432]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16853.268663563536
Iteration 100: Loss = -11711.89169626557
Iteration 200: Loss = -11492.865079505704
Iteration 300: Loss = -11487.60183806795
Iteration 400: Loss = -11487.132302274902
Iteration 500: Loss = -11486.896899863223
Iteration 600: Loss = -11486.758794822572
Iteration 700: Loss = -11486.67111391737
Iteration 800: Loss = -11486.611160796774
Iteration 900: Loss = -11486.56807566679
Iteration 1000: Loss = -11486.535873540846
Iteration 1100: Loss = -11486.511126198651
Iteration 1200: Loss = -11486.491701632534
Iteration 1300: Loss = -11486.476081809646
Iteration 1400: Loss = -11486.463341266572
Iteration 1500: Loss = -11486.452706577213
Iteration 1600: Loss = -11486.443678943939
Iteration 1700: Loss = -11486.435116086017
Iteration 1800: Loss = -11486.42315206054
Iteration 1900: Loss = -11486.417129421372
Iteration 2000: Loss = -11486.412254701472
Iteration 2100: Loss = -11486.408131750526
Iteration 2200: Loss = -11486.404532243467
Iteration 2300: Loss = -11486.4012947259
Iteration 2400: Loss = -11486.398496566357
Iteration 2500: Loss = -11486.395950824219
Iteration 2600: Loss = -11486.393738965038
Iteration 2700: Loss = -11486.391727906144
Iteration 2800: Loss = -11486.389935518475
Iteration 2900: Loss = -11486.388296165389
Iteration 3000: Loss = -11486.386798116771
Iteration 3100: Loss = -11486.38548474003
Iteration 3200: Loss = -11486.384253483664
Iteration 3300: Loss = -11486.383105492836
Iteration 3400: Loss = -11486.3821327743
Iteration 3500: Loss = -11486.381161590041
Iteration 3600: Loss = -11486.38030031685
Iteration 3700: Loss = -11486.37951281809
Iteration 3800: Loss = -11486.37876650113
Iteration 3900: Loss = -11486.378050826386
Iteration 4000: Loss = -11486.37741867704
Iteration 4100: Loss = -11486.376810677777
Iteration 4200: Loss = -11486.376215013799
Iteration 4300: Loss = -11486.377375585149
1
Iteration 4400: Loss = -11486.375137468305
Iteration 4500: Loss = -11486.374727451912
Iteration 4600: Loss = -11486.390526752602
1
Iteration 4700: Loss = -11486.373908348829
Iteration 4800: Loss = -11486.37353855475
Iteration 4900: Loss = -11486.388975215446
1
Iteration 5000: Loss = -11486.3729187736
Iteration 5100: Loss = -11486.372654185687
Iteration 5200: Loss = -11486.372358731813
Iteration 5300: Loss = -11486.372461464045
1
Iteration 5400: Loss = -11486.371916881464
Iteration 5500: Loss = -11486.37171175052
Iteration 5600: Loss = -11486.379568806378
1
Iteration 5700: Loss = -11486.37132908186
Iteration 5800: Loss = -11486.371148285538
Iteration 5900: Loss = -11486.371008486403
Iteration 6000: Loss = -11486.370836895354
Iteration 6100: Loss = -11486.37068980233
Iteration 6200: Loss = -11486.3741170329
1
Iteration 6300: Loss = -11486.370946134672
2
Iteration 6400: Loss = -11486.3704911985
Iteration 6500: Loss = -11486.376585787908
1
Iteration 6600: Loss = -11486.370075019706
Iteration 6700: Loss = -11486.37089664971
1
Iteration 6800: Loss = -11486.370063428622
Iteration 6900: Loss = -11486.369882746409
Iteration 7000: Loss = -11486.36980270033
Iteration 7100: Loss = -11486.382393974993
1
Iteration 7200: Loss = -11486.369659924638
Iteration 7300: Loss = -11486.369565059938
Iteration 7400: Loss = -11486.382396861074
1
Iteration 7500: Loss = -11486.453104945895
2
Iteration 7600: Loss = -11486.370816072464
3
Iteration 7700: Loss = -11486.371027439676
4
Iteration 7800: Loss = -11486.380773589024
5
Iteration 7900: Loss = -11486.38941468477
6
Iteration 8000: Loss = -11486.370072066884
7
Iteration 8100: Loss = -11486.36935519166
Iteration 8200: Loss = -11486.370536524139
1
Iteration 8300: Loss = -11486.369552728978
2
Iteration 8400: Loss = -11486.369044238707
Iteration 8500: Loss = -11486.378963151175
1
Iteration 8600: Loss = -11486.36987598848
2
Iteration 8700: Loss = -11486.374339593194
3
Iteration 8800: Loss = -11486.37327009385
4
Iteration 8900: Loss = -11486.36899505306
Iteration 9000: Loss = -11486.369673275325
1
Iteration 9100: Loss = -11486.374477846903
2
Iteration 9200: Loss = -11486.39089065873
3
Iteration 9300: Loss = -11486.370677742223
4
Iteration 9400: Loss = -11486.374034782219
5
Iteration 9500: Loss = -11486.372588506727
6
Iteration 9600: Loss = -11486.40510721839
7
Iteration 9700: Loss = -11486.374464255243
8
Iteration 9800: Loss = -11486.402749687684
9
Iteration 9900: Loss = -11486.415336656077
10
Stopping early at iteration 9900 due to no improvement.
tensor([[ -4.3807,   2.8528],
        [  6.9390,  -9.2045],
        [  6.6107,  -8.4175],
        [  7.4686,  -8.8993],
        [  7.0087,  -8.6141],
        [ -8.0821,   6.6086],
        [ -5.5927,   4.2019],
        [ -8.4030,   5.9164],
        [ -8.4649,   7.0786],
        [ -8.8127,   6.6230],
        [ -7.7910,   5.8939],
        [ -7.2780,   5.7026],
        [ -8.0382,   5.1421],
        [ -7.4786,   6.0864],
        [  5.6934,  -7.1983],
        [ -8.9878,   6.8831],
        [ -6.2134,   4.4022],
        [  0.9012,  -2.4541],
        [  7.0632,  -8.5704],
        [  6.3330,  -8.7963],
        [  5.4394,  -7.3043],
        [  7.1392,  -8.7638],
        [ -6.2003,   4.5269],
        [  6.6263,  -8.0939],
        [  7.1664,  -9.7177],
        [  7.3096,  -9.9147],
        [ -8.7403,   6.0419],
        [ -9.2053,   6.7503],
        [  4.2659,  -6.0735],
        [ -8.0266,   6.1212],
        [ -7.5633,   5.5565],
        [  7.3199,  -9.6548],
        [  7.0699,  -8.4767],
        [ -7.1054,   4.4434],
        [  4.9737,  -8.5488],
        [  6.7457,  -8.1338],
        [  3.1793,  -5.6804],
        [  7.5747,  -9.5427],
        [ -8.2394,   6.1032],
        [  6.5312,  -8.8647],
        [  7.3097,  -8.7782],
        [ -8.1027,   6.6951],
        [  6.5562,  -7.9472],
        [  1.0691,  -3.5260],
        [ -9.0557,   5.1396],
        [  7.0661,  -9.1193],
        [  6.9750,  -8.3889],
        [  5.7162,  -7.1882],
        [  4.5601,  -5.9572],
        [  6.8830,  -8.9415],
        [  5.9667,  -7.3613],
        [ -8.1934,   6.7852],
        [ -8.1084,   6.4117],
        [ -9.9563,   5.3421],
        [ -6.0999,   4.4473],
        [ -6.2552,   4.6342],
        [  6.8797, -10.1911],
        [  7.1936,  -8.5904],
        [ -7.8866,   6.4944],
        [  5.1025,  -7.3783],
        [ -8.3541,   6.8437],
        [  4.2688,  -5.9908],
        [ -7.9117,   6.0323],
        [  1.5888,  -5.6102],
        [ -6.6140,   4.4582],
        [  5.8515,  -9.1894],
        [ -7.1284,   5.7416],
        [ -4.8474,   3.2300],
        [ -5.4270,   4.0171],
        [ -9.6579,   5.7731],
        [ -8.0019,   6.2865],
        [  7.7359,  -9.3442],
        [ -8.2673,   6.0820],
        [ -8.6630,   4.6559],
        [ -9.3271,   5.0493],
        [ -8.8951,   6.7308],
        [ -9.6509,   5.9749],
        [  2.8901,  -4.9552],
        [ -7.6246,   6.1733],
        [  6.9726,  -8.9031],
        [  6.8024,  -8.7836],
        [ -8.0506,   5.9974],
        [ -7.7244,   6.3217],
        [ -5.9902,   4.6005],
        [  6.3621,  -7.8746],
        [  7.1275, -10.3247],
        [  6.7828,  -8.6534],
        [ -7.4807,   5.2485],
        [  6.9813,  -8.3888],
        [  6.7604,  -8.9013],
        [  6.5616,  -8.8519],
        [  6.4830,  -9.7013],
        [  7.5908,  -9.6072],
        [ -7.5741,   6.1877],
        [ -7.3118,   5.4872],
        [ -7.8707,   5.4340],
        [  7.3163,  -9.0725],
        [  3.7223,  -5.6042],
        [  7.6706,  -9.1492],
        [ -6.8418,   5.4060]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6941, 0.3059],
        [0.2648, 0.7352]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5102, 0.4898], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4026, 0.0974],
         [0.6522, 0.1973]],

        [[0.4273, 0.1023],
         [0.4905, 0.1984]],

        [[0.9604, 0.0992],
         [0.3950, 0.9496]],

        [[0.1518, 0.0977],
         [0.5209, 0.3417]],

        [[0.8552, 0.1062],
         [0.5566, 0.4432]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
11490.417433882372
new:  [0.9840320634059188, 0.2137256165729212, 0.23272478047643252, 0.9919999997943784] [0.9839964884296097, 0.6340832018762659, 0.6203282869126203, 0.9919993417272899] [11491.549919661602, 11746.611324841915, 11776.007709800144, 11486.415336656077]
prior:  [0.9919999997943784, 0.5290426865070819, 0.9919999997943784, 0.9919999997943784] [0.9919993417272899, 0.813740083222088, 0.9919993417272899, 0.9919993417272899] [11487.953545419, 11650.455240622709, 11487.953551644934, 11487.953551644934]
-----------------------------------------------------------------------------------------
This iteration is 1
True Objective function: Loss = -11513.289081775833
Iteration 0: Loss = -27744.32620724975
Iteration 10: Loss = -11504.900294352523
Iteration 20: Loss = -11504.899871006202
Iteration 30: Loss = -11504.899871005571
Iteration 40: Loss = -11504.899871005571
1
Iteration 50: Loss = -11504.899871005571
2
Iteration 60: Loss = -11504.899871005571
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7604, 0.2396],
        [0.2036, 0.7964]], dtype=torch.float64)
alpha: tensor([0.4757, 0.5243])
beta: tensor([[[0.3944, 0.1037],
         [0.4331, 0.1947]],

        [[0.5470, 0.0965],
         [0.4283, 0.5017]],

        [[0.7805, 0.1063],
         [0.6273, 0.0664]],

        [[0.9014, 0.1077],
         [0.3551, 0.6471]],

        [[0.1141, 0.0997],
         [0.7469, 0.5960]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919992163297293
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27506.726129505332
Iteration 100: Loss = -12256.90785801267
Iteration 200: Loss = -12254.397457082037
Iteration 300: Loss = -12253.321885358477
Iteration 400: Loss = -12252.629176993401
Iteration 500: Loss = -12251.618418989523
Iteration 600: Loss = -12246.571517690776
Iteration 700: Loss = -12240.977403420815
Iteration 800: Loss = -12223.332071515706
Iteration 900: Loss = -12099.850330893334
Iteration 1000: Loss = -12056.598441236287
Iteration 1100: Loss = -12040.443625521495
Iteration 1200: Loss = -11853.754912768634
Iteration 1300: Loss = -11761.77630435245
Iteration 1400: Loss = -11733.161380516427
Iteration 1500: Loss = -11698.54873238466
Iteration 1600: Loss = -11642.73188382222
Iteration 1700: Loss = -11604.19871393921
Iteration 1800: Loss = -11604.020656635836
Iteration 1900: Loss = -11582.94570212262
Iteration 2000: Loss = -11582.875919254418
Iteration 2100: Loss = -11582.804075994514
Iteration 2200: Loss = -11574.733618361974
Iteration 2300: Loss = -11574.697474873075
Iteration 2400: Loss = -11574.67280812933
Iteration 2500: Loss = -11574.650686366984
Iteration 2600: Loss = -11574.829174741728
1
Iteration 2700: Loss = -11574.57236816669
Iteration 2800: Loss = -11564.399850219828
Iteration 2900: Loss = -11564.37228820928
Iteration 3000: Loss = -11564.352828546005
Iteration 3100: Loss = -11563.099338752818
Iteration 3200: Loss = -11550.187426609915
Iteration 3300: Loss = -11550.177705723474
Iteration 3400: Loss = -11550.170244522556
Iteration 3500: Loss = -11550.167120057838
Iteration 3600: Loss = -11550.157427534043
Iteration 3700: Loss = -11550.150252915564
Iteration 3800: Loss = -11550.137146767265
Iteration 3900: Loss = -11538.721264636528
Iteration 4000: Loss = -11527.56011328021
Iteration 4100: Loss = -11527.569311179881
1
Iteration 4200: Loss = -11527.538965338388
Iteration 4300: Loss = -11517.223237647167
Iteration 4400: Loss = -11517.277627594463
1
Iteration 4500: Loss = -11517.216077761832
Iteration 4600: Loss = -11517.213573089197
Iteration 4700: Loss = -11517.211530608247
Iteration 4800: Loss = -11517.209255390897
Iteration 4900: Loss = -11517.207466996122
Iteration 5000: Loss = -11517.20576058129
Iteration 5100: Loss = -11517.204177868825
Iteration 5200: Loss = -11517.202771777345
Iteration 5300: Loss = -11517.201491019438
Iteration 5400: Loss = -11517.200277661983
Iteration 5500: Loss = -11517.199200909561
Iteration 5600: Loss = -11517.198516771146
Iteration 5700: Loss = -11517.197258044373
Iteration 5800: Loss = -11517.19637220229
Iteration 5900: Loss = -11517.197230592697
1
Iteration 6000: Loss = -11517.194857894841
Iteration 6100: Loss = -11517.19414628544
Iteration 6200: Loss = -11517.200323676596
1
Iteration 6300: Loss = -11517.192833871302
Iteration 6400: Loss = -11517.1922449063
Iteration 6500: Loss = -11517.269429758593
1
Iteration 6600: Loss = -11517.191211562025
Iteration 6700: Loss = -11517.190717186386
Iteration 6800: Loss = -11517.235722835612
1
Iteration 6900: Loss = -11517.189824337813
Iteration 7000: Loss = -11517.189417787251
Iteration 7100: Loss = -11517.193697504312
1
Iteration 7200: Loss = -11517.188615556275
Iteration 7300: Loss = -11517.188242352113
Iteration 7400: Loss = -11517.215868515206
1
Iteration 7500: Loss = -11517.2396636774
2
Iteration 7600: Loss = -11517.18605215268
Iteration 7700: Loss = -11512.391027636426
Iteration 7800: Loss = -11512.397609772805
1
Iteration 7900: Loss = -11512.383649308547
Iteration 8000: Loss = -11512.383518362252
Iteration 8100: Loss = -11512.390058623521
1
Iteration 8200: Loss = -11512.382761142846
Iteration 8300: Loss = -11512.385011640237
1
Iteration 8400: Loss = -11512.385190353034
2
Iteration 8500: Loss = -11512.390661952651
3
Iteration 8600: Loss = -11512.410639849852
4
Iteration 8700: Loss = -11512.381300617448
Iteration 8800: Loss = -11512.382218807124
1
Iteration 8900: Loss = -11512.38086370368
Iteration 9000: Loss = -11512.542795689295
1
Iteration 9100: Loss = -11512.39900006407
2
Iteration 9200: Loss = -11512.388746431678
3
Iteration 9300: Loss = -11512.380753209956
Iteration 9400: Loss = -11512.380672509407
Iteration 9500: Loss = -11512.382861478718
1
Iteration 9600: Loss = -11512.380153112214
Iteration 9700: Loss = -11512.39283752573
1
Iteration 9800: Loss = -11512.381780795848
2
Iteration 9900: Loss = -11512.38010298793
Iteration 10000: Loss = -11512.384668361035
1
Iteration 10100: Loss = -11512.381709371532
2
Iteration 10200: Loss = -11512.317985117728
Iteration 10300: Loss = -11503.42735541955
Iteration 10400: Loss = -11503.42660348009
Iteration 10500: Loss = -11503.431926994084
1
Iteration 10600: Loss = -11503.427647840344
2
Iteration 10700: Loss = -11503.428636135153
3
Iteration 10800: Loss = -11503.51623693476
4
Iteration 10900: Loss = -11503.42721576517
5
Iteration 11000: Loss = -11503.426179610055
Iteration 11100: Loss = -11503.444185258095
1
Iteration 11200: Loss = -11503.425801327961
Iteration 11300: Loss = -11503.46483632106
1
Iteration 11400: Loss = -11503.425527151507
Iteration 11500: Loss = -11503.401624726952
Iteration 11600: Loss = -11503.399270235685
Iteration 11700: Loss = -11503.40819650788
1
Iteration 11800: Loss = -11503.41041819297
2
Iteration 11900: Loss = -11503.397692747918
Iteration 12000: Loss = -11503.397569844743
Iteration 12100: Loss = -11503.410162505166
1
Iteration 12200: Loss = -11503.406024928947
2
Iteration 12300: Loss = -11503.400248904294
3
Iteration 12400: Loss = -11503.39725590449
Iteration 12500: Loss = -11503.401887554728
1
Iteration 12600: Loss = -11503.415130704097
2
Iteration 12700: Loss = -11503.398582609565
3
Iteration 12800: Loss = -11503.443319340988
4
Iteration 12900: Loss = -11503.40302122549
5
Iteration 13000: Loss = -11503.39870632872
6
Iteration 13100: Loss = -11503.441101197863
7
Iteration 13200: Loss = -11503.397042595323
Iteration 13300: Loss = -11503.3999684776
1
Iteration 13400: Loss = -11503.451169846077
2
Iteration 13500: Loss = -11503.413359243294
3
Iteration 13600: Loss = -11503.39829450122
4
Iteration 13700: Loss = -11503.398837598936
5
Iteration 13800: Loss = -11503.39847513846
6
Iteration 13900: Loss = -11503.400806876407
7
Iteration 14000: Loss = -11503.46009800214
8
Iteration 14100: Loss = -11503.39729235853
9
Iteration 14200: Loss = -11503.396226699173
Iteration 14300: Loss = -11503.39695753595
1
Iteration 14400: Loss = -11503.397851511747
2
Iteration 14500: Loss = -11503.42719075443
3
Iteration 14600: Loss = -11503.397139813564
4
Iteration 14700: Loss = -11503.39638052236
5
Iteration 14800: Loss = -11503.396944603452
6
Iteration 14900: Loss = -11503.40910293064
7
Iteration 15000: Loss = -11503.396458218343
8
Iteration 15100: Loss = -11503.39630603264
9
Iteration 15200: Loss = -11503.430622703921
10
Stopping early at iteration 15200 due to no improvement.
tensor([[  5.5416, -10.1568],
        [ -5.6142,   0.9990],
        [ -7.3753,   2.7601],
        [ -9.8618,   5.2466],
        [  3.2242,  -7.8394],
        [  0.5805,  -5.1957],
        [  3.8676,  -8.4828],
        [  2.1017,  -6.7169],
        [ -7.9956,   3.3804],
        [-10.0069,   5.3916],
        [  3.3635,  -7.9787],
        [ -8.4565,   3.8413],
        [-10.7823,   6.1671],
        [  4.8364,  -9.4516],
        [-10.1351,   5.5199],
        [  4.3692,  -8.9844],
        [  0.6990,  -5.3143],
        [  0.0739,  -4.6891],
        [-10.6187,   6.0035],
        [-10.9010,   6.2858],
        [-10.7569,   6.1417],
        [  4.9351,  -9.5503],
        [-10.1864,   5.5712],
        [  6.5843, -11.1996],
        [ -9.4571,   4.8419],
        [  5.5325, -10.1477],
        [  5.1001,  -9.7154],
        [-10.7252,   6.1100],
        [  1.0780,  -5.6932],
        [ -9.8746,   5.2594],
        [ -9.5200,   4.9047],
        [  0.7000,  -5.3152],
        [-10.6547,   6.0395],
        [  5.1418,  -9.7570],
        [ -9.0799,   4.4646],
        [  0.9622,  -5.5775],
        [  2.8418,  -7.4570],
        [-10.8541,   6.2389],
        [  5.2167,  -9.8319],
        [  3.7241,  -8.3393],
        [ -8.7998,   4.1846],
        [-11.0961,   6.4809],
        [-10.6696,   6.0544],
        [ -9.1462,   4.5309],
        [-10.6442,   6.0290],
        [ -9.5050,   4.8897],
        [-10.5738,   5.9585],
        [  3.8582,  -8.4734],
        [  5.3305,  -9.9457],
        [  2.4803,  -7.0955],
        [  5.3781,  -9.9933],
        [-11.3894,   6.7742],
        [ -9.6416,   5.0264],
        [  4.6728,  -9.2880],
        [-10.2745,   5.6593],
        [  5.0806,  -9.6959],
        [ -9.3652,   4.7500],
        [ -9.8467,   5.2315],
        [  4.0841,  -8.6993],
        [  5.4288, -10.0440],
        [  4.9680,  -9.5832],
        [-10.1394,   5.5241],
        [  5.8791, -10.4943],
        [  5.3056,  -9.9208],
        [  6.8692, -11.4844],
        [  4.5791,  -9.1943],
        [-10.5175,   5.9022],
        [-10.2935,   5.6783],
        [  4.6301,  -9.2453],
        [  4.6651,  -9.2803],
        [  2.4644,  -7.0796],
        [ -7.7549,   3.1397],
        [  5.1507,  -9.7659],
        [  1.2094,  -5.8246],
        [ -6.2373,   1.6220],
        [  2.2230,  -6.8382],
        [  4.6915,  -9.3067],
        [  5.3433,  -9.9585],
        [-10.5684,   5.9532],
        [  4.7386,  -9.3538],
        [-11.3073,   6.6921],
        [ -7.6184,   3.0032],
        [  4.4423,  -9.0575],
        [  4.7714,  -9.3867],
        [-10.3437,   5.7285],
        [  5.4403, -10.0555],
        [ -6.4962,   1.8810],
        [  3.3430,  -7.9582],
        [  5.0210,  -9.6362],
        [-10.9126,   6.2974],
        [ -9.5165,   4.9013],
        [-11.0615,   6.4463],
        [  4.8257,  -9.4409],
        [-11.3903,   6.7750],
        [-10.3861,   5.7709],
        [-10.8271,   6.2118],
        [  4.2645,  -8.8797],
        [-10.7717,   6.1565],
        [-11.3114,   6.6962],
        [  5.0552,  -9.6704]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7972, 0.2028],
        [0.2395, 0.7605]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.1036],
         [0.4331, 0.4032]],

        [[0.5470, 0.0953],
         [0.4283, 0.5017]],

        [[0.7805, 0.1065],
         [0.6273, 0.0664]],

        [[0.9014, 0.1078],
         [0.3551, 0.6471]],

        [[0.1141, 0.0999],
         [0.7469, 0.5960]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919992163297293
Iteration 0: Loss = -23039.406087687115
Iteration 10: Loss = -11504.902384496023
Iteration 20: Loss = -11504.899871005573
Iteration 30: Loss = -11504.899871005571
Iteration 40: Loss = -11504.899871005571
1
Iteration 50: Loss = -11504.899871005571
2
Iteration 60: Loss = -11504.899871005571
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7604, 0.2396],
        [0.2036, 0.7964]], dtype=torch.float64)
alpha: tensor([0.4757, 0.5243])
beta: tensor([[[0.3944, 0.1037],
         [0.2946, 0.1947]],

        [[0.3636, 0.0965],
         [0.3958, 0.6365]],

        [[0.3678, 0.1063],
         [0.6955, 0.5298]],

        [[0.4065, 0.1077],
         [0.8181, 0.4707]],

        [[0.9455, 0.0997],
         [0.4073, 0.2706]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23039.53434496927
Iteration 100: Loss = -12294.076344944639
Iteration 200: Loss = -12229.052696419425
Iteration 300: Loss = -11932.43108219113
Iteration 400: Loss = -11891.631907795509
Iteration 500: Loss = -11853.345133243653
Iteration 600: Loss = -11808.991779302738
Iteration 700: Loss = -11789.651624254055
Iteration 800: Loss = -11759.8105247904
Iteration 900: Loss = -11729.863863147373
Iteration 1000: Loss = -11712.53023268724
Iteration 1100: Loss = -11685.056567517104
Iteration 1200: Loss = -11668.94407744534
Iteration 1300: Loss = -11641.01775384328
Iteration 1400: Loss = -11626.886854890226
Iteration 1500: Loss = -11601.575190779207
Iteration 1600: Loss = -11601.104920364822
Iteration 1700: Loss = -11588.443309650955
Iteration 1800: Loss = -11588.327704751395
Iteration 1900: Loss = -11572.182783823293
Iteration 2000: Loss = -11572.128213091106
Iteration 2100: Loss = -11572.09796161826
Iteration 2200: Loss = -11572.073560074508
Iteration 2300: Loss = -11572.053613897113
Iteration 2400: Loss = -11572.037753710683
Iteration 2500: Loss = -11572.025482672103
Iteration 2600: Loss = -11572.014184804942
Iteration 2700: Loss = -11572.004841019387
Iteration 2800: Loss = -11571.996490677071
Iteration 2900: Loss = -11565.3163562927
Iteration 3000: Loss = -11565.291880205607
Iteration 3100: Loss = -11565.285516802647
Iteration 3200: Loss = -11565.278799597936
Iteration 3300: Loss = -11565.271296535058
Iteration 3400: Loss = -11565.25311714798
Iteration 3500: Loss = -11551.84656203333
Iteration 3600: Loss = -11551.839683452272
Iteration 3700: Loss = -11551.835593779955
Iteration 3800: Loss = -11551.832003713715
Iteration 3900: Loss = -11551.828895694804
Iteration 4000: Loss = -11551.826009834595
Iteration 4100: Loss = -11551.82341409316
Iteration 4200: Loss = -11551.82109348913
Iteration 4300: Loss = -11551.818874267732
Iteration 4400: Loss = -11551.820840564313
1
Iteration 4500: Loss = -11551.814984658842
Iteration 4600: Loss = -11551.813154432068
Iteration 4700: Loss = -11551.811809571742
Iteration 4800: Loss = -11551.809875729505
Iteration 4900: Loss = -11551.808572268617
Iteration 5000: Loss = -11551.807320471418
Iteration 5100: Loss = -11551.806237140183
Iteration 5200: Loss = -11551.805168854075
Iteration 5300: Loss = -11551.80418856029
Iteration 5400: Loss = -11551.8036482076
Iteration 5500: Loss = -11551.80237434199
Iteration 5600: Loss = -11551.80156944703
Iteration 5700: Loss = -11551.81175282785
1
Iteration 5800: Loss = -11551.799991542224
Iteration 5900: Loss = -11551.799157763024
Iteration 6000: Loss = -11551.799177343662
1
Iteration 6100: Loss = -11551.797386733731
Iteration 6200: Loss = -11551.796859999202
Iteration 6300: Loss = -11551.79434668746
Iteration 6400: Loss = -11541.960290837378
Iteration 6500: Loss = -11541.946525787274
Iteration 6600: Loss = -11541.927468893356
Iteration 6700: Loss = -11541.934239660703
1
Iteration 6800: Loss = -11541.936235786645
2
Iteration 6900: Loss = -11541.927250612354
Iteration 7000: Loss = -11541.925978664802
Iteration 7100: Loss = -11541.929433502795
1
Iteration 7200: Loss = -11541.924715890213
Iteration 7300: Loss = -11541.924873707932
1
Iteration 7400: Loss = -11541.924260764372
Iteration 7500: Loss = -11541.923857779504
Iteration 7600: Loss = -11541.924096389612
1
Iteration 7700: Loss = -11541.924023107185
2
Iteration 7800: Loss = -11541.936198654405
3
Iteration 7900: Loss = -11541.94490813285
4
Iteration 8000: Loss = -11541.930208833888
5
Iteration 8100: Loss = -11541.93457391113
6
Iteration 8200: Loss = -11541.922553991437
Iteration 8300: Loss = -11541.922362993782
Iteration 8400: Loss = -11541.925329178343
1
Iteration 8500: Loss = -11541.933456554749
2
Iteration 8600: Loss = -11541.921816585102
Iteration 8700: Loss = -11541.954345230326
1
Iteration 8800: Loss = -11541.924414903697
2
Iteration 8900: Loss = -11541.92381369383
3
Iteration 9000: Loss = -11541.922640901825
4
Iteration 9100: Loss = -11541.928670321277
5
Iteration 9200: Loss = -11541.940244428171
6
Iteration 9300: Loss = -11541.921180230944
Iteration 9400: Loss = -11542.01796861609
1
Iteration 9500: Loss = -11541.920199277143
Iteration 9600: Loss = -11541.932567662654
1
Iteration 9700: Loss = -11542.014903742025
2
Iteration 9800: Loss = -11541.92001588277
Iteration 9900: Loss = -11541.920013416391
Iteration 10000: Loss = -11541.920932279765
1
Iteration 10100: Loss = -11541.928899040931
2
Iteration 10200: Loss = -11541.920925946935
3
Iteration 10300: Loss = -11541.922668879337
4
Iteration 10400: Loss = -11541.920299816757
5
Iteration 10500: Loss = -11541.937297044455
6
Iteration 10600: Loss = -11541.99680446811
7
Iteration 10700: Loss = -11541.926791901284
8
Iteration 10800: Loss = -11541.926765472032
9
Iteration 10900: Loss = -11541.922559236882
10
Stopping early at iteration 10900 due to no improvement.
tensor([[ -8.4950,   7.0563],
        [ -7.5641,   5.7080],
        [  3.1809,  -4.5714],
        [  1.3932,  -2.7904],
        [ -5.3895,   3.8508],
        [ -4.1881,   2.3147],
        [ -5.6704,   4.2839],
        [ -3.9889,   2.3926],
        [  4.9630,  -6.4749],
        [  2.8198,  -7.4350],
        [ -5.2154,   3.5723],
        [  2.9885,  -4.6471],
        [  5.7749,  -7.9416],
        [ -7.7594,   5.8288],
        [  6.1952,  -7.6958],
        [ -6.4702,   4.7779],
        [ -3.1932,   1.5754],
        [ -2.2829,   0.7447],
        [  5.4942,  -7.5653],
        [  6.6446,  -9.3013],
        [  4.2720,  -5.6598],
        [ -7.7093,   5.4423],
        [  6.1263,  -8.1758],
        [ -9.3898,   6.7488],
        [  5.1929,  -6.5914],
        [ -7.6363,   6.0934],
        [ -7.3090,   5.8848],
        [  5.8217,  -7.2995],
        [ -3.2984,   1.8248],
        [  4.8832,  -7.6813],
        [ -9.1807,   7.3016],
        [ -5.2053,   1.5259],
        [  5.3861,  -7.0245],
        [ -7.3905,   5.9238],
        [  2.4064,  -4.2523],
        [ -5.4255,   3.8231],
        [ -6.1250,   4.6773],
        [  4.5745,  -6.1534],
        [ -7.3768,   5.8647],
        [ -5.9851,   4.1885],
        [  5.2154,  -6.9106],
        [  5.8601,  -8.5796],
        [  5.9609,  -7.3805],
        [  4.6978,  -6.0848],
        [  2.9059,  -6.6458],
        [  5.2448,  -6.8970],
        [  7.7155, -10.5541],
        [ -5.5880,   4.0114],
        [ -7.3536,   5.8076],
        [ -5.4615,   3.6285],
        [ -7.9260,   6.5044],
        [  6.9939, -10.0521],
        [  6.0551,  -8.2480],
        [ -6.8450,   5.2213],
        [  6.7981,  -8.4821],
        [ -7.3885,   5.6920],
        [  5.8461,  -7.2364],
        [  5.6907,  -8.7597],
        [ -7.1244,   5.4099],
        [ -7.7929,   6.2127],
        [ -8.5197,   6.4157],
        [  7.5282,  -9.5209],
        [ -7.7823,   5.6319],
        [ -7.4046,   6.0182],
        [ -8.8428,   4.5788],
        [ -7.3010,   5.4363],
        [  7.3368,  -8.7281],
        [  0.7950,  -2.2938],
        [ -6.6255,   5.2380],
        [ -5.7246,   3.5199],
        [ -5.8651,   4.4331],
        [ -8.6364,   7.0508],
        [ -7.0141,   5.6135],
        [ -3.8039,   1.0842],
        [ -7.7610,   6.1454],
        [ -4.1335,   2.7083],
        [ -7.1156,   5.5573],
        [ -7.5980,   6.1902],
        [  6.0759,  -7.4657],
        [ -6.8626,   5.4627],
        [  7.2007, -10.4401],
        [  2.7379,  -5.8410],
        [ -7.3605,   4.8648],
        [ -6.9625,   5.1497],
        [  5.0237,  -6.4141],
        [ -8.6903,   6.9584],
        [  3.3468,  -4.7456],
        [ -6.7622,   4.9910],
        [ -8.7796,   6.0631],
        [  5.1636,  -7.7430],
        [  4.9950,  -6.5187],
        [  5.8634,  -7.4909],
        [ -7.4030,   6.0032],
        [  6.6372,  -8.3055],
        [  5.9015,  -7.5465],
        [  5.0605,  -8.3932],
        [ -7.1383,   5.2177],
        [  4.5832,  -7.9627],
        [  6.3661,  -7.7626],
        [ -6.9485,   5.3801]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7563, 0.2437],
        [0.2174, 0.7826]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4501, 0.5499], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4051, 0.1206],
         [0.2946, 0.1956]],

        [[0.3636, 0.0960],
         [0.3958, 0.6365]],

        [[0.3678, 0.1064],
         [0.6955, 0.5298]],

        [[0.4065, 0.1076],
         [0.8181, 0.4707]],

        [[0.9455, 0.0997],
         [0.4073, 0.2706]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603206466266674
Average Adjusted Rand Index: 0.9616161616161616
Iteration 0: Loss = -27633.1989169822
Iteration 10: Loss = -11504.899978572817
Iteration 20: Loss = -11504.899870530995
Iteration 30: Loss = -11504.899870530995
1
Iteration 40: Loss = -11504.899870530995
2
Iteration 50: Loss = -11504.899870530995
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7964, 0.2036],
        [0.2396, 0.7604]], dtype=torch.float64)
alpha: tensor([0.5243, 0.4757])
beta: tensor([[[0.1947, 0.1037],
         [0.8610, 0.3944]],

        [[0.6797, 0.0965],
         [0.8289, 0.4001]],

        [[0.7062, 0.1063],
         [0.1380, 0.5416]],

        [[0.9279, 0.1077],
         [0.0164, 0.0887]],

        [[0.7938, 0.0997],
         [0.1723, 0.9580]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27632.569913734234
Iteration 100: Loss = -12309.25892276328
Iteration 200: Loss = -12266.383380400946
Iteration 300: Loss = -12246.89901209148
Iteration 400: Loss = -12023.720756991395
Iteration 500: Loss = -11983.114259559787
Iteration 600: Loss = -11931.981347175371
Iteration 700: Loss = -11912.87366476637
Iteration 800: Loss = -11907.055704229717
Iteration 900: Loss = -11891.717268066799
Iteration 1000: Loss = -11887.017173832475
Iteration 1100: Loss = -11872.733233900302
Iteration 1200: Loss = -11872.433397119916
Iteration 1300: Loss = -11872.323117739743
Iteration 1400: Loss = -11864.823585522172
Iteration 1500: Loss = -11864.744058875593
Iteration 1600: Loss = -11864.658459535933
Iteration 1700: Loss = -11850.572106180247
Iteration 1800: Loss = -11850.517550210818
Iteration 1900: Loss = -11850.474317162238
Iteration 2000: Loss = -11850.28816449157
Iteration 2100: Loss = -11848.274496958093
Iteration 2200: Loss = -11848.136525717913
Iteration 2300: Loss = -11842.631294416682
Iteration 2400: Loss = -11842.607531246713
Iteration 2500: Loss = -11842.587334892447
Iteration 2600: Loss = -11842.563978315004
Iteration 2700: Loss = -11841.349792933244
Iteration 2800: Loss = -11841.321239200646
Iteration 2900: Loss = -11834.926581103358
Iteration 3000: Loss = -11834.90216932373
Iteration 3100: Loss = -11827.784533493228
Iteration 3200: Loss = -11827.77391867784
Iteration 3300: Loss = -11827.766422442844
Iteration 3400: Loss = -11827.760158377994
Iteration 3500: Loss = -11827.754621488844
Iteration 3600: Loss = -11827.749551137918
Iteration 3700: Loss = -11827.74502861777
Iteration 3800: Loss = -11827.740896451382
Iteration 3900: Loss = -11827.737107951212
Iteration 4000: Loss = -11827.733613783512
Iteration 4100: Loss = -11827.730368649049
Iteration 4200: Loss = -11827.7274474034
Iteration 4300: Loss = -11827.72453212044
Iteration 4400: Loss = -11827.721835333814
Iteration 4500: Loss = -11827.719147814028
Iteration 4600: Loss = -11827.716174385641
Iteration 4700: Loss = -11827.713690330336
Iteration 4800: Loss = -11827.711257051998
Iteration 4900: Loss = -11827.706734624751
Iteration 5000: Loss = -11822.596957346366
Iteration 5100: Loss = -11822.588134630683
Iteration 5200: Loss = -11822.587038965246
Iteration 5300: Loss = -11822.58297916844
Iteration 5400: Loss = -11822.581356388444
Iteration 5500: Loss = -11822.579114884576
Iteration 5600: Loss = -11807.204116130373
Iteration 5700: Loss = -11807.202101063625
Iteration 5800: Loss = -11807.163323371662
Iteration 5900: Loss = -11807.160847267825
Iteration 6000: Loss = -11789.846676683508
Iteration 6100: Loss = -11789.694072224638
Iteration 6200: Loss = -11789.689803490803
Iteration 6300: Loss = -11782.841952793102
Iteration 6400: Loss = -11782.824972553979
Iteration 6500: Loss = -11782.818572572658
Iteration 6600: Loss = -11778.887762117469
Iteration 6700: Loss = -11767.806160714359
Iteration 6800: Loss = -11765.140179107797
Iteration 6900: Loss = -11762.054984782444
Iteration 7000: Loss = -11756.890395325565
Iteration 7100: Loss = -11754.533809047422
Iteration 7200: Loss = -11753.801485767995
Iteration 7300: Loss = -11750.99518398434
Iteration 7400: Loss = -11749.26254268355
Iteration 7500: Loss = -11746.42609601701
Iteration 7600: Loss = -11744.94737536721
Iteration 7700: Loss = -11738.36855468456
Iteration 7800: Loss = -11738.360051545244
Iteration 7900: Loss = -11730.299406721202
Iteration 8000: Loss = -11728.94126702555
Iteration 8100: Loss = -11716.49917194067
Iteration 8200: Loss = -11706.52987328527
Iteration 8300: Loss = -11697.589154882011
Iteration 8400: Loss = -11694.101641918849
Iteration 8500: Loss = -11686.86515283465
Iteration 8600: Loss = -11663.697376812575
Iteration 8700: Loss = -11663.682109091817
Iteration 8800: Loss = -11652.522653771732
Iteration 8900: Loss = -11632.73527893694
Iteration 9000: Loss = -11605.665578891096
Iteration 9100: Loss = -11595.423303293044
Iteration 9200: Loss = -11595.382962658214
Iteration 9300: Loss = -11595.37410132094
Iteration 9400: Loss = -11562.953138389988
Iteration 9500: Loss = -11550.2015954918
Iteration 9600: Loss = -11550.197528979936
Iteration 9700: Loss = -11544.611396663382
Iteration 9800: Loss = -11542.886056429914
Iteration 9900: Loss = -11542.901032474312
1
Iteration 10000: Loss = -11542.88568940061
Iteration 10100: Loss = -11533.204827003998
Iteration 10200: Loss = -11533.194121017657
Iteration 10300: Loss = -11533.191021153438
Iteration 10400: Loss = -11520.363928948973
Iteration 10500: Loss = -11520.10562353537
Iteration 10600: Loss = -11520.165379748652
1
Iteration 10700: Loss = -11520.102644971943
Iteration 10800: Loss = -11520.10304941073
1
Iteration 10900: Loss = -11520.106037871004
2
Iteration 11000: Loss = -11520.111598273268
3
Iteration 11100: Loss = -11520.13177406915
4
Iteration 11200: Loss = -11520.106301211943
5
Iteration 11300: Loss = -11520.133747436017
6
Iteration 11400: Loss = -11520.101973015859
Iteration 11500: Loss = -11520.104455430554
1
Iteration 11600: Loss = -11520.080259141163
Iteration 11700: Loss = -11520.079951456353
Iteration 11800: Loss = -11520.083907582826
1
Iteration 11900: Loss = -11520.082846714502
2
Iteration 12000: Loss = -11520.075073983462
Iteration 12100: Loss = -11520.073058111364
Iteration 12200: Loss = -11520.084436808886
1
Iteration 12300: Loss = -11520.07389772957
2
Iteration 12400: Loss = -11520.130489800631
3
Iteration 12500: Loss = -11520.077489731817
4
Iteration 12600: Loss = -11520.123990026175
5
Iteration 12700: Loss = -11520.175417394485
6
Iteration 12800: Loss = -11520.073093275607
7
Iteration 12900: Loss = -11520.073159566617
8
Iteration 13000: Loss = -11520.103928050836
9
Iteration 13100: Loss = -11520.101996846497
10
Stopping early at iteration 13100 due to no improvement.
tensor([[  6.5481, -10.3351],
        [ -4.3456,   2.1520],
        [ -4.5458,   3.0000],
        [ -2.8884,   0.9637],
        [  4.6089,  -6.0312],
        [  1.4654,  -3.8840],
        [  5.1815,  -6.5681],
        [  3.2177,  -4.7994],
        [ -6.5266,   4.7333],
        [ -8.2156,   6.1927],
        [  5.9020, -10.3985],
        [ -6.8796,   5.0549],
        [ -7.7888,   6.3436],
        [  5.9860,  -7.4581],
        [-10.3692,   6.1390],
        [  6.0836,  -8.5705],
        [  1.8074,  -3.8564],
        [  1.1906,  -3.0426],
        [ -7.6413,   6.1451],
        [ -8.9733,   7.5740],
        [ -7.6040,   6.1991],
        [  6.0867,  -8.1909],
        [ -8.1840,   6.7505],
        [  7.0805,  -9.6381],
        [ -9.2677,   7.6444],
        [  6.0317,  -9.7726],
        [  6.1543,  -8.1413],
        [ -7.9851,   6.0126],
        [  2.4323,  -3.8192],
        [ -7.8076,   6.4152],
        [ -8.0072,   5.8634],
        [  1.7046,  -3.8551],
        [ -7.4975,   4.9989],
        [  6.4191,  -8.5636],
        [ -7.6423,   5.4310],
        [  2.3139,  -3.7301],
        [  4.1622,  -5.7773],
        [ -4.9993,   3.4627],
        [  6.5753,  -8.5872],
        [  4.9775,  -6.4759],
        [ -7.3539,   5.1689],
        [ -8.6475,   5.9557],
        [ -7.6819,   6.0954],
        [ -7.4870,   5.7632],
        [ -7.6701,   6.2070],
        [-11.4911,   6.8759],
        [ -8.7327,   7.2325],
        [  4.9678,  -6.4343],
        [  6.8755,  -8.5925],
        [  3.5124,  -5.1943],
        [  6.7093,  -8.6829],
        [ -9.3491,   7.7530],
        [ -7.5038,   5.9874],
        [  6.0631,  -7.4500],
        [ -8.2234,   6.8058],
        [  6.6806,  -8.1093],
        [ -8.1652,   6.0782],
        [ -7.8777,   5.9415],
        [  4.7756,  -7.3811],
        [  5.9740,  -8.1469],
        [  5.9608,  -7.6294],
        [ -8.2735,   6.8711],
        [  6.7351,  -8.7987],
        [  6.9855,  -8.5015],
        [  6.6965,  -8.3147],
        [  5.4545,  -7.4183],
        [ -8.8669,   7.0659],
        [ -2.2748,   0.2888],
        [  5.7883,  -7.9639],
        [  3.1110,  -5.3253],
        [  3.9445,  -5.3441],
        [ -6.2272,   4.4600],
        [  6.4614,  -7.8922],
        [  3.6828,  -5.0999],
        [ -9.6931,   7.2054],
        [  7.0748,  -9.3998],
        [  5.0763,  -8.3448],
        [  6.8799,  -8.3930],
        [  7.2076, -10.2535],
        [  6.2825,  -7.8050],
        [ -9.8866,   7.7722],
        [ -5.9361,   4.4685],
        [  5.4658,  -7.5368],
        [  6.2092,  -7.5956],
        [ -8.6746,   6.6323],
        [  6.2237, -10.8389],
        [ -6.3824,   1.7672],
        [  3.8977,  -6.8777],
        [  6.2977,  -7.6995],
        [ -8.5447,   7.1582],
        [ -6.6289,   5.0091],
        [ -7.9279,   6.4243],
        [  5.9380,  -7.8743],
        [ -8.1813,   6.4429],
        [ -8.7620,   7.2677],
        [ -8.2795,   6.2750],
        [  4.5071,  -8.0207],
        [ -8.4873,   6.2357],
        [ -8.4551,   7.0687],
        [  5.7328,  -9.1640]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7937, 0.2063],
        [0.2395, 0.7605]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5171, 0.4829], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1979, 0.1100],
         [0.8610, 0.4027]],

        [[0.6797, 0.0969],
         [0.8289, 0.4001]],

        [[0.7062, 0.1070],
         [0.1380, 0.5416]],

        [[0.9279, 0.1076],
         [0.0164, 0.0887]],

        [[0.7938, 0.0996],
         [0.1723, 0.9580]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320648505942
Average Adjusted Rand Index: 0.9841602586080228
Iteration 0: Loss = -17670.557376800243
Iteration 10: Loss = -11504.905897585271
Iteration 20: Loss = -11504.899871108708
Iteration 30: Loss = -11504.899871005571
Iteration 40: Loss = -11504.899871005571
1
Iteration 50: Loss = -11504.899871005571
2
Iteration 60: Loss = -11504.899871005571
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7604, 0.2396],
        [0.2036, 0.7964]], dtype=torch.float64)
alpha: tensor([0.4757, 0.5243])
beta: tensor([[[0.3944, 0.1037],
         [0.3683, 0.1947]],

        [[0.5896, 0.0965],
         [0.7774, 0.8780]],

        [[0.5042, 0.1063],
         [0.2763, 0.5640]],

        [[0.4987, 0.1077],
         [0.6342, 0.0170]],

        [[0.1429, 0.0997],
         [0.3794, 0.2113]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17663.886058639546
Iteration 100: Loss = -12030.027022831737
Iteration 200: Loss = -11575.971879705416
Iteration 300: Loss = -11532.732177488511
Iteration 400: Loss = -11519.899819429893
Iteration 500: Loss = -11510.932090205923
Iteration 600: Loss = -11510.622879363646
Iteration 700: Loss = -11510.431636756448
Iteration 800: Loss = -11510.3030293491
Iteration 900: Loss = -11510.212161044183
Iteration 1000: Loss = -11510.14550110171
Iteration 1100: Loss = -11510.094725864788
Iteration 1200: Loss = -11510.054979532302
Iteration 1300: Loss = -11510.023228263102
Iteration 1400: Loss = -11509.997366597554
Iteration 1500: Loss = -11509.975899008574
Iteration 1600: Loss = -11509.957926524043
Iteration 1700: Loss = -11509.942568607756
Iteration 1800: Loss = -11509.92927336856
Iteration 1900: Loss = -11509.917353142404
Iteration 2000: Loss = -11509.906286454423
Iteration 2100: Loss = -11509.89502329045
Iteration 2200: Loss = -11509.883400936264
Iteration 2300: Loss = -11509.87537553196
Iteration 2400: Loss = -11509.86941031733
Iteration 2500: Loss = -11509.86411233822
Iteration 2600: Loss = -11509.85946206042
Iteration 2700: Loss = -11509.855215411391
Iteration 2800: Loss = -11509.85139270641
Iteration 2900: Loss = -11509.847901271693
Iteration 3000: Loss = -11509.844680332844
Iteration 3100: Loss = -11509.841716931453
Iteration 3200: Loss = -11509.839051676277
Iteration 3300: Loss = -11509.836680017808
Iteration 3400: Loss = -11509.834512356721
Iteration 3500: Loss = -11509.832596480697
Iteration 3600: Loss = -11509.83079558996
Iteration 3700: Loss = -11509.829140423957
Iteration 3800: Loss = -11509.827596406607
Iteration 3900: Loss = -11509.825522762125
Iteration 4000: Loss = -11503.4057163323
Iteration 4100: Loss = -11503.404142867503
Iteration 4200: Loss = -11503.402754761635
Iteration 4300: Loss = -11503.401479536795
Iteration 4400: Loss = -11503.40036042413
Iteration 4500: Loss = -11503.399453531387
Iteration 4600: Loss = -11503.39859556138
Iteration 4700: Loss = -11503.397826723685
Iteration 4800: Loss = -11503.397114337873
Iteration 4900: Loss = -11503.396445886947
Iteration 5000: Loss = -11503.395803609968
Iteration 5100: Loss = -11503.39516397346
Iteration 5200: Loss = -11503.394654299847
Iteration 5300: Loss = -11503.394159093057
Iteration 5400: Loss = -11503.393603652838
Iteration 5500: Loss = -11503.393186490148
Iteration 5600: Loss = -11503.39275600691
Iteration 5700: Loss = -11503.392364834248
Iteration 5800: Loss = -11503.391973366597
Iteration 5900: Loss = -11503.391673827138
Iteration 6000: Loss = -11503.581502985595
1
Iteration 6100: Loss = -11503.391020400128
Iteration 6200: Loss = -11503.390741762048
Iteration 6300: Loss = -11503.390452022539
Iteration 6400: Loss = -11503.391220143181
1
Iteration 6500: Loss = -11503.3899535872
Iteration 6600: Loss = -11503.389694622254
Iteration 6700: Loss = -11503.39280570184
1
Iteration 6800: Loss = -11503.389289757532
Iteration 6900: Loss = -11503.389146037072
Iteration 7000: Loss = -11503.391855055917
1
Iteration 7100: Loss = -11503.388784540333
Iteration 7200: Loss = -11503.392917472527
1
Iteration 7300: Loss = -11503.425871890875
2
Iteration 7400: Loss = -11503.431970440664
3
Iteration 7500: Loss = -11503.393259562028
4
Iteration 7600: Loss = -11503.389197964181
5
Iteration 7700: Loss = -11503.399559150681
6
Iteration 7800: Loss = -11503.387906597176
Iteration 7900: Loss = -11503.38888936057
1
Iteration 8000: Loss = -11503.390837271272
2
Iteration 8100: Loss = -11503.452871371122
3
Iteration 8200: Loss = -11503.387631710077
Iteration 8300: Loss = -11503.437999316657
1
Iteration 8400: Loss = -11503.38792834238
2
Iteration 8500: Loss = -11503.391355966583
3
Iteration 8600: Loss = -11503.393107202717
4
Iteration 8700: Loss = -11503.387535875561
Iteration 8800: Loss = -11503.478434759323
1
Iteration 8900: Loss = -11503.412886547863
2
Iteration 9000: Loss = -11503.392357659506
3
Iteration 9100: Loss = -11503.387237500878
Iteration 9200: Loss = -11503.387144461718
Iteration 9300: Loss = -11503.452405822085
1
Iteration 9400: Loss = -11503.386729848065
Iteration 9500: Loss = -11503.393333720065
1
Iteration 9600: Loss = -11503.386718200734
Iteration 9700: Loss = -11503.394731058608
1
Iteration 9800: Loss = -11503.386563256989
Iteration 9900: Loss = -11503.393451207232
1
Iteration 10000: Loss = -11503.390815435914
2
Iteration 10100: Loss = -11503.396453982417
3
Iteration 10200: Loss = -11503.393657793154
4
Iteration 10300: Loss = -11503.391390828112
5
Iteration 10400: Loss = -11503.386513357696
Iteration 10500: Loss = -11503.386667586068
1
Iteration 10600: Loss = -11503.389280843943
2
Iteration 10700: Loss = -11503.387059687457
3
Iteration 10800: Loss = -11503.404042363018
4
Iteration 10900: Loss = -11503.384900168374
Iteration 11000: Loss = -11503.385709489812
1
Iteration 11100: Loss = -11503.38720209607
2
Iteration 11200: Loss = -11503.429644525808
3
Iteration 11300: Loss = -11503.392583840088
4
Iteration 11400: Loss = -11503.388069063005
5
Iteration 11500: Loss = -11503.390839149788
6
Iteration 11600: Loss = -11503.385382614373
7
Iteration 11700: Loss = -11503.385815379655
8
Iteration 11800: Loss = -11503.385652392206
9
Iteration 11900: Loss = -11503.386497995798
10
Stopping early at iteration 11900 due to no improvement.
tensor([[ -9.0427,   7.5002],
        [  2.3904,  -4.2188],
        [  6.7394,  -8.2251],
        [  1.8760,  -4.7073],
        [ -6.3666,   4.8335],
        [ -3.8413,   1.9321],
        [ -6.9151,   5.4570],
        [ -5.1695,   3.6473],
        [  4.6813,  -6.7058],
        [  5.9494,  -8.3670],
        [ -7.1600,   4.1887],
        [  7.3444, -10.2200],
        [  7.8887,  -9.5081],
        [ -7.6719,   6.1601],
        [  6.8375,  -8.7192],
        [ -7.2603,   5.8709],
        [ -3.7747,   2.2381],
        [ -3.5923,   1.1712],
        [  7.6811,  -9.0677],
        [  7.4415,  -8.8467],
        [  6.4338,  -8.0192],
        [ -9.3283,   5.7305],
        [  6.6401,  -9.1437],
        [ -9.6386,   6.6357],
        [  7.8908,  -9.6679],
        [ -8.3488,   6.7626],
        [ -7.5088,   6.1153],
        [  7.9848,  -9.3876],
        [ -4.3952,   2.3773],
        [  6.6767, -11.2919],
        [  6.0425,  -7.9831],
        [ -4.0879,   1.9257],
        [  6.3850,  -7.8031],
        [ -8.1937,   6.5580],
        [  7.3010,  -8.7032],
        [ -4.3955,   2.1404],
        [ -5.8774,   4.4562],
        [  4.8871,  -6.2779],
        [ -7.8598,   6.4410],
        [ -7.2002,   4.8748],
        [  7.8058,  -9.8786],
        [  6.9221,  -8.8591],
        [  7.0821,  -8.4788],
        [  5.8378,  -7.7157],
        [  6.1040,  -7.6507],
        [  6.2984,  -7.8024],
        [  7.2794,  -8.9149],
        [ -9.2688,   7.2005],
        [ -8.2526,   6.8129],
        [ -5.4858,   4.0950],
        [ -8.7424,   7.0362],
        [  7.8139,  -9.2305],
        [  5.9170,  -7.8395],
        [ -7.3691,   5.7467],
        [  7.5771,  -9.2831],
        [ -8.2054,   6.7422],
        [  6.3145,  -8.0277],
        [  7.3920,  -9.2715],
        [ -7.1578,   5.6964],
        [ -8.0253,   6.4977],
        [ -8.3791,   6.5923],
        [  6.7114,  -8.9773],
        [ -8.7668,   6.8211],
        [ -8.1900,   6.8017],
        [ -8.3309,   6.5436],
        [ -8.4481,   5.8246],
        [  8.0608,  -9.7334],
        [  1.8352,  -3.2385],
        [ -7.5184,   6.0133],
        [ -5.6040,   3.4646],
        [ -5.5180,   4.0484],
        [  7.0582,  -8.4506],
        [ -8.2241,   6.8263],
        [ -4.4050,   2.6257],
        [  7.6138,  -9.0006],
        [ -6.3507,   2.7063],
        [ -8.7670,   6.8809],
        [ -8.5833,   6.2885],
        [  8.0013,  -9.5643],
        [ -7.4085,   6.0215],
        [  8.0404, -10.0585],
        [  4.5930,  -6.0378],
        [ -8.2243,   5.3643],
        [ -7.9030,   6.5159],
        [  6.7609,  -8.5289],
        [ -9.3051,   7.9127],
        [  2.9551,  -5.4218],
        [ -6.8012,   4.5386],
        [ -9.4018,   6.1783],
        [  6.9478,  -8.3353],
        [  6.6057, -10.6283],
        [  7.4921,  -9.1185],
        [ -8.3590,   6.4196],
        [  8.3537, -10.7754],
        [  7.4530,  -8.8439],
        [  7.2787,  -9.9588],
        [ -8.0091,   5.3801],
        [  6.3518,  -9.6082],
        [  7.6159,  -9.1297],
        [ -7.7557,   6.0447]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7613, 0.2387],
        [0.2029, 0.7971]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4901, 0.5099], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4030, 0.1036],
         [0.3683, 0.1987]],

        [[0.5896, 0.0962],
         [0.7774, 0.8780]],

        [[0.5042, 0.1065],
         [0.2763, 0.5640]],

        [[0.4987, 0.1073],
         [0.6342, 0.0170]],

        [[0.1429, 0.0996],
         [0.3794, 0.2113]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919992163297293
Iteration 0: Loss = -20956.303803475224
Iteration 10: Loss = -11504.899857193122
Iteration 20: Loss = -11504.899871005571
1
Iteration 30: Loss = -11504.899871005571
2
Iteration 40: Loss = -11504.899871005571
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7604, 0.2396],
        [0.2036, 0.7964]], dtype=torch.float64)
alpha: tensor([0.4757, 0.5243])
beta: tensor([[[0.3944, 0.1037],
         [0.6634, 0.1947]],

        [[0.5184, 0.0965],
         [0.9661, 0.3284]],

        [[0.8363, 0.1063],
         [0.4908, 0.4763]],

        [[0.9369, 0.1077],
         [0.7483, 0.7788]],

        [[0.3329, 0.0997],
         [0.5468, 0.9142]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20956.066881307117
Iteration 100: Loss = -12324.379158660418
Iteration 200: Loss = -12045.527474605231
Iteration 300: Loss = -11883.362473504269
Iteration 400: Loss = -11822.615983316235
Iteration 500: Loss = -11806.025263815365
Iteration 600: Loss = -11771.632903091038
Iteration 700: Loss = -11761.875793478139
Iteration 800: Loss = -11740.770498378753
Iteration 900: Loss = -11718.137468268547
Iteration 1000: Loss = -11714.602098481066
Iteration 1100: Loss = -11707.652619578575
Iteration 1200: Loss = -11693.292757126677
Iteration 1300: Loss = -11672.86141517292
Iteration 1400: Loss = -11612.53173095328
Iteration 1500: Loss = -11585.592259166096
Iteration 1600: Loss = -11558.183550063479
Iteration 1700: Loss = -11557.515116317973
Iteration 1800: Loss = -11541.06101072231
Iteration 1900: Loss = -11539.706739942387
Iteration 2000: Loss = -11539.658564294268
Iteration 2100: Loss = -11539.618581084533
Iteration 2200: Loss = -11539.566880622831
Iteration 2300: Loss = -11539.425174708225
Iteration 2400: Loss = -11539.402737497181
Iteration 2500: Loss = -11539.38942937183
Iteration 2600: Loss = -11539.442898307028
1
Iteration 2700: Loss = -11539.36838269459
Iteration 2800: Loss = -11539.359871100693
Iteration 2900: Loss = -11539.35221832232
Iteration 3000: Loss = -11539.343606805813
Iteration 3100: Loss = -11531.988671492176
Iteration 3200: Loss = -11531.981528410983
Iteration 3300: Loss = -11531.976478669867
Iteration 3400: Loss = -11531.972010244903
Iteration 3500: Loss = -11531.967979302835
Iteration 3600: Loss = -11531.964380647036
Iteration 3700: Loss = -11531.962640013622
Iteration 3800: Loss = -11531.95822288207
Iteration 3900: Loss = -11531.955531395477
Iteration 4000: Loss = -11532.00611769382
1
Iteration 4100: Loss = -11531.950827614206
Iteration 4200: Loss = -11531.948763031132
Iteration 4300: Loss = -11531.94691664283
Iteration 4400: Loss = -11531.945528934093
Iteration 4500: Loss = -11531.943611119377
Iteration 4600: Loss = -11531.942121754166
Iteration 4700: Loss = -11531.969933924456
1
Iteration 4800: Loss = -11531.939440498643
Iteration 4900: Loss = -11531.938368743571
Iteration 5000: Loss = -11531.999139847983
1
Iteration 5100: Loss = -11531.938009906396
Iteration 5200: Loss = -11531.94854271443
1
Iteration 5300: Loss = -11531.939133046075
2
Iteration 5400: Loss = -11531.933502209638
Iteration 5500: Loss = -11531.937962200484
1
Iteration 5600: Loss = -11531.960165215898
2
Iteration 5700: Loss = -11531.931440538634
Iteration 5800: Loss = -11531.9313780288
Iteration 5900: Loss = -11531.935717478784
1
Iteration 6000: Loss = -11531.929651009654
Iteration 6100: Loss = -11531.943064998566
1
Iteration 6200: Loss = -11531.93609361678
2
Iteration 6300: Loss = -11531.929338191054
Iteration 6400: Loss = -11531.92871400256
Iteration 6500: Loss = -11531.931759470423
1
Iteration 6600: Loss = -11531.930814275453
2
Iteration 6700: Loss = -11531.926943765959
Iteration 6800: Loss = -11531.942589201246
1
Iteration 6900: Loss = -11531.9261419356
Iteration 7000: Loss = -11531.925620457901
Iteration 7100: Loss = -11531.942590398405
1
Iteration 7200: Loss = -11531.92503183265
Iteration 7300: Loss = -11531.927342144496
1
Iteration 7400: Loss = -11531.924527711208
Iteration 7500: Loss = -11531.927247580088
1
Iteration 7600: Loss = -11531.924074715382
Iteration 7700: Loss = -11531.925940961759
1
Iteration 7800: Loss = -11531.928161482389
2
Iteration 7900: Loss = -11531.924443941516
3
Iteration 8000: Loss = -11531.927801962596
4
Iteration 8100: Loss = -11531.924313716767
5
Iteration 8200: Loss = -11531.926383641427
6
Iteration 8300: Loss = -11531.923559251638
Iteration 8400: Loss = -11531.923345490393
Iteration 8500: Loss = -11531.930861862147
1
Iteration 8600: Loss = -11531.924185978058
2
Iteration 8700: Loss = -11531.94512373385
3
Iteration 8800: Loss = -11531.924085450239
4
Iteration 8900: Loss = -11531.923524509062
5
Iteration 9000: Loss = -11531.922693024406
Iteration 9100: Loss = -11531.953808449807
1
Iteration 9200: Loss = -11531.922694619556
2
Iteration 9300: Loss = -11531.922073041282
Iteration 9400: Loss = -11531.926335112219
1
Iteration 9500: Loss = -11531.962759727689
2
Iteration 9600: Loss = -11531.961724179555
3
Iteration 9700: Loss = -11531.95984355234
4
Iteration 9800: Loss = -11531.937683134427
5
Iteration 9900: Loss = -11531.922724846909
6
Iteration 10000: Loss = -11531.92202251526
Iteration 10100: Loss = -11531.92375289154
1
Iteration 10200: Loss = -11532.074930563856
2
Iteration 10300: Loss = -11531.925414086563
3
Iteration 10400: Loss = -11531.931291393825
4
Iteration 10500: Loss = -11531.978776118232
5
Iteration 10600: Loss = -11531.967999107823
6
Iteration 10700: Loss = -11531.928507804909
7
Iteration 10800: Loss = -11531.921486303843
Iteration 10900: Loss = -11531.92239922704
1
Iteration 11000: Loss = -11531.93180016954
2
Iteration 11100: Loss = -11531.936476044242
3
Iteration 11200: Loss = -11531.95041357603
4
Iteration 11300: Loss = -11531.93638009963
5
Iteration 11400: Loss = -11531.920638581678
Iteration 11500: Loss = -11531.960886480074
1
Iteration 11600: Loss = -11517.349979308292
Iteration 11700: Loss = -11517.243359635848
Iteration 11800: Loss = -11517.247091055013
1
Iteration 11900: Loss = -11517.244798043723
2
Iteration 12000: Loss = -11517.282194390616
3
Iteration 12100: Loss = -11517.243222467123
Iteration 12200: Loss = -11517.250698974027
1
Iteration 12300: Loss = -11517.325612888502
2
Iteration 12400: Loss = -11517.24498037894
3
Iteration 12500: Loss = -11517.251791955312
4
Iteration 12600: Loss = -11517.248305047484
5
Iteration 12700: Loss = -11517.245414403438
6
Iteration 12800: Loss = -11517.243057309533
Iteration 12900: Loss = -11517.244822005512
1
Iteration 13000: Loss = -11517.247476479426
2
Iteration 13100: Loss = -11517.276287618675
3
Iteration 13200: Loss = -11517.248637252329
4
Iteration 13300: Loss = -11517.243284157314
5
Iteration 13400: Loss = -11517.244726625717
6
Iteration 13500: Loss = -11517.247229070328
7
Iteration 13600: Loss = -11517.24342654713
8
Iteration 13700: Loss = -11517.263662901862
9
Iteration 13800: Loss = -11517.245199846853
10
Stopping early at iteration 13800 due to no improvement.
tensor([[-10.2575,   8.6421],
        [  1.5238,  -5.0428],
        [  4.2493,  -5.8494],
        [  2.4106,  -4.1837],
        [ -6.2784,   4.7955],
        [ -3.6303,   2.1332],
        [ -6.9724,   5.3819],
        [ -5.5893,   3.2586],
        [  4.8745,  -6.4745],
        [  6.9067,  -8.2964],
        [ -6.4916,   4.8361],
        [  5.2815,  -6.8399],
        [  7.0515,  -9.7109],
        [ -8.5562,   6.8771],
        [  6.5692, -10.6008],
        [ -7.7156,   5.8849],
        [ -3.8279,   2.1786],
        [ -3.2186,   1.5841],
        [  6.9439,  -8.9555],
        [  8.2993,  -9.9884],
        [  6.4274,  -8.6417],
        [ -9.3901,   6.6595],
        [  7.9053,  -9.3412],
        [ -9.2855,   7.7188],
        [  6.3830,  -8.9403],
        [-10.6239,   7.0681],
        [ -8.8543,   7.4490],
        [  6.2528,  -9.2962],
        [ -4.1488,   2.6677],
        [  8.5007, -10.0177],
        [  6.4607,  -7.8536],
        [ -3.7268,   2.3344],
        [  6.9295,  -8.3852],
        [ -8.9917,   7.4503],
        [  6.1379,  -7.5266],
        [ -4.0386,   2.4834],
        [ -6.8377,   3.4641],
        [  4.8058,  -6.3709],
        [-10.5704,   6.9334],
        [ -7.3565,   4.7211],
        [  5.7462,  -7.1424],
        [  7.5900,  -9.1360],
        [  6.9383,  -8.6448],
        [  5.8299,  -7.8714],
        [  6.5982,  -8.0585],
        [  7.8174,  -9.2804],
        [  8.3668,  -9.9102],
        [ -8.4895,   3.8743],
        [ -9.3306,   7.9433],
        [-10.1374,   8.2904],
        [ -8.8176,   7.3867],
        [  6.5987,  -9.8947],
        [  6.3753,  -7.8527],
        [ -8.1803,   6.5942],
        [  8.1277,  -9.5159],
        [ -9.1335,   6.8953],
        [  6.8630,  -8.2509],
        [  7.3665,  -8.8098],
        [ -7.1308,   5.7175],
        [ -8.7342,   6.5841],
        [ -9.3970,   6.7021],
        [  8.6642, -10.0956],
        [ -8.7042,   7.2893],
        [ -9.1529,   7.7660],
        [-10.1052,   7.0290],
        [ -7.8835,   6.4899],
        [  7.8883, -10.5092],
        [  1.8337,  -3.2540],
        [ -8.4902,   6.4898],
        [ -5.9436,   3.0972],
        [ -5.6041,   3.9509],
        [  7.6267,  -9.2959],
        [ -8.9435,   7.5001],
        [ -4.1976,   2.8112],
        [  3.1460,  -4.6797],
        [ -5.7454,   3.3497],
        [ -9.9996,   8.4391],
        [ -8.8709,   7.4762],
        [  6.9049,  -9.3663],
        [ -8.3718,   6.7994],
        [  7.5241,  -8.9216],
        [  7.4969,  -9.9002],
        [ -8.2980,   5.6551],
        [ -8.2901,   6.8955],
        [  7.0148,  -8.4013],
        [-10.7348,   9.1550],
        [  3.4683,  -4.9218],
        [ -6.3870,   4.9083],
        [ -9.8575,   6.5219],
        [  7.2315,  -9.3868],
        [  6.2691,  -8.1606],
        [  7.2433, -10.4340],
        [ -8.5270,   7.1330],
        [  7.8558, -10.3539],
        [  7.8345,  -9.3607],
        [  7.1114,  -9.3263],
        [ -8.8418,   4.4936],
        [  7.0780,  -9.3682],
        [  8.0306,  -9.4944],
        [ -8.7269,   6.7827]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7551, 0.2449],
        [0.2062, 0.7938]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4900, 0.5100], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4028, 0.1036],
         [0.6634, 0.1986]],

        [[0.5184, 0.0962],
         [0.9661, 0.3284]],

        [[0.8363, 0.1107],
         [0.4908, 0.4763]],

        [[0.9369, 0.1076],
         [0.7483, 0.7788]],

        [[0.3329, 0.0996],
         [0.5468, 0.9142]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320648505942
Average Adjusted Rand Index: 0.9839985580570193
11513.289081775833
new:  [0.9603206466266674, 0.9840320648505942, 0.9920000001562724, 0.9840320648505942] [0.9616161616161616, 0.9841602586080228, 0.9919992163297293, 0.9839985580570193] [11541.922559236882, 11520.101996846497, 11503.386497995798, 11517.245199846853]
prior:  [0.9920000001562724, 0.9920000001562724, 0.9920000001562724, 0.9920000001562724] [0.9919992163297293, 0.9919992163297293, 0.9919992163297293, 0.9919992163297293] [11504.899871005571, 11504.899870530995, 11504.899871005571, 11504.899871005571]
-----------------------------------------------------------------------------------------
This iteration is 2
True Objective function: Loss = -11502.48610111485
Iteration 0: Loss = -21321.816854614255
Iteration 10: Loss = -11745.30028677793
Iteration 20: Loss = -11735.980722956541
Iteration 30: Loss = -11728.488664526925
Iteration 40: Loss = -11725.945364149033
Iteration 50: Loss = -11720.552974408725
Iteration 60: Loss = -11720.11109510347
Iteration 70: Loss = -11720.117573048645
1
Iteration 80: Loss = -11720.117993734999
2
Iteration 90: Loss = -11720.117997962938
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.4381, 0.5619],
        [0.6045, 0.3955]], dtype=torch.float64)
alpha: tensor([0.5354, 0.4646])
beta: tensor([[[0.2586, 0.1001],
         [0.4455, 0.3264]],

        [[0.7714, 0.1008],
         [0.8859, 0.5684]],

        [[0.5554, 0.0915],
         [0.9655, 0.0072]],

        [[0.6308, 0.0899],
         [0.6011, 0.1818]],

        [[0.5284, 0.0983],
         [0.0216, 0.6608]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 11
Adjusted Rand Index: 0.6045993611318492
time is 1
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 9
Adjusted Rand Index: 0.6691181331636993
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.06783559262718615
Average Adjusted Rand Index: 0.8232263728805679
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21201.306051458843
Iteration 100: Loss = -12233.935881187947
Iteration 200: Loss = -12224.18259368343
Iteration 300: Loss = -12213.163579575647
Iteration 400: Loss = -12086.574953505033
Iteration 500: Loss = -11696.433081169087
Iteration 600: Loss = -11592.056528742276
Iteration 700: Loss = -11584.5485154478
Iteration 800: Loss = -11568.182896069344
Iteration 900: Loss = -11556.988512146552
Iteration 1000: Loss = -11541.190299319012
Iteration 1100: Loss = -11539.509113519227
Iteration 1200: Loss = -11520.660201060664
Iteration 1300: Loss = -11513.714221004248
Iteration 1400: Loss = -11510.22801369751
Iteration 1500: Loss = -11501.781876518387
Iteration 1600: Loss = -11501.679030328709
Iteration 1700: Loss = -11501.647345210271
Iteration 1800: Loss = -11501.624431556318
Iteration 1900: Loss = -11501.606406456029
Iteration 2000: Loss = -11501.59150873684
Iteration 2100: Loss = -11501.699756133206
1
Iteration 2200: Loss = -11501.568541586004
Iteration 2300: Loss = -11501.559673638085
Iteration 2400: Loss = -11501.551875380936
Iteration 2500: Loss = -11501.544892601847
Iteration 2600: Loss = -11501.529951280741
Iteration 2700: Loss = -11501.524596166928
Iteration 2800: Loss = -11501.520146448524
Iteration 2900: Loss = -11501.51567620337
Iteration 3000: Loss = -11501.511881427305
Iteration 3100: Loss = -11501.547937229921
1
Iteration 3200: Loss = -11501.505273280769
Iteration 3300: Loss = -11501.50240793812
Iteration 3400: Loss = -11501.611423913548
1
Iteration 3500: Loss = -11501.497237015057
Iteration 3600: Loss = -11501.494941899555
Iteration 3700: Loss = -11501.492644614958
Iteration 3800: Loss = -11501.490415148684
Iteration 3900: Loss = -11501.502273712551
1
Iteration 4000: Loss = -11501.484748631206
Iteration 4100: Loss = -11501.47618829705
Iteration 4200: Loss = -11494.421178453715
Iteration 4300: Loss = -11494.404036854194
Iteration 4400: Loss = -11494.402093418368
Iteration 4500: Loss = -11494.531761965036
1
Iteration 4600: Loss = -11494.399338249517
Iteration 4700: Loss = -11494.39822465029
Iteration 4800: Loss = -11494.398712637185
1
Iteration 4900: Loss = -11494.396301119517
Iteration 5000: Loss = -11494.395458524798
Iteration 5100: Loss = -11494.417043619398
1
Iteration 5200: Loss = -11494.394025524984
Iteration 5300: Loss = -11494.393352477378
Iteration 5400: Loss = -11494.392744471032
Iteration 5500: Loss = -11494.398112563555
1
Iteration 5600: Loss = -11494.391647473592
Iteration 5700: Loss = -11494.391128661508
Iteration 5800: Loss = -11494.397299485381
1
Iteration 5900: Loss = -11494.390270542346
Iteration 6000: Loss = -11494.389877672475
Iteration 6100: Loss = -11494.389586986144
Iteration 6200: Loss = -11494.389164334916
Iteration 6300: Loss = -11494.388813149879
Iteration 6400: Loss = -11494.388569315102
Iteration 6500: Loss = -11494.388216896581
Iteration 6600: Loss = -11494.38795729486
Iteration 6700: Loss = -11494.388026349166
1
Iteration 6800: Loss = -11494.387399437885
Iteration 6900: Loss = -11494.392498571618
1
Iteration 7000: Loss = -11494.387128306384
Iteration 7100: Loss = -11494.386823177594
Iteration 7200: Loss = -11494.38726048079
1
Iteration 7300: Loss = -11494.4014718182
2
Iteration 7400: Loss = -11494.386258440189
Iteration 7500: Loss = -11494.386087452716
Iteration 7600: Loss = -11494.385934467102
Iteration 7700: Loss = -11494.38575980308
Iteration 7800: Loss = -11494.385649945565
Iteration 7900: Loss = -11494.590056919826
1
Iteration 8000: Loss = -11494.385580150742
Iteration 8100: Loss = -11494.385168121365
Iteration 8200: Loss = -11494.385063327552
Iteration 8300: Loss = -11494.38618271093
1
Iteration 8400: Loss = -11494.385173630813
2
Iteration 8500: Loss = -11494.38505621786
Iteration 8600: Loss = -11494.386098176243
1
Iteration 8700: Loss = -11494.38697708251
2
Iteration 8800: Loss = -11494.383928167468
Iteration 8900: Loss = -11494.442512702593
1
Iteration 9000: Loss = -11494.392922942401
2
Iteration 9100: Loss = -11494.406516943447
3
Iteration 9200: Loss = -11494.384092838876
4
Iteration 9300: Loss = -11494.422013317942
5
Iteration 9400: Loss = -11494.384126877287
6
Iteration 9500: Loss = -11494.400217357901
7
Iteration 9600: Loss = -11494.382950603715
Iteration 9700: Loss = -11494.41178640873
1
Iteration 9800: Loss = -11494.383909805374
2
Iteration 9900: Loss = -11494.442447794874
3
Iteration 10000: Loss = -11494.40234369624
4
Iteration 10100: Loss = -11494.520625922336
5
Iteration 10200: Loss = -11494.383768930858
6
Iteration 10300: Loss = -11494.383356551398
7
Iteration 10400: Loss = -11494.389790216479
8
Iteration 10500: Loss = -11494.38260075467
Iteration 10600: Loss = -11494.392793609522
1
Iteration 10700: Loss = -11494.382510192938
Iteration 10800: Loss = -11494.382889339679
1
Iteration 10900: Loss = -11494.401248303318
2
Iteration 11000: Loss = -11494.39212832404
3
Iteration 11100: Loss = -11494.404020583237
4
Iteration 11200: Loss = -11494.391585434025
5
Iteration 11300: Loss = -11494.383426803786
6
Iteration 11400: Loss = -11494.38834385759
7
Iteration 11500: Loss = -11494.395790860019
8
Iteration 11600: Loss = -11494.38252559146
9
Iteration 11700: Loss = -11494.382302159303
Iteration 11800: Loss = -11494.385560729896
1
Iteration 11900: Loss = -11494.382260556027
Iteration 12000: Loss = -11494.383176703917
1
Iteration 12100: Loss = -11494.382658114773
2
Iteration 12200: Loss = -11494.430075830387
3
Iteration 12300: Loss = -11494.38247859992
4
Iteration 12400: Loss = -11494.384624015489
5
Iteration 12500: Loss = -11494.384127597204
6
Iteration 12600: Loss = -11494.384030930776
7
Iteration 12700: Loss = -11494.423972716173
8
Iteration 12800: Loss = -11494.382715451648
9
Iteration 12900: Loss = -11494.382544373546
10
Stopping early at iteration 12900 due to no improvement.
tensor([[  4.7637,  -9.3789],
        [  4.5733,  -9.1885],
        [ -9.3981,   4.7829],
        [-10.5230,   5.9078],
        [ -9.7164,   5.1011],
        [  1.3570,  -5.9722],
        [  3.1205,  -7.7357],
        [-11.3248,   6.7095],
        [  4.8344,  -9.4496],
        [ -9.5830,   4.9678],
        [ -8.8286,   4.2134],
        [ -8.7653,   4.1501],
        [ -6.2982,   1.6830],
        [  6.9531, -11.5684],
        [ -5.9651,   1.3499],
        [ -6.0137,   1.3985],
        [ -8.3127,   3.6975],
        [ -0.3509,  -4.2643],
        [  5.1049,  -9.7201],
        [  1.7488,  -6.3641],
        [  2.3317,  -6.9469],
        [ -1.2158,  -3.3994],
        [ -4.2738,  -0.3414],
        [  3.7273,  -8.3426],
        [  5.0473,  -9.6625],
        [ -6.2410,   1.6258],
        [ -9.5647,   4.9495],
        [-10.8271,   6.2119],
        [ -9.6743,   5.0591],
        [ -9.4517,   4.8365],
        [ -8.8285,   4.2133],
        [  2.9222,  -7.5375],
        [ -9.0080,   4.3928],
        [  4.7911,  -9.4064],
        [  4.3723,  -8.9875],
        [-11.2835,   6.6682],
        [  6.3985, -11.0137],
        [  3.9495,  -8.5647],
        [ -7.1317,   2.5165],
        [ -9.0945,   4.4792],
        [  6.7939, -11.4092],
        [  6.0606, -10.6758],
        [  6.1714, -10.7866],
        [ -9.4426,   4.8274],
        [  4.4492,  -9.0644],
        [ -9.2330,   4.6178],
        [  3.2558,  -7.8710],
        [ -9.3800,   4.7648],
        [ -9.5909,   4.9757],
        [  1.9047,  -6.5199],
        [ -6.6153,   2.0001],
        [-10.9638,   6.3486],
        [  4.6599,  -9.2752],
        [ -5.5687,   0.9535],
        [-11.3197,   6.7045],
        [ -9.3527,   4.7375],
        [  4.5264,  -9.1416],
        [  4.0007,  -8.6159],
        [ -8.5629,   3.9477],
        [  4.5356,  -9.1509],
        [  1.9715,  -6.5867],
        [  5.0582,  -9.6734],
        [-10.6794,   6.0642],
        [  2.8674,  -7.4826],
        [  6.1970, -10.8122],
        [ -9.2082,   4.5930],
        [ -9.2391,   4.6238],
        [  4.8818,  -9.4971],
        [  7.6102, -12.2254],
        [  6.4032, -11.0184],
        [  6.6320, -11.2473],
        [ -8.5754,   3.9602],
        [  4.6447,  -9.2599],
        [ -9.9236,   5.3084],
        [  5.9233, -10.5385],
        [  5.2145,  -9.8297],
        [ -8.3407,   3.7255],
        [  5.1139,  -9.7292],
        [  6.9657, -11.5809],
        [ -0.7825,  -3.8328],
        [  1.3822,  -5.9974],
        [  5.3241,  -9.9393],
        [-10.9371,   6.3218],
        [ -8.1337,   3.5185],
        [-11.1656,   6.5503],
        [ -9.1707,   4.5555],
        [  5.2137,  -9.8289],
        [  4.4528,  -9.0680],
        [ -8.7968,   4.1816],
        [ -9.2309,   4.6157],
        [ -1.2692,  -3.3461],
        [ -6.4618,   1.8465],
        [ -5.1481,   0.5329],
        [  4.9365,  -9.5517],
        [  4.3944,  -9.0097],
        [ -9.8123,   5.1971],
        [  4.8848,  -9.5000],
        [ -9.0366,   4.4213],
        [ -9.8464,   5.2312],
        [  5.3371,  -9.9523]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7410, 0.2590],
        [0.2378, 0.7622]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5073, 0.4927], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3879, 0.1010],
         [0.4455, 0.2065]],

        [[0.7714, 0.1030],
         [0.8859, 0.5684]],

        [[0.5554, 0.0920],
         [0.9655, 0.0072]],

        [[0.6308, 0.0964],
         [0.6011, 0.1818]],

        [[0.5284, 0.0996],
         [0.0216, 0.6608]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.9919992163297293
Iteration 0: Loss = -26084.127923511234
Iteration 10: Loss = -11499.132834819871
Iteration 20: Loss = -11496.57114516387
Iteration 30: Loss = -11496.571070556007
Iteration 40: Loss = -11496.571067580255
Iteration 50: Loss = -11496.571067788123
1
Iteration 60: Loss = -11496.571067788125
2
Iteration 70: Loss = -11496.571067788125
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7372, 0.2628],
        [0.2435, 0.7565]], dtype=torch.float64)
alpha: tensor([0.4919, 0.5081])
beta: tensor([[[0.3799, 0.1019],
         [0.0717, 0.2024]],

        [[0.5206, 0.1029],
         [0.7564, 0.3414]],

        [[0.3083, 0.0920],
         [0.8762, 0.6653]],

        [[0.9007, 0.0962],
         [0.6321, 0.7924]],

        [[0.0331, 0.0995],
         [0.8313, 0.9813]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961254157898
Average Adjusted Rand Index: 0.9761596921619656
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26011.110890030053
Iteration 100: Loss = -12025.982695891833
Iteration 200: Loss = -11752.573146381437
Iteration 300: Loss = -11639.307297919986
Iteration 400: Loss = -11638.220072790422
Iteration 500: Loss = -11637.700454625274
Iteration 600: Loss = -11637.506584805173
Iteration 700: Loss = -11637.38733378004
Iteration 800: Loss = -11637.308378739704
Iteration 900: Loss = -11637.254089958811
Iteration 1000: Loss = -11637.214683868327
Iteration 1100: Loss = -11637.184787624303
Iteration 1200: Loss = -11637.16151070535
Iteration 1300: Loss = -11637.143025162537
Iteration 1400: Loss = -11637.12799820923
Iteration 1500: Loss = -11637.115668766486
Iteration 1600: Loss = -11637.105389000493
Iteration 1700: Loss = -11637.096668391387
Iteration 1800: Loss = -11637.089258281396
Iteration 1900: Loss = -11637.082942313835
Iteration 2000: Loss = -11637.077455052135
Iteration 2100: Loss = -11637.07263161816
Iteration 2200: Loss = -11637.068419197354
Iteration 2300: Loss = -11637.06470375088
Iteration 2400: Loss = -11637.061447575703
Iteration 2500: Loss = -11637.058519037055
Iteration 2600: Loss = -11637.055914083166
Iteration 2700: Loss = -11637.053573091329
Iteration 2800: Loss = -11637.051412554854
Iteration 2900: Loss = -11637.049510446626
Iteration 3000: Loss = -11637.04778109597
Iteration 3100: Loss = -11637.046213589274
Iteration 3200: Loss = -11637.044807069473
Iteration 3300: Loss = -11637.043489892876
Iteration 3400: Loss = -11637.042274272304
Iteration 3500: Loss = -11637.044676359956
1
Iteration 3600: Loss = -11637.04018903969
Iteration 3700: Loss = -11637.039250705402
Iteration 3800: Loss = -11637.038397485061
Iteration 3900: Loss = -11637.037606214537
Iteration 4000: Loss = -11637.036893772009
Iteration 4100: Loss = -11637.036226867898
Iteration 4200: Loss = -11637.03575954469
Iteration 4300: Loss = -11637.034984825974
Iteration 4400: Loss = -11637.034444998648
Iteration 4500: Loss = -11637.034656559734
1
Iteration 4600: Loss = -11637.033488928528
Iteration 4700: Loss = -11637.033246403354
Iteration 4800: Loss = -11637.03350822432
1
Iteration 4900: Loss = -11637.032261619206
Iteration 5000: Loss = -11637.031904696749
Iteration 5100: Loss = -11637.03177713237
Iteration 5200: Loss = -11637.03127837455
Iteration 5300: Loss = -11637.030965441323
Iteration 5400: Loss = -11637.03330689238
1
Iteration 5500: Loss = -11637.032545037486
2
Iteration 5600: Loss = -11637.030246549066
Iteration 5700: Loss = -11637.030140767878
Iteration 5800: Loss = -11637.02976673825
Iteration 5900: Loss = -11637.039563416
1
Iteration 6000: Loss = -11637.029351318512
Iteration 6100: Loss = -11637.029476821219
1
Iteration 6200: Loss = -11637.030079909026
2
Iteration 6300: Loss = -11637.029382255128
3
Iteration 6400: Loss = -11637.030050707483
4
Iteration 6500: Loss = -11637.02916574909
Iteration 6600: Loss = -11637.033316379026
1
Iteration 6700: Loss = -11637.03138924182
2
Iteration 6800: Loss = -11637.028631834208
Iteration 6900: Loss = -11637.02996967433
1
Iteration 7000: Loss = -11637.029081163064
2
Iteration 7100: Loss = -11637.028167871373
Iteration 7200: Loss = -11637.028953953091
1
Iteration 7300: Loss = -11637.034209431677
2
Iteration 7400: Loss = -11637.028000325794
Iteration 7500: Loss = -11637.030015422191
1
Iteration 7600: Loss = -11637.037767628124
2
Iteration 7700: Loss = -11637.02826726136
3
Iteration 7800: Loss = -11637.030641095173
4
Iteration 7900: Loss = -11637.027320710955
Iteration 8000: Loss = -11637.035733096296
1
Iteration 8100: Loss = -11637.030671825545
2
Iteration 8200: Loss = -11637.029199658919
3
Iteration 8300: Loss = -11637.027145806169
Iteration 8400: Loss = -11637.028399413233
1
Iteration 8500: Loss = -11637.049883132837
2
Iteration 8600: Loss = -11637.032087266383
3
Iteration 8700: Loss = -11637.163933105077
4
Iteration 8800: Loss = -11637.026912662743
Iteration 8900: Loss = -11637.039680941667
1
Iteration 9000: Loss = -11637.027466338968
2
Iteration 9100: Loss = -11637.100610730424
3
Iteration 9200: Loss = -11637.026839426046
Iteration 9300: Loss = -11637.026924640282
1
Iteration 9400: Loss = -11637.109028686917
2
Iteration 9500: Loss = -11637.027085020634
3
Iteration 9600: Loss = -11637.032513679602
4
Iteration 9700: Loss = -11637.034104929506
5
Iteration 9800: Loss = -11637.026808389004
Iteration 9900: Loss = -11637.027774624445
1
Iteration 10000: Loss = -11637.034031357904
2
Iteration 10100: Loss = -11637.185654168943
3
Iteration 10200: Loss = -11637.026608603157
Iteration 10300: Loss = -11637.028244458606
1
Iteration 10400: Loss = -11637.02659339605
Iteration 10500: Loss = -11637.026921073299
1
Iteration 10600: Loss = -11637.026528050294
Iteration 10700: Loss = -11637.026602991902
1
Iteration 10800: Loss = -11637.026475815948
Iteration 10900: Loss = -11637.042861257987
1
Iteration 11000: Loss = -11637.02645815083
Iteration 11100: Loss = -11637.02645044211
Iteration 11200: Loss = -11637.026722778015
1
Iteration 11300: Loss = -11637.02642798339
Iteration 11400: Loss = -11637.052375827783
1
Iteration 11500: Loss = -11637.02644156501
2
Iteration 11600: Loss = -11637.05716310827
3
Iteration 11700: Loss = -11637.02874518415
4
Iteration 11800: Loss = -11637.028621965279
5
Iteration 11900: Loss = -11637.061759274835
6
Iteration 12000: Loss = -11637.026414445128
Iteration 12100: Loss = -11637.032311111827
1
Iteration 12200: Loss = -11637.026391271753
Iteration 12300: Loss = -11637.02715422307
1
Iteration 12400: Loss = -11637.026900616458
2
Iteration 12500: Loss = -11637.02560222384
Iteration 12600: Loss = -11637.026164944315
1
Iteration 12700: Loss = -11637.02881407548
2
Iteration 12800: Loss = -11637.026934462097
3
Iteration 12900: Loss = -11637.043532978485
4
Iteration 13000: Loss = -11637.025446226608
Iteration 13100: Loss = -11637.123507118158
1
Iteration 13200: Loss = -11637.025427565139
Iteration 13300: Loss = -11637.02821836514
1
Iteration 13400: Loss = -11637.025447477175
2
Iteration 13500: Loss = -11637.025406579052
Iteration 13600: Loss = -11637.028052113532
1
Iteration 13700: Loss = -11637.025395174061
Iteration 13800: Loss = -11637.036641095197
1
Iteration 13900: Loss = -11637.025457738644
2
Iteration 14000: Loss = -11637.02548464424
3
Iteration 14100: Loss = -11637.095183155248
4
Iteration 14200: Loss = -11637.025430409398
5
Iteration 14300: Loss = -11637.028297101835
6
Iteration 14400: Loss = -11637.02550772815
7
Iteration 14500: Loss = -11637.02542438111
8
Iteration 14600: Loss = -11637.061480901746
9
Iteration 14700: Loss = -11637.037519304113
10
Stopping early at iteration 14700 due to no improvement.
tensor([[  7.1077,  -8.6152],
        [  6.7033,  -8.1273],
        [ -9.1085,   7.4494],
        [ -8.8769,   7.3503],
        [ -8.5863,   7.1259],
        [  3.4746,  -4.9315],
        [  4.5220,  -6.7355],
        [ -5.0555,   3.6286],
        [  7.5181,  -8.9148],
        [ -8.6051,   7.2169],
        [ -6.9079,   5.4483],
        [ -7.6144,   4.6400],
        [ -4.4774,   2.8840],
        [  8.1957, -10.6646],
        [ -3.9765,   2.5620],
        [ -4.3309,   2.6953],
        [ -6.8519,   5.0698],
        [  1.4687,  -2.8737],
        [  7.8385,  -9.2315],
        [  3.3414,  -5.2275],
        [  4.5442,  -5.9319],
        [  0.2419,  -2.1532],
        [ -2.2597,   0.5684],
        [  5.3316,  -7.2008],
        [  7.6364,  -9.1499],
        [ -4.2167,   2.8304],
        [ -9.1360,   7.6091],
        [ -5.7808,   4.3945],
        [ -9.0590,   7.5906],
        [ -8.1739,   6.7853],
        [ -4.8467,   2.5664],
        [  4.6548,  -6.0998],
        [ -7.6882,   5.2231],
        [  7.4339,  -9.0584],
        [  6.3391,  -8.1810],
        [ -9.1893,   7.7654],
        [  8.1288,  -9.6038],
        [  4.1689,  -8.7841],
        [ -5.7545,   3.6001],
        [ -8.1423,   6.2266],
        [  8.3877, -10.1359],
        [  7.3158,  -9.3775],
        [  7.4801,  -9.2038],
        [ -7.9363,   6.4578],
        [  6.6887,  -8.1406],
        [ -8.2103,   6.7855],
        [  4.7546,  -6.8785],
        [ -8.2177,   6.4014],
        [ -8.4166,   7.0201],
        [  4.3566,  -5.8077],
        [ -4.2298,   2.7963],
        [ -7.0897,   4.5975],
        [  7.1412,  -8.5602],
        [ -3.5446,   1.9668],
        [ -8.0289,   6.6174],
        [ -7.6453,   5.4028],
        [  6.6401,  -8.5447],
        [  5.4461,  -7.1127],
        [ -6.5166,   4.9922],
        [  6.3944,  -8.0388],
        [  3.3965,  -5.6058],
        [  7.0534,  -9.4783],
        [ -8.2675,   6.8801],
        [  3.4384,  -8.0537],
        [  7.4847,  -9.1971],
        [ -8.2562,   6.7707],
        [ -7.7583,   6.0698],
        [  7.4491,  -8.9273],
        [  4.1731,  -7.7035],
        [  8.0963,  -9.6377],
        [  8.2119,  -9.6669],
        [ -6.9966,   4.1316],
        [  6.9325,  -8.9668],
        [ -8.4897,   6.1257],
        [  7.2836,  -8.7955],
        [  7.3182,  -8.7144],
        [ -7.3203,   4.4110],
        [  7.3602,  -9.7682],
        [  8.2844,  -9.6767],
        [  0.1714,  -3.1871],
        [  3.1256,  -4.5702],
        [  7.7383, -10.3172],
        [ -5.6568,   3.3278],
        [ -6.4433,   4.3122],
        [ -3.8296,   2.4309],
        [ -7.8765,   6.2728],
        [  8.0298,  -9.7734],
        [  5.2392,  -9.5027],
        [ -6.8255,   5.4245],
        [ -7.9501,   5.9713],
        [  0.6275,  -2.1726],
        [ -4.4631,   2.6381],
        [ -3.4094,   1.0448],
        [  7.2981,  -8.7592],
        [  5.6701,  -8.0150],
        [ -9.6511,   7.8833],
        [  6.9778,  -8.3656],
        [ -6.8212,   5.4320],
        [ -6.4827,   5.0312],
        [  7.7972, -10.8492]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6180, 0.3820],
        [0.2763, 0.7237]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5091, 0.4909], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3763, 0.1007],
         [0.0717, 0.2231]],

        [[0.5206, 0.1025],
         [0.7564, 0.3414]],

        [[0.3083, 0.0919],
         [0.8762, 0.6653]],

        [[0.9007, 0.0960],
         [0.6321, 0.7924]],

        [[0.0331, 0.1053],
         [0.8313, 0.9813]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 77
Adjusted Rand Index: 0.28514463947159197
Global Adjusted Rand Index: 0.4613270204225441
Average Adjusted Rand Index: 0.8331886200562838
Iteration 0: Loss = -21992.112211100703
Iteration 10: Loss = -12216.605430209733
Iteration 20: Loss = -11496.531069364732
Iteration 30: Loss = -11496.571079535135
1
Iteration 40: Loss = -11496.57107163152
2
Iteration 50: Loss = -11496.571066521194
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7372, 0.2628],
        [0.2435, 0.7565]], dtype=torch.float64)
alpha: tensor([0.4919, 0.5081])
beta: tensor([[[0.3799, 0.1019],
         [0.4112, 0.2024]],

        [[0.2521, 0.1029],
         [0.2737, 0.7864]],

        [[0.9832, 0.0920],
         [0.2905, 0.8436]],

        [[0.7457, 0.0962],
         [0.5577, 0.5189]],

        [[0.5207, 0.0995],
         [0.6996, 0.4671]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961254157898
Average Adjusted Rand Index: 0.9761596921619656
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21992.48019784186
Iteration 100: Loss = -12278.092224786318
Iteration 200: Loss = -12213.532346842112
Iteration 300: Loss = -11894.478900625109
Iteration 400: Loss = -11879.574199918125
Iteration 500: Loss = -11866.198405870733
Iteration 600: Loss = -11859.02444440795
Iteration 700: Loss = -11851.859157880834
Iteration 800: Loss = -11850.511057532767
Iteration 900: Loss = -11830.659189190406
Iteration 1000: Loss = -11793.89615592563
Iteration 1100: Loss = -11772.308595720913
Iteration 1200: Loss = -11764.036222801813
Iteration 1300: Loss = -11736.63450620791
Iteration 1400: Loss = -11729.137893111103
Iteration 1500: Loss = -11717.850407348094
Iteration 1600: Loss = -11714.177576591046
Iteration 1700: Loss = -11699.250896380772
Iteration 1800: Loss = -11685.885926654852
Iteration 1900: Loss = -11674.939653670932
Iteration 2000: Loss = -11674.86413482244
Iteration 2100: Loss = -11674.831208442503
Iteration 2200: Loss = -11667.994131816968
Iteration 2300: Loss = -11667.392158976461
Iteration 2400: Loss = -11667.373019487506
Iteration 2500: Loss = -11667.346447328864
Iteration 2600: Loss = -11654.588054324433
Iteration 2700: Loss = -11654.224881541251
Iteration 2800: Loss = -11637.747252150622
Iteration 2900: Loss = -11637.702804332273
Iteration 3000: Loss = -11637.678724985419
Iteration 3100: Loss = -11620.195013874785
Iteration 3200: Loss = -11603.48546818672
Iteration 3300: Loss = -11603.403068031213
Iteration 3400: Loss = -11603.395525498167
Iteration 3500: Loss = -11603.38931319236
Iteration 3600: Loss = -11603.384700051101
Iteration 3700: Loss = -11600.433013212909
Iteration 3800: Loss = -11599.033075802772
Iteration 3900: Loss = -11599.030198753373
Iteration 4000: Loss = -11598.890270412914
Iteration 4100: Loss = -11579.109077509385
Iteration 4200: Loss = -11579.102018172573
Iteration 4300: Loss = -11579.09664370611
Iteration 4400: Loss = -11579.09271775062
Iteration 4500: Loss = -11579.081455999289
Iteration 4600: Loss = -11574.12650511082
Iteration 4700: Loss = -11555.649811459356
Iteration 4800: Loss = -11542.141080011437
Iteration 4900: Loss = -11542.135139600878
Iteration 5000: Loss = -11542.132121580004
Iteration 5100: Loss = -11542.129862209282
Iteration 5200: Loss = -11542.128015322247
Iteration 5300: Loss = -11542.126451935737
Iteration 5400: Loss = -11542.125054960537
Iteration 5500: Loss = -11542.12384095429
Iteration 5600: Loss = -11542.12437572269
1
Iteration 5700: Loss = -11522.79579176938
Iteration 5800: Loss = -11522.753702325283
Iteration 5900: Loss = -11522.753380549082
Iteration 6000: Loss = -11522.76016194661
1
Iteration 6100: Loss = -11522.750668379718
Iteration 6200: Loss = -11522.752055137833
1
Iteration 6300: Loss = -11522.749159919089
Iteration 6400: Loss = -11522.749957318007
1
Iteration 6500: Loss = -11522.747950950688
Iteration 6600: Loss = -11522.748136205244
1
Iteration 6700: Loss = -11522.747641336151
Iteration 6800: Loss = -11522.752816069793
1
Iteration 6900: Loss = -11522.745554617353
Iteration 7000: Loss = -11511.242523671855
Iteration 7100: Loss = -11507.37877441273
Iteration 7200: Loss = -11507.379655133202
1
Iteration 7300: Loss = -11507.373536778334
Iteration 7400: Loss = -11507.372925905855
Iteration 7500: Loss = -11507.388253407762
1
Iteration 7600: Loss = -11507.373184254107
2
Iteration 7700: Loss = -11507.37035264619
Iteration 7800: Loss = -11507.376325071715
1
Iteration 7900: Loss = -11507.367103579563
Iteration 8000: Loss = -11507.381207272618
1
Iteration 8100: Loss = -11507.383910009816
2
Iteration 8200: Loss = -11507.407839182371
3
Iteration 8300: Loss = -11507.385608257202
4
Iteration 8400: Loss = -11507.36315629487
Iteration 8500: Loss = -11507.363708052668
1
Iteration 8600: Loss = -11507.363317232703
2
Iteration 8700: Loss = -11507.411016677765
3
Iteration 8800: Loss = -11507.373566779082
4
Iteration 8900: Loss = -11507.431691637354
5
Iteration 9000: Loss = -11507.363136065065
Iteration 9100: Loss = -11507.461146194917
1
Iteration 9200: Loss = -11507.377814651703
2
Iteration 9300: Loss = -11507.362218797765
Iteration 9400: Loss = -11507.366299387579
1
Iteration 9500: Loss = -11507.369699779387
2
Iteration 9600: Loss = -11507.361915368101
Iteration 9700: Loss = -11507.363513829929
1
Iteration 9800: Loss = -11507.372870779913
2
Iteration 9900: Loss = -11507.368008445368
3
Iteration 10000: Loss = -11507.373556244927
4
Iteration 10100: Loss = -11507.36205137058
5
Iteration 10200: Loss = -11507.371304833623
6
Iteration 10300: Loss = -11507.372065039632
7
Iteration 10400: Loss = -11507.36435445811
8
Iteration 10500: Loss = -11507.365496058379
9
Iteration 10600: Loss = -11507.362201064238
10
Stopping early at iteration 10600 due to no improvement.
tensor([[  6.1719,  -8.0301],
        [  5.4384,  -7.1291],
        [ -9.1735,   6.6444],
        [ -7.8268,   6.0840],
        [ -8.5935,   6.1812],
        [  2.6538,  -4.7091],
        [  4.4183,  -6.1756],
        [ -5.6256,   3.9657],
        [  6.5105,  -8.5764],
        [ -7.4967,   6.0615],
        [ -6.8364,   5.4159],
        [ -6.9524,   5.5241],
        [ -4.4603,   3.0390],
        [  6.9053,  -9.2246],
        [ -4.1064,   2.6188],
        [ -4.2952,   2.5198],
        [ -6.7527,   4.5204],
        [  1.3108,  -2.6971],
        [  6.3170,  -7.7415],
        [  3.3271,  -4.7370],
        [  3.7617,  -5.4139],
        [  0.4981,  -1.8915],
        [ -2.5002,   1.0809],
        [  5.2251,  -6.6139],
        [  5.8363,  -9.4534],
        [ -8.6554,   6.2258],
        [ -7.9430,   6.2679],
        [ -6.3683,   4.9414],
        [ -8.6910,   6.6679],
        [ -7.5778,   6.0401],
        [ -4.6811,   3.2818],
        [  4.5087,  -5.9160],
        [ -7.2332,   5.2811],
        [  5.9669,  -7.4235],
        [  3.7522,  -7.7560],
        [ -8.0033,   6.4250],
        [  7.0153,  -8.4612],
        [  5.2796,  -6.7225],
        [ -6.3697,   4.9741],
        [ -9.1328,   5.2628],
        [  7.2718,  -8.6606],
        [  6.0784,  -7.7738],
        [  6.3849,  -7.8921],
        [ -7.9892,   5.3007],
        [  5.3814,  -8.2232],
        [ -7.6960,   6.0862],
        [  3.3122,  -5.2710],
        [ -7.1292,   5.7428],
        [ -7.4346,   5.8221],
        [  3.3748,  -4.7849],
        [ -4.8067,   3.3839],
        [ -7.9283,   4.3582],
        [  5.4325,  -8.5512],
        [ -3.7564,   2.3324],
        [ -7.5459,   6.0687],
        [ -7.1547,   5.5355],
        [  5.9840,  -7.4234],
        [  5.7433,  -7.1811],
        [ -7.3486,   4.5874],
        [  4.5621,  -8.5180],
        [  2.3253,  -3.7922],
        [  5.6544,  -7.3662],
        [ -7.5997,   5.8232],
        [  3.2298,  -6.8754],
        [  6.2455,  -8.1553],
        [ -7.9306,   5.8863],
        [ -6.9603,   5.4397],
        [  6.1269,  -8.4469],
        [  4.8968,  -6.2851],
        [  6.1674,  -8.2271],
        [  6.9751,  -8.9773],
        [ -6.6235,   5.2171],
        [  5.6830,  -7.3096],
        [ -7.6007,   5.7808],
        [  5.6189,  -7.6228],
        [  6.3569,  -7.7432],
        [ -6.5752,   4.7542],
        [  6.4223,  -8.2235],
        [  7.2290,  -8.6218],
        [  0.7682,  -2.4298],
        [  1.4136,  -3.4903],
        [  7.1678, -10.0808],
        [ -5.8796,   4.0292],
        [ -6.9756,   5.5856],
        [ -4.4199,   2.5525],
        [ -8.0359,   6.5309],
        [  6.6427,  -8.1692],
        [  5.7976,  -7.2419],
        [ -6.8594,   5.4256],
        [ -7.5273,   5.8263],
        [  0.2781,  -1.8747],
        [ -4.6579,   3.1154],
        [ -3.4918,   1.7707],
        [  5.8177,  -7.8882],
        [ -7.8217,   5.8954],
        [ -8.4626,   6.4562],
        [  5.1664,  -7.4066],
        [ -6.8926,   5.3526],
        [ -6.5899,   5.1808],
        [  6.2667,  -8.2936]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7396, 0.2604],
        [0.2415, 0.7585]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4982, 0.5018], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3880, 0.1057],
         [0.4112, 0.2058]],

        [[0.2521, 0.1031],
         [0.2737, 0.7864]],

        [[0.9832, 0.0919],
         [0.2905, 0.8436]],

        [[0.7457, 0.0964],
         [0.5577, 0.5189]],

        [[0.5207, 0.0995],
         [0.6996, 0.4671]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320230862317
Average Adjusted Rand Index: 0.9841599999999999
Iteration 0: Loss = -40474.57309394357
Iteration 10: Loss = -12150.563727448227
Iteration 20: Loss = -11496.56693523782
Iteration 30: Loss = -11496.571058031714
1
Iteration 40: Loss = -11496.571072556482
2
Iteration 50: Loss = -11496.57106840289
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7372, 0.2628],
        [0.2435, 0.7565]], dtype=torch.float64)
alpha: tensor([0.4919, 0.5081])
beta: tensor([[[0.3799, 0.1019],
         [0.3720, 0.2024]],

        [[0.4738, 0.1029],
         [0.5843, 0.0694]],

        [[0.9951, 0.0920],
         [0.8557, 0.4167]],

        [[0.4306, 0.0962],
         [0.9061, 0.9725]],

        [[0.3466, 0.0995],
         [0.0748, 0.5768]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961254157898
Average Adjusted Rand Index: 0.9761596921619656
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40474.561857610315
Iteration 100: Loss = -12357.006426763255
Iteration 200: Loss = -12315.060975346389
Iteration 300: Loss = -12284.66646459708
Iteration 400: Loss = -12265.323915771074
Iteration 500: Loss = -12252.443109738948
Iteration 600: Loss = -12234.091601848124
Iteration 700: Loss = -12230.678447167575
Iteration 800: Loss = -12229.197556837831
Iteration 900: Loss = -12226.87052581915
Iteration 1000: Loss = -12150.45141919594
Iteration 1100: Loss = -12094.522623112663
Iteration 1200: Loss = -12019.079400109504
Iteration 1300: Loss = -11998.401539545199
Iteration 1400: Loss = -11980.817808742268
Iteration 1500: Loss = -11974.845328683907
Iteration 1600: Loss = -11956.192364879731
Iteration 1700: Loss = -11938.894670219994
Iteration 1800: Loss = -11935.654231875984
Iteration 1900: Loss = -11913.498594286317
Iteration 2000: Loss = -11884.528919358947
Iteration 2100: Loss = -11864.741068508038
Iteration 2200: Loss = -11860.776785588601
Iteration 2300: Loss = -11850.414412474813
Iteration 2400: Loss = -11846.429473589444
Iteration 2500: Loss = -11830.971753937582
Iteration 2600: Loss = -11830.900715865902
Iteration 2700: Loss = -11830.854206258537
Iteration 2800: Loss = -11830.81798107983
Iteration 2900: Loss = -11830.788302287676
Iteration 3000: Loss = -11830.763456909981
Iteration 3100: Loss = -11830.742089883071
Iteration 3200: Loss = -11830.723559958065
Iteration 3300: Loss = -11830.707325249025
Iteration 3400: Loss = -11830.692935491972
Iteration 3500: Loss = -11830.68012347672
Iteration 3600: Loss = -11830.668618682213
Iteration 3700: Loss = -11830.658227311409
Iteration 3800: Loss = -11830.648805233555
Iteration 3900: Loss = -11830.64027981543
Iteration 4000: Loss = -11830.632481362614
Iteration 4100: Loss = -11830.625388403942
Iteration 4200: Loss = -11830.618821123295
Iteration 4300: Loss = -11830.61285162841
Iteration 4400: Loss = -11830.607327854486
Iteration 4500: Loss = -11830.602189056706
Iteration 4600: Loss = -11830.597466041247
Iteration 4700: Loss = -11830.593079361668
Iteration 4800: Loss = -11830.58900028009
Iteration 4900: Loss = -11830.58520112436
Iteration 5000: Loss = -11830.581668714885
Iteration 5100: Loss = -11830.578368158467
Iteration 5200: Loss = -11830.575309548194
Iteration 5300: Loss = -11830.572409440916
Iteration 5400: Loss = -11830.569706155145
Iteration 5500: Loss = -11830.567207916829
Iteration 5600: Loss = -11830.564786742407
Iteration 5700: Loss = -11830.562559700671
Iteration 5800: Loss = -11830.563113222552
1
Iteration 5900: Loss = -11830.558562271293
Iteration 6000: Loss = -11830.556912981723
Iteration 6100: Loss = -11830.556496716858
Iteration 6200: Loss = -11830.553486614885
Iteration 6300: Loss = -11830.551762437186
Iteration 6400: Loss = -11830.551296410216
Iteration 6500: Loss = -11830.548999020863
Iteration 6600: Loss = -11830.547668425119
Iteration 6700: Loss = -11830.546309620475
Iteration 6800: Loss = -11830.545128758453
Iteration 6900: Loss = -11830.549774583946
1
Iteration 7000: Loss = -11830.542609380407
Iteration 7100: Loss = -11830.539703456696
Iteration 7200: Loss = -11828.490112970896
Iteration 7300: Loss = -11828.489549490252
Iteration 7400: Loss = -11828.48759751574
Iteration 7500: Loss = -11828.486694677997
Iteration 7600: Loss = -11828.485874696315
Iteration 7700: Loss = -11828.487790944513
1
Iteration 7800: Loss = -11828.484060590292
Iteration 7900: Loss = -11828.485282339407
1
Iteration 8000: Loss = -11828.516211950899
2
Iteration 8100: Loss = -11828.498304463019
3
Iteration 8200: Loss = -11827.357250566376
Iteration 8300: Loss = -11824.654357875155
Iteration 8400: Loss = -11824.653984395318
Iteration 8500: Loss = -11824.651005751337
Iteration 8600: Loss = -11824.647477792103
Iteration 8700: Loss = -11824.635148158144
Iteration 8800: Loss = -11820.789026567021
Iteration 8900: Loss = -11809.144909691893
Iteration 9000: Loss = -11806.005506200023
Iteration 9100: Loss = -11805.864752736585
Iteration 9200: Loss = -11802.063638225023
Iteration 9300: Loss = -11801.965778000544
Iteration 9400: Loss = -11801.16067463013
Iteration 9500: Loss = -11799.081093622981
Iteration 9600: Loss = -11796.111205974605
Iteration 9700: Loss = -11795.13921035333
Iteration 9800: Loss = -11777.857909052476
Iteration 9900: Loss = -11753.311398335858
Iteration 10000: Loss = -11703.77647384416
Iteration 10100: Loss = -11693.508834559987
Iteration 10200: Loss = -11674.709960666953
Iteration 10300: Loss = -11645.444350623575
Iteration 10400: Loss = -11639.250963808056
Iteration 10500: Loss = -11639.255012852502
1
Iteration 10600: Loss = -11639.237062103051
Iteration 10700: Loss = -11639.231494839736
Iteration 10800: Loss = -11639.29154158719
1
Iteration 10900: Loss = -11639.236799563565
2
Iteration 11000: Loss = -11639.226724452177
Iteration 11100: Loss = -11639.226538189148
Iteration 11200: Loss = -11639.22861470386
1
Iteration 11300: Loss = -11639.225894936253
Iteration 11400: Loss = -11639.225501102062
Iteration 11500: Loss = -11639.224634598018
Iteration 11600: Loss = -11639.224377794491
Iteration 11700: Loss = -11639.225614596706
1
Iteration 11800: Loss = -11639.229467896408
2
Iteration 11900: Loss = -11639.225730955759
3
Iteration 12000: Loss = -11639.239627880641
4
Iteration 12100: Loss = -11639.237878451851
5
Iteration 12200: Loss = -11639.327251619407
6
Iteration 12300: Loss = -11639.22370768681
Iteration 12400: Loss = -11639.22303023799
Iteration 12500: Loss = -11639.229763908375
1
Iteration 12600: Loss = -11639.337039750386
2
Iteration 12700: Loss = -11639.235390398702
3
Iteration 12800: Loss = -11639.222527727501
Iteration 12900: Loss = -11639.228769168729
1
Iteration 13000: Loss = -11639.22238787025
Iteration 13100: Loss = -11639.222432452383
1
Iteration 13200: Loss = -11639.222328658236
Iteration 13300: Loss = -11639.224238775892
1
Iteration 13400: Loss = -11639.230229574387
2
Iteration 13500: Loss = -11639.23536962389
3
Iteration 13600: Loss = -11639.223393689299
4
Iteration 13700: Loss = -11639.225332612014
5
Iteration 13800: Loss = -11639.226885728092
6
Iteration 13900: Loss = -11639.228691430411
7
Iteration 14000: Loss = -11639.227042148364
8
Iteration 14100: Loss = -11639.229004634326
9
Iteration 14200: Loss = -11639.223117703032
10
Stopping early at iteration 14200 due to no improvement.
tensor([[  7.0700,  -8.8457],
        [  7.7585,  -9.1448],
        [ -8.6357,   6.3233],
        [ -8.0352,   6.6357],
        [ -8.4826,   6.1212],
        [  3.6277,  -5.2576],
        [  6.7150,  -8.3357],
        [ -4.8433,   3.4410],
        [  6.8492,  -8.5538],
        [ -9.1973,   6.5944],
        [ -6.7402,   5.3215],
        [ -7.3970,   5.5580],
        [ -4.6559,   3.2692],
        [  8.6734, -10.2281],
        [ -4.2192,   1.8515],
        [ -9.4805,   7.8372],
        [ -7.2279,   5.4233],
        [  7.6913,  -9.0813],
        [  7.8674,  -9.3257],
        [  7.5685,  -8.9568],
        [  4.6146,  -6.6672],
        [  7.9647, -10.0185],
        [ -2.0040,   0.3646],
        [  8.4594,  -9.8466],
        [  6.7990,  -8.2117],
        [ -4.5518,   3.1573],
        [ -9.1187,   7.2286],
        [ -5.9940,   3.7589],
        [ -9.8724,   7.2496],
        [ -8.5140,   5.5647],
        [ -4.1218,   2.6889],
        [  7.5594,  -9.5730],
        [ -7.7127,   5.8994],
        [  7.3555,  -8.7603],
        [  7.4490,  -8.8516],
        [ -8.4850,   7.0927],
        [  8.2436,  -9.6629],
        [  7.6172,  -9.8693],
        [ -5.6401,   4.1435],
        [ -8.7536,   5.0551],
        [  8.5599, -10.4950],
        [  8.1494,  -9.5938],
        [  6.2117, -10.8269],
        [ -8.1573,   5.5474],
        [  7.4495,  -9.3274],
        [ -7.7459,   6.2737],
        [  7.6152, -10.0488],
        [ -9.7232,   7.8861],
        [ -8.0715,   6.5558],
        [  3.0763,  -7.6916],
        [ -3.8790,   2.4909],
        [ -7.1534,   4.5388],
        [  8.0032, -10.0103],
        [ -3.2812,   1.8330],
        [ -8.2360,   6.7144],
        [ -7.1838,   5.4881],
        [  8.0764,  -9.4633],
        [  8.1661,  -9.9138],
        [ -6.3271,   4.8111],
        [  6.7619,  -8.1694],
        [  7.8948,  -9.2903],
        [  6.0495,  -8.9777],
        [ -7.7675,   6.3636],
        [  5.1267,  -6.8948],
        [  8.2017,  -9.9445],
        [-10.2658,   7.0804],
        [ -8.2203,   5.1213],
        [  7.2865,  -8.6749],
        [  8.3470,  -9.7540],
        [  6.4069, -11.0024],
        [  8.0325,  -9.8483],
        [ -7.4427,   3.2615],
        [  6.6089, -10.1405],
        [ -9.8304,   6.9042],
        [  7.6029,  -9.1136],
        [  7.0742,  -8.9260],
        [ -7.3825,   5.1005],
        [  7.8639,  -9.3292],
        [  8.5080, -11.5367],
        [  7.3993,  -8.8310],
        [  7.0515,  -9.7678],
        [  7.3673, -11.9825],
        [ -4.9487,   3.3234],
        [ -5.9277,   4.4779],
        [ -9.5173,   7.9829],
        [ -8.1917,   6.2857],
        [  8.0497, -10.5167],
        [  7.4737,  -8.8628],
        [ -6.6502,   5.2567],
        [ -7.7978,   5.5606],
        [  0.2688,  -1.8976],
        [ -4.1649,   2.3546],
        [ -2.8883,   1.0365],
        [  8.1795, -10.2274],
        [  8.0925,  -9.4816],
        [ -9.7712,   7.3584],
        [  6.3635,  -8.0759],
        [ -6.7220,   5.2166],
        [ -6.5873,   4.5684],
        [  7.2020,  -8.7051]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4946, 0.5054],
        [0.4050, 0.5950]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5101, 0.4899], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3777, 0.1006],
         [0.3720, 0.2255]],

        [[0.4738, 0.1025],
         [0.5843, 0.0694]],

        [[0.9951, 0.0920],
         [0.8557, 0.4167]],

        [[0.4306, 0.0916],
         [0.9061, 0.9725]],

        [[0.3466, 0.0993],
         [0.0748, 0.5768]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 78
Adjusted Rand Index: 0.30755685986793835
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4667944991906003
Average Adjusted Rand Index: 0.8535105883033169
Iteration 0: Loss = -24733.490421548147
Iteration 10: Loss = -12231.23294842584
Iteration 20: Loss = -12181.959169541951
Iteration 30: Loss = -11496.552784907259
Iteration 40: Loss = -11496.57107840447
1
Iteration 50: Loss = -11496.571072405992
2
Iteration 60: Loss = -11496.571069927375
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7372, 0.2628],
        [0.2435, 0.7565]], dtype=torch.float64)
alpha: tensor([0.4919, 0.5081])
beta: tensor([[[0.3799, 0.1019],
         [0.1292, 0.2024]],

        [[0.5522, 0.1029],
         [0.6265, 0.6542]],

        [[0.5622, 0.0920],
         [0.8805, 0.4040]],

        [[0.1624, 0.0962],
         [0.2435, 0.1518]],

        [[0.0585, 0.0995],
         [0.2803, 0.9370]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961254157898
Average Adjusted Rand Index: 0.9761596921619656
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24668.28468865839
Iteration 100: Loss = -12228.650483989377
Iteration 200: Loss = -12214.05939785878
Iteration 300: Loss = -11990.0552654347
Iteration 400: Loss = -11821.279606606555
Iteration 500: Loss = -11767.070411306739
Iteration 600: Loss = -11713.195386373662
Iteration 700: Loss = -11667.271818765525
Iteration 800: Loss = -11641.357785187774
Iteration 900: Loss = -11640.501060708977
Iteration 1000: Loss = -11613.069692677065
Iteration 1100: Loss = -11612.89368305706
Iteration 1200: Loss = -11602.908357927006
Iteration 1300: Loss = -11588.34389510425
Iteration 1400: Loss = -11588.183093467043
Iteration 1500: Loss = -11588.13285476226
Iteration 1600: Loss = -11588.095101528335
Iteration 1700: Loss = -11588.065149298998
Iteration 1800: Loss = -11588.040580087229
Iteration 1900: Loss = -11588.021220157587
Iteration 2000: Loss = -11588.001540163974
Iteration 2100: Loss = -11587.978975002206
Iteration 2200: Loss = -11585.786451790136
Iteration 2300: Loss = -11585.753441191198
Iteration 2400: Loss = -11585.668291588048
Iteration 2500: Loss = -11585.659627090128
Iteration 2600: Loss = -11585.651984243108
Iteration 2700: Loss = -11585.646190674966
Iteration 2800: Loss = -11585.639116242506
Iteration 2900: Loss = -11585.633413428566
Iteration 3000: Loss = -11573.987987550614
Iteration 3100: Loss = -11562.523523598536
Iteration 3200: Loss = -11562.507795292706
Iteration 3300: Loss = -11562.502397623255
Iteration 3400: Loss = -11562.497307426605
Iteration 3500: Loss = -11562.493420861201
Iteration 3600: Loss = -11562.49006446799
Iteration 3700: Loss = -11562.48715588277
Iteration 3800: Loss = -11562.48447685849
Iteration 3900: Loss = -11562.482101269863
Iteration 4000: Loss = -11562.48000676479
Iteration 4100: Loss = -11562.477921819496
Iteration 4200: Loss = -11562.476005058732
Iteration 4300: Loss = -11562.475398518845
Iteration 4400: Loss = -11558.917705519962
Iteration 4500: Loss = -11558.898340230236
Iteration 4600: Loss = -11558.896316832654
Iteration 4700: Loss = -11558.89468827589
Iteration 4800: Loss = -11558.893364770896
Iteration 4900: Loss = -11558.892148853536
Iteration 5000: Loss = -11558.891050569971
Iteration 5100: Loss = -11558.889841889599
Iteration 5200: Loss = -11558.888098010773
Iteration 5300: Loss = -11541.537996349596
Iteration 5400: Loss = -11541.530884724009
Iteration 5500: Loss = -11541.529809884396
Iteration 5600: Loss = -11541.529793773458
Iteration 5700: Loss = -11541.528256819496
Iteration 5800: Loss = -11541.527537774824
Iteration 5900: Loss = -11541.526933777954
Iteration 6000: Loss = -11541.533629497324
1
Iteration 6100: Loss = -11541.52583772212
Iteration 6200: Loss = -11541.525311199697
Iteration 6300: Loss = -11541.539935246048
1
Iteration 6400: Loss = -11541.524636932487
Iteration 6500: Loss = -11541.523970770571
Iteration 6600: Loss = -11541.523705337208
Iteration 6700: Loss = -11541.52344121288
Iteration 6800: Loss = -11541.522895752674
Iteration 6900: Loss = -11541.523243616888
1
Iteration 7000: Loss = -11541.522263968753
Iteration 7100: Loss = -11541.527903156917
1
Iteration 7200: Loss = -11541.521517602798
Iteration 7300: Loss = -11541.521145491564
Iteration 7400: Loss = -11541.520732172801
Iteration 7500: Loss = -11541.520591904382
Iteration 7600: Loss = -11541.523577012422
1
Iteration 7700: Loss = -11541.533792812854
2
Iteration 7800: Loss = -11541.521022670167
3
Iteration 7900: Loss = -11541.519564648033
Iteration 8000: Loss = -11541.558213839742
1
Iteration 8100: Loss = -11541.524314108841
2
Iteration 8200: Loss = -11541.519134198708
Iteration 8300: Loss = -11541.519652471734
1
Iteration 8400: Loss = -11541.522777878981
2
Iteration 8500: Loss = -11541.52224679131
3
Iteration 8600: Loss = -11540.741111677533
Iteration 8700: Loss = -11540.711030084147
Iteration 8800: Loss = -11540.72934289179
1
Iteration 8900: Loss = -11529.721828346783
Iteration 9000: Loss = -11529.732341852345
1
Iteration 9100: Loss = -11529.712305042549
Iteration 9200: Loss = -11529.72042083745
1
Iteration 9300: Loss = -11529.729514385886
2
Iteration 9400: Loss = -11529.724190337096
3
Iteration 9500: Loss = -11529.708658006215
Iteration 9600: Loss = -11529.71349313432
1
Iteration 9700: Loss = -11529.715789987742
2
Iteration 9800: Loss = -11529.712521145071
3
Iteration 9900: Loss = -11529.707976033289
Iteration 10000: Loss = -11529.710031821154
1
Iteration 10100: Loss = -11529.7076141095
Iteration 10200: Loss = -11529.707992717862
1
Iteration 10300: Loss = -11529.709399840935
2
Iteration 10400: Loss = -11529.709548487454
3
Iteration 10500: Loss = -11529.71829492291
4
Iteration 10600: Loss = -11529.71402020034
5
Iteration 10700: Loss = -11529.738395772114
6
Iteration 10800: Loss = -11529.74264523878
7
Iteration 10900: Loss = -11529.707406960222
Iteration 11000: Loss = -11529.707635651073
1
Iteration 11100: Loss = -11529.726105949061
2
Iteration 11200: Loss = -11529.707367813106
Iteration 11300: Loss = -11529.706808016124
Iteration 11400: Loss = -11529.706777460417
Iteration 11500: Loss = -11529.710105257627
1
Iteration 11600: Loss = -11529.711570489771
2
Iteration 11700: Loss = -11529.706483400467
Iteration 11800: Loss = -11529.734069650036
1
Iteration 11900: Loss = -11507.844196146081
Iteration 12000: Loss = -11507.845833551639
1
Iteration 12100: Loss = -11507.887410413765
2
Iteration 12200: Loss = -11507.841992789092
Iteration 12300: Loss = -11507.843016319559
1
Iteration 12400: Loss = -11507.842420453251
2
Iteration 12500: Loss = -11507.841935099235
Iteration 12600: Loss = -11507.865546598312
1
Iteration 12700: Loss = -11507.85289940731
2
Iteration 12800: Loss = -11507.844652887725
3
Iteration 12900: Loss = -11507.848304555164
4
Iteration 13000: Loss = -11507.846070577172
5
Iteration 13100: Loss = -11507.844986607224
6
Iteration 13200: Loss = -11507.844888144242
7
Iteration 13300: Loss = -11507.845980914066
8
Iteration 13400: Loss = -11507.841667689308
Iteration 13500: Loss = -11507.949939478316
1
Iteration 13600: Loss = -11507.844863104856
2
Iteration 13700: Loss = -11507.878472593424
3
Iteration 13800: Loss = -11507.850387364717
4
Iteration 13900: Loss = -11507.885312123932
5
Iteration 14000: Loss = -11507.889438946786
6
Iteration 14100: Loss = -11507.841654200043
Iteration 14200: Loss = -11507.84608730155
1
Iteration 14300: Loss = -11507.841863564088
2
Iteration 14400: Loss = -11507.842650092553
3
Iteration 14500: Loss = -11507.85374855112
4
Iteration 14600: Loss = -11507.934503622802
5
Iteration 14700: Loss = -11507.859135363711
6
Iteration 14800: Loss = -11496.393517987874
Iteration 14900: Loss = -11496.376832468277
Iteration 15000: Loss = -11496.479982797713
1
Iteration 15100: Loss = -11496.37012105963
Iteration 15200: Loss = -11494.430282889096
Iteration 15300: Loss = -11494.43079057211
1
Iteration 15400: Loss = -11494.431905803744
2
Iteration 15500: Loss = -11494.431945493605
3
Iteration 15600: Loss = -11494.436045145712
4
Iteration 15700: Loss = -11494.430973729479
5
Iteration 15800: Loss = -11494.433122798308
6
Iteration 15900: Loss = -11494.430781676421
7
Iteration 16000: Loss = -11494.431051006359
8
Iteration 16100: Loss = -11494.439066897932
9
Iteration 16200: Loss = -11494.432413405664
10
Stopping early at iteration 16200 due to no improvement.
tensor([[  6.9412,  -8.4855],
        [  7.2932,  -8.7107],
        [ -8.7020,   7.0690],
        [ -8.8699,   7.4821],
        [ -8.9237,   7.5319],
        [  3.0777,  -4.5600],
        [  4.5848,  -6.1903],
        [ -5.7656,   4.3126],
        [  6.6891, -10.2197],
        [ -8.8602,   7.3666],
        [ -7.7108,   5.6539],
        [ -7.5127,   5.9915],
        [ -4.6827,   3.2953],
        [  8.4447, -13.0600],
        [ -4.8464,   2.4735],
        [ -4.3144,   2.8726],
        [ -6.8544,   5.2631],
        [  5.6074,  -7.0563],
        [  7.1290,  -9.2303],
        [  3.3412,  -4.7459],
        [  3.5909,  -5.6328],
        [  0.3406,  -1.8065],
        [ -2.8725,   1.0973],
        [  5.3241,  -7.0143],
        [  7.8373, -10.9131],
        [ -6.1654,   1.7361],
        [ -8.7629,   7.3761],
        [ -7.8940,   3.9577],
        [ -9.3193,   7.9328],
        [ -9.6606,   6.6844],
        [ -4.9974,   3.5677],
        [  4.1186,  -6.3123],
        [ -8.1910,   5.0003],
        [  7.4760,  -9.2897],
        [  5.7354,  -8.5514],
        [-10.8294,   7.1568],
        [  7.8301,  -9.5097],
        [  5.4989,  -7.0628],
        [ -9.7421,   8.2107],
        [ -8.5138,   6.8031],
        [  8.4254,  -9.8358],
        [  8.8648, -10.2788],
        [  7.4545, -10.1614],
        [ -8.7903,   6.8357],
        [  6.5082,  -8.3435],
        [ -8.0028,   6.6165],
        [  7.8872, -10.3391],
        [ -8.0027,   6.6153],
        [ -8.5641,   7.1671],
        [  2.8919,  -5.5773],
        [ -5.1100,   3.5479],
        [ -8.4717,   4.9190],
        [  7.2281,  -8.6144],
        [ -4.0767,   2.4890],
        [ -8.2267,   6.8376],
        [ -8.3122,   6.1665],
        [  6.7862,  -8.1742],
        [  5.5501,  -7.0979],
        [ -8.3503,   4.2487],
        [  5.8144,  -7.6037],
        [  3.6949,  -5.1198],
        [  6.9066,  -8.3184],
        [ -8.8905,   7.5022],
        [  4.2726,  -6.0187],
        [  8.5467,  -9.9801],
        [ -9.8550,   7.5252],
        [ -8.1781,   6.7870],
        [  7.0032,  -9.5092],
        [  4.7218,  -6.9505],
        [  7.9738, -10.7235],
        [  8.1066,  -9.5447],
        [ -8.2432,   4.4569],
        [  7.0167,  -8.6175],
        [ -8.2511,   6.7550],
        [  7.3222,  -8.7852],
        [  7.1335,  -8.7626],
        [ -6.8825,   5.2768],
        [  7.2159,  -9.6263],
        [  8.2745,  -9.6713],
        [  0.6742,  -2.3438],
        [  2.6895,  -4.6668],
        [  8.3950, -13.0102],
        [ -5.9461,   4.5202],
        [ -7.1281,   4.5934],
        [ -4.4968,   2.9602],
        [ -7.9965,   6.5321],
        [  8.2162, -12.2011],
        [  7.6103,  -9.2378],
        [ -7.4420,   6.0516],
        [ -8.5373,   6.3243],
        [  3.0331,  -4.9896],
        [ -5.0125,   3.3481],
        [ -4.1713,   1.4998],
        [  7.3780,  -8.8989],
        [  5.8645,  -7.5367],
        [ -9.1922,   7.6962],
        [  6.8142,  -8.2245],
        [ -7.5225,   5.9307],
        [ -7.2228,   5.3991],
        [  7.6606,  -9.1169]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7399, 0.2601],
        [0.2354, 0.7646]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5086, 0.4914], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3878, 0.1006],
         [0.1292, 0.2064]],

        [[0.5522, 0.1029],
         [0.6265, 0.6542]],

        [[0.5622, 0.0920],
         [0.8805, 0.4040]],

        [[0.1624, 0.0966],
         [0.2435, 0.1518]],

        [[0.0585, 0.0993],
         [0.2803, 0.9370]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.9919992163297293
11502.48610111485
new:  [0.4613270204225441, 0.9840320230862317, 0.4667944991906003, 0.9919999775871758] [0.8331886200562838, 0.9841599999999999, 0.8535105883033169, 0.9919992163297293] [11637.037519304113, 11507.362201064238, 11639.223117703032, 11494.432413405664]
prior:  [0.9760961254157898, 0.9760961254157898, 0.9760961254157898, 0.9760961254157898] [0.9761596921619656, 0.9761596921619656, 0.9761596921619656, 0.9761596921619656] [11496.571067788125, 11496.571066521194, 11496.57106840289, 11496.571069927375]
-----------------------------------------------------------------------------------------
This iteration is 3
True Objective function: Loss = -11703.824783587863
Iteration 0: Loss = -22470.431451011067
Iteration 10: Loss = -11908.770971986674
Iteration 20: Loss = -11905.371801331366
Iteration 30: Loss = -11905.343383955933
Iteration 40: Loss = -11905.343346061527
Iteration 50: Loss = -11905.343352331267
1
Iteration 60: Loss = -11905.343351930283
2
Iteration 70: Loss = -11905.343351912565
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.5597, 0.4403],
        [0.4737, 0.5263]], dtype=torch.float64)
alpha: tensor([0.5174, 0.4826])
beta: tensor([[[0.3643, 0.1071],
         [0.1821, 0.2571]],

        [[0.5034, 0.0964],
         [0.6260, 0.0895]],

        [[0.6571, 0.0874],
         [0.1178, 0.7631]],

        [[0.1925, 0.0842],
         [0.4968, 0.7575]],

        [[0.8212, 0.0887],
         [0.6549, 0.4188]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8079912862954653
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.35871776452354065
Average Adjusted Rand Index: 0.9220824474712316
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22433.674525762348
Iteration 100: Loss = -12680.991353249763
Iteration 200: Loss = -12677.963231978823
Iteration 300: Loss = -12627.748164544248
Iteration 400: Loss = -12558.494913102017
Iteration 500: Loss = -12482.03400736198
Iteration 600: Loss = -11971.97734984332
Iteration 700: Loss = -11848.101118669498
Iteration 800: Loss = -11742.885235881946
Iteration 900: Loss = -11731.655433467402
Iteration 1000: Loss = -11710.369131962369
Iteration 1100: Loss = -11709.334532569741
Iteration 1200: Loss = -11709.184736979294
Iteration 1300: Loss = -11700.97158037381
Iteration 1400: Loss = -11700.906228453243
Iteration 1500: Loss = -11700.86072197291
Iteration 1600: Loss = -11700.816953083602
Iteration 1700: Loss = -11697.256544669677
Iteration 1800: Loss = -11697.215117369557
Iteration 1900: Loss = -11697.190371410861
Iteration 2000: Loss = -11697.165062956465
Iteration 2100: Loss = -11697.149678887952
Iteration 2200: Loss = -11697.13882682738
Iteration 2300: Loss = -11697.128534196749
Iteration 2400: Loss = -11697.119007064914
Iteration 2500: Loss = -11697.111555879745
Iteration 2600: Loss = -11697.106226850083
Iteration 2700: Loss = -11697.09931900823
Iteration 2800: Loss = -11697.094171343928
Iteration 2900: Loss = -11697.089575683802
Iteration 3000: Loss = -11697.085413589792
Iteration 3100: Loss = -11697.081704821456
Iteration 3200: Loss = -11697.078247905758
Iteration 3300: Loss = -11697.075149848903
Iteration 3400: Loss = -11697.07232485007
Iteration 3500: Loss = -11697.069805584177
Iteration 3600: Loss = -11697.067500129473
Iteration 3700: Loss = -11697.070189610586
1
Iteration 3800: Loss = -11697.135061056248
2
Iteration 3900: Loss = -11697.061232852986
Iteration 4000: Loss = -11697.059587804093
Iteration 4100: Loss = -11697.060832070014
1
Iteration 4200: Loss = -11697.09507602899
2
Iteration 4300: Loss = -11697.055235012887
Iteration 4400: Loss = -11697.053157735456
Iteration 4500: Loss = -11697.05106609085
Iteration 4600: Loss = -11697.04920249569
Iteration 4700: Loss = -11697.04465010641
Iteration 4800: Loss = -11697.042217870807
Iteration 4900: Loss = -11697.065730302693
1
Iteration 5000: Loss = -11697.040037622239
Iteration 5100: Loss = -11697.082725022547
1
Iteration 5200: Loss = -11697.038405121642
Iteration 5300: Loss = -11697.045178232649
1
Iteration 5400: Loss = -11697.037043859204
Iteration 5500: Loss = -11697.038254334233
1
Iteration 5600: Loss = -11697.035768813957
Iteration 5700: Loss = -11697.11132999772
1
Iteration 5800: Loss = -11697.034712787847
Iteration 5900: Loss = -11697.034400341285
Iteration 6000: Loss = -11697.03374883671
Iteration 6100: Loss = -11697.034028242964
1
Iteration 6200: Loss = -11697.032929324583
Iteration 6300: Loss = -11697.12105050029
1
Iteration 6400: Loss = -11697.032183672824
Iteration 6500: Loss = -11697.216073001793
1
Iteration 6600: Loss = -11697.031569270566
Iteration 6700: Loss = -11697.03767835257
1
Iteration 6800: Loss = -11697.03102621
Iteration 6900: Loss = -11697.048495767463
1
Iteration 7000: Loss = -11697.030472028393
Iteration 7100: Loss = -11697.032322526773
1
Iteration 7200: Loss = -11697.03022642591
Iteration 7300: Loss = -11697.036561985773
1
Iteration 7400: Loss = -11697.029597819503
Iteration 7500: Loss = -11697.02942496635
Iteration 7600: Loss = -11697.029288482712
Iteration 7700: Loss = -11697.03372960568
1
Iteration 7800: Loss = -11697.028881590359
Iteration 7900: Loss = -11697.02873500975
Iteration 8000: Loss = -11697.030143145785
1
Iteration 8100: Loss = -11697.104225964975
2
Iteration 8200: Loss = -11697.028348816184
Iteration 8300: Loss = -11697.034338089068
1
Iteration 8400: Loss = -11697.02809735741
Iteration 8500: Loss = -11697.028118117927
1
Iteration 8600: Loss = -11697.02789304462
Iteration 8700: Loss = -11697.028116596846
1
Iteration 8800: Loss = -11697.02786387653
Iteration 8900: Loss = -11697.027745662703
Iteration 9000: Loss = -11697.02981253818
1
Iteration 9100: Loss = -11697.049125371654
2
Iteration 9200: Loss = -11697.027522721752
Iteration 9300: Loss = -11697.047980226007
1
Iteration 9400: Loss = -11697.028231645088
2
Iteration 9500: Loss = -11697.030654136423
3
Iteration 9600: Loss = -11697.3431642031
4
Iteration 9700: Loss = -11697.027726833601
5
Iteration 9800: Loss = -11697.029494516659
6
Iteration 9900: Loss = -11697.106976912108
7
Iteration 10000: Loss = -11697.05989775062
8
Iteration 10100: Loss = -11697.02820166837
9
Iteration 10200: Loss = -11697.026927413543
Iteration 10300: Loss = -11697.029829518118
1
Iteration 10400: Loss = -11697.07324938187
2
Iteration 10500: Loss = -11697.026879881107
Iteration 10600: Loss = -11697.061188960293
1
Iteration 10700: Loss = -11697.034585124426
2
Iteration 10800: Loss = -11697.030916461023
3
Iteration 10900: Loss = -11697.030370667107
4
Iteration 11000: Loss = -11697.026639154326
Iteration 11100: Loss = -11697.028943824584
1
Iteration 11200: Loss = -11697.02692215684
2
Iteration 11300: Loss = -11697.035037082102
3
Iteration 11400: Loss = -11697.027113892858
4
Iteration 11500: Loss = -11697.02677169042
5
Iteration 11600: Loss = -11697.057653520113
6
Iteration 11700: Loss = -11697.20777688286
7
Iteration 11800: Loss = -11697.026744825987
8
Iteration 11900: Loss = -11697.041311208997
9
Iteration 12000: Loss = -11697.04287679855
10
Stopping early at iteration 12000 due to no improvement.
tensor([[-11.3328,   6.7175],
        [ -4.4249,  -0.1903],
        [-10.9124,   6.2972],
        [  5.0316,  -9.6468],
        [ -8.8324,   4.2172],
        [-11.6939,   7.0786],
        [-10.4182,   5.8030],
        [-11.0481,   6.4328],
        [ -8.2084,   3.5932],
        [ -1.7698,  -2.8454],
        [  4.4748,  -9.0900],
        [  6.1484, -10.7636],
        [-10.0105,   5.3953],
        [ -7.1485,   2.5332],
        [ -8.3596,   3.7444],
        [  4.7627,  -9.3779],
        [-10.0233,   5.4080],
        [ -4.7358,   0.1206],
        [ -9.6086,   4.9934],
        [ -9.8897,   5.2745],
        [  4.6431,  -9.2583],
        [-10.3716,   5.7564],
        [  5.7303, -10.3455],
        [  4.5191,  -9.1344],
        [ -3.6838,  -0.9314],
        [  4.3527,  -8.9679],
        [ -7.8520,   3.2368],
        [  2.5017,  -7.1169],
        [-11.0598,   6.4446],
        [-11.0357,   6.4205],
        [  4.2964,  -8.9116],
        [  1.3360,  -5.9512],
        [  0.0433,  -4.6585],
        [ -6.6029,   1.9877],
        [ -9.9580,   5.3428],
        [ -8.2277,   3.6125],
        [-10.6140,   5.9988],
        [  3.3715,  -7.9867],
        [  5.1116,  -9.7268],
        [  3.6303,  -8.2455],
        [ -7.5119,   2.8967],
        [  5.5088, -10.1240],
        [  3.9844,  -8.5997],
        [-10.5820,   5.9668],
        [ -9.9363,   5.3211],
        [ -0.2869,  -4.3284],
        [  0.6383,  -5.2536],
        [ -9.9716,   5.3564],
        [ -7.8820,   3.2668],
        [  4.4298,  -9.0451],
        [  6.1060, -10.7213],
        [  4.3698,  -8.9850],
        [  6.1217, -10.7369],
        [ -9.8379,   5.2227],
        [  4.5925,  -9.2078],
        [ -6.0513,   1.4361],
        [-10.0799,   5.4647],
        [ -9.0005,   4.3853],
        [ -6.4879,   1.8727],
        [  4.5844,  -9.1996],
        [  2.7815,  -7.3967],
        [  4.2372,  -8.8524],
        [-10.9248,   6.3096],
        [ -8.6664,   4.0512],
        [  0.6805,  -5.2957],
        [  4.9054,  -9.5206],
        [  2.2028,  -6.8180],
        [-11.1126,   6.4974],
        [  5.5671, -10.1823],
        [  5.6882, -10.3034],
        [  6.3773, -10.9925],
        [ -4.9257,   0.3105],
        [-10.6849,   6.0696],
        [  6.2864, -10.9016],
        [-10.0918,   5.4766],
        [  4.3904,  -9.0056],
        [ -9.8145,   5.1993],
        [-11.0730,   6.4578],
        [  5.2327,  -9.8480],
        [-11.7481,   7.1329],
        [  3.1217,  -7.7369],
        [-11.6159,   7.0006],
        [ -8.7417,   4.1265],
        [-11.3170,   6.7018],
        [-10.2250,   5.6098],
        [  5.6349, -10.2502],
        [  5.9084, -10.5237],
        [-10.3566,   5.7413],
        [  3.3778,  -7.9931],
        [-11.2214,   6.6062],
        [  4.6048,  -9.2200],
        [  5.1094,  -9.7246],
        [  4.4983,  -9.1136],
        [  5.0301,  -9.6454],
        [  2.4800,  -7.0952],
        [  2.3348,  -6.9500],
        [  4.9689,  -9.5841],
        [ -4.6886,   0.0734],
        [  2.7120,  -7.3273],
        [ -9.4994,   4.8842]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6791, 0.3209],
        [0.2361, 0.7639]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4867, 0.5133], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2062, 0.1078],
         [0.1821, 0.3952]],

        [[0.5034, 0.0986],
         [0.6260, 0.0895]],

        [[0.6571, 0.0887],
         [0.1178, 0.7631]],

        [[0.1925, 0.0841],
         [0.4968, 0.7575]],

        [[0.8212, 0.0898],
         [0.6549, 0.4188]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840308245660749
Average Adjusted Rand Index: 0.983998338570873
Iteration 0: Loss = -25021.371829031545
Iteration 10: Loss = -11694.041190706499
Iteration 20: Loss = -11694.041241508225
1
Iteration 30: Loss = -11694.041241508368
2
Iteration 40: Loss = -11694.041241508368
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.6860, 0.3140],
        [0.2344, 0.7656]], dtype=torch.float64)
alpha: tensor([0.4472, 0.5528])
beta: tensor([[[0.2004, 0.1084],
         [0.9829, 0.3910]],

        [[0.1673, 0.0988],
         [0.0973, 0.2738]],

        [[0.4242, 0.0901],
         [0.8079, 0.5182]],

        [[0.2053, 0.0842],
         [0.4916, 0.3715]],

        [[0.1623, 0.0901],
         [0.5340, 0.0877]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919994334721989
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25021.04324319345
Iteration 100: Loss = -11978.777935353939
Iteration 200: Loss = -11868.76088201938
Iteration 300: Loss = -11864.302825274664
Iteration 400: Loss = -11861.751764290253
Iteration 500: Loss = -11859.43874669459
Iteration 600: Loss = -11858.409869042842
Iteration 700: Loss = -11843.648549209245
Iteration 800: Loss = -11842.310705610613
Iteration 900: Loss = -11824.65158130011
Iteration 1000: Loss = -11799.758413334506
Iteration 1100: Loss = -11773.049740895487
Iteration 1200: Loss = -11766.449569451746
Iteration 1300: Loss = -11758.416261103992
Iteration 1400: Loss = -11734.944929086221
Iteration 1500: Loss = -11734.041117073917
Iteration 1600: Loss = -11721.586714341664
Iteration 1700: Loss = -11721.382440942141
Iteration 1800: Loss = -11721.008087815262
Iteration 1900: Loss = -11720.950855223295
Iteration 2000: Loss = -11710.339304581647
Iteration 2100: Loss = -11710.227419349378
Iteration 2200: Loss = -11710.199584682603
Iteration 2300: Loss = -11710.177074834395
Iteration 2400: Loss = -11710.1575488493
Iteration 2500: Loss = -11710.138191029371
Iteration 2600: Loss = -11710.078258773894
Iteration 2700: Loss = -11707.497099837812
Iteration 2800: Loss = -11707.452434776407
Iteration 2900: Loss = -11707.408319875669
Iteration 3000: Loss = -11707.397109168305
Iteration 3100: Loss = -11707.38428773312
Iteration 3200: Loss = -11707.341531075714
Iteration 3300: Loss = -11692.528605597812
Iteration 3400: Loss = -11692.519410561414
Iteration 3500: Loss = -11692.513081224004
Iteration 3600: Loss = -11692.507740554665
Iteration 3700: Loss = -11692.503634663626
Iteration 3800: Loss = -11692.498813324028
Iteration 3900: Loss = -11692.495033986548
Iteration 4000: Loss = -11692.491566794979
Iteration 4100: Loss = -11692.488449372357
Iteration 4200: Loss = -11692.485561335267
Iteration 4300: Loss = -11692.482947486707
Iteration 4400: Loss = -11692.480503403316
Iteration 4500: Loss = -11692.494358768861
1
Iteration 4600: Loss = -11692.476235882647
Iteration 4700: Loss = -11692.474319103712
Iteration 4800: Loss = -11692.472511463127
Iteration 4900: Loss = -11692.47625757345
1
Iteration 5000: Loss = -11692.469346275568
Iteration 5100: Loss = -11692.46792911959
Iteration 5200: Loss = -11692.467010864768
Iteration 5300: Loss = -11692.466150937891
Iteration 5400: Loss = -11692.464288456476
Iteration 5500: Loss = -11692.463230579235
Iteration 5600: Loss = -11692.462247773728
Iteration 5700: Loss = -11692.461260893877
Iteration 5800: Loss = -11692.460363281089
Iteration 5900: Loss = -11692.45957192748
Iteration 6000: Loss = -11692.472734109466
1
Iteration 6100: Loss = -11692.458077642355
Iteration 6200: Loss = -11692.457374414287
Iteration 6300: Loss = -11692.465644145446
1
Iteration 6400: Loss = -11692.45613688838
Iteration 6500: Loss = -11692.455583323004
Iteration 6600: Loss = -11692.455041347352
Iteration 6700: Loss = -11692.454860611533
Iteration 6800: Loss = -11692.46662758066
1
Iteration 6900: Loss = -11692.453650776679
Iteration 7000: Loss = -11692.483572696925
1
Iteration 7100: Loss = -11692.45356508311
Iteration 7200: Loss = -11692.45284890025
Iteration 7300: Loss = -11692.452957990969
1
Iteration 7400: Loss = -11692.453861522014
2
Iteration 7500: Loss = -11692.452440787794
Iteration 7600: Loss = -11692.462609778999
1
Iteration 7700: Loss = -11692.45106726786
Iteration 7800: Loss = -11692.453204638477
1
Iteration 7900: Loss = -11692.45195735424
2
Iteration 8000: Loss = -11692.512827966391
3
Iteration 8100: Loss = -11692.471825792958
4
Iteration 8200: Loss = -11692.450234887414
Iteration 8300: Loss = -11692.450497036181
1
Iteration 8400: Loss = -11692.449675925373
Iteration 8500: Loss = -11692.453342318691
1
Iteration 8600: Loss = -11692.449163908226
Iteration 8700: Loss = -11692.450259357905
1
Iteration 8800: Loss = -11692.449313021985
2
Iteration 8900: Loss = -11692.448763914974
Iteration 9000: Loss = -11692.449901381176
1
Iteration 9100: Loss = -11692.448703052969
Iteration 9200: Loss = -11692.449126066218
1
Iteration 9300: Loss = -11692.448299994465
Iteration 9400: Loss = -11692.448580940263
1
Iteration 9500: Loss = -11692.448131624384
Iteration 9600: Loss = -11692.452701973512
1
Iteration 9700: Loss = -11692.491154699088
2
Iteration 9800: Loss = -11692.4489529181
3
Iteration 9900: Loss = -11692.447776857443
Iteration 10000: Loss = -11692.44839589668
1
Iteration 10100: Loss = -11692.588855680528
2
Iteration 10200: Loss = -11692.44739401201
Iteration 10300: Loss = -11692.447927204577
1
Iteration 10400: Loss = -11692.452122702103
2
Iteration 10500: Loss = -11692.464396348956
3
Iteration 10600: Loss = -11692.466765993853
4
Iteration 10700: Loss = -11692.454487890996
5
Iteration 10800: Loss = -11692.504902003879
6
Iteration 10900: Loss = -11692.446979865681
Iteration 11000: Loss = -11692.458709529526
1
Iteration 11100: Loss = -11692.4512983169
2
Iteration 11200: Loss = -11692.447008022235
3
Iteration 11300: Loss = -11692.446873416358
Iteration 11400: Loss = -11692.462863785064
1
Iteration 11500: Loss = -11692.451759219288
2
Iteration 11600: Loss = -11692.447099010102
3
Iteration 11700: Loss = -11692.448813268044
4
Iteration 11800: Loss = -11692.449619553692
5
Iteration 11900: Loss = -11692.447334556322
6
Iteration 12000: Loss = -11692.447328622005
7
Iteration 12100: Loss = -11692.474256374097
8
Iteration 12200: Loss = -11692.446952579176
9
Iteration 12300: Loss = -11692.44839550073
10
Stopping early at iteration 12300 due to no improvement.
tensor([[ -8.2103,   6.7917],
        [ -2.8842,   1.2849],
        [ -7.6439,   6.1022],
        [  6.0201,  -7.6544],
        [ -6.3660,   3.5605],
        [ -9.1082,   7.1626],
        [ -7.8874,   5.8637],
        [ -8.5456,   6.7026],
        [ -6.0744,   4.6195],
        [ -0.9363,  -2.0912],
        [  6.1307,  -7.7821],
        [  5.7731,  -8.2191],
        [ -9.0300,   6.3698],
        [ -5.5907,   4.0880],
        [ -6.7895,   5.1214],
        [  6.0539,  -7.4402],
        [ -7.5963,   6.1984],
        [ -3.1162,   1.6351],
        [ -7.5153,   4.8913],
        [ -7.5916,   6.0489],
        [  6.2534,  -7.7204],
        [ -9.4020,   6.0601],
        [  6.3216,  -8.0665],
        [  5.1967,  -6.8259],
        [ -2.3664,   0.3302],
        [  5.5203,  -7.1626],
        [ -6.1549,   4.7645],
        [  4.0125,  -5.7359],
        [ -8.5343,   6.4591],
        [ -7.8170,   6.3714],
        [  5.5646,  -7.4532],
        [  2.9810,  -4.4510],
        [  1.6987,  -3.0983],
        [ -4.9881,   3.5638],
        [ -7.7344,   6.2695],
        [ -7.5913,   4.1372],
        [ -8.1786,   6.5882],
        [  4.4825,  -6.7821],
        [  6.6914,  -8.0801],
        [  5.1179,  -6.5421],
        [ -5.8935,   4.5071],
        [  6.1919,  -7.6341],
        [  5.4768,  -7.1694],
        [ -7.7274,   6.2992],
        [ -7.4018,   5.9093],
        [  0.7242,  -3.3257],
        [  2.0846,  -3.8477],
        [ -8.2433,   6.4950],
        [ -6.2558,   4.8685],
        [  5.8551,  -7.2561],
        [  6.7556,  -8.4215],
        [  5.4582,  -8.3933],
        [  4.7894,  -7.4955],
        [ -8.0514,   6.6493],
        [  5.7378,  -7.3553],
        [ -4.5152,   2.8903],
        [ -9.3682,   6.2755],
        [ -6.1504,   3.2030],
        [ -5.2331,   3.0685],
        [  6.3381,  -8.1197],
        [  3.7476,  -6.5211],
        [  5.5976,  -7.0542],
        [ -8.4226,   6.7346],
        [ -7.1444,   5.4545],
        [  1.7877,  -4.2830],
        [  5.5150,  -8.1562],
        [  3.6241,  -5.5757],
        [ -8.2926,   6.4448],
        [  5.0611,  -8.0236],
        [  6.5502,  -8.7088],
        [  6.3040,  -7.6945],
        [ -3.4167,   1.7572],
        [ -7.9367,   6.2342],
        [  4.4864,  -6.0631],
        [ -7.4749,   6.0681],
        [  5.6993,  -7.2730],
        [ -7.6080,   6.1265],
        [ -8.6544,   7.2673],
        [  5.4174,  -8.0294],
        [ -8.7830,   7.3905],
        [  4.2841,  -6.6811],
        [ -9.3735,   5.7682],
        [ -7.4786,   4.8773],
        [-10.3836,   6.4346],
        [ -7.9903,   6.4021],
        [  6.1910,  -7.6587],
        [  5.0347,  -6.4259],
        [ -7.7541,   6.3662],
        [  4.8093,  -6.7285],
        [ -8.2786,   6.8848],
        [  5.9204,  -9.2583],
        [  6.5845,  -8.4811],
        [  5.5365,  -6.9341],
        [  6.4802,  -8.0414],
        [  4.1469,  -5.5415],
        [  5.7348,  -7.1260],
        [  4.1522,  -7.0014],
        [ -3.0963,   1.5475],
        [  4.3747,  -5.8331],
        [ -7.9232,   5.5018]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6880, 0.3120],
        [0.2345, 0.7655]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4888, 0.5112], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2048, 0.1081],
         [0.9829, 0.3985]],

        [[0.1673, 0.0994],
         [0.0973, 0.2738]],

        [[0.4242, 0.0902],
         [0.8079, 0.5182]],

        [[0.2053, 0.0842],
         [0.4916, 0.3715]],

        [[0.1623, 0.0903],
         [0.5340, 0.0877]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919994334721989
Average Adjusted Rand Index: 0.9919995611635631
Iteration 0: Loss = -16006.818210800844
Iteration 10: Loss = -11694.04123380479
Iteration 20: Loss = -11694.041239871
1
Iteration 30: Loss = -11694.041239871007
2
Iteration 40: Loss = -11694.041239871007
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7656, 0.2344],
        [0.3140, 0.6860]], dtype=torch.float64)
alpha: tensor([0.5528, 0.4472])
beta: tensor([[[0.3910, 0.1084],
         [0.9811, 0.2004]],

        [[0.4675, 0.0988],
         [0.7966, 0.2232]],

        [[0.6708, 0.0901],
         [0.3596, 0.1830]],

        [[0.4759, 0.0842],
         [0.9367, 0.4009]],

        [[0.7153, 0.0901],
         [0.3826, 0.7419]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919994334721989
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16004.599749424435
Iteration 100: Loss = -11847.985186969396
Iteration 200: Loss = -11694.761536172846
Iteration 300: Loss = -11693.546101481068
Iteration 400: Loss = -11693.087712228808
Iteration 500: Loss = -11692.850656483111
Iteration 600: Loss = -11692.709282999378
Iteration 700: Loss = -11692.61724957859
Iteration 800: Loss = -11692.553494755171
Iteration 900: Loss = -11692.507141728544
Iteration 1000: Loss = -11692.50171555332
Iteration 1100: Loss = -11692.445405451412
Iteration 1200: Loss = -11692.424118826408
Iteration 1300: Loss = -11692.406992034843
Iteration 1400: Loss = -11692.393014818734
Iteration 1500: Loss = -11692.381329648206
Iteration 1600: Loss = -11692.371569324598
Iteration 1700: Loss = -11692.36533993834
Iteration 1800: Loss = -11692.356191313631
Iteration 1900: Loss = -11692.350049462213
Iteration 2000: Loss = -11692.353746613862
1
Iteration 2100: Loss = -11692.340207222756
Iteration 2200: Loss = -11692.336083017377
Iteration 2300: Loss = -11692.332498254173
Iteration 2400: Loss = -11692.330926492406
Iteration 2500: Loss = -11692.326446976018
Iteration 2600: Loss = -11692.323877156874
Iteration 2700: Loss = -11692.36711164753
1
Iteration 2800: Loss = -11692.319568125238
Iteration 2900: Loss = -11692.317674547137
Iteration 3000: Loss = -11692.316006309205
Iteration 3100: Loss = -11692.315430456661
Iteration 3200: Loss = -11692.313085717442
Iteration 3300: Loss = -11692.311739636976
Iteration 3400: Loss = -11692.3503893744
1
Iteration 3500: Loss = -11692.309510745465
Iteration 3600: Loss = -11692.322597703484
1
Iteration 3700: Loss = -11692.343852264807
2
Iteration 3800: Loss = -11692.306798028072
Iteration 3900: Loss = -11692.30600268585
Iteration 4000: Loss = -11692.305328143879
Iteration 4100: Loss = -11692.304654419848
Iteration 4200: Loss = -11692.304016120941
Iteration 4300: Loss = -11692.303428617572
Iteration 4400: Loss = -11692.302916187684
Iteration 4500: Loss = -11692.302467220172
Iteration 4600: Loss = -11692.301971449235
Iteration 4700: Loss = -11692.3159618133
1
Iteration 4800: Loss = -11692.301159049719
Iteration 4900: Loss = -11692.302969561386
1
Iteration 5000: Loss = -11692.300411156273
Iteration 5100: Loss = -11692.3013975291
1
Iteration 5200: Loss = -11692.311588656354
2
Iteration 5300: Loss = -11692.305890701364
3
Iteration 5400: Loss = -11692.300966077044
4
Iteration 5500: Loss = -11692.395469938825
5
Iteration 5600: Loss = -11692.298745015949
Iteration 5700: Loss = -11692.29991361295
1
Iteration 5800: Loss = -11692.29833676856
Iteration 5900: Loss = -11692.298238929921
Iteration 6000: Loss = -11692.298534504725
1
Iteration 6100: Loss = -11692.297792176983
Iteration 6200: Loss = -11692.326979915782
1
Iteration 6300: Loss = -11692.298636629905
2
Iteration 6400: Loss = -11692.297340539439
Iteration 6500: Loss = -11692.298248513758
1
Iteration 6600: Loss = -11692.297989306497
2
Iteration 6700: Loss = -11692.296996062692
Iteration 6800: Loss = -11692.29905475567
1
Iteration 6900: Loss = -11692.296757386812
Iteration 7000: Loss = -11692.305454892972
1
Iteration 7100: Loss = -11692.296748512153
Iteration 7200: Loss = -11692.296455236768
Iteration 7300: Loss = -11692.318074263603
1
Iteration 7400: Loss = -11692.328684072898
2
Iteration 7500: Loss = -11692.300491628768
3
Iteration 7600: Loss = -11692.296100404481
Iteration 7700: Loss = -11692.309934036706
1
Iteration 7800: Loss = -11692.286244055053
Iteration 7900: Loss = -11692.286183658345
Iteration 8000: Loss = -11692.286037077323
Iteration 8100: Loss = -11692.295263641146
1
Iteration 8200: Loss = -11692.286038433142
2
Iteration 8300: Loss = -11692.285831828362
Iteration 8400: Loss = -11692.290462797939
1
Iteration 8500: Loss = -11692.285831144836
Iteration 8600: Loss = -11692.324793678697
1
Iteration 8700: Loss = -11692.285703538999
Iteration 8800: Loss = -11692.295753917426
1
Iteration 8900: Loss = -11692.285752852633
2
Iteration 9000: Loss = -11692.288992406608
3
Iteration 9100: Loss = -11692.286862997205
4
Iteration 9200: Loss = -11692.300722273505
5
Iteration 9300: Loss = -11692.285551663534
Iteration 9400: Loss = -11692.285688709779
1
Iteration 9500: Loss = -11692.285963277496
2
Iteration 9600: Loss = -11692.285571313614
3
Iteration 9700: Loss = -11692.288120491985
4
Iteration 9800: Loss = -11692.285510680193
Iteration 9900: Loss = -11692.387697845632
1
Iteration 10000: Loss = -11692.2854247044
Iteration 10100: Loss = -11692.285379447132
Iteration 10200: Loss = -11692.287540121633
1
Iteration 10300: Loss = -11692.288160629525
2
Iteration 10400: Loss = -11692.32939256821
3
Iteration 10500: Loss = -11692.285398951311
4
Iteration 10600: Loss = -11692.28761953536
5
Iteration 10700: Loss = -11692.331346822373
6
Iteration 10800: Loss = -11692.285959536406
7
Iteration 10900: Loss = -11692.291117802934
8
Iteration 11000: Loss = -11692.31785300414
9
Iteration 11100: Loss = -11692.28525157858
Iteration 11200: Loss = -11692.285499940162
1
Iteration 11300: Loss = -11692.293288302606
2
Iteration 11400: Loss = -11692.318487731938
3
Iteration 11500: Loss = -11692.288843258892
4
Iteration 11600: Loss = -11692.285309536055
5
Iteration 11700: Loss = -11692.525717189397
6
Iteration 11800: Loss = -11692.285595668342
7
Iteration 11900: Loss = -11692.329965395182
8
Iteration 12000: Loss = -11692.289668915984
9
Iteration 12100: Loss = -11692.290055439418
10
Stopping early at iteration 12100 due to no improvement.
tensor([[  7.9133,  -9.4655],
        [ -0.2195,  -4.3958],
        [  6.2767,  -8.3582],
        [ -8.3887,   6.8189],
        [  3.8027,  -6.1301],
        [  7.2080,  -9.0150],
        [  5.4358, -10.0511],
        [  6.9184,  -9.9707],
        [  4.6545,  -6.0412],
        [ -1.9497,  -0.8024],
        [ -7.9035,   6.5154],
        [ -8.5074,   5.8048],
        [  6.8883,  -9.2118],
        [  4.1177,  -5.5638],
        [  5.2338,  -6.8975],
        [ -7.9156,   6.4681],
        [  6.1726,  -9.1104],
        [  1.5785,  -3.1867],
        [  5.2543,  -7.1637],
        [  6.1169,  -8.3723],
        [-10.4015,   6.1024],
        [  6.7955,  -9.1593],
        [ -8.1410,   6.4715],
        [ -7.7133,   5.2993],
        [  0.3118,  -2.3908],
        [ -7.6612,   6.2729],
        [  4.7743,  -6.1686],
        [ -6.0953,   3.6275],
        [  6.7140,  -9.4981],
        [  6.8615,  -8.4228],
        [ -7.2858,   5.8614],
        [ -4.4223,   2.9971],
        [ -3.5866,   1.1990],
        [  3.2762,  -5.2826],
        [  6.3526,  -8.0490],
        [  5.1196,  -6.6776],
        [  6.8689,  -8.2639],
        [ -6.4868,   5.0398],
        [ -8.5345,   7.1125],
        [ -6.8466,   5.1221],
        [  2.8980,  -7.5132],
        [ -8.2378,   6.4454],
        [ -7.2190,   5.7284],
        [  5.8385,  -9.6181],
        [  6.1185,  -8.6148],
        [ -3.1437,   0.9041],
        [ -4.0693,   1.8561],
        [  7.2899,  -8.6936],
        [  4.4914,  -6.6478],
        [ -7.6968,   6.1625],
        [ -9.1931,   6.9086],
        [ -8.0560,   5.6566],
        [ -8.3226,   3.9552],
        [  6.7113,  -8.2236],
        [ -8.0235,   6.5682],
        [  2.6793,  -4.7389],
        [  7.1169,  -8.5041],
        [  3.8252,  -5.5293],
        [  3.1577,  -5.1543],
        [ -8.2403,   6.8368],
        [ -5.9129,   4.3452],
        [ -7.4199,   5.9017],
        [  7.4802,  -9.6216],
        [  5.4296,  -7.1468],
        [ -3.8805,   2.1832],
        [ -8.2768,   6.8319],
        [ -5.5430,   3.6336],
        [  6.7988,  -8.1875],
        [ -7.8544,   6.3981],
        [ -8.5465,   6.4831],
        [ -8.2453,   6.7802],
        [  1.1659,  -4.0171],
        [  6.9815,  -8.4162],
        [ -6.1340,   4.4022],
        [  6.5526,  -9.1283],
        [ -7.7777,   6.3908],
        [  7.1217,  -9.5680],
        [  7.4103,  -8.9019],
        [ -9.3670,   5.6642],
        [  7.9323,  -9.3408],
        [ -6.2902,   4.6678],
        [  6.3246,  -9.7323],
        [  5.6390,  -7.1720],
        [  6.3103,  -9.0981],
        [  6.8356,  -8.8501],
        [ -8.3713,   6.6393],
        [ -6.5792,   4.9779],
        [  6.1689,  -8.5315],
        [ -6.4534,   5.0654],
        [  6.1931, -10.1516],
        [ -7.9867,   6.5999],
        [ -8.3849,   6.9848],
        [ -7.4052,   5.9180],
        [ -8.3730,   6.9099],
        [ -5.5598,   4.1100],
        [ -5.4135,   3.9350],
        [ -6.5954,   4.5465],
        [  1.5976,  -3.0544],
        [ -5.7992,   4.4123],
        [  6.7842,  -8.9881]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7667, 0.2333],
        [0.3142, 0.6858]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5124, 0.4876], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3977, 0.1078],
         [0.9811, 0.2052]],

        [[0.4675, 0.0989],
         [0.7966, 0.2232]],

        [[0.6708, 0.0901],
         [0.3596, 0.1830]],

        [[0.4759, 0.0842],
         [0.9367, 0.4009]],

        [[0.7153, 0.0900],
         [0.3826, 0.7419]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919994334721989
Average Adjusted Rand Index: 0.9919995611635631
Iteration 0: Loss = -25895.331309196335
Iteration 10: Loss = -12678.899490419508
Iteration 20: Loss = -12678.662037005113
Iteration 30: Loss = -12415.594032669966
Iteration 40: Loss = -11694.041235881487
Iteration 50: Loss = -11694.041239871005
1
Iteration 60: Loss = -11694.041239871007
2
Iteration 70: Loss = -11694.041239871007
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7656, 0.2344],
        [0.3140, 0.6860]], dtype=torch.float64)
alpha: tensor([0.5528, 0.4472])
beta: tensor([[[0.3910, 0.1084],
         [0.5920, 0.2004]],

        [[0.7584, 0.0988],
         [0.9799, 0.2157]],

        [[0.9337, 0.0901],
         [0.3481, 0.4713]],

        [[0.5221, 0.0842],
         [0.5074, 0.7243]],

        [[0.9651, 0.0901],
         [0.7422, 0.0907]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919994334721989
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25896.75470185321
Iteration 100: Loss = -12690.538072116098
Iteration 200: Loss = -12681.615839367072
Iteration 300: Loss = -12679.614062533687
Iteration 400: Loss = -12664.18734128297
Iteration 500: Loss = -12620.362290246374
Iteration 600: Loss = -12540.326829421449
Iteration 700: Loss = -12456.260050269402
Iteration 800: Loss = -12333.814716926116
Iteration 900: Loss = -12312.544317587068
Iteration 1000: Loss = -12305.969041434493
Iteration 1100: Loss = -12304.694828932852
Iteration 1200: Loss = -12300.349889024821
Iteration 1300: Loss = -12291.575348874083
Iteration 1400: Loss = -12277.745506883282
Iteration 1500: Loss = -12250.991398409986
Iteration 1600: Loss = -12231.760279121612
Iteration 1700: Loss = -12222.835891109413
Iteration 1800: Loss = -12212.219135938825
Iteration 1900: Loss = -12209.633109484024
Iteration 2000: Loss = -12201.02938527806
Iteration 2100: Loss = -12190.442059906753
Iteration 2200: Loss = -12188.378048646424
Iteration 2300: Loss = -12185.914567382926
Iteration 2400: Loss = -12179.450293419584
Iteration 2500: Loss = -12158.238055671703
Iteration 2600: Loss = -12134.815837044289
Iteration 2700: Loss = -12128.519217962132
Iteration 2800: Loss = -12115.413622703134
Iteration 2900: Loss = -12114.851420846282
Iteration 3000: Loss = -12111.174872110882
Iteration 3100: Loss = -12111.114637843462
Iteration 3200: Loss = -12111.075347219126
Iteration 3300: Loss = -12111.043514535006
Iteration 3400: Loss = -12111.005487016533
Iteration 3500: Loss = -12110.233802100081
Iteration 3600: Loss = -12110.193190979893
Iteration 3700: Loss = -12110.132965561255
Iteration 3800: Loss = -12106.597175376935
Iteration 3900: Loss = -12106.57083515043
Iteration 4000: Loss = -12106.529859608989
Iteration 4100: Loss = -12093.476040160454
Iteration 4200: Loss = -12093.418643413863
Iteration 4300: Loss = -12093.410640772932
Iteration 4400: Loss = -12093.404184877982
Iteration 4500: Loss = -12093.398656039642
Iteration 4600: Loss = -12093.393790931677
Iteration 4700: Loss = -12093.389355388528
Iteration 4800: Loss = -12093.385327669685
Iteration 4900: Loss = -12093.38156244957
Iteration 5000: Loss = -12093.436795514624
1
Iteration 5100: Loss = -12087.00062616582
Iteration 5200: Loss = -12086.958397814595
Iteration 5300: Loss = -12086.922337005282
Iteration 5400: Loss = -12087.01217893356
1
Iteration 5500: Loss = -12086.916719597266
Iteration 5600: Loss = -12086.914421007948
Iteration 5700: Loss = -12086.91232711788
Iteration 5800: Loss = -12086.946813305174
1
Iteration 5900: Loss = -12086.931526513392
2
Iteration 6000: Loss = -12080.667987679677
Iteration 6100: Loss = -12080.647429093257
Iteration 6200: Loss = -12080.645877900484
Iteration 6300: Loss = -12080.640471432942
Iteration 6400: Loss = -12062.749867724811
Iteration 6500: Loss = -12062.71962381951
Iteration 6600: Loss = -12062.710529175865
Iteration 6700: Loss = -12062.707978963179
Iteration 6800: Loss = -12062.707388601457
Iteration 6900: Loss = -12062.722049179585
1
Iteration 7000: Loss = -12062.715283912821
2
Iteration 7100: Loss = -12062.70507722158
Iteration 7200: Loss = -12062.703233956836
Iteration 7300: Loss = -12062.70438336557
1
Iteration 7400: Loss = -12062.697447419423
Iteration 7500: Loss = -12061.029205119534
Iteration 7600: Loss = -12061.019433973182
Iteration 7700: Loss = -12061.016873595976
Iteration 7800: Loss = -12045.558617661658
Iteration 7900: Loss = -12045.549793716034
Iteration 8000: Loss = -12045.545798872256
Iteration 8100: Loss = -12045.538306611552
Iteration 8200: Loss = -12045.53772835544
Iteration 8300: Loss = -12045.5381881859
1
Iteration 8400: Loss = -12045.539593148053
2
Iteration 8500: Loss = -12045.54061737728
3
Iteration 8600: Loss = -12045.044743562567
Iteration 8700: Loss = -12033.119421076854
Iteration 8800: Loss = -12033.114060884693
Iteration 8900: Loss = -12033.105407613042
Iteration 9000: Loss = -12033.104005990377
Iteration 9100: Loss = -12012.09556032851
Iteration 9200: Loss = -12012.057851955875
Iteration 9300: Loss = -12012.058318045334
1
Iteration 9400: Loss = -12012.056372069024
Iteration 9500: Loss = -12012.05756212676
1
Iteration 9600: Loss = -12012.203316639501
2
Iteration 9700: Loss = -12012.064257430615
3
Iteration 9800: Loss = -12012.054442700559
Iteration 9900: Loss = -12012.054373720099
Iteration 10000: Loss = -12012.133737470615
1
Iteration 10100: Loss = -12012.20674608737
2
Iteration 10200: Loss = -12012.062965271909
3
Iteration 10300: Loss = -12012.053589584277
Iteration 10400: Loss = -12012.053419732802
Iteration 10500: Loss = -12012.07119500916
1
Iteration 10600: Loss = -12012.0515399094
Iteration 10700: Loss = -12012.05223787775
1
Iteration 10800: Loss = -12012.068245765597
2
Iteration 10900: Loss = -12012.050980672147
Iteration 11000: Loss = -12012.05078522072
Iteration 11100: Loss = -12012.051304893113
1
Iteration 11200: Loss = -12012.059470176468
2
Iteration 11300: Loss = -12012.049735884744
Iteration 11400: Loss = -12012.049874101069
1
Iteration 11500: Loss = -12012.051968211535
2
Iteration 11600: Loss = -12012.107765877963
3
Iteration 11700: Loss = -12012.051862871653
4
Iteration 11800: Loss = -12012.048986858039
Iteration 11900: Loss = -12012.049761533573
1
Iteration 12000: Loss = -12012.05467344722
2
Iteration 12100: Loss = -12012.064043988874
3
Iteration 12200: Loss = -12012.048787588947
Iteration 12300: Loss = -12012.049637551212
1
Iteration 12400: Loss = -12012.048211939165
Iteration 12500: Loss = -12012.048351603324
1
Iteration 12600: Loss = -12012.064897793454
2
Iteration 12700: Loss = -12012.053738439468
3
Iteration 12800: Loss = -12012.066383308704
4
Iteration 12900: Loss = -12012.05157085874
5
Iteration 13000: Loss = -12012.047821466333
Iteration 13100: Loss = -12012.050058022198
1
Iteration 13200: Loss = -12012.052659236908
2
Iteration 13300: Loss = -12012.096426917778
3
Iteration 13400: Loss = -12012.047546640282
Iteration 13500: Loss = -12012.048079334021
1
Iteration 13600: Loss = -12012.059206769765
2
Iteration 13700: Loss = -12012.050183381696
3
Iteration 13800: Loss = -12012.049474300076
4
Iteration 13900: Loss = -12012.196921301631
5
Iteration 14000: Loss = -12012.049558990046
6
Iteration 14100: Loss = -12012.047448399588
Iteration 14200: Loss = -12012.048383945561
1
Iteration 14300: Loss = -12012.071875109274
2
Iteration 14400: Loss = -12012.060973555619
3
Iteration 14500: Loss = -12012.04872882682
4
Iteration 14600: Loss = -12012.04865531533
5
Iteration 14700: Loss = -12012.047699358176
6
Iteration 14800: Loss = -12012.045950870683
Iteration 14900: Loss = -12012.052234728939
1
Iteration 15000: Loss = -12012.05513582229
2
Iteration 15100: Loss = -12012.110370209002
3
Iteration 15200: Loss = -12012.050375656352
4
Iteration 15300: Loss = -12012.045963045737
5
Iteration 15400: Loss = -12012.046465579879
6
Iteration 15500: Loss = -12012.057695213873
7
Iteration 15600: Loss = -12012.048352306676
8
Iteration 15700: Loss = -12012.051689424556
9
Iteration 15800: Loss = -12012.046328967668
10
Stopping early at iteration 15800 due to no improvement.
tensor([[ -8.8057,   7.3925],
        [ -5.5103,   4.0316],
        [ -8.7238,   7.1116],
        [  7.4146,  -8.8134],
        [ -8.1418,   6.4301],
        [ -9.2101,   7.8147],
        [ -8.9188,   6.8833],
        [ -8.2407,   6.6060],
        [-10.0983,   6.9143],
        [ -3.2391,   1.7776],
        [  3.1278,  -6.6457],
        [  3.4209,  -4.8435],
        [ -8.9940,   7.4355],
        [ -5.9187,   4.5115],
        [ -7.4118,   5.8661],
        [  4.5083,  -6.1277],
        [ -9.0532,   7.4177],
        [ -7.2306,   4.3819],
        [ -8.3092,   6.8837],
        [ -9.3601,   7.1404],
        [  6.3970,  -8.4796],
        [ -8.9685,   7.2714],
        [  5.4870,  -9.1033],
        [  1.1307,  -4.5667],
        [ -5.8286,   4.2034],
        [  3.1620,  -5.0032],
        [ -7.7038,   6.2505],
        [  0.8577,  -2.6668],
        [ -9.3974,   7.4690],
        [-11.1218,   6.5065],
        [  4.0299,  -5.8039],
        [ -0.2285,  -1.2045],
        [ -2.2702,  -0.3373],
        [ -6.5219,   5.0722],
        [ -8.7187,   6.4581],
        [ -9.4557,   7.3468],
        [ -8.8164,   7.1963],
        [  1.8092,  -3.4539],
        [  6.6715,  -8.3234],
        [  3.4226,  -4.9563],
        [ -6.6329,   5.0628],
        [  4.5194,  -7.6656],
        [  1.0487,  -3.1804],
        [ -8.4304,   7.0070],
        [ -8.1152,   6.6973],
        [  1.9928,  -3.3796],
        [  1.6029,  -2.9922],
        [ -9.4991,   7.8852],
        [ -7.5539,   5.4696],
        [  3.6010,  -5.1636],
        [  7.0861,  -8.5586],
        [  2.3810,  -4.4538],
        [  7.6033,  -9.2037],
        [ -9.8229,   6.9017],
        [  4.5927,  -6.4751],
        [ -7.9923,   5.0551],
        [ -8.8480,   6.5877],
        [ -6.7155,   5.3291],
        [ -8.2662,   4.7226],
        [  6.4920,  -8.0662],
        [  2.5493,  -4.4752],
        [  3.2407,  -4.6281],
        [ -9.0785,   7.6759],
        [ -8.0804,   6.5028],
        [  0.7890,  -2.8039],
        [  5.9774,  -7.8970],
        [ -0.8815,  -0.5841],
        [ -8.9941,   6.9316],
        [  5.2439,  -6.6318],
        [  5.2643,  -8.8193],
        [  6.3022,  -8.0318],
        [ -5.4841,   4.0449],
        [ -9.4259,   7.3101],
        [  1.8942,  -3.4234],
        [ -8.2454,   6.8357],
        [  3.3364,  -4.7330],
        [ -9.1422,   7.7082],
        [ -8.6598,   7.2660],
        [  5.8794,  -7.2659],
        [ -8.8093,   7.3615],
        [  3.1283,  -5.3356],
        [ -9.1298,   6.7341],
        [ -9.5216,   4.9064],
        [ -8.9574,   7.1769],
        [ -9.5346,   7.5079],
        [  5.3154,  -6.9772],
        [  1.9842,  -3.4005],
        [ -8.8054,   7.0672],
        [  0.4707,  -2.5141],
        [ -8.9454,   7.2500],
        [  5.2106,  -6.6017],
        [  6.2827,  -7.6827],
        [  3.1623,  -5.9232],
        [  6.5154,  -7.9143],
        [  2.1188,  -3.5244],
        [  2.3282,  -3.8013],
        [  1.0795,  -5.0930],
        [ -6.9775,   5.5164],
        [ -0.5522,  -1.6632],
        [ -9.1193,   6.3748]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4823, 0.5177],
        [0.5531, 0.4469]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4590, 0.5410], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3147, 0.1058],
         [0.5920, 0.3252]],

        [[0.7584, 0.0943],
         [0.9799, 0.2157]],

        [[0.9337, 0.1008],
         [0.3481, 0.4713]],

        [[0.5221, 0.0834],
         [0.5074, 0.7243]],

        [[0.9651, 0.0881],
         [0.7422, 0.0907]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448543354594036
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080514782056689
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.03342585200501619
Average Adjusted Rand Index: 0.8832173839783589
Iteration 0: Loss = -20011.7178826154
Iteration 10: Loss = -11694.041245847542
Iteration 20: Loss = -11694.041239871007
Iteration 30: Loss = -11694.041239871007
1
Iteration 40: Loss = -11694.041239871007
2
Iteration 50: Loss = -11694.041239871007
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7656, 0.2344],
        [0.3140, 0.6860]], dtype=torch.float64)
alpha: tensor([0.5528, 0.4472])
beta: tensor([[[0.3910, 0.1084],
         [0.1545, 0.2004]],

        [[0.0621, 0.0988],
         [0.1414, 0.9409]],

        [[0.9117, 0.0901],
         [0.3726, 0.5872]],

        [[0.5411, 0.0842],
         [0.0027, 0.3982]],

        [[0.9855, 0.0901],
         [0.4990, 0.7035]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919994334721989
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20011.375487349313
Iteration 100: Loss = -12752.335984522315
Iteration 200: Loss = -12499.155268666038
Iteration 300: Loss = -12201.855521764288
Iteration 400: Loss = -12171.16431992152
Iteration 500: Loss = -12072.52780809069
Iteration 600: Loss = -11954.178927640802
Iteration 700: Loss = -11905.491440845888
Iteration 800: Loss = -11823.099224703226
Iteration 900: Loss = -11776.586113084815
Iteration 1000: Loss = -11752.26640837962
Iteration 1100: Loss = -11752.126653082945
Iteration 1200: Loss = -11751.967102230283
Iteration 1300: Loss = -11732.531177869185
Iteration 1400: Loss = -11732.48161060838
Iteration 1500: Loss = -11732.443603706319
Iteration 1600: Loss = -11732.413403071236
Iteration 1700: Loss = -11732.388840381245
Iteration 1800: Loss = -11732.368543025692
Iteration 1900: Loss = -11732.351541330681
Iteration 2000: Loss = -11732.337145269765
Iteration 2100: Loss = -11732.324777865151
Iteration 2200: Loss = -11732.31404567699
Iteration 2300: Loss = -11732.30471878448
Iteration 2400: Loss = -11732.296553001119
Iteration 2500: Loss = -11732.28932048729
Iteration 2600: Loss = -11732.282865723848
Iteration 2700: Loss = -11732.27718348867
Iteration 2800: Loss = -11732.272028821788
Iteration 2900: Loss = -11732.267414935523
Iteration 3000: Loss = -11732.26320758526
Iteration 3100: Loss = -11732.259474563774
Iteration 3200: Loss = -11732.257889318917
Iteration 3300: Loss = -11732.252938546233
Iteration 3400: Loss = -11732.250046035482
Iteration 3500: Loss = -11732.247421249616
Iteration 3600: Loss = -11732.245084061971
Iteration 3700: Loss = -11732.242868921207
Iteration 3800: Loss = -11732.240877930766
Iteration 3900: Loss = -11732.238978546387
Iteration 4000: Loss = -11732.23735625326
Iteration 4100: Loss = -11732.235641368345
Iteration 4200: Loss = -11732.234148678
Iteration 4300: Loss = -11732.234519330286
1
Iteration 4400: Loss = -11732.231475128638
Iteration 4500: Loss = -11732.230872116806
Iteration 4600: Loss = -11732.229336966266
Iteration 4700: Loss = -11732.285918113372
1
Iteration 4800: Loss = -11732.227188145393
Iteration 4900: Loss = -11732.246496616237
1
Iteration 5000: Loss = -11732.225554156175
Iteration 5100: Loss = -11732.224702803462
Iteration 5200: Loss = -11732.224192164611
Iteration 5300: Loss = -11732.223245544887
Iteration 5400: Loss = -11732.222747135022
Iteration 5500: Loss = -11732.221973704025
Iteration 5600: Loss = -11732.222600462906
1
Iteration 5700: Loss = -11732.239600488318
2
Iteration 5800: Loss = -11732.220376813673
Iteration 5900: Loss = -11732.333193618057
1
Iteration 6000: Loss = -11732.219433087204
Iteration 6100: Loss = -11732.264646706963
1
Iteration 6200: Loss = -11732.218620200374
Iteration 6300: Loss = -11732.218272679595
Iteration 6400: Loss = -11732.218130570998
Iteration 6500: Loss = -11732.217684845271
Iteration 6600: Loss = -11732.21761024143
Iteration 6700: Loss = -11732.224771861476
1
Iteration 6800: Loss = -11732.216737390429
Iteration 6900: Loss = -11732.229240593395
1
Iteration 7000: Loss = -11732.216926803316
2
Iteration 7100: Loss = -11732.216005066231
Iteration 7200: Loss = -11732.215795073684
Iteration 7300: Loss = -11732.21983194699
1
Iteration 7400: Loss = -11732.226497878355
2
Iteration 7500: Loss = -11732.283616801393
3
Iteration 7600: Loss = -11732.229323133133
4
Iteration 7700: Loss = -11732.217893939625
5
Iteration 7800: Loss = -11732.216779711258
6
Iteration 7900: Loss = -11732.229912561574
7
Iteration 8000: Loss = -11732.214864604837
Iteration 8100: Loss = -11732.221221401296
1
Iteration 8200: Loss = -11732.217828189885
2
Iteration 8300: Loss = -11732.21396992758
Iteration 8400: Loss = -11732.214754502085
1
Iteration 8500: Loss = -11732.214331127016
2
Iteration 8600: Loss = -11732.213746815834
Iteration 8700: Loss = -11732.247843139428
1
Iteration 8800: Loss = -11732.213582162529
Iteration 8900: Loss = -11732.213452103764
Iteration 9000: Loss = -11732.21377727755
1
Iteration 9100: Loss = -11732.325491756876
2
Iteration 9200: Loss = -11732.213416129147
Iteration 9300: Loss = -11732.213379747489
Iteration 9400: Loss = -11732.285501706318
1
Iteration 9500: Loss = -11732.213790687038
2
Iteration 9600: Loss = -11732.213243692284
Iteration 9700: Loss = -11732.213909412978
1
Iteration 9800: Loss = -11732.232827455875
2
Iteration 9900: Loss = -11732.221370160534
3
Iteration 10000: Loss = -11732.21333112869
4
Iteration 10100: Loss = -11732.212980165477
Iteration 10200: Loss = -11732.221286600781
1
Iteration 10300: Loss = -11732.212544691674
Iteration 10400: Loss = -11732.217007305584
1
Iteration 10500: Loss = -11732.212964359996
2
Iteration 10600: Loss = -11732.213847361645
3
Iteration 10700: Loss = -11732.214955864796
4
Iteration 10800: Loss = -11732.232905495846
5
Iteration 10900: Loss = -11732.212419296997
Iteration 11000: Loss = -11732.21279140995
1
Iteration 11100: Loss = -11732.213504051997
2
Iteration 11200: Loss = -11732.214599356997
3
Iteration 11300: Loss = -11732.233591815047
4
Iteration 11400: Loss = -11732.233063091127
5
Iteration 11500: Loss = -11732.212366516258
Iteration 11600: Loss = -11732.227142287169
1
Iteration 11700: Loss = -11732.216039743655
2
Iteration 11800: Loss = -11732.212176879553
Iteration 11900: Loss = -11732.217476680731
1
Iteration 12000: Loss = -11732.253221339855
2
Iteration 12100: Loss = -11732.218334166237
3
Iteration 12200: Loss = -11732.218687680463
4
Iteration 12300: Loss = -11732.244205906585
5
Iteration 12400: Loss = -11732.21315740764
6
Iteration 12500: Loss = -11732.212076573029
Iteration 12600: Loss = -11732.215710461289
1
Iteration 12700: Loss = -11732.212397193527
2
Iteration 12800: Loss = -11732.229117127637
3
Iteration 12900: Loss = -11732.216161373177
4
Iteration 13000: Loss = -11732.236246606677
5
Iteration 13100: Loss = -11732.211992830864
Iteration 13200: Loss = -11732.212257894376
1
Iteration 13300: Loss = -11732.212255722648
2
Iteration 13400: Loss = -11732.21203049904
3
Iteration 13500: Loss = -11732.22156260639
4
Iteration 13600: Loss = -11732.22145910452
5
Iteration 13700: Loss = -11732.253402606733
6
Iteration 13800: Loss = -11732.211983266592
Iteration 13900: Loss = -11732.218302440442
1
Iteration 14000: Loss = -11732.24230637143
2
Iteration 14100: Loss = -11732.212017005386
3
Iteration 14200: Loss = -11732.212190860839
4
Iteration 14300: Loss = -11732.215674030766
5
Iteration 14400: Loss = -11732.214368787769
6
Iteration 14500: Loss = -11732.222415278053
7
Iteration 14600: Loss = -11732.266471629277
8
Iteration 14700: Loss = -11732.213981388108
9
Iteration 14800: Loss = -11732.21565160722
10
Stopping early at iteration 14800 due to no improvement.
tensor([[  8.6314, -10.6674],
        [  1.1216,  -3.1664],
        [  7.5340,  -9.4396],
        [-10.3516,   7.8874],
        [  3.5973,  -6.4577],
        [  8.4976,  -9.9265],
        [  7.5702,  -9.2761],
        [  6.5985, -10.1653],
        [  4.8616,  -6.8290],
        [ -1.1889,  -0.1978],
        [ -8.3777,   6.9914],
        [ -8.8241,   6.6647],
        [  7.7648,  -9.8779],
        [  4.1282,  -5.5574],
        [  4.4908,  -7.6064],
        [ -9.0844,   7.6830],
        [  6.9285,  -8.3148],
        [  1.7642,  -3.1839],
        [  5.1641,  -7.3576],
        [  6.4742,  -8.6320],
        [ -9.7199,   7.1198],
        [  7.4600,  -8.8920],
        [ -9.2477,   7.4630],
        [ -6.8104,   4.3285],
        [  0.3607,  -2.4498],
        [ -8.0340,   6.6032],
        [  4.0033,  -6.9808],
        [ -6.5118,   2.9880],
        [  7.7526, -10.3690],
        [  7.5861,  -9.1022],
        [ -7.4775,   5.6321],
        [ -5.2532,   1.8965],
        [ -3.0932,   1.5096],
        [  2.6980,  -5.9159],
        [  6.1770,  -9.2470],
        [  4.9576,  -6.9178],
        [  7.2982,  -9.1268],
        [ -6.9897,   4.2496],
        [-11.3600,   6.7529],
        [ -6.6313,   5.1308],
        [  4.3660,  -6.0435],
        [-12.3814,   7.7662],
        [ -7.1858,   5.6683],
        [  7.8666,  -9.5593],
        [  6.4261,  -8.4292],
        [ -3.0648,   0.9177],
        [ -3.6142,   2.1996],
        [  7.8657,  -9.2520],
        [  7.8291,  -9.4318],
        [ -8.8286,   5.7313],
        [ -9.9585,   8.1767],
        [ -7.6152,   6.2287],
        [ -6.8698,   5.1196],
        [  6.1979, -10.8131],
        [ -8.6685,   6.6233],
        [  2.9773,  -4.5819],
        [  7.4469,  -9.0585],
        [  3.3579,  -6.0885],
        [  3.2343,  -5.1656],
        [ -9.3655,   7.6208],
        [ -6.0412,   4.0312],
        [ -8.0388,   5.5112],
        [  7.9256,  -9.3532],
        [  5.6550,  -7.1025],
        [ -3.6196,   2.2283],
        [ -9.9190,   7.1163],
        [ -5.2096,   3.6333],
        [  7.8628,  -9.7692],
        [ -9.6703,   6.8699],
        [ -8.9001,   7.4883],
        [ -9.9566,   8.0585],
        [  1.7588,  -3.5446],
        [  6.7732,  -9.1930],
        [ -6.3566,   3.9108],
        [  6.9460,  -8.3674],
        [ -8.1676,   6.7808],
        [  7.8313,  -9.2246],
        [  7.9291, -10.1464],
        [ -9.0350,   7.5974],
        [  8.0699,  -9.4568],
        [-10.6980,   9.1246],
        [  8.1947,  -9.7953],
        [  5.5424,  -7.3241],
        [  7.7538,  -9.3054],
        [  8.1981,  -9.6459],
        [ -9.0462,   7.3999],
        [ -6.3406,   4.9230],
        [  7.5621,  -8.9819],
        [ -6.5673,   4.6171],
        [  7.8516,  -9.4177],
        [ -9.5376,   6.7297],
        [ -9.6325,   8.1023],
        [ -8.5482,   4.9702],
        [ -9.0953,   7.7058],
        [ -5.4441,   4.0153],
        [ -5.4309,   3.7583],
        [ -6.2427,   4.6685],
        [  1.4632,  -3.3902],
        [ -6.8430,   3.0273],
        [  7.3830,  -8.8800]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7598, 0.2402],
        [0.3248, 0.6752]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5122, 0.4878], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3937, 0.1077],
         [0.1545, 0.2064]],

        [[0.0621, 0.0969],
         [0.1414, 0.9409]],

        [[0.9117, 0.0980],
         [0.3726, 0.5872]],

        [[0.5411, 0.0842],
         [0.0027, 0.3982]],

        [[0.9855, 0.0898],
         [0.4990, 0.7035]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681899156014986
Average Adjusted Rand Index: 0.9683212541881167
11703.824783587863
new:  [0.9919994334721989, 0.9919994334721989, 0.03342585200501619, 0.9681899156014986] [0.9919995611635631, 0.9919995611635631, 0.8832173839783589, 0.9683212541881167] [11692.44839550073, 11692.290055439418, 12012.046328967668, 11732.21565160722]
prior:  [0.9919994334721989, 0.9919994334721989, 0.9919994334721989, 0.9919994334721989] [0.9919995611635631, 0.9919995611635631, 0.9919995611635631, 0.9919995611635631] [11694.041241508368, 11694.041239871007, 11694.041239871007, 11694.041239871007]
-----------------------------------------------------------------------------------------
This iteration is 4
True Objective function: Loss = -11393.596514248846
Iteration 0: Loss = -38555.74004235127
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[       nan,        nan],
         [3.8228e-01,        nan]],

        [[4.7621e-02,        nan],
         [9.5557e-01, 2.7461e-01]],

        [[3.9107e-02,        nan],
         [3.3919e-01, 4.6745e-04]],

        [[7.6090e-01,        nan],
         [3.4021e-02, 8.1163e-01]],

        [[9.6474e-01,        nan],
         [4.6434e-01, 3.3423e-01]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38573.85516050139
Iteration 100: Loss = -12188.07959047046
Iteration 200: Loss = -12183.959278706865
Iteration 300: Loss = -12177.419444753688
Iteration 400: Loss = -12163.813092660948
Iteration 500: Loss = -12147.689005774211
Iteration 600: Loss = -12127.440481198675
Iteration 700: Loss = -12069.016852925897
Iteration 800: Loss = -12008.191825773903
Iteration 900: Loss = -11879.110142965683
Iteration 1000: Loss = -11800.683517711625
Iteration 1100: Loss = -11788.189418612392
Iteration 1200: Loss = -11782.8103724418
Iteration 1300: Loss = -11782.093220575545
Iteration 1400: Loss = -11781.654719226855
Iteration 1500: Loss = -11781.38834771114
Iteration 1600: Loss = -11781.196312670038
Iteration 1700: Loss = -11781.045296275683
Iteration 1800: Loss = -11780.926640555992
Iteration 1900: Loss = -11780.835470554382
Iteration 2000: Loss = -11780.760786949862
Iteration 2100: Loss = -11780.699853018432
Iteration 2200: Loss = -11780.645537255534
Iteration 2300: Loss = -11780.599431769606
Iteration 2400: Loss = -11780.565219966269
Iteration 2500: Loss = -11780.527970023717
Iteration 2600: Loss = -11780.498744858416
Iteration 2700: Loss = -11780.472398784876
Iteration 2800: Loss = -11780.449846132533
Iteration 2900: Loss = -11780.42958565219
Iteration 3000: Loss = -11780.411500801427
Iteration 3100: Loss = -11780.399406512404
Iteration 3200: Loss = -11780.380574499235
Iteration 3300: Loss = -11780.367263179898
Iteration 3400: Loss = -11780.383035485233
1
Iteration 3500: Loss = -11780.344235065037
Iteration 3600: Loss = -11780.334114352516
Iteration 3700: Loss = -11780.324862524441
Iteration 3800: Loss = -11780.39024445632
1
Iteration 3900: Loss = -11780.308620698403
Iteration 4000: Loss = -11780.301452041358
Iteration 4100: Loss = -11780.294792995739
Iteration 4200: Loss = -11780.288890607573
Iteration 4300: Loss = -11780.283030088442
Iteration 4400: Loss = -11780.277809276364
Iteration 4500: Loss = -11780.273313040801
Iteration 4600: Loss = -11780.268460199022
Iteration 4700: Loss = -11780.264215105162
Iteration 4800: Loss = -11780.260326206238
Iteration 4900: Loss = -11780.257049916543
Iteration 5000: Loss = -11780.253461591345
Iteration 5100: Loss = -11780.250183230293
Iteration 5200: Loss = -11780.609026912998
1
Iteration 5300: Loss = -11780.244573819451
Iteration 5400: Loss = -11780.242019342166
Iteration 5500: Loss = -11780.243105517322
1
Iteration 5600: Loss = -11780.237430555597
Iteration 5700: Loss = -11780.236802833264
Iteration 5800: Loss = -11780.233411379728
Iteration 5900: Loss = -11780.255654824672
1
Iteration 6000: Loss = -11780.22983543752
Iteration 6100: Loss = -11780.253361981471
1
Iteration 6200: Loss = -11780.226691109183
Iteration 6300: Loss = -11780.230274433254
1
Iteration 6400: Loss = -11780.22443991496
Iteration 6500: Loss = -11780.222764225316
Iteration 6600: Loss = -11780.231714644939
1
Iteration 6700: Loss = -11780.220314371836
Iteration 6800: Loss = -11780.220107632962
Iteration 6900: Loss = -11780.218852693719
Iteration 7000: Loss = -11780.233335444995
1
Iteration 7100: Loss = -11780.216415739473
Iteration 7200: Loss = -11780.21546324875
Iteration 7300: Loss = -11780.22352253059
1
Iteration 7400: Loss = -11780.217754420608
2
Iteration 7500: Loss = -11780.223948060966
3
Iteration 7600: Loss = -11780.21213507725
Iteration 7700: Loss = -11780.213893608603
1
Iteration 7800: Loss = -11780.204010534055
Iteration 7900: Loss = -11776.912868639884
Iteration 8000: Loss = -11776.925151233056
1
Iteration 8100: Loss = -11776.909660921194
Iteration 8200: Loss = -11776.799495290452
Iteration 8300: Loss = -11769.911084666375
Iteration 8400: Loss = -11769.907163090778
Iteration 8500: Loss = -11766.792982772731
Iteration 8600: Loss = -11764.515847325805
Iteration 8700: Loss = -11753.387654149199
Iteration 8800: Loss = -11748.1566852576
Iteration 8900: Loss = -11734.62574023114
Iteration 9000: Loss = -11711.509431219758
Iteration 9100: Loss = -11685.284566395028
Iteration 9200: Loss = -11600.549400600035
Iteration 9300: Loss = -11543.707666052083
Iteration 9400: Loss = -11529.32185729419
Iteration 9500: Loss = -11520.782710797926
Iteration 9600: Loss = -11468.95558938478
Iteration 9700: Loss = -11445.09815379922
Iteration 9800: Loss = -11418.587141372695
Iteration 9900: Loss = -11381.870486946022
Iteration 10000: Loss = -11380.6581209495
Iteration 10100: Loss = -11380.647489885234
Iteration 10200: Loss = -11380.660392480284
1
Iteration 10300: Loss = -11380.644048214333
Iteration 10400: Loss = -11380.650578114735
1
Iteration 10500: Loss = -11373.30764103388
Iteration 10600: Loss = -11373.248840536997
Iteration 10700: Loss = -11373.23892190413
Iteration 10800: Loss = -11373.241148503137
1
Iteration 10900: Loss = -11373.238791420963
Iteration 11000: Loss = -11373.257074364423
1
Iteration 11100: Loss = -11373.234121504234
Iteration 11200: Loss = -11373.177156653024
Iteration 11300: Loss = -11373.175586156463
Iteration 11400: Loss = -11373.177020567793
1
Iteration 11500: Loss = -11373.171405158106
Iteration 11600: Loss = -11373.176461362476
1
Iteration 11700: Loss = -11373.184103909422
2
Iteration 11800: Loss = -11373.176141502014
3
Iteration 11900: Loss = -11373.174027976569
4
Iteration 12000: Loss = -11373.171350304618
Iteration 12100: Loss = -11373.17160046201
1
Iteration 12200: Loss = -11373.169990801045
Iteration 12300: Loss = -11373.170205424569
1
Iteration 12400: Loss = -11373.173402886672
2
Iteration 12500: Loss = -11373.170782461668
3
Iteration 12600: Loss = -11373.17003043849
4
Iteration 12700: Loss = -11373.179446833066
5
Iteration 12800: Loss = -11373.226439166376
6
Iteration 12900: Loss = -11373.169387297505
Iteration 13000: Loss = -11373.201711729544
1
Iteration 13100: Loss = -11373.17436328668
2
Iteration 13200: Loss = -11373.178310468411
3
Iteration 13300: Loss = -11373.271311279566
4
Iteration 13400: Loss = -11373.174294967186
5
Iteration 13500: Loss = -11373.17844263113
6
Iteration 13600: Loss = -11373.198870715636
7
Iteration 13700: Loss = -11373.179354571479
8
Iteration 13800: Loss = -11373.17150528553
9
Iteration 13900: Loss = -11373.183328605675
10
Stopping early at iteration 13900 due to no improvement.
tensor([[  4.9693,  -9.5845],
        [  3.0410,  -7.6563],
        [ -3.6928,  -0.9224],
        [ -9.6990,   5.0838],
        [ -9.4682,   4.8530],
        [ -9.4767,   4.8614],
        [  3.2405,  -7.8557],
        [  0.7197,  -5.3350],
        [  5.0917,  -9.7069],
        [ -8.0881,   3.4729],
        [ -8.5336,   3.9183],
        [ -9.5160,   4.9008],
        [  1.6213,  -6.2365],
        [  4.6213,  -9.2365],
        [  3.8364,  -8.4516],
        [ -8.1129,   3.4977],
        [  4.4271,  -9.0423],
        [  3.8362,  -8.4514],
        [ -9.5558,   4.9406],
        [ -8.9533,   4.3381],
        [  4.6420,  -9.2572],
        [  5.6504, -10.2657],
        [ -9.3892,   4.7739],
        [  5.2857,  -9.9010],
        [  2.5714,  -7.1866],
        [  3.2400,  -7.8553],
        [  4.1879,  -8.8031],
        [  5.1247,  -9.7399],
        [  2.6420,  -7.2572],
        [  3.4397,  -8.0549],
        [ -2.8665,  -1.7487],
        [ -6.5134,   1.8981],
        [  3.9346,  -8.5498],
        [ -9.2392,   4.6240],
        [  4.2646,  -8.8798],
        [ -6.8971,   2.2819],
        [ -7.1261,   2.5109],
        [ -9.5591,   4.9439],
        [  4.8279,  -9.4431],
        [ -7.9495,   3.3343],
        [ -9.5467,   4.9315],
        [  2.8404,  -7.4557],
        [  3.7719,  -8.3872],
        [ -9.5104,   4.8952],
        [  5.0362,  -9.6514],
        [  5.3444,  -9.9596],
        [  1.7397,  -6.3549],
        [  5.0489,  -9.6641],
        [ -8.7680,   4.1528],
        [ -8.7906,   4.1754],
        [  4.0863,  -8.7015],
        [  4.5772,  -9.1924],
        [ -7.3237,   2.7085],
        [  5.1318,  -9.7470],
        [  4.3725,  -8.9877],
        [  4.5805,  -9.1958],
        [ -9.4362,   4.8210],
        [ -9.3677,   4.7525],
        [ -9.1376,   4.5224],
        [ -8.8782,   4.2630],
        [  1.2201,  -5.8353],
        [  2.6420,  -7.2572],
        [ -9.5013,   4.8861],
        [  4.9279,  -9.5431],
        [ -9.2581,   4.6429],
        [  5.1536,  -9.7688],
        [ -7.7282,   3.1130],
        [  1.4294,  -6.0446],
        [  4.4420,  -9.0572],
        [ -8.2587,   3.6435],
        [ -9.2147,   4.5995],
        [ -9.3301,   4.7149],
        [  3.1130,  -7.7282],
        [ -9.0696,   4.4544],
        [  5.3757,  -9.9910],
        [ -8.7953,   4.1801],
        [ -8.2503,   3.6351],
        [ -7.0573,   2.4421],
        [  3.5419,  -8.1571],
        [  4.9352,  -9.5505],
        [ -9.2160,   4.6008],
        [ -8.9662,   4.3510],
        [ -7.9620,   3.3468],
        [  5.2649,  -9.8801],
        [  4.6659,  -9.2811],
        [  5.2441,  -9.8593],
        [ -9.1411,   4.5259],
        [  2.6045,  -7.2197],
        [  2.7408,  -7.3561],
        [  0.5348,  -5.1500],
        [  4.9803,  -9.5955],
        [  4.3720,  -8.9872],
        [  3.7158,  -8.3310],
        [  4.8164,  -9.4316],
        [ -9.5863,   4.9711],
        [  4.3447,  -8.9600],
        [  3.0410,  -7.6563],
        [ -7.6969,   3.0817],
        [ -9.2986,   4.6834],
        [  0.0408,  -4.6560]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7474, 0.2526],
        [0.2381, 0.7619]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5613, 0.4387], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.9531e-01, 9.8717e-02],
         [3.8228e-01, 3.9584e-01]],

        [[4.7621e-02, 1.0017e-01],
         [9.5557e-01, 2.7461e-01]],

        [[3.9107e-02, 1.1223e-01],
         [3.3919e-01, 4.6745e-04]],

        [[7.6090e-01, 8.5909e-02],
         [3.4021e-02, 8.1163e-01]],

        [[9.6474e-01, 8.6539e-02],
         [4.6434e-01, 3.3423e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320165194881
Average Adjusted Rand Index: 0.9839996238662728
Iteration 0: Loss = -19326.55936936651
Iteration 10: Loss = -11375.379206369185
Iteration 20: Loss = -11375.445414519249
1
Iteration 30: Loss = -11375.445409206925
2
Iteration 40: Loss = -11375.445409206925
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7615, 0.2385],
        [0.2544, 0.7456]], dtype=torch.float64)
alpha: tensor([0.4910, 0.5090])
beta: tensor([[[0.3884, 0.0988],
         [0.1760, 0.1914]],

        [[0.7519, 0.1004],
         [0.9511, 0.6389]],

        [[0.5854, 0.1119],
         [0.1525, 0.7223]],

        [[0.5489, 0.0860],
         [0.3403, 0.3469]],

        [[0.2105, 0.0865],
         [0.5770, 0.9018]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19326.256887543077
Iteration 100: Loss = -11911.470308675925
Iteration 200: Loss = -11530.083668649018
Iteration 300: Loss = -11517.850212349305
Iteration 400: Loss = -11490.019312753597
Iteration 500: Loss = -11449.901082221926
Iteration 600: Loss = -11408.764824983642
Iteration 700: Loss = -11394.325949590118
Iteration 800: Loss = -11393.810546609962
Iteration 900: Loss = -11383.896037536533
Iteration 1000: Loss = -11383.825617599252
Iteration 1100: Loss = -11383.776865485139
Iteration 1200: Loss = -11383.741543143595
Iteration 1300: Loss = -11383.71461196814
Iteration 1400: Loss = -11383.693403472178
Iteration 1500: Loss = -11383.680070744274
Iteration 1600: Loss = -11383.662279617862
Iteration 1700: Loss = -11383.650568756784
Iteration 1800: Loss = -11383.64173595531
Iteration 1900: Loss = -11383.632322169828
Iteration 2000: Loss = -11383.625049037957
Iteration 2100: Loss = -11383.618790218552
Iteration 2200: Loss = -11383.613372225203
Iteration 2300: Loss = -11383.60852290459
Iteration 2400: Loss = -11383.604306495155
Iteration 2500: Loss = -11383.601377533163
Iteration 2600: Loss = -11383.597107440772
Iteration 2700: Loss = -11383.59410452862
Iteration 2800: Loss = -11383.591382329852
Iteration 2900: Loss = -11383.601025302893
1
Iteration 3000: Loss = -11383.586750112692
Iteration 3100: Loss = -11383.584697678943
Iteration 3200: Loss = -11383.582890148316
Iteration 3300: Loss = -11383.589813530982
1
Iteration 3400: Loss = -11383.57961951455
Iteration 3500: Loss = -11383.5782871974
Iteration 3600: Loss = -11383.576957223584
Iteration 3700: Loss = -11383.57663942144
Iteration 3800: Loss = -11383.574705469731
Iteration 3900: Loss = -11383.573688158309
Iteration 4000: Loss = -11383.572724668218
Iteration 4100: Loss = -11383.571942675266
Iteration 4200: Loss = -11383.571076315153
Iteration 4300: Loss = -11383.570281984345
Iteration 4400: Loss = -11383.569624612255
Iteration 4500: Loss = -11383.56903568216
Iteration 4600: Loss = -11383.56839012485
Iteration 4700: Loss = -11383.567841486934
Iteration 4800: Loss = -11383.567350392446
Iteration 4900: Loss = -11383.566806791878
Iteration 5000: Loss = -11383.57292003607
1
Iteration 5100: Loss = -11383.565930570736
Iteration 5200: Loss = -11383.566542545828
1
Iteration 5300: Loss = -11383.56599880422
2
Iteration 5400: Loss = -11383.564855002898
Iteration 5500: Loss = -11383.564499511509
Iteration 5600: Loss = -11383.56422223784
Iteration 5700: Loss = -11383.563964823465
Iteration 5800: Loss = -11383.563641889305
Iteration 5900: Loss = -11383.56343028914
Iteration 6000: Loss = -11383.563143719211
Iteration 6100: Loss = -11383.562958936158
Iteration 6200: Loss = -11383.562678970433
Iteration 6300: Loss = -11383.562752913234
1
Iteration 6400: Loss = -11383.565122290496
2
Iteration 6500: Loss = -11383.574343417658
3
Iteration 6600: Loss = -11383.562023868255
Iteration 6700: Loss = -11383.561326386898
Iteration 6800: Loss = -11383.561324684457
Iteration 6900: Loss = -11383.561166414142
Iteration 7000: Loss = -11383.560871750118
Iteration 7100: Loss = -11383.572310919883
1
Iteration 7200: Loss = -11383.560597223419
Iteration 7300: Loss = -11383.561345978716
1
Iteration 7400: Loss = -11383.560401880226
Iteration 7500: Loss = -11383.577501212549
1
Iteration 7600: Loss = -11383.56049436791
2
Iteration 7700: Loss = -11383.560244662234
Iteration 7800: Loss = -11383.5612515562
1
Iteration 7900: Loss = -11383.560157895889
Iteration 8000: Loss = -11383.560082541188
Iteration 8100: Loss = -11383.560066783723
Iteration 8200: Loss = -11383.560882735253
1
Iteration 8300: Loss = -11383.560275925589
2
Iteration 8400: Loss = -11383.559830738805
Iteration 8500: Loss = -11383.559819490585
Iteration 8600: Loss = -11383.59669970032
1
Iteration 8700: Loss = -11383.564391160095
2
Iteration 8800: Loss = -11383.559587702935
Iteration 8900: Loss = -11383.600099363419
1
Iteration 9000: Loss = -11383.55968430991
2
Iteration 9100: Loss = -11383.560527456058
3
Iteration 9200: Loss = -11383.565466502465
4
Iteration 9300: Loss = -11383.559226538066
Iteration 9400: Loss = -11383.594748885049
1
Iteration 9500: Loss = -11383.626390841484
2
Iteration 9600: Loss = -11383.561320285287
3
Iteration 9700: Loss = -11383.560461456289
4
Iteration 9800: Loss = -11383.568717727714
5
Iteration 9900: Loss = -11383.559627983701
6
Iteration 10000: Loss = -11383.56699661018
7
Iteration 10100: Loss = -11383.574550997417
8
Iteration 10200: Loss = -11383.558900468062
Iteration 10300: Loss = -11383.575343785675
1
Iteration 10400: Loss = -11383.562201859088
2
Iteration 10500: Loss = -11383.566111887358
3
Iteration 10600: Loss = -11383.562309580817
4
Iteration 10700: Loss = -11383.579946357222
5
Iteration 10800: Loss = -11383.562373298872
6
Iteration 10900: Loss = -11383.55870294593
Iteration 11000: Loss = -11383.578060453947
1
Iteration 11100: Loss = -11383.568201715025
2
Iteration 11200: Loss = -11383.57612233455
3
Iteration 11300: Loss = -11383.559649883582
4
Iteration 11400: Loss = -11383.565131240119
5
Iteration 11500: Loss = -11383.592725412918
6
Iteration 11600: Loss = -11383.592050775851
7
Iteration 11700: Loss = -11383.562157356826
8
Iteration 11800: Loss = -11383.615441870892
9
Iteration 11900: Loss = -11383.56286620502
10
Stopping early at iteration 11900 due to no improvement.
tensor([[-8.5583,  6.4132],
        [-6.2568,  4.4559],
        [-0.4217, -3.2037],
        [ 7.1361, -8.5236],
        [ 6.8822, -8.3857],
        [ 6.1964, -8.9523],
        [-6.2706,  4.8401],
        [-4.1135,  1.9925],
        [-7.8165,  6.4302],
        [ 3.4880, -8.1033],
        [ 5.4209, -7.7241],
        [ 6.2833, -8.4976],
        [-4.7319,  3.1711],
        [-8.1188,  6.7043],
        [-6.9702,  5.2264],
        [ 5.0849, -6.4951],
        [-8.2610,  6.3086],
        [-6.9025,  5.4179],
        [ 6.1035, -7.5673],
        [ 5.9380, -8.5393],
        [-8.2731,  6.0025],
        [-8.4862,  7.0259],
        [ 5.8402, -7.6753],
        [-8.1894,  6.7676],
        [-5.6451,  4.0604],
        [-6.4124,  4.7136],
        [-8.5035,  4.5914],
        [-7.8879,  6.5009],
        [-5.6603,  4.2344],
        [-6.6099,  4.9166],
        [-0.1686, -1.2294],
        [ 3.4600, -4.9475],
        [-8.7087,  7.2826],
        [ 3.8616, -5.4555],
        [-7.4006,  5.5476],
        [ 3.9075, -5.2957],
        [ 3.8860, -5.7185],
        [ 6.4429, -9.5459],
        [-8.0822,  6.0856],
        [ 4.9139, -6.5496],
        [ 6.0119, -8.7509],
        [-5.8337,  4.4466],
        [-8.8874,  5.4904],
        [ 6.1271, -9.6064],
        [-8.6110,  6.6227],
        [-8.1352,  6.6908],
        [-4.7355,  3.3484],
        [-9.2349,  5.4407],
        [ 6.4086, -8.1164],
        [ 6.1600, -7.9275],
        [-8.2914,  5.6070],
        [-9.4466,  4.8314],
        [ 3.8048, -6.2091],
        [-8.6258,  6.8630],
        [-7.4869,  6.0964],
        [-8.4069,  6.8066],
        [ 4.3956, -5.7976],
        [ 6.9940, -8.8146],
        [ 7.0845, -8.4774],
        [ 6.4615, -8.8227],
        [-4.2694,  2.8363],
        [-5.6455,  4.2475],
        [ 6.2816, -9.3622],
        [-7.8267,  6.4019],
        [ 6.6195, -8.0277],
        [-8.8126,  6.5698],
        [ 3.8418, -6.3457],
        [-9.4909,  5.6821],
        [-7.7016,  5.9289],
        [ 1.8660, -4.9347],
        [ 6.7603, -8.5537],
        [ 6.7933, -8.2145],
        [-6.2404,  4.5935],
        [ 5.9922, -7.5915],
        [-8.7718,  7.3759],
        [ 6.1971, -7.6860],
        [ 5.3925, -6.9049],
        [ 4.0272, -5.4434],
        [-7.2713,  4.2319],
        [-7.6567,  6.1595],
        [ 6.6464, -8.2819],
        [ 6.2779, -7.9676],
        [ 5.0321, -6.5316],
        [-8.2445,  6.7377],
        [-9.4088,  4.8240],
        [-8.2056,  6.2599],
        [ 7.3485, -9.2373],
        [-5.7428,  3.9684],
        [-5.8532,  4.2560],
        [-3.6252,  2.0515],
        [-7.8043,  6.4134],
        [-8.7126,  4.6556],
        [-7.1617,  4.8966],
        [-7.6766,  6.0052],
        [ 6.8750, -8.3746],
        [-9.0756,  6.2358],
        [-6.2009,  4.4954],
        [ 4.5838, -6.2172],
        [ 6.7206, -8.2597],
        [-3.6439,  1.0424]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7558, 0.2442],
        [0.2560, 0.7440]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4372, 0.5628], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3974, 0.0985],
         [0.1760, 0.1951]],

        [[0.7519, 0.1004],
         [0.9511, 0.6389]],

        [[0.5854, 0.1126],
         [0.1525, 0.7223]],

        [[0.5489, 0.0880],
         [0.3403, 0.3469]],

        [[0.2105, 0.0865],
         [0.5770, 0.9018]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961289547104
Average Adjusted Rand Index: 0.9759994357994091
Iteration 0: Loss = -20389.520638892267
Iteration 10: Loss = -11375.442784138966
Iteration 20: Loss = -11375.445415524116
1
Iteration 30: Loss = -11375.44541552487
2
Iteration 40: Loss = -11375.44541552487
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7456, 0.2544],
        [0.2385, 0.7615]], dtype=torch.float64)
alpha: tensor([0.5090, 0.4910])
beta: tensor([[[0.1914, 0.0988],
         [0.7335, 0.3884]],

        [[0.1142, 0.1004],
         [0.5551, 0.7746]],

        [[0.0643, 0.1119],
         [0.3844, 0.9236]],

        [[0.9937, 0.0860],
         [0.9727, 0.7426]],

        [[0.1994, 0.0865],
         [0.4602, 0.7494]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20389.13498697566
Iteration 100: Loss = -12271.89741686124
Iteration 200: Loss = -11822.861833207326
Iteration 300: Loss = -11816.406650148117
Iteration 400: Loss = -11813.003321119955
Iteration 500: Loss = -11806.899242975634
Iteration 600: Loss = -11748.968885900478
Iteration 700: Loss = -11529.86577171484
Iteration 800: Loss = -11471.812927156456
Iteration 900: Loss = -11449.74467526644
Iteration 1000: Loss = -11429.855279428162
Iteration 1100: Loss = -11418.097474243716
Iteration 1200: Loss = -11416.112370249513
Iteration 1300: Loss = -11415.55469499321
Iteration 1400: Loss = -11409.48990540948
Iteration 1500: Loss = -11409.45829807071
Iteration 1600: Loss = -11409.433906723294
Iteration 1700: Loss = -11409.414490571584
Iteration 1800: Loss = -11409.397200807625
Iteration 1900: Loss = -11401.869676057862
Iteration 2000: Loss = -11401.856684302284
Iteration 2100: Loss = -11401.846686930245
Iteration 2200: Loss = -11401.840821172162
Iteration 2300: Loss = -11401.829967403559
Iteration 2400: Loss = -11401.788998168764
Iteration 2500: Loss = -11397.467824049181
Iteration 2600: Loss = -11397.45823194015
Iteration 2700: Loss = -11397.453650166983
Iteration 2800: Loss = -11397.449522926738
Iteration 2900: Loss = -11397.4460515002
Iteration 3000: Loss = -11397.444820894367
Iteration 3100: Loss = -11397.453313547338
1
Iteration 3200: Loss = -11397.437628176367
Iteration 3300: Loss = -11397.434406459668
Iteration 3400: Loss = -11397.432271681057
Iteration 3500: Loss = -11397.429659889418
Iteration 3600: Loss = -11397.426929899171
Iteration 3700: Loss = -11397.413477429645
Iteration 3800: Loss = -11389.865869882517
Iteration 3900: Loss = -11389.864501675858
Iteration 4000: Loss = -11389.863357070017
Iteration 4100: Loss = -11389.86248765108
Iteration 4200: Loss = -11389.86558181987
1
Iteration 4300: Loss = -11389.865853207291
2
Iteration 4400: Loss = -11389.860252872142
Iteration 4500: Loss = -11389.863847034656
1
Iteration 4600: Loss = -11389.857996355247
Iteration 4700: Loss = -11389.85738489072
Iteration 4800: Loss = -11389.861529812722
1
Iteration 4900: Loss = -11389.86100395538
2
Iteration 5000: Loss = -11389.854265156415
Iteration 5100: Loss = -11389.8535214696
Iteration 5200: Loss = -11389.85326545598
Iteration 5300: Loss = -11389.857282864032
1
Iteration 5400: Loss = -11389.852153765167
Iteration 5500: Loss = -11389.851687117822
Iteration 5600: Loss = -11389.851537856395
Iteration 5700: Loss = -11389.852508776596
1
Iteration 5800: Loss = -11389.943965985389
2
Iteration 5900: Loss = -11389.849843160284
Iteration 6000: Loss = -11389.849635443361
Iteration 6100: Loss = -11389.84904059034
Iteration 6200: Loss = -11389.84897632999
Iteration 6300: Loss = -11389.84769981845
Iteration 6400: Loss = -11389.804968549659
Iteration 6500: Loss = -11373.174349712224
Iteration 6600: Loss = -11373.222545313893
1
Iteration 6700: Loss = -11373.172854752063
Iteration 6800: Loss = -11373.17200242194
Iteration 6900: Loss = -11373.174435813538
1
Iteration 7000: Loss = -11373.290361580664
2
Iteration 7100: Loss = -11373.174297703541
3
Iteration 7200: Loss = -11373.172252791932
4
Iteration 7300: Loss = -11373.173161963361
5
Iteration 7400: Loss = -11373.18053714242
6
Iteration 7500: Loss = -11373.17044718764
Iteration 7600: Loss = -11373.171504027105
1
Iteration 7700: Loss = -11373.176791603748
2
Iteration 7800: Loss = -11373.178066280896
3
Iteration 7900: Loss = -11373.172482096941
4
Iteration 8000: Loss = -11373.180381000893
5
Iteration 8100: Loss = -11373.170702881875
6
Iteration 8200: Loss = -11373.170781934454
7
Iteration 8300: Loss = -11373.19461012836
8
Iteration 8400: Loss = -11373.1693014011
Iteration 8500: Loss = -11373.16941610534
1
Iteration 8600: Loss = -11373.270333708368
2
Iteration 8700: Loss = -11373.169013293367
Iteration 8800: Loss = -11373.17256939333
1
Iteration 8900: Loss = -11373.171258780028
2
Iteration 9000: Loss = -11373.18581660298
3
Iteration 9100: Loss = -11373.172461211225
4
Iteration 9200: Loss = -11373.173035791453
5
Iteration 9300: Loss = -11373.170869844873
6
Iteration 9400: Loss = -11373.172335480796
7
Iteration 9500: Loss = -11373.169246178823
8
Iteration 9600: Loss = -11373.17045706224
9
Iteration 9700: Loss = -11373.23392256426
10
Stopping early at iteration 9700 due to no improvement.
tensor([[  6.6499,  -8.2108],
        [  4.5664,  -6.1480],
        [ -2.1616,   0.6226],
        [ -9.3283,   7.1169],
        [ -9.4336,   6.7231],
        [ -9.2211,   7.2415],
        [  4.9123,  -6.3419],
        [  2.2273,  -3.8247],
        [  6.9839,  -8.5388],
        [ -7.1448,   4.3604],
        [ -7.2183,   5.7785],
        [ -8.1505,   6.7450],
        [  3.1990,  -4.6515],
        [  6.6137,  -9.1237],
        [  5.6813,  -7.0928],
        [ -6.5649,   5.1571],
        [  6.5063,  -7.9559],
        [  4.4343,  -7.8660],
        [ -8.1149,   6.3224],
        [ -8.4464,   6.5205],
        [  7.0441,  -8.4519],
        [  7.4327,  -8.8491],
        [ -8.1391,   6.1406],
        [  6.7343,  -8.1907],
        [  3.9061,  -5.7514],
        [  4.6103,  -6.5058],
        [  5.8028,  -7.2559],
        [  6.9221,  -8.3120],
        [  4.2190,  -5.7051],
        [  4.9295,  -6.5475],
        [ -2.1333,  -0.9976],
        [ -5.3729,   3.1196],
        [  5.5900,  -7.2258],
        [ -6.2159,   3.1787],
        [  5.4399,  -6.9372],
        [ -5.4404,   3.7436],
        [ -5.5175,   4.1267],
        [ -9.0688,   7.5943],
        [  5.2311,  -9.0805],
        [ -6.5996,   4.9108],
        [ -8.3847,   6.1258],
        [  4.3664,  -6.0171],
        [  5.4915,  -6.8946],
        [ -9.4569,   5.6370],
        [  6.7732,  -8.2308],
        [  6.7044,  -8.5238],
        [  3.1206,  -5.0040],
        [  6.2866,  -8.3490],
        [ -9.6557,   5.0404],
        [ -7.9932,   6.4320],
        [  6.1542,  -7.5540],
        [  5.9730,  -7.5228],
        [-10.0698,   6.0509],
        [  5.1971,  -9.8124],
        [  5.0584,  -8.8830],
        [  6.3877,  -8.6617],
        [ -6.7368,   3.5149],
        [ -9.2692,   7.4095],
        [ -9.3709,   6.3055],
        [ -8.1341,   6.7312],
        [  2.7713,  -4.2785],
        [  4.2687,  -5.6595],
        [ -8.5239,   6.7734],
        [  5.9847,  -7.3801],
        [ -8.7350,   6.7428],
        [  6.7037,  -8.2605],
        [ -7.4356,   2.8203],
        [  2.6054,  -4.8607],
        [  6.1096,  -7.5005],
        [ -4.2578,   2.5846],
        [ -8.5716,   7.0901],
        [ -8.4157,   7.0246],
        [  3.9564,  -6.9081],
        [ -9.3727,   7.7673],
        [  6.9868,  -8.3771],
        [ -7.6792,   6.2371],
        [ -7.2423,   5.2367],
        [ -9.5821,   8.0183],
        [  7.7255, -10.1949],
        [  6.8169,  -8.2929],
        [ -9.0233,   7.4713],
        [ -8.2879,   6.5792],
        [ -6.6689,   4.9586],
        [  6.6591,  -8.4716],
        [  5.0894,  -9.1212],
        [  7.0704,  -8.4584],
        [ -8.6628,   7.2083],
        [  3.5772,  -6.0704],
        [  7.4925,  -9.0556],
        [  2.1104,  -3.5660],
        [  6.4955,  -9.0112],
        [  5.9044,  -7.4388],
        [  6.2654,  -9.4639],
        [  5.6537,  -7.6070],
        [ -8.9923,   7.1289],
        [  6.2922,  -7.8786],
        [  4.6452,  -6.1247],
        [ -6.1203,   4.7068],
        [ -9.0736,   6.9239],
        [  1.6347,  -3.0500]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7456, 0.2544],
        [0.2382, 0.7618]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5641, 0.4359], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1937, 0.0987],
         [0.7335, 0.3980]],

        [[0.1142, 0.1009],
         [0.5551, 0.7746]],

        [[0.0643, 0.1126],
         [0.3844, 0.9236]],

        [[0.9937, 0.0859],
         [0.9727, 0.7426]],

        [[0.1994, 0.0865],
         [0.4602, 0.7494]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320165194881
Average Adjusted Rand Index: 0.9839996238662728
Iteration 0: Loss = -29192.96768126928
Iteration 10: Loss = -12180.148998678033
Iteration 20: Loss = -11896.870496174708
Iteration 30: Loss = -11375.445369248118
Iteration 40: Loss = -11375.445415483131
1
Iteration 50: Loss = -11375.44541548329
2
Iteration 60: Loss = -11375.44541548329
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7456, 0.2544],
        [0.2385, 0.7615]], dtype=torch.float64)
alpha: tensor([0.5090, 0.4910])
beta: tensor([[[0.1914, 0.0988],
         [0.9991, 0.3884]],

        [[0.4331, 0.1004],
         [0.0240, 0.3414]],

        [[0.4096, 0.1119],
         [0.5176, 0.8960]],

        [[0.2981, 0.0860],
         [0.4120, 0.3048]],

        [[0.7416, 0.0865],
         [0.5338, 0.3445]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29194.998058358353
Iteration 100: Loss = -12183.361337261655
Iteration 200: Loss = -12156.129865393732
Iteration 300: Loss = -12118.944869429675
Iteration 400: Loss = -12031.953136083168
Iteration 500: Loss = -11833.426770400227
Iteration 600: Loss = -11758.048759018307
Iteration 700: Loss = -11710.786945450634
Iteration 800: Loss = -11702.364757254254
Iteration 900: Loss = -11684.17908199965
Iteration 1000: Loss = -11682.968791137248
Iteration 1100: Loss = -11669.904332911397
Iteration 1200: Loss = -11669.513620532809
Iteration 1300: Loss = -11658.637725668015
Iteration 1400: Loss = -11652.486722683621
Iteration 1500: Loss = -11651.87910594164
Iteration 1600: Loss = -11651.506358064002
Iteration 1700: Loss = -11646.724163378613
Iteration 1800: Loss = -11646.590059450047
Iteration 1900: Loss = -11646.519386478974
Iteration 2000: Loss = -11646.462173198264
Iteration 2100: Loss = -11646.411549081271
Iteration 2200: Loss = -11646.339817577906
Iteration 2300: Loss = -11642.2848926144
Iteration 2400: Loss = -11642.086529974806
Iteration 2500: Loss = -11641.911691217498
Iteration 2600: Loss = -11636.783481014227
Iteration 2700: Loss = -11636.652671925061
Iteration 2800: Loss = -11636.395667773053
Iteration 2900: Loss = -11636.367604817426
Iteration 3000: Loss = -11636.327233202972
Iteration 3100: Loss = -11635.881540152379
Iteration 3200: Loss = -11635.776660980866
Iteration 3300: Loss = -11626.373422617144
Iteration 3400: Loss = -11626.194090153822
Iteration 3500: Loss = -11610.777989319255
Iteration 3600: Loss = -11610.208421327217
Iteration 3700: Loss = -11598.289186718415
Iteration 3800: Loss = -11596.555782403804
Iteration 3900: Loss = -11594.596146783626
Iteration 4000: Loss = -11589.331783635684
Iteration 4100: Loss = -11578.474367781011
Iteration 4200: Loss = -11574.054408605582
Iteration 4300: Loss = -11573.918441650663
Iteration 4400: Loss = -11568.96428415292
Iteration 4500: Loss = -11568.926590624738
Iteration 4600: Loss = -11562.098764071423
Iteration 4700: Loss = -11562.043295741916
Iteration 4800: Loss = -11562.027307906927
Iteration 4900: Loss = -11555.324121379368
Iteration 5000: Loss = -11552.857035992556
Iteration 5100: Loss = -11552.79301845048
Iteration 5200: Loss = -11547.842728413943
Iteration 5300: Loss = -11546.513963705815
Iteration 5400: Loss = -11537.149651549322
Iteration 5500: Loss = -11531.55812761007
Iteration 5600: Loss = -11531.436700743036
Iteration 5700: Loss = -11531.431979015324
Iteration 5800: Loss = -11531.42796887784
Iteration 5900: Loss = -11531.40835358347
Iteration 6000: Loss = -11519.253401055033
Iteration 6100: Loss = -11519.250704171678
Iteration 6200: Loss = -11519.240634687636
Iteration 6300: Loss = -11514.95025486226
Iteration 6400: Loss = -11514.870719316752
Iteration 6500: Loss = -11514.86825705706
Iteration 6600: Loss = -11514.865335952352
Iteration 6700: Loss = -11514.862369550876
Iteration 6800: Loss = -11514.845235383367
Iteration 6900: Loss = -11506.060655828862
Iteration 7000: Loss = -11493.930068294158
Iteration 7100: Loss = -11493.739169985896
Iteration 7200: Loss = -11493.73732937307
Iteration 7300: Loss = -11493.735763345816
Iteration 7400: Loss = -11493.734679901087
Iteration 7500: Loss = -11493.73370165923
Iteration 7600: Loss = -11493.733650197777
Iteration 7700: Loss = -11493.732215659784
Iteration 7800: Loss = -11493.76101667914
1
Iteration 7900: Loss = -11493.730968750162
Iteration 8000: Loss = -11493.73128203546
1
Iteration 8100: Loss = -11493.718532742067
Iteration 8200: Loss = -11483.298737414227
Iteration 8300: Loss = -11483.295829731558
Iteration 8400: Loss = -11483.295038079976
Iteration 8500: Loss = -11473.538667197405
Iteration 8600: Loss = -11473.434917347246
Iteration 8700: Loss = -11473.436552789832
1
Iteration 8800: Loss = -11473.446019982028
2
Iteration 8900: Loss = -11473.434873573458
Iteration 9000: Loss = -11473.454662519965
1
Iteration 9100: Loss = -11473.445737249165
2
Iteration 9200: Loss = -11473.438980591938
3
Iteration 9300: Loss = -11473.432140534207
Iteration 9400: Loss = -11473.433947752823
1
Iteration 9500: Loss = -11473.439650931636
2
Iteration 9600: Loss = -11473.43429471125
3
Iteration 9700: Loss = -11464.657171779034
Iteration 9800: Loss = -11464.497997760442
Iteration 9900: Loss = -11464.503213757305
1
Iteration 10000: Loss = -11464.49536846872
Iteration 10100: Loss = -11464.498905710323
1
Iteration 10200: Loss = -11464.493749993817
Iteration 10300: Loss = -11450.245315906008
Iteration 10400: Loss = -11450.255194706038
1
Iteration 10500: Loss = -11450.252849005203
2
Iteration 10600: Loss = -11447.580693074822
Iteration 10700: Loss = -11430.592495147283
Iteration 10800: Loss = -11430.618478452694
1
Iteration 10900: Loss = -11430.593483635841
2
Iteration 11000: Loss = -11430.596014897843
3
Iteration 11100: Loss = -11430.591808221827
Iteration 11200: Loss = -11430.593928046977
1
Iteration 11300: Loss = -11430.59734795428
2
Iteration 11400: Loss = -11430.620462599849
3
Iteration 11500: Loss = -11430.598609660694
4
Iteration 11600: Loss = -11430.589708445863
Iteration 11700: Loss = -11430.590742458544
1
Iteration 11800: Loss = -11430.590121486432
2
Iteration 11900: Loss = -11430.606341680388
3
Iteration 12000: Loss = -11430.599213938918
4
Iteration 12100: Loss = -11430.60285311091
5
Iteration 12200: Loss = -11430.600774233299
6
Iteration 12300: Loss = -11430.602362674725
7
Iteration 12400: Loss = -11430.619675910495
8
Iteration 12500: Loss = -11430.589504092783
Iteration 12600: Loss = -11430.616655105505
1
Iteration 12700: Loss = -11430.588711281316
Iteration 12800: Loss = -11430.593855888796
1
Iteration 12900: Loss = -11430.58789462511
Iteration 13000: Loss = -11430.592127180524
1
Iteration 13100: Loss = -11430.588861964685
2
Iteration 13200: Loss = -11430.589214775697
3
Iteration 13300: Loss = -11430.588178841257
4
Iteration 13400: Loss = -11430.591644604425
5
Iteration 13500: Loss = -11430.590069998489
6
Iteration 13600: Loss = -11430.593076025823
7
Iteration 13700: Loss = -11430.600788004891
8
Iteration 13800: Loss = -11430.588659727957
9
Iteration 13900: Loss = -11430.615308107364
10
Stopping early at iteration 13900 due to no improvement.
tensor([[  5.3067,  -7.6819],
        [  4.9339,  -6.4798],
        [ -0.9912,  -0.5340],
        [ -7.9688,   6.4077],
        [ -8.6620,   6.5908],
        [ -7.9696,   6.5104],
        [  4.2346,  -5.6234],
        [  0.2349,  -4.8501],
        [  5.7833,  -7.1702],
        [ -5.0735,   3.5673],
        [ -5.8339,   4.3949],
        [ -8.0506,   5.6922],
        [  1.8062,  -4.9380],
        [  6.2154,  -7.6107],
        [  4.5033,  -5.9288],
        [ -5.2574,   3.8587],
        [  7.4968, -10.1741],
        [  4.5345,  -5.9251],
        [ -8.0682,   5.6419],
        [ -7.2926,   5.8572],
        [  5.4898,  -6.8855],
        [  7.6178,  -9.4722],
        [ -6.6874,   4.9668],
        [  5.9204,  -7.6590],
        [  6.5447,  -7.9331],
        [  5.2121,  -6.8608],
        [  7.0511,  -9.0638],
        [  5.4396,  -8.9026],
        [  1.9317,  -6.5469],
        [  4.0352,  -5.4554],
        [ -1.4251,  -0.1709],
        [ -4.1546,   1.1208],
        [  4.7211,  -6.6874],
        [ -5.8249,   3.0257],
        [  5.8495,  -7.8780],
        [ -5.7287,   3.2785],
        [ -4.2609,   2.8491],
        [ -7.6431,   6.2331],
        [  5.2389,  -7.5127],
        [ -6.4283,   1.8131],
        [ -8.6767,   5.6374],
        [  3.3052,  -5.0908],
        [  4.3885,  -6.0054],
        [ -7.0498,   5.5713],
        [  5.6332,  -7.2331],
        [  6.2035,  -7.7426],
        [  2.5611,  -4.2380],
        [  5.7875,  -7.2693],
        [ -6.9246,   5.0148],
        [ -7.0178,   5.4841],
        [  6.4135, -10.0032],
        [  5.2907,  -7.0949],
        [ -7.8597,   6.2427],
        [  5.9571,  -8.2400],
        [  5.2000,  -6.5866],
        [  6.3816,  -7.9685],
        [ -3.2876,   1.8391],
        [ -8.5485,   6.8562],
        [ -7.0732,   5.5313],
        [ -8.1273,   6.1859],
        [  2.3297,  -3.7644],
        [  3.5258,  -4.9501],
        [ -6.0807,   4.6742],
        [  5.5028,  -7.0659],
        [ -7.9078,   6.0220],
        [  5.9057,  -7.6759],
        [ -4.9789,   2.4153],
        [  3.5878,  -4.9752],
        [  5.3626,  -6.7584],
        [ -5.1504,   1.5883],
        [ -8.5476,   6.0542],
        [ -7.9602,   6.4775],
        [  4.0685,  -5.9543],
        [ -5.9753,   4.5869],
        [  6.2229,  -7.7169],
        [ -6.9973,   5.3813],
        [ -6.7294,   5.1463],
        [ -3.8262,   2.3774],
        [  7.2646,  -8.7856],
        [  5.4195,  -7.5608],
        [  6.6204,  -9.8410],
        [ -7.0710,   5.6380],
        [ -5.3573,   3.8585],
        [  5.4756,  -8.6852],
        [  5.5270,  -7.7993],
        [  6.1982,  -7.5846],
        [  6.6950,  -9.4706],
        [  6.8376,  -8.8176],
        [  3.2904,  -5.5472],
        [  2.6232,  -4.5337],
        [  5.8390,  -7.2348],
        [  6.0042,  -7.9267],
        [  4.9264,  -6.3839],
        [  5.5285,  -6.9300],
        [ -7.9562,   6.4208],
        [  7.0281,  -8.4315],
        [  3.5801,  -5.5509],
        [ -4.6504,   1.0242],
        [ -7.8378,   6.4072],
        [  1.7705,  -4.3876]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7342, 0.2658],
        [0.2532, 0.7468]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5854, 0.4146], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1941, 0.1104],
         [0.9991, 0.3980]],

        [[0.4331, 0.1004],
         [0.0240, 0.3414]],

        [[0.4096, 0.1172],
         [0.5176, 0.8960]],

        [[0.2981, 0.0886],
         [0.4120, 0.3048]],

        [[0.7416, 0.0865],
         [0.5338, 0.3445]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524808995851451
Average Adjusted Rand Index: 0.9523212581991836
Iteration 0: Loss = -20978.908466557263
Iteration 10: Loss = -11375.442830009684
Iteration 20: Loss = -11375.445412495097
1
Iteration 30: Loss = -11375.44541552487
2
Iteration 40: Loss = -11375.44541552487
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7456, 0.2544],
        [0.2385, 0.7615]], dtype=torch.float64)
alpha: tensor([0.5090, 0.4910])
beta: tensor([[[0.1914, 0.0988],
         [0.9123, 0.3884]],

        [[0.0199, 0.1004],
         [0.2044, 0.0672]],

        [[0.5461, 0.1119],
         [0.4786, 0.4925]],

        [[0.3544, 0.0860],
         [0.6319, 0.7306]],

        [[0.3730, 0.0865],
         [0.4242, 0.7248]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20979.04577701409
Iteration 100: Loss = -11726.526567191206
Iteration 200: Loss = -11512.768717307425
Iteration 300: Loss = -11497.752362384086
Iteration 400: Loss = -11466.578603247708
Iteration 500: Loss = -11431.551381593652
Iteration 600: Loss = -11422.466653184005
Iteration 700: Loss = -11422.14217836188
Iteration 800: Loss = -11421.937178772476
Iteration 900: Loss = -11421.643532814413
Iteration 1000: Loss = -11397.92793250469
Iteration 1100: Loss = -11383.475124845934
Iteration 1200: Loss = -11383.4033404642
Iteration 1300: Loss = -11383.350564266351
Iteration 1400: Loss = -11383.309156753108
Iteration 1500: Loss = -11383.274420548974
Iteration 1600: Loss = -11383.240561493778
Iteration 1700: Loss = -11383.129967893157
Iteration 1800: Loss = -11373.31113284942
Iteration 1900: Loss = -11373.292127069348
Iteration 2000: Loss = -11373.277416912337
Iteration 2100: Loss = -11373.265034358603
Iteration 2200: Loss = -11373.254416494081
Iteration 2300: Loss = -11373.245199467618
Iteration 2400: Loss = -11373.237158077587
Iteration 2500: Loss = -11373.230083604878
Iteration 2600: Loss = -11373.223793944162
Iteration 2700: Loss = -11373.218211776053
Iteration 2800: Loss = -11373.21320236056
Iteration 2900: Loss = -11373.208742841995
Iteration 3000: Loss = -11373.204672206126
Iteration 3100: Loss = -11373.200990798221
Iteration 3200: Loss = -11373.197718401861
Iteration 3300: Loss = -11373.194706878758
Iteration 3400: Loss = -11373.191952069446
Iteration 3500: Loss = -11373.189422167423
Iteration 3600: Loss = -11373.187141397077
Iteration 3700: Loss = -11373.185055246027
Iteration 3800: Loss = -11373.183107864592
Iteration 3900: Loss = -11373.18127425315
Iteration 4000: Loss = -11373.17962991877
Iteration 4100: Loss = -11373.17806311106
Iteration 4200: Loss = -11373.176689072376
Iteration 4300: Loss = -11373.175325736116
Iteration 4400: Loss = -11373.174146196821
Iteration 4500: Loss = -11373.17300414364
Iteration 4600: Loss = -11373.17192129252
Iteration 4700: Loss = -11373.170905491686
Iteration 4800: Loss = -11373.170008294404
Iteration 4900: Loss = -11373.169170606558
Iteration 5000: Loss = -11373.168373960285
Iteration 5100: Loss = -11373.167583890676
Iteration 5200: Loss = -11373.166945971441
Iteration 5300: Loss = -11373.166321374092
Iteration 5400: Loss = -11373.165614840002
Iteration 5500: Loss = -11373.165087873478
Iteration 5600: Loss = -11373.169052048823
1
Iteration 5700: Loss = -11373.164058438013
Iteration 5800: Loss = -11373.164252670134
1
Iteration 5900: Loss = -11373.163163655925
Iteration 6000: Loss = -11373.162732282697
Iteration 6100: Loss = -11373.164464926329
1
Iteration 6200: Loss = -11373.162010110778
Iteration 6300: Loss = -11373.162448324409
1
Iteration 6400: Loss = -11373.161297103601
Iteration 6500: Loss = -11373.161330992405
1
Iteration 6600: Loss = -11373.160735879388
Iteration 6700: Loss = -11373.16374252122
1
Iteration 6800: Loss = -11373.165185501326
2
Iteration 6900: Loss = -11373.172665731458
3
Iteration 7000: Loss = -11373.160337165813
Iteration 7100: Loss = -11373.161405156443
1
Iteration 7200: Loss = -11373.159845727861
Iteration 7300: Loss = -11373.168044252801
1
Iteration 7400: Loss = -11373.159120960941
Iteration 7500: Loss = -11373.159026164845
Iteration 7600: Loss = -11373.16666220382
1
Iteration 7700: Loss = -11373.159964510043
2
Iteration 7800: Loss = -11373.199578796472
3
Iteration 7900: Loss = -11373.240401156243
4
Iteration 8000: Loss = -11373.158138180659
Iteration 8100: Loss = -11373.166816574152
1
Iteration 8200: Loss = -11373.16022167818
2
Iteration 8300: Loss = -11373.184520463501
3
Iteration 8400: Loss = -11373.164264014835
4
Iteration 8500: Loss = -11373.185755692422
5
Iteration 8600: Loss = -11373.162130659024
6
Iteration 8700: Loss = -11373.172708737153
7
Iteration 8800: Loss = -11373.159119341602
8
Iteration 8900: Loss = -11373.158612394363
9
Iteration 9000: Loss = -11373.157922898332
Iteration 9100: Loss = -11373.165733697488
1
Iteration 9200: Loss = -11373.192182103061
2
Iteration 9300: Loss = -11373.162451546359
3
Iteration 9400: Loss = -11373.16045584407
4
Iteration 9500: Loss = -11373.159265703123
5
Iteration 9600: Loss = -11373.159343441297
6
Iteration 9700: Loss = -11373.158631625958
7
Iteration 9800: Loss = -11373.15850225987
8
Iteration 9900: Loss = -11373.202580780584
9
Iteration 10000: Loss = -11373.158297311249
10
Stopping early at iteration 10000 due to no improvement.
tensor([[ 5.9097, -7.5892],
        [ 4.5751, -6.1195],
        [-2.0789,  0.6910],
        [-8.6087,  6.1957],
        [-8.2529,  6.7303],
        [-8.3965,  6.9625],
        [ 4.8128, -6.2065],
        [ 2.1473, -3.9088],
        [ 6.0153, -8.7359],
        [-6.7050,  4.8303],
        [-7.2598,  5.4694],
        [-7.8131,  6.3071],
        [ 2.9251, -4.9270],
        [ 7.1033, -8.7479],
        [ 5.3359, -6.8001],
        [-6.5864,  5.0226],
        [ 6.1013, -7.4876],
        [ 4.8948, -7.0487],
        [-7.2460,  5.4533],
        [-7.9603,  6.5548],
        [ 5.5039, -7.2006],
        [ 5.9337, -7.3355],
        [-7.4902,  5.8642],
        [ 5.9862, -7.4078],
        [ 3.7477, -5.9017],
        [ 4.6156, -6.2800],
        [ 5.0060, -7.1355],
        [ 5.7819, -7.5096],
        [ 4.2769, -5.6657],
        [ 4.5554, -6.9051],
        [-1.5586, -0.4377],
        [-6.5135,  1.8983],
        [ 5.4028, -6.7955],
        [-5.7183,  3.6705],
        [ 5.3789, -6.7834],
        [-6.8972,  2.2819],
        [-5.7076,  3.9276],
        [-8.8873,  7.2875],
        [ 6.1160, -7.5148],
        [-6.4508,  5.0394],
        [-7.5638,  6.1530],
        [ 4.1707, -6.1316],
        [ 5.8877, -7.3740],
        [-9.0787,  5.9900],
        [ 5.8617, -7.3658],
        [ 5.6616, -7.7843],
        [ 3.1354, -4.9591],
        [ 5.5903, -7.1319],
        [-7.8175,  5.4247],
        [-8.7518,  4.7913],
        [ 5.5050, -7.5255],
        [ 5.9514, -7.4557],
        [-6.0279,  4.0376],
        [ 6.0443, -8.1557],
        [ 5.1611, -6.8617],
        [ 7.0094, -8.5312],
        [-6.0823,  4.1298],
        [-8.9905,  6.5132],
        [-8.0489,  6.6465],
        [-7.3155,  5.8625],
        [ 1.9867, -5.0655],
        [ 4.0583, -5.8533],
        [-7.3593,  5.9727],
        [ 5.3270, -7.2319],
        [-8.4389,  5.8004],
        [ 4.6359, -9.2120],
        [-5.9604,  4.2565],
        [ 3.0412, -4.4283],
        [ 5.3541, -6.7422],
        [-4.1197,  2.7201],
        [-7.9398,  6.5457],
        [-8.0982,  6.6986],
        [ 4.7072, -6.1524],
        [-7.6743,  5.7889],
        [ 5.5891, -8.6170],
        [-7.5873,  6.0462],
        [-7.3207,  5.0520],
        [-5.8331,  3.6793],
        [ 4.9063, -6.2986],
        [ 5.7283, -7.6051],
        [-8.4240,  7.0284],
        [-8.3693,  6.1950],
        [-6.9641,  4.5914],
        [ 5.9598, -7.8090],
        [ 5.7266, -7.2717],
        [ 5.9668, -7.3533],
        [-7.8917,  6.3300],
        [ 4.0349, -5.6175],
        [ 3.6946, -6.4068],
        [ 1.9277, -3.7551],
        [ 5.4968, -7.1484],
        [ 5.3085, -6.8024],
        [ 4.9819, -6.7882],
        [ 5.7046, -7.1106],
        [-8.2596,  5.8448],
        [ 5.9835, -7.3915],
        [ 4.5748, -6.1527],
        [-6.3806,  4.3883],
        [-8.8206,  6.7156],
        [ 1.6512, -3.0422]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7461, 0.2539],
        [0.2381, 0.7619]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5624, 0.4376], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1953, 0.0985],
         [0.9123, 0.3970]],

        [[0.0199, 0.1005],
         [0.2044, 0.0672]],

        [[0.5461, 0.1126],
         [0.4786, 0.4925]],

        [[0.3544, 0.0859],
         [0.6319, 0.7306]],

        [[0.3730, 0.0865],
         [0.4242, 0.7248]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320165194881
Average Adjusted Rand Index: 0.9839996238662728
11393.596514248846
new:  [0.9760961289547104, 0.9840320165194881, 0.9524808995851451, 0.9840320165194881] [0.9759994357994091, 0.9839996238662728, 0.9523212581991836, 0.9839996238662728] [11383.56286620502, 11373.23392256426, 11430.615308107364, 11373.158297311249]
prior:  [0.9919999740011123, 0.9919999740011123, 0.9919999740011123, 0.9919999740011123] [0.9919998119331364, 0.9919998119331364, 0.9919998119331364, 0.9919998119331364] [11375.445409206925, 11375.44541552487, 11375.44541548329, 11375.44541552487]
-----------------------------------------------------------------------------------------
This iteration is 5
True Objective function: Loss = -11587.538732870691
Iteration 0: Loss = -27565.625809500583
Iteration 10: Loss = -12504.933265469976
Iteration 20: Loss = -12300.739097903683
Iteration 30: Loss = -11585.250885290377
Iteration 40: Loss = -11585.250978111158
1
Iteration 50: Loss = -11585.250978111158
2
Iteration 60: Loss = -11585.250978111158
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7636, 0.2364],
        [0.2858, 0.7142]], dtype=torch.float64)
alpha: tensor([0.5309, 0.4691])
beta: tensor([[[0.3906, 0.0962],
         [0.9078, 0.1983]],

        [[0.9816, 0.0953],
         [0.9346, 0.6605]],

        [[0.1402, 0.0922],
         [0.7682, 0.8993]],

        [[0.6949, 0.1020],
         [0.6299, 0.9856]],

        [[0.5060, 0.0907],
         [0.3590, 0.1337]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27253.109328815885
Iteration 100: Loss = -12505.150504746967
Iteration 200: Loss = -12453.579076383628
Iteration 300: Loss = -12213.397740337521
Iteration 400: Loss = -11792.286503505502
Iteration 500: Loss = -11652.338968145437
Iteration 600: Loss = -11627.481701005805
Iteration 700: Loss = -11602.452117155372
Iteration 800: Loss = -11587.172661201792
Iteration 900: Loss = -11586.782632145827
Iteration 1000: Loss = -11586.550805543986
Iteration 1100: Loss = -11586.390882367788
Iteration 1200: Loss = -11586.273768064348
Iteration 1300: Loss = -11586.184533752532
Iteration 1400: Loss = -11586.114377425749
Iteration 1500: Loss = -11586.057596862816
Iteration 1600: Loss = -11586.010936492112
Iteration 1700: Loss = -11585.972703712398
Iteration 1800: Loss = -11585.940662448085
Iteration 1900: Loss = -11585.913513787908
Iteration 2000: Loss = -11585.890285800442
Iteration 2100: Loss = -11585.870237219833
Iteration 2200: Loss = -11585.852762085517
Iteration 2300: Loss = -11585.837435040552
Iteration 2400: Loss = -11585.823933703536
Iteration 2500: Loss = -11585.811995775797
Iteration 2600: Loss = -11585.801336228897
Iteration 2700: Loss = -11585.79186265084
Iteration 2800: Loss = -11585.783322947238
Iteration 2900: Loss = -11585.775653199837
Iteration 3000: Loss = -11585.768713029576
Iteration 3100: Loss = -11585.76242884498
Iteration 3200: Loss = -11585.756633243891
Iteration 3300: Loss = -11585.751344291259
Iteration 3400: Loss = -11585.746483599616
Iteration 3500: Loss = -11585.742013863264
Iteration 3600: Loss = -11585.737880369561
Iteration 3700: Loss = -11585.73413519707
Iteration 3800: Loss = -11585.730748359554
Iteration 3900: Loss = -11585.727636230058
Iteration 4000: Loss = -11585.72474481268
Iteration 4100: Loss = -11585.7221359967
Iteration 4200: Loss = -11585.719653365204
Iteration 4300: Loss = -11585.717345593086
Iteration 4400: Loss = -11585.71519559433
Iteration 4500: Loss = -11585.713240682597
Iteration 4600: Loss = -11585.711423972443
Iteration 4700: Loss = -11585.709684105637
Iteration 4800: Loss = -11585.708121653675
Iteration 4900: Loss = -11585.707438215335
Iteration 5000: Loss = -11585.705252953348
Iteration 5100: Loss = -11585.703951793601
Iteration 5200: Loss = -11585.702681405503
Iteration 5300: Loss = -11585.720877936898
1
Iteration 5400: Loss = -11585.700472606273
Iteration 5500: Loss = -11585.700294151138
Iteration 5600: Loss = -11585.698542777514
Iteration 5700: Loss = -11585.697665563595
Iteration 5800: Loss = -11585.696781962524
Iteration 5900: Loss = -11585.695985721954
Iteration 6000: Loss = -11585.6961335803
1
Iteration 6100: Loss = -11585.694596115276
Iteration 6200: Loss = -11585.693935581749
Iteration 6300: Loss = -11585.69335490852
Iteration 6400: Loss = -11585.692697583729
Iteration 6500: Loss = -11585.766239863771
1
Iteration 6600: Loss = -11585.69269483048
Iteration 6700: Loss = -11585.691367100484
Iteration 6800: Loss = -11585.698967070026
1
Iteration 6900: Loss = -11585.692855374296
2
Iteration 7000: Loss = -11585.69002061814
Iteration 7100: Loss = -11585.699958208761
1
Iteration 7200: Loss = -11585.691090913946
2
Iteration 7300: Loss = -11585.700459182091
3
Iteration 7400: Loss = -11585.697366616081
4
Iteration 7500: Loss = -11585.703340827371
5
Iteration 7600: Loss = -11585.695699025084
6
Iteration 7700: Loss = -11585.687530594774
Iteration 7800: Loss = -11585.69627300037
1
Iteration 7900: Loss = -11585.687566578945
2
Iteration 8000: Loss = -11585.686779503209
Iteration 8100: Loss = -11585.702408540283
1
Iteration 8200: Loss = -11585.692468636793
2
Iteration 8300: Loss = -11585.686159402301
Iteration 8400: Loss = -11585.686511652999
1
Iteration 8500: Loss = -11585.744501933666
2
Iteration 8600: Loss = -11585.692768026836
3
Iteration 8700: Loss = -11585.685468137788
Iteration 8800: Loss = -11585.703363010944
1
Iteration 8900: Loss = -11585.716037059454
2
Iteration 9000: Loss = -11585.69083412935
3
Iteration 9100: Loss = -11585.685794448416
4
Iteration 9200: Loss = -11585.695551909535
5
Iteration 9300: Loss = -11584.00049595592
Iteration 9400: Loss = -11584.030080560635
1
Iteration 9500: Loss = -11583.997906113853
Iteration 9600: Loss = -11583.864124023758
Iteration 9700: Loss = -11583.864553873793
1
Iteration 9800: Loss = -11583.847986058041
Iteration 9900: Loss = -11583.901295184514
1
Iteration 10000: Loss = -11583.847863632662
Iteration 10100: Loss = -11583.84857338162
1
Iteration 10200: Loss = -11583.87764718779
2
Iteration 10300: Loss = -11583.848114364973
3
Iteration 10400: Loss = -11583.848673337206
4
Iteration 10500: Loss = -11583.871561863181
5
Iteration 10600: Loss = -11583.853046290005
6
Iteration 10700: Loss = -11583.85881929448
7
Iteration 10800: Loss = -11583.846927374332
Iteration 10900: Loss = -11583.86246118776
1
Iteration 11000: Loss = -11583.849575379856
2
Iteration 11100: Loss = -11583.846930342219
3
Iteration 11200: Loss = -11583.848721709695
4
Iteration 11300: Loss = -11583.855617421854
5
Iteration 11400: Loss = -11583.848386135263
6
Iteration 11500: Loss = -11583.879026358585
7
Iteration 11600: Loss = -11583.848932073719
8
Iteration 11700: Loss = -11583.87399434944
9
Iteration 11800: Loss = -11583.846730645695
Iteration 11900: Loss = -11583.893356171882
1
Iteration 12000: Loss = -11583.900317561729
2
Iteration 12100: Loss = -11583.849293679203
3
Iteration 12200: Loss = -11583.853404491016
4
Iteration 12300: Loss = -11583.855114825114
5
Iteration 12400: Loss = -11583.849977953381
6
Iteration 12500: Loss = -11583.895103243727
7
Iteration 12600: Loss = -11583.846379935601
Iteration 12700: Loss = -11583.84959892319
1
Iteration 12800: Loss = -11583.854290131789
2
Iteration 12900: Loss = -11583.85501904576
3
Iteration 13000: Loss = -11583.847845886723
4
Iteration 13100: Loss = -11583.869181631637
5
Iteration 13200: Loss = -11583.855185545894
6
Iteration 13300: Loss = -11583.84630968551
Iteration 13400: Loss = -11583.847569432008
1
Iteration 13500: Loss = -11583.847032797426
2
Iteration 13600: Loss = -11583.850232059489
3
Iteration 13700: Loss = -11583.848453470066
4
Iteration 13800: Loss = -11583.856603085065
5
Iteration 13900: Loss = -11583.85115373787
6
Iteration 14000: Loss = -11583.863395089076
7
Iteration 14100: Loss = -11583.885203869322
8
Iteration 14200: Loss = -11583.84947688018
9
Iteration 14300: Loss = -11583.847131949633
10
Stopping early at iteration 14300 due to no improvement.
tensor([[-10.9252,   6.3100],
        [  4.9922,  -9.6074],
        [ -8.9678,   4.3526],
        [-10.9656,   6.3504],
        [  5.8489, -10.4641],
        [ -7.4620,   2.8468],
        [ -8.0244,   3.4092],
        [-10.6947,   6.0795],
        [  5.1854,  -9.8006],
        [-10.4172,   5.8020],
        [ -9.9457,   5.3305],
        [-11.0468,   6.4316],
        [  3.7789,  -8.3941],
        [-10.9321,   6.3169],
        [-11.0298,   6.4146],
        [  5.1372,  -9.7525],
        [-10.0422,   5.4269],
        [ -9.9591,   5.3438],
        [  3.0259,  -7.6411],
        [-11.1925,   6.5773],
        [ -9.6069,   4.9917],
        [  2.4562,  -7.0714],
        [  4.9217,  -9.5369],
        [-10.7753,   6.1601],
        [ -7.3941,   2.7789],
        [  4.7283,  -9.3435],
        [  4.6131,  -9.2283],
        [  5.2199,  -9.8351],
        [  4.1943,  -8.8095],
        [-10.1399,   5.5247],
        [-10.2786,   5.6634],
        [-10.7178,   6.1026],
        [  4.2857,  -8.9009],
        [-10.0490,   5.4337],
        [  5.2389,  -9.8541],
        [  5.1296,  -9.7448],
        [  4.7674,  -9.3826],
        [  4.5437,  -9.1589],
        [ -7.9336,   3.3183],
        [ -9.6577,   5.0425],
        [  4.9514,  -9.5666],
        [ -7.4994,   2.8842],
        [  1.0196,  -5.6348],
        [  1.0147,  -5.6299],
        [  4.6265,  -9.2417],
        [  5.3474,  -9.9626],
        [-10.9861,   6.3709],
        [ -9.1674,   4.5521],
        [-10.6124,   5.9972],
        [-10.7770,   6.1618],
        [  4.5846,  -9.1998],
        [  5.1709,  -9.7861],
        [-10.9036,   6.2884],
        [  4.5667,  -9.1819],
        [  4.9981,  -9.6133],
        [ -0.3745,  -4.2407],
        [-10.8919,   6.2767],
        [  4.3142,  -8.9294],
        [  5.0956,  -9.7108],
        [ -6.5631,   1.9479],
        [-10.0624,   5.4472],
        [ -9.6739,   5.0587],
        [ -9.2479,   4.6327],
        [ -6.1206,   1.5054],
        [  4.1219,  -8.7371],
        [-10.6758,   6.0605],
        [  4.9138,  -9.5290],
        [  4.5587,  -9.1739],
        [-10.0934,   5.4782],
        [  5.3958, -10.0110],
        [ -5.1941,   0.5789],
        [-10.4655,   5.8503],
        [ -7.9124,   3.2972],
        [ -8.7475,   4.1323],
        [-10.0922,   5.4770],
        [  1.6296,  -6.2448],
        [  5.4414, -10.0566],
        [-10.9582,   6.3429],
        [  4.9587,  -9.5739],
        [ -9.7322,   5.1169],
        [-10.5868,   5.9716],
        [  1.5412,  -6.1564],
        [  3.3874,  -8.0026],
        [  5.0581,  -9.6734],
        [  3.7026,  -8.3178],
        [  4.8766,  -9.4918],
        [  4.7460,  -9.3612],
        [-11.2143,   6.5991],
        [-10.5437,   5.9284],
        [ -7.0901,   2.4749],
        [  4.8845,  -9.4998],
        [  3.3239,  -7.9391],
        [  4.3293,  -8.9446],
        [  3.4144,  -8.0297],
        [  4.9587,  -9.5740],
        [-11.2203,   6.6051],
        [  4.6862,  -9.3014],
        [ -7.3126,   2.6974],
        [ -9.4567,   4.8415],
        [ -9.2759,   4.6607]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7149, 0.2851],
        [0.2367, 0.7633]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4814, 0.5186], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2024, 0.0961],
         [0.9078, 0.3986]],

        [[0.9816, 0.0957],
         [0.9346, 0.6605]],

        [[0.1402, 0.0920],
         [0.7682, 0.8993]],

        [[0.6949, 0.1020],
         [0.6299, 0.9856]],

        [[0.5060, 0.0907],
         [0.3590, 0.1337]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -23530.775287464126
Iteration 10: Loss = -11585.252894340372
Iteration 20: Loss = -11585.250978111158
Iteration 30: Loss = -11585.250978111158
1
Iteration 40: Loss = -11585.250978111158
2
Iteration 50: Loss = -11585.250978111158
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7636, 0.2364],
        [0.2858, 0.7142]], dtype=torch.float64)
alpha: tensor([0.5309, 0.4691])
beta: tensor([[[0.3906, 0.0962],
         [0.4128, 0.1983]],

        [[0.0931, 0.0953],
         [0.9812, 0.7525]],

        [[0.8471, 0.0922],
         [0.6040, 0.6723]],

        [[0.5959, 0.1020],
         [0.0561, 0.7442]],

        [[0.8097, 0.0907],
         [0.5103, 0.8852]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23530.827091123192
Iteration 100: Loss = -12518.909845690618
Iteration 200: Loss = -12224.797955673966
Iteration 300: Loss = -12012.95473236496
Iteration 400: Loss = -11886.249366716638
Iteration 500: Loss = -11878.860442746627
Iteration 600: Loss = -11864.379084660164
Iteration 700: Loss = -11853.701881095449
Iteration 800: Loss = -11845.224469867351
Iteration 900: Loss = -11833.390351923394
Iteration 1000: Loss = -11825.226965795695
Iteration 1100: Loss = -11814.454744333472
Iteration 1200: Loss = -11807.723777682202
Iteration 1300: Loss = -11791.849259314893
Iteration 1400: Loss = -11787.500264197248
Iteration 1500: Loss = -11787.405850800622
Iteration 1600: Loss = -11785.56088827175
Iteration 1700: Loss = -11777.253064672852
Iteration 1800: Loss = -11777.220707243481
Iteration 1900: Loss = -11777.196025464276
Iteration 2000: Loss = -11777.176650739659
Iteration 2100: Loss = -11777.15647692587
Iteration 2200: Loss = -11771.202626849015
Iteration 2300: Loss = -11771.188565638722
Iteration 2400: Loss = -11766.466963726798
Iteration 2500: Loss = -11766.432673692703
Iteration 2600: Loss = -11764.7954766269
Iteration 2700: Loss = -11764.770918061626
Iteration 2800: Loss = -11764.763511906045
Iteration 2900: Loss = -11764.743493299595
Iteration 3000: Loss = -11761.796454831914
Iteration 3100: Loss = -11761.785244751196
Iteration 3200: Loss = -11756.259938902034
Iteration 3300: Loss = -11756.248007671777
Iteration 3400: Loss = -11756.186647317636
Iteration 3500: Loss = -11742.833056394711
Iteration 3600: Loss = -11727.447370485977
Iteration 3700: Loss = -11727.440754671374
Iteration 3800: Loss = -11727.4352978522
Iteration 3900: Loss = -11718.529553834655
Iteration 4000: Loss = -11718.524816781395
Iteration 4100: Loss = -11718.52218838217
Iteration 4200: Loss = -11718.519858836085
Iteration 4300: Loss = -11718.517951823804
Iteration 4400: Loss = -11718.515528629114
Iteration 4500: Loss = -11709.049433926355
Iteration 4600: Loss = -11709.046626896528
Iteration 4700: Loss = -11709.044485480557
Iteration 4800: Loss = -11697.585592119398
Iteration 4900: Loss = -11697.58187892744
Iteration 5000: Loss = -11684.825922871627
Iteration 5100: Loss = -11684.79249544432
Iteration 5200: Loss = -11684.78972879889
Iteration 5300: Loss = -11684.787853568821
Iteration 5400: Loss = -11684.786425662034
Iteration 5500: Loss = -11684.785351710736
Iteration 5600: Loss = -11684.784410754744
Iteration 5700: Loss = -11684.784139252215
Iteration 5800: Loss = -11684.7829159533
Iteration 5900: Loss = -11684.78220850046
Iteration 6000: Loss = -11684.78152926844
Iteration 6100: Loss = -11684.78351463441
1
Iteration 6200: Loss = -11684.780193498724
Iteration 6300: Loss = -11684.780144413648
Iteration 6400: Loss = -11684.779366622943
Iteration 6500: Loss = -11684.77894000099
Iteration 6600: Loss = -11684.783138668008
1
Iteration 6700: Loss = -11672.903216281084
Iteration 6800: Loss = -11672.889868835386
Iteration 6900: Loss = -11672.88688703993
Iteration 7000: Loss = -11672.897751858487
1
Iteration 7100: Loss = -11672.920746741143
2
Iteration 7200: Loss = -11672.886378854875
Iteration 7300: Loss = -11656.650462523241
Iteration 7400: Loss = -11656.656245747183
1
Iteration 7500: Loss = -11656.649692515455
Iteration 7600: Loss = -11656.654793435802
1
Iteration 7700: Loss = -11656.661914731629
2
Iteration 7800: Loss = -11656.650124801832
3
Iteration 7900: Loss = -11656.659995068481
4
Iteration 8000: Loss = -11656.649675031611
Iteration 8100: Loss = -11656.6553187129
1
Iteration 8200: Loss = -11656.662097848686
2
Iteration 8300: Loss = -11656.646890276808
Iteration 8400: Loss = -11656.648715898116
1
Iteration 8500: Loss = -11656.649797509828
2
Iteration 8600: Loss = -11656.64615640792
Iteration 8700: Loss = -11634.771866063284
Iteration 8800: Loss = -11634.771691676271
Iteration 8900: Loss = -11634.771966192397
1
Iteration 9000: Loss = -11634.856188371714
2
Iteration 9100: Loss = -11634.771449181377
Iteration 9200: Loss = -11634.771503534843
1
Iteration 9300: Loss = -11634.780683676407
2
Iteration 9400: Loss = -11634.781681326403
3
Iteration 9500: Loss = -11619.206822026623
Iteration 9600: Loss = -11619.229579938221
1
Iteration 9700: Loss = -11619.19951142915
Iteration 9800: Loss = -11619.229272601313
1
Iteration 9900: Loss = -11619.19870664622
Iteration 10000: Loss = -11619.199142846046
1
Iteration 10100: Loss = -11619.20520258094
2
Iteration 10200: Loss = -11619.197638429167
Iteration 10300: Loss = -11619.22133906129
1
Iteration 10400: Loss = -11619.200733182097
2
Iteration 10500: Loss = -11619.20462168407
3
Iteration 10600: Loss = -11619.27401110273
4
Iteration 10700: Loss = -11619.196754858522
Iteration 10800: Loss = -11619.202286028323
1
Iteration 10900: Loss = -11606.331129999428
Iteration 11000: Loss = -11606.320129226984
Iteration 11100: Loss = -11606.292388023037
Iteration 11200: Loss = -11606.291842862622
Iteration 11300: Loss = -11606.297030986874
1
Iteration 11400: Loss = -11606.299781366844
2
Iteration 11500: Loss = -11606.308480986267
3
Iteration 11600: Loss = -11606.294246449024
4
Iteration 11700: Loss = -11606.295914901133
5
Iteration 11800: Loss = -11606.29444461407
6
Iteration 11900: Loss = -11606.297358703883
7
Iteration 12000: Loss = -11606.316696623371
8
Iteration 12100: Loss = -11606.31264788566
9
Iteration 12200: Loss = -11606.293928528337
10
Stopping early at iteration 12200 due to no improvement.
tensor([[  7.5919,  -9.7279],
        [ -9.5522,   6.1532],
        [  6.6861, -11.3013],
        [  7.0417,  -8.7355],
        [ -7.9962,   6.0276],
        [  7.2962, -10.4617],
        [  6.9444,  -8.4356],
        [  7.3144,  -8.9202],
        [ -7.7510,   5.9933],
        [  6.5380,  -8.9963],
        [  7.3430,  -8.8894],
        [  7.5274,  -9.0993],
        [ -7.5253,   4.6816],
        [  7.9833,  -9.8881],
        [  7.4839,  -8.9073],
        [ -9.3369,   7.0386],
        [  7.5284,  -9.2555],
        [  7.2399,  -8.8058],
        [ -6.1094,   4.6359],
        [  7.8953,  -9.4802],
        [  6.3649,  -7.7652],
        [ -7.0670,   2.4518],
        [ -9.8877,   5.7399],
        [  7.5164,  -9.0105],
        [  4.3753,  -5.8595],
        [ -7.9707,   6.3997],
        [-11.7245,   7.1092],
        [ -8.4828,   7.0603],
        [ -7.3555,   5.8915],
        [  7.4118,  -8.8858],
        [  6.4914,  -9.0270],
        [  7.2098,  -9.0986],
        [ -9.4154,   6.8186],
        [  7.2997,  -8.7761],
        [ -8.3044,   6.9148],
        [ -9.6237,   7.6451],
        [ -9.3131,   6.2037],
        [ -8.9984,   5.2305],
        [  6.5950,  -8.6876],
        [  6.7350,  -8.1213],
        [ -9.5732,   7.3255],
        [  7.2923,  -8.6787],
        [ -8.0425,   6.5468],
        [ -5.1758,   1.4247],
        [ -7.6531,   6.2661],
        [ -8.2978,   6.7879],
        [  7.6554, -10.2932],
        [  7.0916, -10.7986],
        [  7.6887, -10.5924],
        [  7.7394,  -9.1257],
        [ -8.6910,   5.3221],
        [ -8.1433,   6.6766],
        [  7.5140,  -9.2128],
        [ -7.4835,   6.0962],
        [ -7.6869,   6.1522],
        [ -8.6456,   7.0938],
        [  7.9698,  -9.5125],
        [-12.0044,   7.3892],
        [ -7.9184,   6.4015],
        [  7.3675,  -8.8036],
        [  7.5912,  -9.0063],
        [  6.1322,  -8.2741],
        [  7.1945,  -8.5841],
        [  3.0899,  -4.5503],
        [ -8.7591,   7.1392],
        [  8.0297,  -9.5116],
        [ -9.7794,   5.1642],
        [ -7.6037,   6.2031],
        [  6.7235,  -8.1364],
        [ -9.2598,   7.7038],
        [  7.0326,  -8.4189],
        [  6.9101,  -8.3292],
        [  7.3034,  -8.8290],
        [  7.1387,  -8.9883],
        [  7.5687, -10.1656],
        [ -4.6606,   3.2479],
        [ -8.4684,   6.9502],
        [  7.1509, -10.3890],
        [ -7.6453,   6.2135],
        [  7.3060, -10.2287],
        [  7.9022, -10.2681],
        [ -4.8643,   2.8271],
        [ -6.5707,   4.9678],
        [ -8.3936,   6.9119],
        [ -8.8850,   7.1294],
        [ -7.7345,   6.3050],
        [ -9.1149,   7.5626],
        [  7.3368, -10.1142],
        [  7.7073,  -9.1647],
        [  6.3918,  -7.8103],
        [-10.3955,   5.7803],
        [ -6.5755,   4.6624],
        [ -8.3930,   6.9130],
        [ -7.1802,   4.3644],
        [ -8.1534,   6.7258],
        [  7.3407,  -8.7966],
        [ -9.3773,   7.9370],
        [  4.3055,  -5.7168],
        [  6.0903,  -7.6783],
        [  7.2587,  -9.1364]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7505, 0.2495],
        [0.2952, 0.7048]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5175, 0.4825], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3995, 0.0962],
         [0.4128, 0.2010]],

        [[0.0931, 0.0957],
         [0.9812, 0.7525]],

        [[0.8471, 0.0922],
         [0.6040, 0.6723]],

        [[0.5959, 0.1098],
         [0.0561, 0.7442]],

        [[0.8097, 0.0907],
         [0.5103, 0.8852]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320460697916
Average Adjusted Rand Index: 0.9841602586080228
Iteration 0: Loss = -21044.400093456905
Iteration 10: Loss = -12266.158435747475
Iteration 20: Loss = -11585.250976980771
Iteration 30: Loss = -11585.25097599101
Iteration 40: Loss = -11585.25097599101
1
Iteration 50: Loss = -11585.25097599101
2
Iteration 60: Loss = -11585.25097599101
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7142, 0.2858],
        [0.2364, 0.7636]], dtype=torch.float64)
alpha: tensor([0.4691, 0.5309])
beta: tensor([[[0.1983, 0.0962],
         [0.0057, 0.3906]],

        [[0.3159, 0.0953],
         [0.4181, 0.3317]],

        [[0.7790, 0.0922],
         [0.9228, 0.9330]],

        [[0.6933, 0.1020],
         [0.2685, 0.7974]],

        [[0.4331, 0.0907],
         [0.8943, 0.0100]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21044.467853176222
Iteration 100: Loss = -12479.09293702953
Iteration 200: Loss = -12085.74412014565
Iteration 300: Loss = -11966.241030025369
Iteration 400: Loss = -11864.63661197565
Iteration 500: Loss = -11761.851495842346
Iteration 600: Loss = -11747.736644235772
Iteration 700: Loss = -11731.438407700442
Iteration 800: Loss = -11730.78605284054
Iteration 900: Loss = -11713.990288712448
Iteration 1000: Loss = -11713.83768302102
Iteration 1100: Loss = -11713.732272754667
Iteration 1200: Loss = -11713.654436247814
Iteration 1300: Loss = -11713.59427768864
Iteration 1400: Loss = -11713.546594383683
Iteration 1500: Loss = -11713.508716673268
Iteration 1600: Loss = -11713.477349586155
Iteration 1700: Loss = -11713.449781714497
Iteration 1800: Loss = -11713.414916927277
Iteration 1900: Loss = -11709.512428156162
Iteration 2000: Loss = -11709.491300649486
Iteration 2100: Loss = -11709.473046143074
Iteration 2200: Loss = -11705.911515579119
Iteration 2300: Loss = -11705.899150679534
Iteration 2400: Loss = -11705.8882058074
Iteration 2500: Loss = -11705.877897470747
Iteration 2600: Loss = -11705.867654446596
Iteration 2700: Loss = -11705.842721256842
Iteration 2800: Loss = -11691.931347410453
Iteration 2900: Loss = -11691.914783077546
Iteration 3000: Loss = -11679.203887051817
Iteration 3100: Loss = -11679.173927852955
Iteration 3200: Loss = -11679.168598401453
Iteration 3300: Loss = -11679.162115954921
Iteration 3400: Loss = -11673.744202233685
Iteration 3500: Loss = -11673.744450593893
1
Iteration 3600: Loss = -11673.7332305506
Iteration 3700: Loss = -11673.73012491453
Iteration 3800: Loss = -11673.727471685368
Iteration 3900: Loss = -11673.72479742098
Iteration 4000: Loss = -11673.722290682592
Iteration 4100: Loss = -11673.731693985968
1
Iteration 4200: Loss = -11667.549988860195
Iteration 4300: Loss = -11667.546776282074
Iteration 4400: Loss = -11667.160903302385
Iteration 4500: Loss = -11656.38035295775
Iteration 4600: Loss = -11656.382695528542
1
Iteration 4700: Loss = -11656.378061493986
Iteration 4800: Loss = -11656.376491442541
Iteration 4900: Loss = -11656.37215534785
Iteration 5000: Loss = -11633.037228018518
Iteration 5100: Loss = -11632.996309808448
Iteration 5200: Loss = -11633.005871070942
1
Iteration 5300: Loss = -11632.993241865757
Iteration 5400: Loss = -11632.992017097062
Iteration 5500: Loss = -11632.991042672116
Iteration 5600: Loss = -11632.990077926648
Iteration 5700: Loss = -11633.000072204219
1
Iteration 5800: Loss = -11625.958823468462
Iteration 5900: Loss = -11625.957260675343
Iteration 6000: Loss = -11625.959451739482
1
Iteration 6100: Loss = -11625.9559572519
Iteration 6200: Loss = -11625.955387405378
Iteration 6300: Loss = -11625.986369724986
1
Iteration 6400: Loss = -11625.954372468548
Iteration 6500: Loss = -11625.95395307062
Iteration 6600: Loss = -11625.954193954103
1
Iteration 6700: Loss = -11625.953521331567
Iteration 6800: Loss = -11625.952763357958
Iteration 6900: Loss = -11625.9525724098
Iteration 7000: Loss = -11625.952182615174
Iteration 7100: Loss = -11625.951836446855
Iteration 7200: Loss = -11625.965606538633
1
Iteration 7300: Loss = -11625.951396997063
Iteration 7400: Loss = -11625.951078380136
Iteration 7500: Loss = -11625.953959682543
1
Iteration 7600: Loss = -11625.950690397547
Iteration 7700: Loss = -11625.955507236155
1
Iteration 7800: Loss = -11625.950814022282
2
Iteration 7900: Loss = -11625.955283530642
3
Iteration 8000: Loss = -11625.94998876407
Iteration 8100: Loss = -11625.949694284212
Iteration 8200: Loss = -11625.951311792232
1
Iteration 8300: Loss = -11625.949389590887
Iteration 8400: Loss = -11625.976002635367
1
Iteration 8500: Loss = -11626.032765386377
2
Iteration 8600: Loss = -11625.95535780591
3
Iteration 8700: Loss = -11625.955598857969
4
Iteration 8800: Loss = -11625.976134404566
5
Iteration 8900: Loss = -11612.042822687452
Iteration 9000: Loss = -11612.041095472909
Iteration 9100: Loss = -11596.945780260388
Iteration 9200: Loss = -11596.936158360435
Iteration 9300: Loss = -11596.946012705834
1
Iteration 9400: Loss = -11596.940175776223
2
Iteration 9500: Loss = -11596.936963549533
3
Iteration 9600: Loss = -11596.939626604086
4
Iteration 9700: Loss = -11597.019776904826
5
Iteration 9800: Loss = -11596.938949174222
6
Iteration 9900: Loss = -11596.934904596468
Iteration 10000: Loss = -11596.944925868585
1
Iteration 10100: Loss = -11596.939849817438
2
Iteration 10200: Loss = -11596.940019817499
3
Iteration 10300: Loss = -11596.942770912641
4
Iteration 10400: Loss = -11596.937068986226
5
Iteration 10500: Loss = -11596.937348793217
6
Iteration 10600: Loss = -11596.937309679093
7
Iteration 10700: Loss = -11596.967704071663
8
Iteration 10800: Loss = -11596.934630811194
Iteration 10900: Loss = -11596.935808292277
1
Iteration 11000: Loss = -11596.937667422548
2
Iteration 11100: Loss = -11596.944619623071
3
Iteration 11200: Loss = -11596.959210012423
4
Iteration 11300: Loss = -11597.071620775514
5
Iteration 11400: Loss = -11596.93590508324
6
Iteration 11500: Loss = -11596.976196600788
7
Iteration 11600: Loss = -11596.945291920289
8
Iteration 11700: Loss = -11596.961101530413
9
Iteration 11800: Loss = -11596.946104828605
10
Stopping early at iteration 11800 due to no improvement.
tensor([[ -8.4441,   7.0356],
        [  6.1732,  -7.9373],
        [ -8.0092,   5.5334],
        [ -8.6334,   7.2143],
        [  6.2510,  -7.6626],
        [ -7.0277,   3.2181],
        [ -6.5647,   4.8295],
        [ -7.9944,   6.5956],
        [  6.1219,  -7.6094],
        [ -8.4885,   7.0953],
        [ -7.5545,   6.1680],
        [ -8.7134,   6.9600],
        [  5.3458,  -6.7378],
        [ -8.2373,   6.7750],
        [ -9.1770,   6.5106],
        [  6.4808,  -7.8856],
        [ -8.8563,   6.6177],
        [ -9.5262,   6.6896],
        [  4.6158,  -6.0913],
        [-10.0226,   8.0504],
        [ -7.7264,   6.3070],
        [  4.0391,  -5.4972],
        [  6.4355,  -7.9145],
        [-11.0332,   6.7516],
        [ -5.8621,   4.3406],
        [  5.9780,  -7.3827],
        [  6.1941,  -8.8030],
        [  6.3861,  -7.8824],
        [  5.6190,  -7.0055],
        [ -8.6619,   6.8668],
        [ -7.3125,   5.9262],
        [ -7.9584,   6.4067],
        [  5.3773,  -7.4603],
        [ -8.1396,   5.5078],
        [  6.4876,  -9.9025],
        [  6.0719,  -7.7676],
        [  6.1685,  -7.7360],
        [  5.8843,  -7.5782],
        [ -6.4927,   4.6357],
        [ -8.0918,   6.4040],
        [  5.8319, -10.4472],
        [ -8.1775,   6.7804],
        [  2.4115,  -4.3255],
        [  2.5545,  -4.0681],
        [  5.8109,  -8.9870],
        [  6.2350,  -8.2457],
        [ -9.3461,   7.2942],
        [ -7.3272,   5.8974],
        [ -8.5659,   6.1271],
        [ -8.0420,   6.3839],
        [  5.6206,  -7.3623],
        [  6.1080,  -7.7249],
        [ -8.7999,   7.2263],
        [  5.2857,  -7.8632],
        [  6.0614,  -7.6690],
        [  1.2689,  -2.6673],
        [ -8.3445,   6.4243],
        [  5.7401,  -7.2720],
        [  6.0317,  -7.5809],
        [ -6.5366,   1.9214],
        [ -8.7205,   6.6169],
        [ -7.7031,   6.1643],
        [ -7.3122,   5.8284],
        [ -4.6503,   3.0093],
        [  5.3883,  -6.8861],
        [ -8.2214,   6.0109],
        [  6.1366,  -7.5598],
        [  5.6645,  -7.4605],
        [ -8.6401,   7.0230],
        [  6.7793,  -8.7407],
        [ -3.5407,   2.1541],
        [ -8.2759,   6.3857],
        [ -6.3741,   4.7316],
        [ -7.2752,   5.4961],
        [-10.0698,   5.8490],
        [  3.2594,  -4.6553],
        [  6.8473,  -9.3898],
        [ -9.0340,   7.0345],
        [  6.1298,  -7.5161],
        [ -7.6818,   6.2909],
        [ -7.9731,   6.2964],
        [  2.8936,  -4.8091],
        [  4.7388,  -6.7089],
        [  6.2515,  -7.6639],
        [  5.2041,  -6.6520],
        [  6.1232,  -7.7890],
        [  6.7715,  -8.1753],
        [ -9.6261,   7.4571],
        [ -8.8331,   7.3770],
        [ -5.8041,   3.7209],
        [  6.6712,  -8.0590],
        [  4.7600,  -6.4074],
        [  5.7756,  -7.1699],
        [  5.0047,  -6.4193],
        [  5.5903,  -8.2462],
        [ -9.4010,   6.5036],
        [  5.6450,  -7.6259],
        [ -5.6926,   4.2740],
        [ -7.1925,   5.8032],
        [ -7.3281,   5.9366]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7110, 0.2890],
        [0.2427, 0.7573]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4797, 0.5203], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2013, 0.0962],
         [0.0057, 0.4001]],

        [[0.3159, 0.1008],
         [0.4181, 0.3317]],

        [[0.7790, 0.0923],
         [0.9228, 0.9330]],

        [[0.6933, 0.1020],
         [0.2685, 0.7974]],

        [[0.4331, 0.0907],
         [0.8943, 0.0100]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999854008352
Average Adjusted Rand Index: 0.9919995611635631
Iteration 0: Loss = -16846.04180145661
Iteration 10: Loss = -11585.352018916934
Iteration 20: Loss = -11585.250978111171
Iteration 30: Loss = -11585.250978111158
Iteration 40: Loss = -11585.250978111158
1
Iteration 50: Loss = -11585.250978111158
2
Iteration 60: Loss = -11585.250978111158
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7636, 0.2364],
        [0.2858, 0.7142]], dtype=torch.float64)
alpha: tensor([0.5309, 0.4691])
beta: tensor([[[0.3906, 0.0962],
         [0.3081, 0.1983]],

        [[0.4248, 0.0953],
         [0.0550, 0.1932]],

        [[0.8993, 0.0922],
         [0.3297, 0.3009]],

        [[0.8685, 0.1020],
         [0.9911, 0.6754]],

        [[0.6623, 0.0907],
         [0.3870, 0.0763]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16846.045332014404
Iteration 100: Loss = -12255.73465824956
Iteration 200: Loss = -11827.764383762822
Iteration 300: Loss = -11799.205305962767
Iteration 400: Loss = -11766.179145706423
Iteration 500: Loss = -11682.086939641264
Iteration 600: Loss = -11649.120970117649
Iteration 700: Loss = -11641.957256597194
Iteration 800: Loss = -11619.190525537006
Iteration 900: Loss = -11598.10448131164
Iteration 1000: Loss = -11588.583939559956
Iteration 1100: Loss = -11588.505982019002
Iteration 1200: Loss = -11588.450612799674
Iteration 1300: Loss = -11588.409200561891
Iteration 1400: Loss = -11588.377436831925
Iteration 1500: Loss = -11588.35256424256
Iteration 1600: Loss = -11588.33252144485
Iteration 1700: Loss = -11588.316059067805
Iteration 1800: Loss = -11588.302306623737
Iteration 1900: Loss = -11588.290574048904
Iteration 2000: Loss = -11588.280571636415
Iteration 2100: Loss = -11588.271979158211
Iteration 2200: Loss = -11588.264470907683
Iteration 2300: Loss = -11588.257860270318
Iteration 2400: Loss = -11588.252044173098
Iteration 2500: Loss = -11588.246903198602
Iteration 2600: Loss = -11588.242307953773
Iteration 2700: Loss = -11588.238190286702
Iteration 2800: Loss = -11588.234453876807
Iteration 2900: Loss = -11588.231162946726
Iteration 3000: Loss = -11588.228123266934
Iteration 3100: Loss = -11588.225393810942
Iteration 3200: Loss = -11588.222851416182
Iteration 3300: Loss = -11588.220568275432
Iteration 3400: Loss = -11588.218495078208
Iteration 3500: Loss = -11588.21659668111
Iteration 3600: Loss = -11588.214833478616
Iteration 3700: Loss = -11588.213228581726
Iteration 3800: Loss = -11588.211817409607
Iteration 3900: Loss = -11588.210465106204
Iteration 4000: Loss = -11588.209216182626
Iteration 4100: Loss = -11588.208030820575
Iteration 4200: Loss = -11588.206993814596
Iteration 4300: Loss = -11588.206144693602
Iteration 4400: Loss = -11588.205062062942
Iteration 4500: Loss = -11588.204246301033
Iteration 4600: Loss = -11588.203458906726
Iteration 4700: Loss = -11588.202737401432
Iteration 4800: Loss = -11588.201967114777
Iteration 4900: Loss = -11588.204474736416
1
Iteration 5000: Loss = -11588.200756948669
Iteration 5100: Loss = -11588.200146691248
Iteration 5200: Loss = -11588.199647688374
Iteration 5300: Loss = -11588.201374820437
1
Iteration 5400: Loss = -11588.198677428092
Iteration 5500: Loss = -11588.199225355307
1
Iteration 5600: Loss = -11588.198826574555
2
Iteration 5700: Loss = -11588.19747742373
Iteration 5800: Loss = -11588.197089761234
Iteration 5900: Loss = -11588.19851855822
1
Iteration 6000: Loss = -11588.196422421022
Iteration 6100: Loss = -11588.201233373213
1
Iteration 6200: Loss = -11588.200376066125
2
Iteration 6300: Loss = -11588.195500138734
Iteration 6400: Loss = -11583.871146298781
Iteration 6500: Loss = -11583.868049554745
Iteration 6600: Loss = -11583.865081233187
Iteration 6700: Loss = -11583.887376791183
1
Iteration 6800: Loss = -11583.865853769039
2
Iteration 6900: Loss = -11583.908992213237
3
Iteration 7000: Loss = -11583.861464896978
Iteration 7100: Loss = -11583.861381626602
Iteration 7200: Loss = -11583.88454730877
1
Iteration 7300: Loss = -11583.900575818387
2
Iteration 7400: Loss = -11583.86611801399
3
Iteration 7500: Loss = -11583.981205134049
4
Iteration 7600: Loss = -11583.860164885378
Iteration 7700: Loss = -11583.867801402908
1
Iteration 7800: Loss = -11583.862078268641
2
Iteration 7900: Loss = -11583.861227527912
3
Iteration 8000: Loss = -11583.861899825597
4
Iteration 8100: Loss = -11583.860423389726
5
Iteration 8200: Loss = -11583.85970064096
Iteration 8300: Loss = -11583.873210916745
1
Iteration 8400: Loss = -11583.85948200223
Iteration 8500: Loss = -11583.974199205035
1
Iteration 8600: Loss = -11583.87014098826
2
Iteration 8700: Loss = -11583.879453051126
3
Iteration 8800: Loss = -11583.8837190355
4
Iteration 8900: Loss = -11583.878502873751
5
Iteration 9000: Loss = -11583.864824427257
6
Iteration 9100: Loss = -11583.866512267547
7
Iteration 9200: Loss = -11583.86414517072
8
Iteration 9300: Loss = -11583.859774163144
9
Iteration 9400: Loss = -11583.865445073401
10
Stopping early at iteration 9400 due to no improvement.
tensor([[  6.2602, -10.8754],
        [ -7.4683,   5.8264],
        [  5.4594,  -6.8608],
        [  6.4681,  -8.6675],
        [ -8.2127,   5.7845],
        [  4.3845,  -5.8737],
        [  4.8922,  -6.2831],
        [  6.1076,  -7.4953],
        [ -7.4211,   6.0284],
        [  6.9844,  -9.6709],
        [  5.9243,  -7.8911],
        [  6.1196,  -8.0282],
        [ -7.0380,   4.3863],
        [  6.0235,  -9.3981],
        [  6.2389,  -7.6665],
        [ -7.4623,   5.4142],
        [  6.1796,  -7.6832],
        [  6.6162,  -9.2556],
        [ -6.1670,   4.2203],
        [  6.8928,  -8.6365],
        [  6.2874,  -7.6779],
        [ -5.4692,   4.0614],
        [-10.3406,   5.7254],
        [  6.3791,  -7.8800],
        [  4.3763,  -5.7997],
        [ -7.0227,   5.5964],
        [ -7.2166,   5.7020],
        [ -8.4673,   6.1549],
        [ -6.6239,   4.9912],
        [  6.1505,  -7.5854],
        [  5.8553,  -8.4440],
        [  5.2262,  -9.5785],
        [ -6.7552,   5.2429],
        [  5.7969,  -7.3866],
        [ -8.5827,   6.0331],
        [ -7.5351,   5.3832],
        [ -7.0008,   5.5739],
        [ -7.6257,   5.6678],
        [  4.7313,  -6.2813],
        [  5.8687,  -7.5993],
        [ -7.1309,   5.6218],
        [  4.4795,  -5.8784],
        [ -4.0205,   2.6339],
        [ -4.1398,   2.5053],
        [ -7.0475,   5.6386],
        [ -7.5657,   6.1639],
        [  5.8774,  -8.0271],
        [  5.6294,  -7.3280],
        [  6.2699,  -7.6726],
        [  6.0488,  -7.4355],
        [ -7.6604,   5.4165],
        [ -7.3872,   5.9479],
        [  6.7252,  -8.3666],
        [ -7.8069,   4.5688],
        [ -7.2349,   5.5150],
        [ -2.9418,   0.9238],
        [  6.4637,  -8.2709],
        [ -7.5127,   5.3214],
        [ -7.1326,   5.6843],
        [  3.0295,  -5.4852],
        [  5.3519,  -7.3440],
        [  5.0648,  -8.9187],
        [  5.3747,  -6.8202],
        [  3.0658,  -4.5601],
        [ -6.5165,   5.0440],
        [  6.2033,  -7.5956],
        [ -7.4182,   5.7659],
        [ -7.9867,   5.4001],
        [  6.1388,  -7.9005],
        [ -8.3195,   5.0455],
        [  2.1936,  -3.5801],
        [  5.0193,  -8.7785],
        [  4.4349,  -6.7548],
        [  5.2977,  -6.6897],
        [  5.7328,  -8.4101],
        [ -4.7189,   3.1668],
        [-10.8492,   6.2340],
        [  6.4320,  -7.8195],
        [ -7.5229,   5.8698],
        [  4.9966,  -8.2173],
        [  6.3016,  -8.1710],
        [ -5.4964,   2.2008],
        [ -6.2024,   4.8159],
        [ -8.2067,   5.9194],
        [ -7.0053,   4.2541],
        [ -7.0624,   5.6668],
        [ -6.9244,   5.3899],
        [  6.7666,  -9.1117],
        [  6.1931,  -9.5149],
        [  3.8468,  -5.7259],
        [ -7.4752,   5.6240],
        [ -6.4857,   4.6850],
        [ -6.7431,   5.3243],
        [ -6.5669,   4.7566],
        [ -7.1268,   5.5212],
        [  6.8193,  -8.2154],
        [ -7.4115,   5.7192],
        [  3.3535,  -6.6344],
        [  5.5241,  -7.0258],
        [  5.6147,  -8.0181]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7638, 0.2362],
        [0.2845, 0.7155]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5162, 0.4838], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3990, 0.0962],
         [0.3081, 0.2025]],

        [[0.4248, 0.0958],
         [0.0550, 0.1932]],

        [[0.8993, 0.0921],
         [0.3297, 0.3009]],

        [[0.8685, 0.1019],
         [0.9911, 0.6754]],

        [[0.6623, 0.0907],
         [0.3870, 0.0763]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -36914.80006098572
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.9344,    nan]],

        [[0.0776,    nan],
         [0.1720, 0.8498]],

        [[0.7800,    nan],
         [0.6159, 0.8030]],

        [[0.5090,    nan],
         [0.9702, 0.7403]],

        [[0.1897,    nan],
         [0.6170, 0.1241]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36915.48510298258
Iteration 100: Loss = -12530.531975933707
Iteration 200: Loss = -12510.602210604531
Iteration 300: Loss = -12502.809278394461
Iteration 400: Loss = -12496.354363605358
Iteration 500: Loss = -12488.304386448635
Iteration 600: Loss = -12475.59688274959
Iteration 700: Loss = -12437.614961238816
Iteration 800: Loss = -12321.882972293575
Iteration 900: Loss = -12216.688217596782
Iteration 1000: Loss = -12116.948166559863
Iteration 1100: Loss = -11965.208209239376
Iteration 1200: Loss = -11914.628681630384
Iteration 1300: Loss = -11890.615917738662
Iteration 1400: Loss = -11883.00186087318
Iteration 1500: Loss = -11870.850309663629
Iteration 1600: Loss = -11870.380172097273
Iteration 1700: Loss = -11854.394207285495
Iteration 1800: Loss = -11854.069286978753
Iteration 1900: Loss = -11853.905875195971
Iteration 2000: Loss = -11853.792585567951
Iteration 2100: Loss = -11853.70318218476
Iteration 2200: Loss = -11853.630004263134
Iteration 2300: Loss = -11853.56848099786
Iteration 2400: Loss = -11853.515331046017
Iteration 2500: Loss = -11853.468444599366
Iteration 2600: Loss = -11853.427345376125
Iteration 2700: Loss = -11853.38769598989
Iteration 2800: Loss = -11853.314876358447
Iteration 2900: Loss = -11849.869324250863
Iteration 3000: Loss = -11849.829927966439
Iteration 3100: Loss = -11849.802904026852
Iteration 3200: Loss = -11849.7817109619
Iteration 3300: Loss = -11849.760494568576
Iteration 3400: Loss = -11849.707599980473
Iteration 3500: Loss = -11847.116094127126
Iteration 3600: Loss = -11847.053322488053
Iteration 3700: Loss = -11846.996336154536
Iteration 3800: Loss = -11843.289708330927
Iteration 3900: Loss = -11837.351242383396
Iteration 4000: Loss = -11837.33576180252
Iteration 4100: Loss = -11837.31930304609
Iteration 4200: Loss = -11831.420571439041
Iteration 4300: Loss = -11831.393619861701
Iteration 4400: Loss = -11831.384837163101
Iteration 4500: Loss = -11831.377599034886
Iteration 4600: Loss = -11831.37107359019
Iteration 4700: Loss = -11823.989033826187
Iteration 4800: Loss = -11823.805099233463
Iteration 4900: Loss = -11823.797073607158
Iteration 5000: Loss = -11823.783444444627
Iteration 5100: Loss = -11820.331679101515
Iteration 5200: Loss = -11820.317209716157
Iteration 5300: Loss = -11820.311377910499
Iteration 5400: Loss = -11820.306951409557
Iteration 5500: Loss = -11820.30284171896
Iteration 5600: Loss = -11820.298829849613
Iteration 5700: Loss = -11820.294467138492
Iteration 5800: Loss = -11820.28787592696
Iteration 5900: Loss = -11820.27158855933
Iteration 6000: Loss = -11820.23557593599
Iteration 6100: Loss = -11813.903068652668
Iteration 6200: Loss = -11813.8983628135
Iteration 6300: Loss = -11813.891670729201
Iteration 6400: Loss = -11813.887572954489
Iteration 6500: Loss = -11813.882530827288
Iteration 6600: Loss = -11813.878748828332
Iteration 6700: Loss = -11798.42973658652
Iteration 6800: Loss = -11798.425929459258
Iteration 6900: Loss = -11798.421625388135
Iteration 7000: Loss = -11788.104990908587
Iteration 7100: Loss = -11780.289047785644
Iteration 7200: Loss = -11780.280627941762
Iteration 7300: Loss = -11780.273203833854
Iteration 7400: Loss = -11780.269494112792
Iteration 7500: Loss = -11771.024335864415
Iteration 7600: Loss = -11771.034945350635
1
Iteration 7700: Loss = -11771.020362530844
Iteration 7800: Loss = -11771.019321343128
Iteration 7900: Loss = -11771.019585873139
1
Iteration 8000: Loss = -11771.016958603777
Iteration 8100: Loss = -11771.01572970685
Iteration 8200: Loss = -11762.046198260316
Iteration 8300: Loss = -11745.669274624877
Iteration 8400: Loss = -11745.653983075756
Iteration 8500: Loss = -11745.661424093623
1
Iteration 8600: Loss = -11736.04930606834
Iteration 8700: Loss = -11736.045380979214
Iteration 8800: Loss = -11736.045236499966
Iteration 8900: Loss = -11736.0461920635
1
Iteration 9000: Loss = -11736.041490598473
Iteration 9100: Loss = -11727.612396753608
Iteration 9200: Loss = -11713.070383598817
Iteration 9300: Loss = -11713.069180799484
Iteration 9400: Loss = -11713.068141058631
Iteration 9500: Loss = -11713.063973086191
Iteration 9600: Loss = -11713.060466916693
Iteration 9700: Loss = -11713.156960853525
1
Iteration 9800: Loss = -11713.058947238062
Iteration 9900: Loss = -11713.073651631472
1
Iteration 10000: Loss = -11709.408390354092
Iteration 10100: Loss = -11709.386139445056
Iteration 10200: Loss = -11709.396688731878
1
Iteration 10300: Loss = -11709.40661458455
2
Iteration 10400: Loss = -11709.385326841279
Iteration 10500: Loss = -11709.419345234139
1
Iteration 10600: Loss = -11709.40023077158
2
Iteration 10700: Loss = -11709.38872170421
3
Iteration 10800: Loss = -11709.38504865971
Iteration 10900: Loss = -11709.419455003828
1
Iteration 11000: Loss = -11709.425451777382
2
Iteration 11100: Loss = -11709.393856133936
3
Iteration 11200: Loss = -11709.428665966287
4
Iteration 11300: Loss = -11709.383604039716
Iteration 11400: Loss = -11709.383797647542
1
Iteration 11500: Loss = -11709.411817007272
2
Iteration 11600: Loss = -11698.83012159479
Iteration 11700: Loss = -11698.813246570735
Iteration 11800: Loss = -11698.829350756072
1
Iteration 11900: Loss = -11698.811229683715
Iteration 12000: Loss = -11698.811848785374
1
Iteration 12100: Loss = -11698.817549223235
2
Iteration 12200: Loss = -11698.821557323787
3
Iteration 12300: Loss = -11698.818473190624
4
Iteration 12400: Loss = -11698.811336424847
5
Iteration 12500: Loss = -11698.809863198832
Iteration 12600: Loss = -11698.808782239763
Iteration 12700: Loss = -11698.808761378465
Iteration 12800: Loss = -11698.881966937492
1
Iteration 12900: Loss = -11698.81928750037
2
Iteration 13000: Loss = -11698.897656117932
3
Iteration 13100: Loss = -11698.81041165234
4
Iteration 13200: Loss = -11698.836601720071
5
Iteration 13300: Loss = -11698.815555398365
6
Iteration 13400: Loss = -11698.818037955736
7
Iteration 13500: Loss = -11698.837702440922
8
Iteration 13600: Loss = -11696.569336914203
Iteration 13700: Loss = -11683.110752326986
Iteration 13800: Loss = -11683.112690047217
1
Iteration 13900: Loss = -11683.112316666498
2
Iteration 14000: Loss = -11683.130701207861
3
Iteration 14100: Loss = -11683.11572351176
4
Iteration 14200: Loss = -11683.136019470025
5
Iteration 14300: Loss = -11674.354519749262
Iteration 14400: Loss = -11674.257768806572
Iteration 14500: Loss = -11674.274686087285
1
Iteration 14600: Loss = -11674.25012127607
Iteration 14700: Loss = -11674.286133032674
1
Iteration 14800: Loss = -11674.282068980689
2
Iteration 14900: Loss = -11662.51801685972
Iteration 15000: Loss = -11662.517869039602
Iteration 15100: Loss = -11662.52472409761
1
Iteration 15200: Loss = -11656.3544910299
Iteration 15300: Loss = -11656.334227534197
Iteration 15400: Loss = -11656.335729209886
1
Iteration 15500: Loss = -11656.332508999994
Iteration 15600: Loss = -11656.342809818123
1
Iteration 15700: Loss = -11656.383526131262
2
Iteration 15800: Loss = -11630.00214928449
Iteration 15900: Loss = -11629.963015705487
Iteration 16000: Loss = -11629.963037869236
1
Iteration 16100: Loss = -11629.965319637142
2
Iteration 16200: Loss = -11629.96384631978
3
Iteration 16300: Loss = -11629.969801095021
4
Iteration 16400: Loss = -11629.975336300386
5
Iteration 16500: Loss = -11629.971326200852
6
Iteration 16600: Loss = -11629.963662709073
7
Iteration 16700: Loss = -11629.962450552877
Iteration 16800: Loss = -11629.96320428616
1
Iteration 16900: Loss = -11629.960293358123
Iteration 17000: Loss = -11629.96039962083
1
Iteration 17100: Loss = -11629.965866090435
2
Iteration 17200: Loss = -11629.973980596458
3
Iteration 17300: Loss = -11629.95967696973
Iteration 17400: Loss = -11629.983055280134
1
Iteration 17500: Loss = -11629.965722647663
2
Iteration 17600: Loss = -11629.963576393122
3
Iteration 17700: Loss = -11629.960111696364
4
Iteration 17800: Loss = -11629.959454187056
Iteration 17900: Loss = -11629.983992638645
1
Iteration 18000: Loss = -11629.961431173117
2
Iteration 18100: Loss = -11629.965114865656
3
Iteration 18200: Loss = -11629.973324612081
4
Iteration 18300: Loss = -11629.959850899191
5
Iteration 18400: Loss = -11629.959302497778
Iteration 18500: Loss = -11629.973914450093
1
Iteration 18600: Loss = -11629.959495632722
2
Iteration 18700: Loss = -11629.961147089494
3
Iteration 18800: Loss = -11629.962856201546
4
Iteration 18900: Loss = -11629.959938483598
5
Iteration 19000: Loss = -11629.975057448395
6
Iteration 19100: Loss = -11629.96963494883
7
Iteration 19200: Loss = -11629.964191815307
8
Iteration 19300: Loss = -11629.96737160571
9
Iteration 19400: Loss = -11629.97189510359
10
Stopping early at iteration 19400 due to no improvement.
tensor([[-10.2746,   7.6683],
        [  6.9268,  -9.1996],
        [ -7.5948,   6.1081],
        [ -9.6403,   8.1068],
        [  7.4183,  -8.8626],
        [ -5.7276,   4.3242],
        [ -9.7207,   8.1050],
        [ -9.8629,   7.5614],
        [  7.8426,  -9.4125],
        [-10.0155,   8.5317],
        [ -8.6488,   5.2299],
        [-10.3295,   8.8759],
        [  5.1633,  -7.0709],
        [-10.0363,   8.3369],
        [-10.1995,   8.3937],
        [  7.6385,  -9.4680],
        [ -9.8423,   8.2564],
        [ -9.5601,   7.7867],
        [  3.6668,  -7.1424],
        [-10.0027,   8.0625],
        [ -8.8133,   7.4044],
        [  3.9729,  -5.5693],
        [  7.7306,  -9.1178],
        [-10.0684,   8.6599],
        [ -6.2169,   4.0224],
        [  6.8491,  -8.5343],
        [  8.2001,  -9.5925],
        [  8.0814,  -9.5105],
        [  6.0802,  -7.4670],
        [-10.6751,   7.8006],
        [ -9.7167,   8.3187],
        [ -9.3050,   7.8907],
        [  6.2095,  -7.6344],
        [-10.7246,   8.7238],
        [  7.4179,  -8.8043],
        [  8.0021, -10.2486],
        [  6.9444,  -8.4007],
        [  6.3985,  -8.4312],
        [ -9.9643,   8.5761],
        [ -8.7335,   7.3420],
        [  8.5342,  -9.9925],
        [-11.9747,   7.9550],
        [  7.9901,  -9.4015],
        [  2.4563,  -4.1178],
        [  6.6534,  -8.2234],
        [  7.7132,  -9.3473],
        [ -9.5715,   8.0987],
        [ -9.5534,   7.9799],
        [ -9.0337,   7.3483],
        [-11.1857,   9.0323],
        [  6.5604,  -8.4260],
        [  7.5443,  -9.9158],
        [-10.0899,   8.7034],
        [  6.5735,  -7.9956],
        [  7.6506,  -9.2353],
        [  0.5155,  -3.5892],
        [-10.7358,   8.1632],
        [  8.5751,  -9.9949],
        [  7.3858,  -8.8220],
        [ -4.8783,   3.4248],
        [ -8.3783,   6.8842],
        [ -9.5932,   7.7332],
        [ -7.5823,   6.1310],
        [ -5.1403,   2.5876],
        [  6.0518,  -7.4794],
        [-10.7048,   7.6029],
        [  6.8440, -10.8359],
        [  6.7562,  -8.1485],
        [ -9.0648,   7.6377],
        [  8.1976,  -9.6235],
        [-10.0516,   8.0441],
        [-11.1100,   6.5923],
        [ -6.3817,   4.6242],
        [ -9.3557,   7.3986],
        [ -8.9246,   6.3144],
        [  2.5787,  -5.3990],
        [  7.7533,  -9.1849],
        [ -9.8807,   7.8833],
        [  6.6728,  -9.3942],
        [-10.1804,   6.8599],
        [ -8.9698,   7.5824],
        [  2.8780,  -4.8248],
        [  4.9013,  -6.5759],
        [  7.1487,  -8.8034],
        [  8.0097,  -9.6087],
        [  6.8403,  -9.1573],
        [  6.9970,  -8.4491],
        [ -9.7389,   8.0872],
        [-10.2021,   8.7260],
        [ -5.8542,   3.5786],
        [  6.9011,  -8.5347],
        [  4.6173,  -6.6246],
        [  6.1430,  -8.0392],
        [  4.1520,  -7.3681],
        [  6.9346,  -9.1278],
        [-11.0087,   9.5378],
        [  6.8629,  -8.3246],
        [ -5.8816,   4.0690],
        [ -7.8692,   6.4003],
        [-10.0347,   8.0021]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6969, 0.3031],
        [0.2603, 0.7397]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4798, 0.5202], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1999, 0.0961],
         [0.9344, 0.4009]],

        [[0.0776, 0.1015],
         [0.1720, 0.8498]],

        [[0.7800, 0.0940],
         [0.6159, 0.8030]],

        [[0.5090, 0.1110],
         [0.9702, 0.7403]],

        [[0.1897, 0.0907],
         [0.6170, 0.1241]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681923880397061
Average Adjusted Rand Index: 0.9681591614988758
11587.538732870691
new:  [0.9840320460697916, 0.9919999854008352, 1.0, 0.9681923880397061] [0.9841602586080228, 0.9919995611635631, 1.0, 0.9681591614988758] [11606.293928528337, 11596.946104828605, 11583.865445073401, 11629.97189510359]
prior:  [1.0, 1.0, 1.0, 0.0] [1.0, 1.0, 1.0, 0.0] [11585.250978111158, 11585.25097599101, 11585.250978111158, nan]
-----------------------------------------------------------------------------------------
This iteration is 6
True Objective function: Loss = -11730.03503185901
Iteration 0: Loss = -35184.971119824164
Iteration 10: Loss = -12273.263004850105
Iteration 20: Loss = -11893.1160996965
Iteration 30: Loss = -11892.607306084552
Iteration 40: Loss = -11892.609237093664
1
Iteration 50: Loss = -11892.609320269412
2
Iteration 60: Loss = -11892.60932354675
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6226, 0.3774],
        [0.2292, 0.7708]], dtype=torch.float64)
alpha: tensor([0.4677, 0.5323])
beta: tensor([[[0.3886, 0.1088],
         [0.9688, 0.2135]],

        [[0.0071, 0.0980],
         [0.2372, 0.8356]],

        [[0.9417, 0.0946],
         [0.5534, 0.3714]],

        [[0.8994, 0.1053],
         [0.7782, 0.2454]],

        [[0.1257, 0.0931],
         [0.9256, 0.5293]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 66
Adjusted Rand Index: 0.09709727968427424
Global Adjusted Rand Index: 0.5407756193673523
Average Adjusted Rand Index: 0.819419455936855
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35487.73435383834
Iteration 100: Loss = -12610.281611605587
Iteration 200: Loss = -12605.331331559904
Iteration 300: Loss = -12603.494588702428
Iteration 400: Loss = -12602.556238408528
Iteration 500: Loss = -12602.004177907944
Iteration 600: Loss = -12601.636771223995
Iteration 700: Loss = -12600.783647187454
Iteration 800: Loss = -12597.962779319194
Iteration 900: Loss = -12588.962239966988
Iteration 1000: Loss = -12584.154466025439
Iteration 1100: Loss = -12577.426029471215
Iteration 1200: Loss = -12563.904618422745
Iteration 1300: Loss = -12544.270165990836
Iteration 1400: Loss = -12504.634214192823
Iteration 1500: Loss = -12283.579010763744
Iteration 1600: Loss = -12223.741923264419
Iteration 1700: Loss = -12157.592707327856
Iteration 1800: Loss = -12117.54246467396
Iteration 1900: Loss = -12103.989903996759
Iteration 2000: Loss = -12067.095718330293
Iteration 2100: Loss = -12011.29380781057
Iteration 2200: Loss = -11974.353531752482
Iteration 2300: Loss = -11918.30841711614
Iteration 2400: Loss = -11870.097292321398
Iteration 2500: Loss = -11826.761447168341
Iteration 2600: Loss = -11759.25594112943
Iteration 2700: Loss = -11759.1684589357
Iteration 2800: Loss = -11759.042975337032
Iteration 2900: Loss = -11728.978869055103
Iteration 3000: Loss = -11728.20389104637
Iteration 3100: Loss = -11728.176669236378
Iteration 3200: Loss = -11728.156147700493
Iteration 3300: Loss = -11728.13595666026
Iteration 3400: Loss = -11728.111587629322
Iteration 3500: Loss = -11728.09980696751
Iteration 3600: Loss = -11728.089129487646
Iteration 3700: Loss = -11728.078942707556
Iteration 3800: Loss = -11728.068458069
Iteration 3900: Loss = -11728.046431210658
Iteration 4000: Loss = -11728.038701533787
Iteration 4100: Loss = -11728.033349633522
Iteration 4200: Loss = -11728.027815250125
Iteration 4300: Loss = -11728.023883933423
Iteration 4400: Loss = -11728.020275388508
Iteration 4500: Loss = -11728.017101926232
Iteration 4600: Loss = -11728.014138674851
Iteration 4700: Loss = -11728.011425373712
Iteration 4800: Loss = -11728.008919390808
Iteration 4900: Loss = -11728.006666335927
Iteration 5000: Loss = -11728.00461949436
Iteration 5100: Loss = -11728.002696074094
Iteration 5200: Loss = -11728.000941032464
Iteration 5300: Loss = -11727.99933004185
Iteration 5400: Loss = -11727.99779340536
Iteration 5500: Loss = -11727.996390142323
Iteration 5600: Loss = -11727.995063389262
Iteration 5700: Loss = -11727.9938455976
Iteration 5800: Loss = -11727.99260968704
Iteration 5900: Loss = -11727.991467537238
Iteration 6000: Loss = -11727.990470839168
Iteration 6100: Loss = -11727.989542271798
Iteration 6200: Loss = -11727.988724309722
Iteration 6300: Loss = -11727.987907565068
Iteration 6400: Loss = -11727.987333813502
Iteration 6500: Loss = -11727.986533770047
Iteration 6600: Loss = -11727.985741258855
Iteration 6700: Loss = -11727.985381404842
Iteration 6800: Loss = -11727.984535990296
Iteration 6900: Loss = -11727.984150145281
Iteration 7000: Loss = -11727.997269364947
1
Iteration 7100: Loss = -11727.982968412458
Iteration 7200: Loss = -11727.991684361512
1
Iteration 7300: Loss = -11727.982079693296
Iteration 7400: Loss = -11728.007940391344
1
Iteration 7500: Loss = -11727.98132735858
Iteration 7600: Loss = -11727.980974641137
Iteration 7700: Loss = -11727.981952530272
1
Iteration 7800: Loss = -11727.980298479217
Iteration 7900: Loss = -11727.97988393177
Iteration 8000: Loss = -11727.980044396962
1
Iteration 8100: Loss = -11727.979374222834
Iteration 8200: Loss = -11727.97946676125
1
Iteration 8300: Loss = -11727.983418067466
2
Iteration 8400: Loss = -11727.980540731163
3
Iteration 8500: Loss = -11728.000744043358
4
Iteration 8600: Loss = -11727.978208863324
Iteration 8700: Loss = -11727.98838541798
1
Iteration 8800: Loss = -11727.978652195516
2
Iteration 8900: Loss = -11727.978609765009
3
Iteration 9000: Loss = -11727.983602838929
4
Iteration 9100: Loss = -11728.020975077734
5
Iteration 9200: Loss = -11727.99021037169
6
Iteration 9300: Loss = -11727.97372618595
Iteration 9400: Loss = -11727.973714976839
Iteration 9500: Loss = -11727.97346394771
Iteration 9600: Loss = -11727.973668464929
1
Iteration 9700: Loss = -11727.97321953534
Iteration 9800: Loss = -11728.015207078548
1
Iteration 9900: Loss = -11727.972988649304
Iteration 10000: Loss = -11727.97289700574
Iteration 10100: Loss = -11727.972896450165
Iteration 10200: Loss = -11727.978995997317
1
Iteration 10300: Loss = -11727.974234833962
2
Iteration 10400: Loss = -11727.972539152881
Iteration 10500: Loss = -11727.976467946924
1
Iteration 10600: Loss = -11727.974111292768
2
Iteration 10700: Loss = -11728.002818935725
3
Iteration 10800: Loss = -11727.983216200098
4
Iteration 10900: Loss = -11727.98235985528
5
Iteration 11000: Loss = -11727.972537389871
Iteration 11100: Loss = -11727.974322159085
1
Iteration 11200: Loss = -11727.972291685306
Iteration 11300: Loss = -11727.974205278768
1
Iteration 11400: Loss = -11727.972003575784
Iteration 11500: Loss = -11727.972458286997
1
Iteration 11600: Loss = -11727.976850376419
2
Iteration 11700: Loss = -11727.976053543245
3
Iteration 11800: Loss = -11727.97407368487
4
Iteration 11900: Loss = -11727.976037040718
5
Iteration 12000: Loss = -11727.983271843597
6
Iteration 12100: Loss = -11727.978613902167
7
Iteration 12200: Loss = -11727.978061568958
8
Iteration 12300: Loss = -11727.97208534063
9
Iteration 12400: Loss = -11727.974915885818
10
Stopping early at iteration 12400 due to no improvement.
tensor([[  2.4193,  -7.0345],
        [  4.1712,  -8.7865],
        [  2.6183,  -7.2335],
        [  6.1608, -10.7760],
        [-10.0499,   5.4347],
        [ -9.5255,   4.9103],
        [  6.6117, -11.2269],
        [ -9.5103,   4.8951],
        [ -8.2477,   3.6325],
        [  6.3183, -10.9336],
        [-10.0562,   5.4410],
        [  1.4826,  -6.0978],
        [-10.0631,   5.4478],
        [  3.8177,  -8.4329],
        [  3.8697,  -8.4850],
        [  6.6349, -11.2501],
        [ -8.9542,   4.3390],
        [  6.6695, -11.2847],
        [-10.0723,   5.4571],
        [ -9.9065,   5.2913],
        [ -8.2465,   3.6313],
        [  6.5271, -11.1423],
        [ -8.6977,   4.0825],
        [  3.9105,  -8.5258],
        [  5.4166, -10.0318],
        [  3.6666,  -8.2818],
        [  6.7722, -11.3874],
        [  6.8415, -11.4567],
        [ -8.6594,   4.0442],
        [  6.5066, -11.1218],
        [ -9.5733,   4.9581],
        [ -9.9835,   5.3683],
        [  4.5725,  -9.1877],
        [  3.5554,  -8.1706],
        [  4.5397,  -9.1549],
        [ -9.9303,   5.3150],
        [  6.6830, -11.2982],
        [ -8.9561,   4.3409],
        [  5.1367,  -9.7519],
        [  6.3828, -10.9980],
        [ -9.0293,   4.4141],
        [  1.2918,  -5.9070],
        [  5.3273,  -9.9426],
        [  6.8404, -11.4557],
        [ -6.4027,   1.7875],
        [ -6.3915,   1.7763],
        [  6.2255, -10.8407],
        [ -9.6524,   5.0372],
        [ -8.3568,   3.7416],
        [ -9.7734,   5.1582],
        [ -8.5700,   3.9547],
        [  6.3690, -10.9842],
        [ -4.2097,  -0.4055],
        [  6.4687, -11.0840],
        [ -8.8282,   4.2130],
        [ -6.9243,   2.3091],
        [  6.6757, -11.2909],
        [  6.7373, -11.3525],
        [  6.1584, -10.7736],
        [  6.7334, -11.3486],
        [  3.5026,  -8.1178],
        [  6.8831, -11.4983],
        [ -9.2616,   4.6464],
        [  6.5595, -11.1747],
        [-10.1921,   5.5769],
        [ -7.8695,   3.2543],
        [  4.3207,  -8.9360],
        [ -8.2396,   3.6244],
        [  5.6941, -10.3093],
        [  0.9394,  -5.5546],
        [  6.7448, -11.3600],
        [  3.6300,  -8.2452],
        [  6.6568, -11.2721],
        [ -5.6467,   1.0315],
        [  6.1906, -10.8059],
        [ -5.5769,   0.9617],
        [  4.3937,  -9.0089],
        [  0.2913,  -4.9065],
        [ -8.7834,   4.1682],
        [  4.8221,  -9.4373],
        [ -7.1601,   2.5449],
        [ -7.6507,   3.0354],
        [  4.5238,  -9.1390],
        [-10.1348,   5.5196],
        [  4.8389,  -9.4541],
        [ -5.0526,   0.4374],
        [  3.4948,  -8.1100],
        [ -8.5019,   3.8867],
        [ -5.4810,   0.8658],
        [  6.6748, -11.2900],
        [ -8.1514,   3.5362],
        [  5.6249, -10.2402],
        [ -8.1524,   3.5372],
        [  6.7514, -11.3666],
        [  6.1843, -10.7995],
        [ -9.1471,   4.5318],
        [  6.5231, -11.1383],
        [ -7.0944,   2.4792],
        [  6.7517, -11.3669],
        [  0.6183,  -5.2335]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7634, 0.2366],
        [0.2388, 0.7612]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5702, 0.4298], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3985, 0.1083],
         [0.9688, 0.1980]],

        [[0.0071, 0.0967],
         [0.2372, 0.8356]],

        [[0.9417, 0.0941],
         [0.5534, 0.3714]],

        [[0.8994, 0.1056],
         [0.7782, 0.2454]],

        [[0.1257, 0.1032],
         [0.9256, 0.5293]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.984031996819225
Average Adjusted Rand Index: 0.9839985580570193
Iteration 0: Loss = -20063.786070398404
Iteration 10: Loss = -11726.870601583454
Iteration 20: Loss = -11726.90573558638
1
Iteration 30: Loss = -11726.905738528485
2
Iteration 40: Loss = -11726.905738528485
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7610, 0.2390],
        [0.2394, 0.7606]], dtype=torch.float64)
alpha: tensor([0.4693, 0.5307])
beta: tensor([[[0.1929, 0.1089],
         [0.9798, 0.3927]],

        [[0.7842, 0.0980],
         [0.2031, 0.1010]],

        [[0.3252, 0.0946],
         [0.2997, 0.4924]],

        [[0.4832, 0.1054],
         [0.5913, 0.6837]],

        [[0.5580, 0.1034],
         [0.8238, 0.6840]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20063.68100839199
Iteration 100: Loss = -12594.347871014077
Iteration 200: Loss = -12213.664275258841
Iteration 300: Loss = -12113.542934079192
Iteration 400: Loss = -12102.287503388327
Iteration 500: Loss = -12089.768682658392
Iteration 600: Loss = -12063.6030063768
Iteration 700: Loss = -12042.449949212107
Iteration 800: Loss = -12036.993299215825
Iteration 900: Loss = -12027.859927777505
Iteration 1000: Loss = -12019.184536922641
Iteration 1100: Loss = -12017.388696832231
Iteration 1200: Loss = -12017.25804926642
Iteration 1300: Loss = -12003.931027875526
Iteration 1400: Loss = -11985.704931054304
Iteration 1500: Loss = -11955.438936571729
Iteration 1600: Loss = -11951.090307442664
Iteration 1700: Loss = -11950.48480258441
Iteration 1800: Loss = -11950.031926107025
Iteration 1900: Loss = -11935.293752941183
Iteration 2000: Loss = -11935.271223630316
Iteration 2100: Loss = -11935.254229275848
Iteration 2200: Loss = -11935.241080621277
Iteration 2300: Loss = -11935.23068336447
Iteration 2400: Loss = -11935.221751156116
Iteration 2500: Loss = -11935.213176596186
Iteration 2600: Loss = -11935.201102764544
Iteration 2700: Loss = -11934.626496774279
Iteration 2800: Loss = -11934.566611883234
Iteration 2900: Loss = -11934.5617522728
Iteration 3000: Loss = -11934.556814396157
Iteration 3100: Loss = -11934.549941464698
Iteration 3200: Loss = -11934.533651545145
Iteration 3300: Loss = -11934.524933460038
Iteration 3400: Loss = -11934.52194719707
Iteration 3500: Loss = -11934.51762821303
Iteration 3600: Loss = -11934.36284000516
Iteration 3700: Loss = -11934.35379990418
Iteration 3800: Loss = -11934.351126812868
Iteration 3900: Loss = -11934.347954765653
Iteration 4000: Loss = -11934.343662925208
Iteration 4100: Loss = -11934.254216260337
Iteration 4200: Loss = -11934.241682097047
Iteration 4300: Loss = -11934.240307833865
Iteration 4400: Loss = -11934.23918838954
Iteration 4500: Loss = -11934.237932366403
Iteration 4600: Loss = -11934.236864609953
Iteration 4700: Loss = -11934.235922405724
Iteration 4800: Loss = -11934.235272842008
Iteration 4900: Loss = -11934.234255483469
Iteration 5000: Loss = -11934.233486002106
Iteration 5100: Loss = -11934.233232041113
Iteration 5200: Loss = -11934.232135521392
Iteration 5300: Loss = -11934.232416899009
1
Iteration 5400: Loss = -11934.231050697284
Iteration 5500: Loss = -11934.230368823626
Iteration 5600: Loss = -11934.229822377167
Iteration 5700: Loss = -11934.229437582771
Iteration 5800: Loss = -11934.231556993744
1
Iteration 5900: Loss = -11934.229954498074
2
Iteration 6000: Loss = -11934.22821492446
Iteration 6100: Loss = -11934.227892755218
Iteration 6200: Loss = -11934.227683414367
Iteration 6300: Loss = -11934.231739599414
1
Iteration 6400: Loss = -11934.230140445397
2
Iteration 6500: Loss = -11934.227620387874
Iteration 6600: Loss = -11929.061675571
Iteration 6700: Loss = -11929.015133304942
Iteration 6800: Loss = -11929.012301710738
Iteration 6900: Loss = -11929.011990995252
Iteration 7000: Loss = -11929.017565336459
1
Iteration 7100: Loss = -11929.010834803286
Iteration 7200: Loss = -11929.010910901903
1
Iteration 7300: Loss = -11929.011257945993
2
Iteration 7400: Loss = -11929.010240414409
Iteration 7500: Loss = -11929.015520724464
1
Iteration 7600: Loss = -11929.090404288472
2
Iteration 7700: Loss = -11929.016072630757
3
Iteration 7800: Loss = -11929.009579835203
Iteration 7900: Loss = -11929.010452110475
1
Iteration 8000: Loss = -11929.00924770993
Iteration 8100: Loss = -11929.009766515568
1
Iteration 8200: Loss = -11929.008810678166
Iteration 8300: Loss = -11929.090685535793
1
Iteration 8400: Loss = -11929.00846629728
Iteration 8500: Loss = -11929.008359254898
Iteration 8600: Loss = -11929.008523686252
1
Iteration 8700: Loss = -11929.008179598857
Iteration 8800: Loss = -11929.009667869583
1
Iteration 8900: Loss = -11929.010039813771
2
Iteration 9000: Loss = -11929.008268085503
3
Iteration 9100: Loss = -11929.013594723214
4
Iteration 9200: Loss = -11929.01089315668
5
Iteration 9300: Loss = -11929.021689727586
6
Iteration 9400: Loss = -11929.021023362946
7
Iteration 9500: Loss = -11929.012658481031
8
Iteration 9600: Loss = -11929.015389848966
9
Iteration 9700: Loss = -11929.088607652153
10
Stopping early at iteration 9700 due to no improvement.
tensor([[-5.0550,  3.5111],
        [-8.3532,  6.3634],
        [-5.5657,  3.8997],
        [-8.0553,  6.2109],
        [ 6.1334, -7.5819],
        [ 5.5695, -7.3271],
        [-6.3921,  4.7362],
        [ 6.4375, -7.8913],
        [ 5.0049, -6.4938],
        [-7.6171,  6.0566],
        [ 6.2780, -8.0497],
        [-8.1022,  6.2607],
        [ 6.1269, -7.9022],
        [-6.6004,  5.2141],
        [-6.8633,  5.3251],
        [-8.5517,  6.2824],
        [ 6.4513, -8.0099],
        [-6.8368,  5.3785],
        [ 5.9407, -7.5452],
        [ 7.0792, -8.5666],
        [ 5.0743, -6.4904],
        [-8.2094,  6.8181],
        [ 5.1087, -8.8814],
        [-7.9203,  6.5187],
        [-7.8709,  6.2195],
        [-7.4042,  5.9754],
        [-9.3704,  7.6064],
        [-8.9643,  6.8879],
        [ 6.6225, -8.0410],
        [-7.9370,  6.2680],
        [ 5.6906, -7.0789],
        [ 6.3697, -7.9375],
        [-7.4049,  5.9184],
        [-7.0909,  4.8833],
        [-8.4042,  7.0015],
        [ 5.8896, -7.3075],
        [-7.6010,  6.1238],
        [ 5.8735, -7.5747],
        [-7.3265,  4.6149],
        [-8.1466,  6.1324],
        [ 4.9956, -6.4026],
        [-4.3625,  2.6226],
        [-9.3357,  6.8759],
        [-8.2289,  6.4699],
        [ 2.1367, -3.9353],
        [ 2.2986, -3.7140],
        [-7.8629,  6.4581],
        [ 4.3474, -8.9626],
        [ 4.7884, -7.1977],
        [ 5.6980, -7.0843],
        [ 5.6499, -7.4724],
        [-5.6652,  4.1365],
        [ 0.1060, -1.5667],
        [-7.4784,  6.0424],
        [ 6.0744, -9.5823],
        [ 2.0692, -5.3202],
        [-7.0519,  5.3996],
        [-8.2741,  6.7152],
        [-7.7228,  6.3062],
        [-7.4028,  5.7091],
        [-7.0588,  5.6339],
        [-8.4422,  6.7678],
        [ 5.6271, -7.6614],
        [-7.5716,  6.1653],
        [ 6.6933, -9.3384],
        [ 4.4209, -5.9933],
        [-8.5968,  6.4167],
        [ 5.1742, -6.5879],
        [-7.9568,  6.2622],
        [-4.0892,  2.0364],
        [-8.2203,  6.5031],
        [-7.6042,  4.6509],
        [-7.8256,  6.3401],
        [ 2.9436, -4.9342],
        [-7.1337,  5.7464],
        [ 2.7652, -4.1609],
        [-8.0870,  6.6636],
        [-4.4357,  3.0431],
        [ 6.7803, -9.0051],
        [-8.6232,  6.5881],
        [ 3.5924, -5.1177],
        [ 3.5663, -5.9577],
        [-7.6641,  5.8785],
        [ 6.1526, -7.7393],
        [-7.9720,  6.2954],
        [ 0.9406, -2.5045],
        [-6.3233,  4.8110],
        [ 5.0451, -7.2802],
        [ 4.1617, -8.4337],
        [-6.7658,  5.1878],
        [ 4.0220, -7.0756],
        [-8.2648,  5.4010],
        [ 6.0139, -7.7843],
        [-8.1698,  6.5166],
        [-8.2548,  6.7423],
        [ 5.4396, -7.4561],
        [-8.5280,  6.0763],
        [ 4.2126, -5.6128],
        [-7.7846,  4.9279],
        [-6.5813,  4.9657]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5577, 0.4423],
        [0.5131, 0.4869]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4263, 0.5737], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2238, 0.1083],
         [0.9798, 0.3907]],

        [[0.7842, 0.1001],
         [0.2031, 0.1010]],

        [[0.3252, 0.0942],
         [0.2997, 0.4924]],

        [[0.4832, 0.1044],
         [0.5913, 0.6837]],

        [[0.5580, 0.1015],
         [0.8238, 0.6840]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 23
Adjusted Rand Index: 0.2859858582610519
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.472289173952589
Average Adjusted Rand Index: 0.8491963879819396
Iteration 0: Loss = -23762.464169444484
Iteration 10: Loss = -12592.432711046067
Iteration 20: Loss = -11726.930950310116
Iteration 30: Loss = -11726.905742318762
Iteration 40: Loss = -11726.905741108114
Iteration 50: Loss = -11726.905741108114
1
Iteration 60: Loss = -11726.905741108114
2
Iteration 70: Loss = -11726.905741108114
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7610, 0.2390],
        [0.2394, 0.7606]], dtype=torch.float64)
alpha: tensor([0.4693, 0.5307])
beta: tensor([[[0.1929, 0.1089],
         [0.5142, 0.3927]],

        [[0.8320, 0.0980],
         [0.1784, 0.2205]],

        [[0.4605, 0.0946],
         [0.2071, 0.5844]],

        [[0.9035, 0.1054],
         [0.3572, 0.4893]],

        [[0.7258, 0.1034],
         [0.8790, 0.3933]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23760.604883072916
Iteration 100: Loss = -12601.500880975755
Iteration 200: Loss = -12573.612901982258
Iteration 300: Loss = -12492.737159811151
Iteration 400: Loss = -12026.236625726557
Iteration 500: Loss = -12009.204949877354
Iteration 600: Loss = -11996.017921141563
Iteration 700: Loss = -11990.26360125693
Iteration 800: Loss = -11979.326599403596
Iteration 900: Loss = -11963.380356305377
Iteration 1000: Loss = -11963.149463989273
Iteration 1100: Loss = -11951.360413691393
Iteration 1200: Loss = -11951.254204949968
Iteration 1300: Loss = -11951.173197886621
Iteration 1400: Loss = -11950.805475179603
Iteration 1500: Loss = -11940.780034982481
Iteration 1600: Loss = -11940.742612317963
Iteration 1700: Loss = -11940.712078152974
Iteration 1800: Loss = -11940.68667332584
Iteration 1900: Loss = -11940.665119157495
Iteration 2000: Loss = -11940.646519772605
Iteration 2100: Loss = -11940.630375136043
Iteration 2200: Loss = -11940.616137904235
Iteration 2300: Loss = -11940.603105632572
Iteration 2400: Loss = -11940.590443075884
Iteration 2500: Loss = -11940.515632687306
Iteration 2600: Loss = -11930.008014850157
Iteration 2700: Loss = -11930.001115643712
Iteration 2800: Loss = -11929.99145719916
Iteration 2900: Loss = -11929.984234458705
Iteration 3000: Loss = -11929.97485195424
Iteration 3100: Loss = -11917.972782269579
Iteration 3200: Loss = -11917.963885167452
Iteration 3300: Loss = -11917.959278516828
Iteration 3400: Loss = -11917.955580811014
Iteration 3500: Loss = -11917.951506915504
Iteration 3600: Loss = -11917.948185046009
Iteration 3700: Loss = -11917.945172335178
Iteration 3800: Loss = -11917.942433358641
Iteration 3900: Loss = -11917.939852159923
Iteration 4000: Loss = -11917.93749472902
Iteration 4100: Loss = -11917.935417178782
Iteration 4200: Loss = -11917.933405644497
Iteration 4300: Loss = -11917.931483525914
Iteration 4400: Loss = -11917.930166811291
Iteration 4500: Loss = -11917.94043311135
1
Iteration 4600: Loss = -11917.92672243309
Iteration 4700: Loss = -11917.952862118622
1
Iteration 4800: Loss = -11917.924070282575
Iteration 4900: Loss = -11917.934148373242
1
Iteration 5000: Loss = -11917.922259624409
Iteration 5100: Loss = -11917.920682328859
Iteration 5200: Loss = -11917.919857047807
Iteration 5300: Loss = -11917.92425543807
1
Iteration 5400: Loss = -11917.920450051384
2
Iteration 5500: Loss = -11917.916509385635
Iteration 5600: Loss = -11907.616461818781
Iteration 5700: Loss = -11907.611822097319
Iteration 5800: Loss = -11907.61128234963
Iteration 5900: Loss = -11907.61016340476
Iteration 6000: Loss = -11907.610955057049
1
Iteration 6100: Loss = -11907.60894776931
Iteration 6200: Loss = -11907.60824798762
Iteration 6300: Loss = -11907.609409683237
1
Iteration 6400: Loss = -11907.607214290365
Iteration 6500: Loss = -11907.607383089447
1
Iteration 6600: Loss = -11907.608954197698
2
Iteration 6700: Loss = -11907.605867694483
Iteration 6800: Loss = -11907.605577402333
Iteration 6900: Loss = -11907.619732096333
1
Iteration 7000: Loss = -11907.6088657311
2
Iteration 7100: Loss = -11907.604553555817
Iteration 7200: Loss = -11907.64573510192
1
Iteration 7300: Loss = -11907.603944975877
Iteration 7400: Loss = -11907.613983149298
1
Iteration 7500: Loss = -11907.603415929634
Iteration 7600: Loss = -11907.60324847534
Iteration 7700: Loss = -11907.603546198394
1
Iteration 7800: Loss = -11907.602769856127
Iteration 7900: Loss = -11907.604384970171
1
Iteration 8000: Loss = -11907.602387078592
Iteration 8100: Loss = -11907.614149727116
1
Iteration 8200: Loss = -11907.602398714273
2
Iteration 8300: Loss = -11907.602536659337
3
Iteration 8400: Loss = -11907.60592950251
4
Iteration 8500: Loss = -11907.61016555133
5
Iteration 8600: Loss = -11907.608539293346
6
Iteration 8700: Loss = -11907.606671844407
7
Iteration 8800: Loss = -11907.602154402792
Iteration 8900: Loss = -11907.601293143114
Iteration 9000: Loss = -11907.607204251228
1
Iteration 9100: Loss = -11907.63037233018
2
Iteration 9200: Loss = -11907.60091990563
Iteration 9300: Loss = -11907.602188264662
1
Iteration 9400: Loss = -11907.611027947029
2
Iteration 9500: Loss = -11907.674326302973
3
Iteration 9600: Loss = -11907.604850159116
4
Iteration 9700: Loss = -11907.602079197479
5
Iteration 9800: Loss = -11907.60183602366
6
Iteration 9900: Loss = -11907.60344102112
7
Iteration 10000: Loss = -11906.505065792879
Iteration 10100: Loss = -11906.226784495293
Iteration 10200: Loss = -11906.223297373046
Iteration 10300: Loss = -11906.223838263586
1
Iteration 10400: Loss = -11906.244812326979
2
Iteration 10500: Loss = -11906.223216420207
Iteration 10600: Loss = -11906.224039562709
1
Iteration 10700: Loss = -11906.390021643198
2
Iteration 10800: Loss = -11906.226628327664
3
Iteration 10900: Loss = -11906.232042542053
4
Iteration 11000: Loss = -11906.225788348993
5
Iteration 11100: Loss = -11906.232819340976
6
Iteration 11200: Loss = -11906.227522749177
7
Iteration 11300: Loss = -11906.24274604537
8
Iteration 11400: Loss = -11906.222907760548
Iteration 11500: Loss = -11906.222840857015
Iteration 11600: Loss = -11906.223745105883
1
Iteration 11700: Loss = -11906.22456376673
2
Iteration 11800: Loss = -11906.244611013846
3
Iteration 11900: Loss = -11906.276846733139
4
Iteration 12000: Loss = -11906.225366926928
5
Iteration 12100: Loss = -11906.223285615884
6
Iteration 12200: Loss = -11906.233479238692
7
Iteration 12300: Loss = -11906.225092205252
8
Iteration 12400: Loss = -11906.262038065035
9
Iteration 12500: Loss = -11906.22228949355
Iteration 12600: Loss = -11906.235230627834
1
Iteration 12700: Loss = -11906.231486612633
2
Iteration 12800: Loss = -11906.222262480984
Iteration 12900: Loss = -11906.233214480766
1
Iteration 13000: Loss = -11899.605263322415
Iteration 13100: Loss = -11899.570071219123
Iteration 13200: Loss = -11899.573104826477
1
Iteration 13300: Loss = -11899.58062824788
2
Iteration 13400: Loss = -11899.572815493602
3
Iteration 13500: Loss = -11899.565290964865
Iteration 13600: Loss = -11899.565451231334
1
Iteration 13700: Loss = -11899.586539750424
2
Iteration 13800: Loss = -11899.564605967451
Iteration 13900: Loss = -11899.582275893918
1
Iteration 14000: Loss = -11899.56342543484
Iteration 14100: Loss = -11899.563887095468
1
Iteration 14200: Loss = -11899.56708313568
2
Iteration 14300: Loss = -11899.577517388276
3
Iteration 14400: Loss = -11899.651228464983
4
Iteration 14500: Loss = -11899.563935310469
5
Iteration 14600: Loss = -11899.56674603684
6
Iteration 14700: Loss = -11899.563275045222
Iteration 14800: Loss = -11899.566402023125
1
Iteration 14900: Loss = -11899.56516229134
2
Iteration 15000: Loss = -11899.565610084232
3
Iteration 15100: Loss = -11899.569035855284
4
Iteration 15200: Loss = -11899.576827835652
5
Iteration 15300: Loss = -11899.568203091614
6
Iteration 15400: Loss = -11899.56634189505
7
Iteration 15500: Loss = -11899.568858795104
8
Iteration 15600: Loss = -11899.563108740378
Iteration 15700: Loss = -11899.568391053857
1
Iteration 15800: Loss = -11899.567722821934
2
Iteration 15900: Loss = -11899.56304342103
Iteration 16000: Loss = -11899.563558830168
1
Iteration 16100: Loss = -11899.591076849725
2
Iteration 16200: Loss = -11899.563033937819
Iteration 16300: Loss = -11899.563405873514
1
Iteration 16400: Loss = -11899.571646531336
2
Iteration 16500: Loss = -11899.567234739447
3
Iteration 16600: Loss = -11899.563139248754
4
Iteration 16700: Loss = -11899.563563705678
5
Iteration 16800: Loss = -11899.576912344462
6
Iteration 16900: Loss = -11899.696056828094
7
Iteration 17000: Loss = -11899.563010977583
Iteration 17100: Loss = -11899.563192296057
1
Iteration 17200: Loss = -11899.578546507357
2
Iteration 17300: Loss = -11899.563029175246
3
Iteration 17400: Loss = -11899.567203520986
4
Iteration 17500: Loss = -11899.446521815893
Iteration 17600: Loss = -11899.496789017805
1
Iteration 17700: Loss = -11899.437229444167
Iteration 17800: Loss = -11899.438115293318
1
Iteration 17900: Loss = -11899.46973002084
2
Iteration 18000: Loss = -11899.437409928783
3
Iteration 18100: Loss = -11899.439570364197
4
Iteration 18200: Loss = -11899.437267801133
5
Iteration 18300: Loss = -11899.441902257064
6
Iteration 18400: Loss = -11899.44954483395
7
Iteration 18500: Loss = -11899.531012715066
8
Iteration 18600: Loss = -11899.437246230433
9
Iteration 18700: Loss = -11899.438634376125
10
Stopping early at iteration 18700 due to no improvement.
tensor([[ -4.6627,   3.1765],
        [ -9.1835,   7.7248],
        [-10.1879,   8.7658],
        [-11.5441,   8.6432],
        [  7.9157, -10.1564],
        [  6.7078,  -8.3746],
        [-10.0188,   8.6244],
        [  6.8819,  -8.4204],
        [  5.5575,  -7.2096],
        [ -9.4477,   7.5062],
        [  7.6549,  -9.4142],
        [ -5.9087,   2.8469],
        [  7.8463,  -9.2330],
        [-11.6938,   7.9901],
        [ -8.0877,   6.5314],
        [-10.8346,   9.4479],
        [  8.6367, -11.0648],
        [ -7.5660,   6.1795],
        [  7.5947,  -9.8682],
        [  7.3765,  -9.2876],
        [  5.5464,  -7.3383],
        [-10.6710,   9.0372],
        [  7.6453,  -9.1071],
        [ -8.9561,   7.4902],
        [ -9.6635,   8.1810],
        [ -7.7063,   5.2799],
        [-10.8251,   9.3387],
        [-10.4616,   8.9714],
        [  7.5392,  -8.9426],
        [-10.2329,   8.7356],
        [  6.8539,  -8.3330],
        [  7.6242,  -9.2150],
        [ -9.8376,   8.3180],
        [ -7.2214,   5.6694],
        [ -9.4829,   8.0855],
        [  8.1682,  -9.6637],
        [-10.6693,   9.0951],
        [  6.0402,  -7.4938],
        [ -7.2398,   5.5827],
        [-10.0603,   8.4903],
        [  5.6510,  -7.0939],
        [ -4.6577,   3.0897],
        [ -9.5728,   7.4484],
        [-10.1372,   7.9812],
        [  2.8250,  -4.2272],
        [  2.8290,  -4.2247],
        [ -9.8112,   8.3428],
        [  5.4148, -10.0300],
        [  6.1931,  -7.5827],
        [  7.0414,  -8.5553],
        [  6.1958,  -9.0779],
        [ -6.1413,   4.3837],
        [ -0.0363,  -2.8359],
        [ -9.8059,   8.3751],
        [  7.5458,  -9.0528],
        [  3.3411,  -5.1661],
        [ -7.5787,   6.1860],
        [-10.5176,   8.5579],
        [-10.4382,   8.9174],
        [-10.3824,   8.8199],
        [ -8.0502,   4.7593],
        [ -9.9657,   8.3237],
        [  4.5519,  -9.1671],
        [-10.5090,   9.1060],
        [  7.8893,  -9.8877],
        [  4.8450,  -6.2317],
        [-10.5118,   9.1151],
        [  5.7381,  -7.1247],
        [ -9.6059,   8.0633],
        [ -8.6492,   4.5789],
        [ -9.5289,   8.1320],
        [ -8.1036,   5.5515],
        [ -9.4195,   7.7224],
        [  1.8044,  -5.1348],
        [ -9.7315,   7.6939],
        [  1.5686,  -4.6330],
        [ -9.1177,   5.7192],
        [ -3.8357,   2.4245],
        [  6.6327, -11.2479],
        [-10.6502,   8.6222],
        [  3.1707,  -6.2579],
        [  3.2010,  -7.0887],
        [ -9.6175,   7.8373],
        [  8.0715,  -9.4597],
        [-10.8328,   6.2176],
        [  1.1672,  -3.3354],
        [ -7.5986,   4.3863],
        [  6.3015,  -8.2864],
        [  0.9844,  -5.0870],
        [-10.0440,   8.3282],
        [  5.3247,  -6.7443],
        [ -8.0729,   6.6219],
        [  3.9611,  -7.2017],
        [-11.0614,   8.9967],
        [ -9.4062,   8.0027],
        [  5.8211,  -7.8536],
        [-11.6560,   8.2798],
        [  3.5756,  -5.7826],
        [ -7.6578,   6.1583],
        [ -5.2583,   1.6811]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7673, 0.2327],
        [0.3830, 0.6170]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4295, 0.5705], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2173, 0.1088],
         [0.5142, 0.3969]],

        [[0.8320, 0.1012],
         [0.1784, 0.2205]],

        [[0.4605, 0.0942],
         [0.2071, 0.5844]],

        [[0.9035, 0.1053],
         [0.3572, 0.4893]],

        [[0.7258, 0.0927],
         [0.8790, 0.3933]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 34
Adjusted Rand Index: 0.09709727968427424
Global Adjusted Rand Index: 0.5348914086196361
Average Adjusted Rand Index: 0.8114186722665842
Iteration 0: Loss = -17340.7729067134
Iteration 10: Loss = -11726.905611015263
Iteration 20: Loss = -11726.90574009693
1
Iteration 30: Loss = -11726.905740096923
2
Iteration 40: Loss = -11726.905740096923
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7606, 0.2394],
        [0.2390, 0.7610]], dtype=torch.float64)
alpha: tensor([0.5307, 0.4693])
beta: tensor([[[0.3927, 0.1089],
         [0.1660, 0.1929]],

        [[0.6591, 0.0980],
         [0.7760, 0.9334]],

        [[0.6851, 0.0946],
         [0.7297, 0.9977]],

        [[0.8080, 0.1054],
         [0.9570, 0.6191]],

        [[0.8855, 0.1034],
         [0.2984, 0.4619]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17340.53168343532
Iteration 100: Loss = -12061.261985940631
Iteration 200: Loss = -11735.246165085722
Iteration 300: Loss = -11727.015591447956
Iteration 400: Loss = -11726.232522651306
Iteration 500: Loss = -11725.859261371288
Iteration 600: Loss = -11725.644205426206
Iteration 700: Loss = -11725.507017998027
Iteration 800: Loss = -11725.413481217021
Iteration 900: Loss = -11725.346299093655
Iteration 1000: Loss = -11725.29619660285
Iteration 1100: Loss = -11725.257650283
Iteration 1200: Loss = -11725.227314620472
Iteration 1300: Loss = -11725.202982662855
Iteration 1400: Loss = -11725.1831003093
Iteration 1500: Loss = -11725.166622789853
Iteration 1600: Loss = -11725.15285794397
Iteration 1700: Loss = -11725.141158669267
Iteration 1800: Loss = -11725.131148587645
Iteration 1900: Loss = -11725.122569933023
Iteration 2000: Loss = -11725.115182814148
Iteration 2100: Loss = -11725.108603926385
Iteration 2200: Loss = -11725.102894778
Iteration 2300: Loss = -11725.097808692613
Iteration 2400: Loss = -11725.093309582046
Iteration 2500: Loss = -11725.089281062985
Iteration 2600: Loss = -11725.085685638458
Iteration 2700: Loss = -11725.082484695682
Iteration 2800: Loss = -11725.079549438306
Iteration 2900: Loss = -11725.076968481639
Iteration 3000: Loss = -11725.074589082342
Iteration 3100: Loss = -11725.072357167137
Iteration 3200: Loss = -11725.070401533294
Iteration 3300: Loss = -11725.068588133514
Iteration 3400: Loss = -11725.066926056117
Iteration 3500: Loss = -11725.065404640776
Iteration 3600: Loss = -11725.06400852684
Iteration 3700: Loss = -11725.06274589821
Iteration 3800: Loss = -11725.061553474941
Iteration 3900: Loss = -11725.060452415779
Iteration 4000: Loss = -11725.059437337834
Iteration 4100: Loss = -11725.058483884546
Iteration 4200: Loss = -11725.057637813215
Iteration 4300: Loss = -11725.056825705773
Iteration 4400: Loss = -11725.05605266281
Iteration 4500: Loss = -11725.055318670931
Iteration 4600: Loss = -11725.05479942691
Iteration 4700: Loss = -11725.054078000765
Iteration 4800: Loss = -11725.053482301299
Iteration 4900: Loss = -11725.053161994256
Iteration 5000: Loss = -11725.052496871147
Iteration 5100: Loss = -11725.052011440228
Iteration 5200: Loss = -11725.052710629352
1
Iteration 5300: Loss = -11725.051144627729
Iteration 5400: Loss = -11725.050807719748
Iteration 5500: Loss = -11725.053798958648
1
Iteration 5600: Loss = -11725.050096090417
Iteration 5700: Loss = -11725.049752474279
Iteration 5800: Loss = -11725.050721243786
1
Iteration 5900: Loss = -11725.05544611721
2
Iteration 6000: Loss = -11725.048939470724
Iteration 6100: Loss = -11725.048682905905
Iteration 6200: Loss = -11725.052271108809
1
Iteration 6300: Loss = -11725.04837166691
Iteration 6400: Loss = -11725.050628628223
1
Iteration 6500: Loss = -11725.04795248854
Iteration 6600: Loss = -11725.077428545495
1
Iteration 6700: Loss = -11725.047700595002
Iteration 6800: Loss = -11725.055341164942
1
Iteration 6900: Loss = -11725.047174418896
Iteration 7000: Loss = -11725.145408818367
1
Iteration 7100: Loss = -11725.047160165224
Iteration 7200: Loss = -11725.046736833661
Iteration 7300: Loss = -11725.04841582926
1
Iteration 7400: Loss = -11725.046863078067
2
Iteration 7500: Loss = -11725.04651652127
Iteration 7600: Loss = -11725.04636891676
Iteration 7700: Loss = -11725.053748372731
1
Iteration 7800: Loss = -11725.06185397498
2
Iteration 7900: Loss = -11725.046134032846
Iteration 8000: Loss = -11725.063859612592
1
Iteration 8100: Loss = -11725.045939916128
Iteration 8200: Loss = -11725.051453877197
1
Iteration 8300: Loss = -11725.045737117814
Iteration 8400: Loss = -11725.047929732726
1
Iteration 8500: Loss = -11725.060286490285
2
Iteration 8600: Loss = -11725.062253513417
3
Iteration 8700: Loss = -11725.047285222476
4
Iteration 8800: Loss = -11725.071398175185
5
Iteration 8900: Loss = -11725.056325322346
6
Iteration 9000: Loss = -11725.07321199965
7
Iteration 9100: Loss = -11725.124012636894
8
Iteration 9200: Loss = -11725.046529724845
9
Iteration 9300: Loss = -11725.04550762208
Iteration 9400: Loss = -11725.046652664236
1
Iteration 9500: Loss = -11725.04535328707
Iteration 9600: Loss = -11725.045381943228
1
Iteration 9700: Loss = -11725.048909396644
2
Iteration 9800: Loss = -11725.046754412018
3
Iteration 9900: Loss = -11725.047005888637
4
Iteration 10000: Loss = -11725.080281383893
5
Iteration 10100: Loss = -11725.045016608825
Iteration 10200: Loss = -11725.047640251807
1
Iteration 10300: Loss = -11725.073023570572
2
Iteration 10400: Loss = -11725.045531428106
3
Iteration 10500: Loss = -11725.045494865975
4
Iteration 10600: Loss = -11725.045442553683
5
Iteration 10700: Loss = -11725.045770943874
6
Iteration 10800: Loss = -11725.124073855506
7
Iteration 10900: Loss = -11725.045012253613
Iteration 11000: Loss = -11725.05930752154
1
Iteration 11100: Loss = -11725.048950575494
2
Iteration 11200: Loss = -11725.045182231463
3
Iteration 11300: Loss = -11725.045858387228
4
Iteration 11400: Loss = -11725.069890232728
5
Iteration 11500: Loss = -11725.056608954363
6
Iteration 11600: Loss = -11725.06358708941
7
Iteration 11700: Loss = -11725.04500974073
Iteration 11800: Loss = -11725.04525379926
1
Iteration 11900: Loss = -11725.04941554034
2
Iteration 12000: Loss = -11725.070841332654
3
Iteration 12100: Loss = -11725.053602340706
4
Iteration 12200: Loss = -11725.056411376052
5
Iteration 12300: Loss = -11725.044837746533
Iteration 12400: Loss = -11725.04540628904
1
Iteration 12500: Loss = -11725.044589756339
Iteration 12600: Loss = -11725.045116711026
1
Iteration 12700: Loss = -11725.04735793727
2
Iteration 12800: Loss = -11725.052946883927
3
Iteration 12900: Loss = -11725.082440499395
4
Iteration 13000: Loss = -11725.048653563905
5
Iteration 13100: Loss = -11725.047688662335
6
Iteration 13200: Loss = -11725.047426549132
7
Iteration 13300: Loss = -11725.075681126273
8
Iteration 13400: Loss = -11725.205359203954
9
Iteration 13500: Loss = -11725.044861979244
10
Stopping early at iteration 13500 due to no improvement.
tensor([[  4.0075,  -5.4133],
        [  6.8841,  -9.4309],
        [  2.6086,  -7.2238],
        [  6.8457,  -9.5790],
        [ -9.3579,   6.4315],
        [ -7.9989,   6.3415],
        [  5.3123,  -6.8174],
        [ -7.7582,   6.2740],
        [ -7.2350,   5.5447],
        [  5.7744, -10.3896],
        [ -9.0769,   6.3340],
        [  2.2098,  -5.3079],
        [ -8.3719,   6.8738],
        [  5.1968,  -7.0096],
        [  6.3780,  -7.8386],
        [  7.1040,  -8.7769],
        [ -9.1065,   7.2518],
        [  5.4338,  -8.2498],
        [ -8.4088,   6.6299],
        [ -8.6378,   6.3997],
        [ -7.5703,   5.8503],
        [  7.3457,  -8.7677],
        [ -8.3080,   6.9014],
        [  6.4950,  -8.2306],
        [  6.8566,  -8.2998],
        [  3.9599,  -7.9514],
        [  7.6578,  -9.0441],
        [  7.5643,  -9.2943],
        [ -9.2079,   6.0337],
        [  6.3978, -10.1871],
        [ -8.0578,   6.1925],
        [ -8.3749,   6.6945],
        [  7.4956,  -9.6443],
        [  5.6483,  -7.1853],
        [  7.8384,  -9.3071],
        [ -8.4770,   6.3308],
        [  7.6481, -10.8443],
        [ -7.4044,   5.9995],
        [  5.4733,  -7.0320],
        [  7.3733,  -9.2836],
        [ -7.4546,   5.9548],
        [  2.8652,  -4.2583],
        [  6.8565,  -8.4750],
        [  7.3579,  -9.1528],
        [ -5.1039,   3.1827],
        [ -5.0310,   3.2578],
        [  7.5051,  -9.0924],
        [ -9.6082,   4.9996],
        [ -8.1305,   5.9357],
        [ -8.0437,   6.4486],
        [ -8.0360,   6.5007],
        [  4.2570,  -6.2257],
        [ -2.8646,   1.0349],
        [  7.2267,  -9.1277],
        [ -9.9433,   5.5632],
        [ -5.3783,   3.9524],
        [  5.9805,  -7.4821],
        [  7.0872,  -9.6280],
        [  7.1647,  -8.5587],
        [  7.3036,  -8.6921],
        [  5.0349,  -6.5394],
        [  7.4035,  -8.9316],
        [ -9.3186,   4.7034],
        [  7.1030,  -9.9909],
        [ -8.6029,   6.9221],
        [ -7.1734,   4.0141],
        [  7.7419,  -9.4954],
        [ -7.9028,   5.1760],
        [  7.1005,  -9.0219],
        [  1.3574,  -5.0674],
        [  7.6597, -10.9809],
        [  5.5565,  -7.5102],
        [  7.2376,  -8.9632],
        [ -4.0749,   2.6770],
        [  6.7062,  -8.5555],
        [ -4.3705,   2.2827],
        [  6.0727,  -7.5627],
        [  1.8754,  -3.2622],
        [ -8.2236,   6.7681],
        [  7.2617,  -9.2243],
        [ -5.5877,   4.1860],
        [ -6.0900,   4.6803],
        [  7.1045,  -8.6374],
        [ -8.5995,   7.1795],
        [  6.6016,  -8.4454],
        [ -3.7620,   1.8277],
        [  4.8348,  -6.6805],
        [ -8.3190,   6.1121],
        [ -3.9693,   2.4337],
        [  5.3118,  -8.0950],
        [ -7.2931,   5.6771],
        [  6.3752,  -7.8896],
        [ -7.3478,   4.4485],
        [  7.2776,  -8.7818],
        [  6.9321, -10.1018],
        [ -7.4450,   5.9220],
        [  7.7150, -10.1004],
        [ -5.5661,   4.1259],
        [  5.8939,  -8.0651],
        [  2.0408,  -3.7493]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7622, 0.2378],
        [0.2375, 0.7625]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5702, 0.4298], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4003, 0.1090],
         [0.1660, 0.1972]],

        [[0.6591, 0.0981],
         [0.7760, 0.9334]],

        [[0.6851, 0.0943],
         [0.7297, 0.9977]],

        [[0.8080, 0.1057],
         [0.9570, 0.6191]],

        [[0.8855, 0.1034],
         [0.2984, 0.4619]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919992163297293
Iteration 0: Loss = -22269.038098992285
Iteration 10: Loss = -12600.405487831975
Iteration 20: Loss = -11726.868259514727
Iteration 30: Loss = -11726.905736492105
1
Iteration 40: Loss = -11726.905742656654
2
Iteration 50: Loss = -11726.905742656654
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7606, 0.2394],
        [0.2390, 0.7610]], dtype=torch.float64)
alpha: tensor([0.5307, 0.4693])
beta: tensor([[[0.3927, 0.1089],
         [0.9447, 0.1929]],

        [[0.6092, 0.0980],
         [0.2554, 0.8615]],

        [[0.5846, 0.0946],
         [0.9147, 0.0371]],

        [[0.8532, 0.1054],
         [0.1648, 0.4954]],

        [[0.4599, 0.1034],
         [0.5054, 0.9035]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22268.698629872346
Iteration 100: Loss = -12594.89390594967
Iteration 200: Loss = -12501.991247325937
Iteration 300: Loss = -12050.361916108388
Iteration 400: Loss = -11830.596337050547
Iteration 500: Loss = -11813.609248949799
Iteration 600: Loss = -11802.032312344221
Iteration 700: Loss = -11801.462510390364
Iteration 800: Loss = -11801.11774239066
Iteration 900: Loss = -11800.888019990649
Iteration 1000: Loss = -11800.72515851335
Iteration 1100: Loss = -11800.60452816891
Iteration 1200: Loss = -11800.512219599776
Iteration 1300: Loss = -11800.439599574587
Iteration 1400: Loss = -11800.381379339815
Iteration 1500: Loss = -11800.3336633052
Iteration 1600: Loss = -11800.293898960925
Iteration 1700: Loss = -11800.259584038584
Iteration 1800: Loss = -11786.72644136402
Iteration 1900: Loss = -11786.67148795723
Iteration 2000: Loss = -11786.64764206046
Iteration 2100: Loss = -11786.627728318455
Iteration 2200: Loss = -11786.61042531579
Iteration 2300: Loss = -11786.606440656009
Iteration 2400: Loss = -11786.581163791705
Iteration 2500: Loss = -11786.568380344348
Iteration 2600: Loss = -11786.556486989619
Iteration 2700: Loss = -11786.545740999614
Iteration 2800: Loss = -11786.52842082452
Iteration 2900: Loss = -11773.952076586442
Iteration 3000: Loss = -11773.978091260748
1
Iteration 3100: Loss = -11773.935849052385
Iteration 3200: Loss = -11773.93009407058
Iteration 3300: Loss = -11773.925531707959
Iteration 3400: Loss = -11773.920149549494
Iteration 3500: Loss = -11773.915817874555
Iteration 3600: Loss = -11773.914972522292
Iteration 3700: Loss = -11773.908298020728
Iteration 3800: Loss = -11773.904958891893
Iteration 3900: Loss = -11773.971461275116
1
Iteration 4000: Loss = -11773.898994830231
Iteration 4100: Loss = -11773.901857771698
1
Iteration 4200: Loss = -11773.902444212496
2
Iteration 4300: Loss = -11773.827254655575
Iteration 4400: Loss = -11761.950408295545
Iteration 4500: Loss = -11761.950619550467
1
Iteration 4600: Loss = -11761.946498050664
Iteration 4700: Loss = -11761.943890247874
Iteration 4800: Loss = -11749.73566349885
Iteration 4900: Loss = -11749.742982132475
1
Iteration 5000: Loss = -11749.731181307057
Iteration 5100: Loss = -11749.729738426793
Iteration 5200: Loss = -11749.728987866034
Iteration 5300: Loss = -11749.727245998849
Iteration 5400: Loss = -11749.726372634977
Iteration 5500: Loss = -11749.725147016083
Iteration 5600: Loss = -11749.724109832547
Iteration 5700: Loss = -11749.724322998487
1
Iteration 5800: Loss = -11749.722365603702
Iteration 5900: Loss = -11749.721536186333
Iteration 6000: Loss = -11749.726238298032
1
Iteration 6100: Loss = -11744.48916030636
Iteration 6200: Loss = -11744.487823765456
Iteration 6300: Loss = -11744.487581683765
Iteration 6400: Loss = -11744.486435038341
Iteration 6500: Loss = -11744.48576237242
Iteration 6600: Loss = -11744.50603558635
1
Iteration 6700: Loss = -11744.484635207493
Iteration 6800: Loss = -11744.48415295957
Iteration 6900: Loss = -11744.50183357504
1
Iteration 7000: Loss = -11744.483360906399
Iteration 7100: Loss = -11744.482800428084
Iteration 7200: Loss = -11744.483924215203
1
Iteration 7300: Loss = -11744.482391330439
Iteration 7400: Loss = -11744.482834407894
1
Iteration 7500: Loss = -11744.440483749437
Iteration 7600: Loss = -11744.44100613034
1
Iteration 7700: Loss = -11744.439971590358
Iteration 7800: Loss = -11744.44830730079
1
Iteration 7900: Loss = -11744.447259394252
2
Iteration 8000: Loss = -11744.441625141397
3
Iteration 8100: Loss = -11744.440444158041
4
Iteration 8200: Loss = -11744.439185259278
Iteration 8300: Loss = -11744.441200055866
1
Iteration 8400: Loss = -11744.444070560869
2
Iteration 8500: Loss = -11744.45128631141
3
Iteration 8600: Loss = -11733.400315957862
Iteration 8700: Loss = -11733.355015061374
Iteration 8800: Loss = -11733.354858223664
Iteration 8900: Loss = -11733.39375809244
1
Iteration 9000: Loss = -11733.358942774108
2
Iteration 9100: Loss = -11733.354361391985
Iteration 9200: Loss = -11733.40993093001
1
Iteration 9300: Loss = -11733.360696752505
2
Iteration 9400: Loss = -11733.353929329907
Iteration 9500: Loss = -11733.361588352225
1
Iteration 9600: Loss = -11733.415923210317
2
Iteration 9700: Loss = -11733.353531323783
Iteration 9800: Loss = -11733.361558183333
1
Iteration 9900: Loss = -11733.353627245055
2
Iteration 10000: Loss = -11733.358093400675
3
Iteration 10100: Loss = -11733.387971593851
4
Iteration 10200: Loss = -11733.357963879753
5
Iteration 10300: Loss = -11733.353499577075
Iteration 10400: Loss = -11733.353866546586
1
Iteration 10500: Loss = -11733.35575885193
2
Iteration 10600: Loss = -11733.373927420545
3
Iteration 10700: Loss = -11733.42517501161
4
Iteration 10800: Loss = -11733.351305127899
Iteration 10900: Loss = -11733.353964658672
1
Iteration 11000: Loss = -11733.527549424925
2
Iteration 11100: Loss = -11733.35788370633
3
Iteration 11200: Loss = -11733.369815774367
4
Iteration 11300: Loss = -11733.487307735502
5
Iteration 11400: Loss = -11733.350929151717
Iteration 11500: Loss = -11733.351399529356
1
Iteration 11600: Loss = -11733.441804268987
2
Iteration 11700: Loss = -11733.366562107143
3
Iteration 11800: Loss = -11733.356330421579
4
Iteration 11900: Loss = -11733.366653865742
5
Iteration 12000: Loss = -11733.365440036465
6
Iteration 12100: Loss = -11733.35339706104
7
Iteration 12200: Loss = -11733.37045192412
8
Iteration 12300: Loss = -11733.354129106707
9
Iteration 12400: Loss = -11732.082454782689
Iteration 12500: Loss = -11731.880505280255
Iteration 12600: Loss = -11731.87589053654
Iteration 12700: Loss = -11731.87649511804
1
Iteration 12800: Loss = -11731.86928143016
Iteration 12900: Loss = -11731.869190006188
Iteration 13000: Loss = -11731.879503280661
1
Iteration 13100: Loss = -11731.8692847734
2
Iteration 13200: Loss = -11731.869259997438
3
Iteration 13300: Loss = -11731.876114043547
4
Iteration 13400: Loss = -11731.88273343155
5
Iteration 13500: Loss = -11731.878925672036
6
Iteration 13600: Loss = -11731.91685245101
7
Iteration 13700: Loss = -11731.872926413029
8
Iteration 13800: Loss = -11731.870066572159
9
Iteration 13900: Loss = -11731.871262356572
10
Stopping early at iteration 13900 due to no improvement.
tensor([[  2.7906,  -4.3066],
        [  6.3340,  -8.2009],
        [  4.1397,  -5.5498],
        [  7.2242,  -9.2933],
        [ -8.2243,   6.4694],
        [ -7.9990,   6.6014],
        [  7.4686,  -9.5232],
        [ -7.5877,   6.1118],
        [ -8.0471,   4.3459],
        [  6.4933,  -8.4723],
        [ -8.2482,   6.7312],
        [  2.9502,  -4.4757],
        [ -8.2800,   6.5078],
        [  6.3111,  -8.0299],
        [  8.0886, -10.7431],
        [  6.3505,  -9.8075],
        [ -8.7080,   7.0132],
        [  5.4331,  -7.8516],
        [-10.2532,   5.6379],
        [ -8.2268,   6.5903],
        [ -7.3315,   5.9184],
        [  7.5605,  -9.6234],
        [ -8.0891,   6.5239],
        [  6.3162,  -7.8161],
        [  7.0518,  -9.2470],
        [  5.2077,  -6.6151],
        [  7.9266,  -9.3532],
        [  8.5861,  -9.9936],
        [ -7.9748,   6.5698],
        [  8.3081, -10.2357],
        [ -8.0027,   6.2244],
        [ -8.4263,   6.4794],
        [  7.2633,  -8.7615],
        [  7.2113, -10.5065],
        [  7.7758,  -9.2342],
        [ -8.4429,   6.9473],
        [  7.1172,  -8.6660],
        [ -7.9561,   6.5563],
        [  6.0400,  -7.4658],
        [  7.6762,  -9.7506],
        [ -7.2442,   5.8551],
        [ -9.4679,   6.2759],
        [  6.2899,  -8.7698],
        [  8.5346,  -9.9209],
        [ -5.9637,   4.3797],
        [ -4.7096,   3.3215],
        [  7.3543,  -8.8544],
        [ -7.9959,   6.1439],
        [ -7.7463,   5.7212],
        [ -8.6563,   6.6024],
        [ -9.7426,   5.1274],
        [  4.5496,  -5.9359],
        [ -2.5750,   1.1332],
        [  8.1119, -11.1691],
        [ -8.1323,   6.6916],
        [ -5.3409,   3.5709],
        [  8.0418, -10.7792],
        [  8.5271, -10.2071],
        [  6.9856,  -8.4405],
        [  7.6927,  -9.4635],
        [  4.9259,  -6.4237],
        [  7.5340,  -9.7671],
        [ -7.3156,   5.9124],
        [  6.4802,  -7.9953],
        [ -8.3555,   6.8832],
        [ -6.2354,   4.3438],
        [  6.5591,  -9.7356],
        [ -7.4478,   5.4822],
        [  8.4652, -10.1035],
        [  2.4921,  -3.8784],
        [  7.6177,  -9.2922],
        [  7.3071, -11.8986],
        [  7.7738,  -9.1956],
        [ -5.3346,   3.2734],
        [  6.6128,  -8.0288],
        [ -3.9308,   2.4442],
        [  5.8265,  -7.3323],
        [  0.2581,  -4.8733],
        [ -9.4502,   6.6707],
        [  7.1067,  -9.7701],
        [ -5.3481,   3.8996],
        [ -5.8267,   4.4330],
        [  7.6817, -10.9399],
        [ -8.9454,   6.9632],
        [  6.1510,  -7.8482],
        [ -3.9629,   1.3974],
        [  3.9086,  -7.4988],
        [ -7.5615,   6.1344],
        [ -3.9400,   2.0088],
        [  7.1856,  -9.4989],
        [ -7.2699,   5.4311],
        [  7.5306,  -9.2453],
        [ -6.3374,   4.9425],
        [  7.9170,  -9.3087],
        [  8.2534,  -9.7574],
        [ -7.7259,   6.3075],
        [  8.1627,  -9.6815],
        [ -5.3255,   3.9385],
        [  6.1260,  -7.5932],
        [  1.6262,  -4.1014]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7609, 0.2391],
        [0.2414, 0.7586]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5631, 0.4369], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4014, 0.1129],
         [0.9447, 0.1961]],

        [[0.6092, 0.0980],
         [0.2554, 0.8615]],

        [[0.5846, 0.0943],
         [0.9147, 0.0371]],

        [[0.8532, 0.1056],
         [0.1648, 0.4954]],

        [[0.4599, 0.1035],
         [0.5054, 0.9035]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999854008352
Average Adjusted Rand Index: 0.9919996552039955
11730.03503185901
new:  [0.472289173952589, 0.5348914086196361, 0.9919999730634713, 0.9919999854008352] [0.8491963879819396, 0.8114186722665842, 0.9919992163297293, 0.9919996552039955] [11929.088607652153, 11899.438634376125, 11725.044861979244, 11731.871262356572]
prior:  [0.9919999730634713, 0.9919999730634713, 0.9919999730634713, 0.9919999730634713] [0.9919992163297293, 0.9919992163297293, 0.9919992163297293, 0.9919992163297293] [11726.905738528485, 11726.905741108114, 11726.905740096923, 11726.905742656654]
-----------------------------------------------------------------------------------------
This iteration is 7
True Objective function: Loss = -11308.031146132842
Iteration 0: Loss = -20434.78226404765
Iteration 10: Loss = -11576.70808954682
Iteration 20: Loss = -11294.904931902629
Iteration 30: Loss = -11294.904779607876
Iteration 40: Loss = -11294.904779594055
Iteration 50: Loss = -11294.904779594055
1
Iteration 60: Loss = -11294.904779594055
2
Iteration 70: Loss = -11294.904779594055
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7091, 0.2909],
        [0.1890, 0.8110]], dtype=torch.float64)
alpha: tensor([0.4407, 0.5593])
beta: tensor([[[0.3889, 0.0871],
         [0.8248, 0.1894]],

        [[0.8454, 0.0889],
         [0.4931, 0.0082]],

        [[0.4870, 0.1026],
         [0.5270, 0.6473]],

        [[0.3051, 0.1212],
         [0.0369, 0.7271]],

        [[0.3359, 0.1084],
         [0.7271, 0.9015]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9919988588426287
Average Adjusted Rand Index: 0.99199877740731
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20552.41924731981
Iteration 100: Loss = -11939.954827514277
Iteration 200: Loss = -11938.608684150015
Iteration 300: Loss = -11934.108615245159
Iteration 400: Loss = -11841.720099082866
Iteration 500: Loss = -11538.158209797595
Iteration 600: Loss = -11339.89471284136
Iteration 700: Loss = -11310.025363804309
Iteration 800: Loss = -11309.81247847147
Iteration 900: Loss = -11306.031620972364
Iteration 1000: Loss = -11305.976787859803
Iteration 1100: Loss = -11305.941938147907
Iteration 1200: Loss = -11305.913332648643
Iteration 1300: Loss = -11305.896720668721
Iteration 1400: Loss = -11305.874572787716
Iteration 1500: Loss = -11305.85968669801
Iteration 1600: Loss = -11305.848769729118
Iteration 1700: Loss = -11305.836224090215
Iteration 1800: Loss = -11305.830778873767
Iteration 1900: Loss = -11305.826078992372
Iteration 2000: Loss = -11305.821940806532
Iteration 2100: Loss = -11305.818060607453
Iteration 2200: Loss = -11305.814261851836
Iteration 2300: Loss = -11305.811263046991
Iteration 2400: Loss = -11305.79964230175
Iteration 2500: Loss = -11305.790618357043
Iteration 2600: Loss = -11305.788886809905
Iteration 2700: Loss = -11305.785680235576
Iteration 2800: Loss = -11305.774663714517
Iteration 2900: Loss = -11305.772882574234
Iteration 3000: Loss = -11305.771671222368
Iteration 3100: Loss = -11305.770566939562
Iteration 3200: Loss = -11305.769617576429
Iteration 3300: Loss = -11305.768832689788
Iteration 3400: Loss = -11305.767903422162
Iteration 3500: Loss = -11305.767136585697
Iteration 3600: Loss = -11305.772041090604
1
Iteration 3700: Loss = -11305.76574377328
Iteration 3800: Loss = -11305.765240814193
Iteration 3900: Loss = -11305.76901268383
1
Iteration 4000: Loss = -11305.764141745034
Iteration 4100: Loss = -11305.773370170007
1
Iteration 4200: Loss = -11305.76356747897
Iteration 4300: Loss = -11305.762928052678
Iteration 4400: Loss = -11305.763137325062
1
Iteration 4500: Loss = -11305.762300365443
Iteration 4600: Loss = -11305.803179781748
1
Iteration 4700: Loss = -11305.76436479677
2
Iteration 4800: Loss = -11305.761340650019
Iteration 4900: Loss = -11305.761494559714
1
Iteration 5000: Loss = -11305.766415202406
2
Iteration 5100: Loss = -11305.76069469734
Iteration 5200: Loss = -11305.759255953264
Iteration 5300: Loss = -11301.181796297587
Iteration 5400: Loss = -11301.186076050852
1
Iteration 5500: Loss = -11301.181032916553
Iteration 5600: Loss = -11301.181261804286
1
Iteration 5700: Loss = -11301.184525553994
2
Iteration 5800: Loss = -11301.182462069504
3
Iteration 5900: Loss = -11301.180900671236
Iteration 6000: Loss = -11301.181667070552
1
Iteration 6100: Loss = -11301.18167514418
2
Iteration 6200: Loss = -11301.180785586894
Iteration 6300: Loss = -11301.201815851808
1
Iteration 6400: Loss = -11301.18622408985
2
Iteration 6500: Loss = -11301.180254282146
Iteration 6600: Loss = -11301.19942378687
1
Iteration 6700: Loss = -11301.179787800076
Iteration 6800: Loss = -11301.183316324596
1
Iteration 6900: Loss = -11301.180733610681
2
Iteration 7000: Loss = -11301.183576410598
3
Iteration 7100: Loss = -11301.183090027944
4
Iteration 7200: Loss = -11301.179416313269
Iteration 7300: Loss = -11301.185862466813
1
Iteration 7400: Loss = -11301.182590644388
2
Iteration 7500: Loss = -11301.189537236156
3
Iteration 7600: Loss = -11301.179012321121
Iteration 7700: Loss = -11301.19806505963
1
Iteration 7800: Loss = -11301.178935072303
Iteration 7900: Loss = -11301.178442839178
Iteration 8000: Loss = -11301.178962242702
1
Iteration 8100: Loss = -11301.179122238571
2
Iteration 8200: Loss = -11301.193585205036
3
Iteration 8300: Loss = -11301.185531142091
4
Iteration 8400: Loss = -11301.18814424001
5
Iteration 8500: Loss = -11301.182279819797
6
Iteration 8600: Loss = -11301.18357392727
7
Iteration 8700: Loss = -11301.178007133296
Iteration 8800: Loss = -11301.17853624595
1
Iteration 8900: Loss = -11295.88345391334
Iteration 9000: Loss = -11295.847103181888
Iteration 9100: Loss = -11295.84649732401
Iteration 9200: Loss = -11295.846545310127
1
Iteration 9300: Loss = -11295.851568641187
2
Iteration 9400: Loss = -11295.92311920889
3
Iteration 9500: Loss = -11295.850587671406
4
Iteration 9600: Loss = -11295.850774067905
5
Iteration 9700: Loss = -11295.850380398568
6
Iteration 9800: Loss = -11295.848319711306
7
Iteration 9900: Loss = -11295.836103565232
Iteration 10000: Loss = -11295.83332731061
Iteration 10100: Loss = -11295.834337377846
1
Iteration 10200: Loss = -11295.84241693998
2
Iteration 10300: Loss = -11295.834474940844
3
Iteration 10400: Loss = -11295.834729904314
4
Iteration 10500: Loss = -11295.834464130601
5
Iteration 10600: Loss = -11295.839813734421
6
Iteration 10700: Loss = -11295.834965537673
7
Iteration 10800: Loss = -11295.843402407925
8
Iteration 10900: Loss = -11295.844212196467
9
Iteration 11000: Loss = -11295.890589651292
10
Stopping early at iteration 11000 due to no improvement.
tensor([[  5.4527, -10.0680],
        [  6.6463, -11.2615],
        [  7.0554, -11.6707],
        [  2.3504,  -6.9656],
        [  5.2921,  -9.9074],
        [-10.1869,   5.5717],
        [-10.5715,   5.9563],
        [ -9.8293,   5.2141],
        [  6.3942, -11.0094],
        [  7.2785, -11.8937],
        [  5.8125, -10.4277],
        [  6.3387, -10.9539],
        [  4.9643,  -9.5795],
        [ -6.6900,   2.0748],
        [  7.1288, -11.7440],
        [  5.1194,  -9.7346],
        [-10.2931,   5.6779],
        [-10.4511,   5.8359],
        [  6.6968, -11.3120],
        [  7.3496, -11.9648],
        [  6.5456, -11.1608],
        [ -9.4362,   4.8210],
        [ -3.7861,  -0.8291],
        [  1.7315,  -6.3467],
        [  7.3026, -11.9179],
        [ -8.0709,   3.4557],
        [ -9.6495,   5.0343],
        [-10.0775,   5.4622],
        [  6.7939, -11.4091],
        [  5.9844, -10.5997],
        [  5.3003,  -9.9156],
        [  6.1557, -10.7709],
        [-11.4163,   6.8011],
        [ -9.8266,   5.2114],
        [  6.0777, -10.6929],
        [  6.3160, -10.9312],
        [ -7.2701,   2.6549],
        [  6.1348, -10.7500],
        [-10.5376,   5.9224],
        [-10.3793,   5.7641],
        [  5.4159, -10.0311],
        [-10.5178,   5.9026],
        [  7.0372, -11.6524],
        [  6.1311, -10.7463],
        [-11.2179,   6.6027],
        [-11.0489,   6.4336],
        [  4.8362,  -9.4514],
        [-10.6694,   6.0541],
        [  7.2779, -11.8931],
        [-10.8146,   6.1994],
        [  5.4608, -10.0761],
        [ -9.8035,   5.1882],
        [ -8.1125,   3.4973],
        [ -9.2640,   4.6488],
        [  4.2639,  -8.8791],
        [ -9.4582,   4.8430],
        [-10.3352,   5.7200],
        [  5.9057, -10.5209],
        [ -6.6071,   1.9919],
        [ -9.1515,   4.5362],
        [-11.2094,   6.5942],
        [ -9.1259,   4.5106],
        [  6.8067, -11.4219],
        [ -6.6990,   2.0838],
        [  5.2359,  -9.8511],
        [  6.9328, -11.5480],
        [ -7.7303,   3.1151],
        [ -9.0471,   4.4319],
        [-11.4954,   6.8802],
        [  4.9959,  -9.6112],
        [ -8.9246,   4.3093],
        [  7.1022, -11.7174],
        [  5.9754, -10.5907],
        [  7.1814, -11.7966],
        [ -9.2162,   4.6010],
        [ -9.1711,   4.5559],
        [  6.2527, -10.8679],
        [  5.5216, -10.1368],
        [ -7.0872,   2.4720],
        [  6.4667, -11.0820],
        [  5.4979, -10.1131],
        [  7.1478, -11.7630],
        [  7.2314, -11.8467],
        [-10.7408,   6.1255],
        [  5.4188, -10.0340],
        [-10.6824,   6.0671],
        [  6.9922, -11.6074],
        [  3.5849,  -8.2001],
        [ -9.8215,   5.2063],
        [  3.2365,  -7.8518],
        [  5.5098, -10.1250],
        [  6.8975, -11.5127],
        [ -8.7345,   4.1193],
        [ -8.5731,   3.9579],
        [-10.7430,   6.1277],
        [ -8.5732,   3.9580],
        [-10.4217,   5.8065],
        [ -9.0165,   4.4013],
        [  7.0721, -11.6874],
        [-10.9776,   6.3624]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7189, 0.2811],
        [0.1821, 0.8179]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5204, 0.4796], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3963, 0.0877],
         [0.8248, 0.1921]],

        [[0.8454, 0.0890],
         [0.4931, 0.0082]],

        [[0.4870, 0.1031],
         [0.5270, 0.6473]],

        [[0.3051, 0.1203],
         [0.0369, 0.7271]],

        [[0.3359, 0.1087],
         [0.7271, 0.9015]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9840299598123581
Average Adjusted Rand Index: 0.98399877740731
Iteration 0: Loss = -29224.399825946686
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.5678,    nan]],

        [[0.7693,    nan],
         [0.0351, 0.4014]],

        [[0.4550,    nan],
         [0.8878, 0.2082]],

        [[0.4072,    nan],
         [0.3969, 0.6200]],

        [[0.6588,    nan],
         [0.3711, 0.3423]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29224.927069232945
Iteration 100: Loss = -11959.349904768776
Iteration 200: Loss = -11939.421453539546
Iteration 300: Loss = -11921.491786305729
Iteration 400: Loss = -11886.008302142194
Iteration 500: Loss = -11839.24818767077
Iteration 600: Loss = -11696.90447852024
Iteration 700: Loss = -11487.909658757895
Iteration 800: Loss = -11352.932968189169
Iteration 900: Loss = -11322.500313835726
Iteration 1000: Loss = -11308.277786258952
Iteration 1100: Loss = -11307.44539505365
Iteration 1200: Loss = -11306.961504643503
Iteration 1300: Loss = -11306.63522449184
Iteration 1400: Loss = -11306.400236723044
Iteration 1500: Loss = -11306.223970108209
Iteration 1600: Loss = -11306.08644352047
Iteration 1700: Loss = -11305.975254992141
Iteration 1800: Loss = -11305.882612321879
Iteration 1900: Loss = -11305.79954966454
Iteration 2000: Loss = -11305.604104207388
Iteration 2100: Loss = -11302.83175780503
Iteration 2200: Loss = -11302.763654579981
Iteration 2300: Loss = -11302.719411619999
Iteration 2400: Loss = -11302.683387018673
Iteration 2500: Loss = -11302.652678158793
Iteration 2600: Loss = -11302.625953422852
Iteration 2700: Loss = -11302.602479554991
Iteration 2800: Loss = -11302.581672226506
Iteration 2900: Loss = -11302.563191355654
Iteration 3000: Loss = -11302.546566943247
Iteration 3100: Loss = -11302.531632274678
Iteration 3200: Loss = -11302.518675114661
Iteration 3300: Loss = -11302.505919904226
Iteration 3400: Loss = -11302.49480935124
Iteration 3500: Loss = -11302.48750814528
Iteration 3600: Loss = -11302.475398528384
Iteration 3700: Loss = -11302.466959530488
Iteration 3800: Loss = -11302.459937809783
Iteration 3900: Loss = -11302.45200150995
Iteration 4000: Loss = -11302.445415190465
Iteration 4100: Loss = -11302.4393216026
Iteration 4200: Loss = -11302.433687972507
Iteration 4300: Loss = -11302.428489100397
Iteration 4400: Loss = -11302.423605930562
Iteration 4500: Loss = -11302.419420052698
Iteration 4600: Loss = -11302.41494731004
Iteration 4700: Loss = -11302.411126962952
Iteration 4800: Loss = -11302.437095335124
1
Iteration 4900: Loss = -11302.404110680684
Iteration 5000: Loss = -11302.402044551745
Iteration 5100: Loss = -11302.398009088281
Iteration 5200: Loss = -11302.39634968738
Iteration 5300: Loss = -11302.392863135026
Iteration 5400: Loss = -11302.390296323523
Iteration 5500: Loss = -11302.388034752646
Iteration 5600: Loss = -11302.38598890772
Iteration 5700: Loss = -11302.383913189518
Iteration 5800: Loss = -11302.38206231727
Iteration 5900: Loss = -11302.381690962642
Iteration 6000: Loss = -11302.378691276543
Iteration 6100: Loss = -11302.377100364152
Iteration 6200: Loss = -11302.37601597029
Iteration 6300: Loss = -11302.375009216163
Iteration 6400: Loss = -11302.373012389808
Iteration 6500: Loss = -11302.424995714366
1
Iteration 6600: Loss = -11302.37065852717
Iteration 6700: Loss = -11302.416138472177
1
Iteration 6800: Loss = -11302.3686597831
Iteration 6900: Loss = -11302.3700275232
1
Iteration 7000: Loss = -11302.36669534891
Iteration 7100: Loss = -11302.367730804352
1
Iteration 7200: Loss = -11291.967483896942
Iteration 7300: Loss = -11291.96887404776
1
Iteration 7400: Loss = -11291.964970486895
Iteration 7500: Loss = -11291.965263010576
1
Iteration 7600: Loss = -11291.963375072115
Iteration 7700: Loss = -11291.962766551313
Iteration 7800: Loss = -11291.962474250824
Iteration 7900: Loss = -11291.960933576922
Iteration 8000: Loss = -11291.964462438324
1
Iteration 8100: Loss = -11291.958340422292
Iteration 8200: Loss = -11291.96441287828
1
Iteration 8300: Loss = -11291.954009907035
Iteration 8400: Loss = -11291.953799532532
Iteration 8500: Loss = -11291.96234670966
1
Iteration 8600: Loss = -11291.953684798671
Iteration 8700: Loss = -11291.961692992849
1
Iteration 8800: Loss = -11291.952517862146
Iteration 8900: Loss = -11291.984568837235
1
Iteration 9000: Loss = -11291.95087604508
Iteration 9100: Loss = -11291.953289895773
1
Iteration 9200: Loss = -11291.998260447854
2
Iteration 9300: Loss = -11291.969029568718
3
Iteration 9400: Loss = -11291.966700910989
4
Iteration 9500: Loss = -11291.955154005733
5
Iteration 9600: Loss = -11291.95472373877
6
Iteration 9700: Loss = -11291.964993348149
7
Iteration 9800: Loss = -11291.970009163097
8
Iteration 9900: Loss = -11291.965941323877
9
Iteration 10000: Loss = -11292.007165973257
10
Stopping early at iteration 10000 due to no improvement.
tensor([[ -8.0790,   5.0372],
        [ -7.6772,   6.2907],
        [ -8.3101,   6.6595],
        [ -5.7287,   3.5637],
        [ -7.6405,   6.1879],
        [  5.1099,  -6.6907],
        [  5.5217,  -6.9810],
        [  5.4247,  -7.1081],
        [ -7.3208,   5.8066],
        [ -8.1895,   6.7977],
        [ -7.9014,   6.4227],
        [ -8.8141,   6.5979],
        [ -7.0632,   4.7922],
        [  3.0638,  -5.8473],
        [ -8.3864,   6.3308],
        [ -8.5935,   6.0276],
        [  5.4783,  -7.2961],
        [  5.9912,  -7.4948],
        [ -7.6642,   6.0981],
        [ -7.1605,   5.7576],
        [ -7.3996,   5.8814],
        [  4.5799,  -8.0605],
        [  0.8469,  -2.2393],
        [ -4.7874,   3.2405],
        [ -7.6798,   6.2090],
        [  4.8700,  -6.2612],
        [  5.6788,  -7.4144],
        [  5.8411,  -7.2304],
        [ -8.5135,   6.6054],
        [ -7.6148,   5.8671],
        [ -7.8349,   6.1596],
        [-10.4457,   5.8305],
        [  5.5575,  -9.2457],
        [  6.3842,  -7.9300],
        [ -7.2770,   5.7724],
        [ -7.6893,   5.8907],
        [  6.4100,  -7.8575],
        [ -7.9215,   6.4982],
        [  5.4011,  -7.5643],
        [  6.3458,  -7.7333],
        [ -7.6872,   5.5277],
        [  5.1002,  -7.7018],
        [ -7.6916,   6.3004],
        [ -9.6337,   5.9550],
        [  5.6919,  -7.7788],
        [  5.7797,  -7.1993],
        [ -7.6551,   6.2665],
        [  5.3620,  -6.7698],
        [ -8.1544,   5.6999],
        [  5.6904,  -9.2404],
        [ -8.2115,   6.8103],
        [  6.3856,  -7.7783],
        [  4.6809,  -6.1240],
        [  4.3308,  -6.6331],
        [ -7.7175,   6.2916],
        [  5.7553,  -7.1430],
        [  4.7646,  -8.5387],
        [ -6.8492,   5.1698],
        [  3.6578,  -5.0746],
        [  5.8555,  -7.2581],
        [  4.9913,  -8.8491],
        [  6.3940,  -7.8891],
        [ -7.8386,   6.4220],
        [  3.6089,  -5.3066],
        [ -8.4812,   6.6655],
        [ -7.8804,   6.4476],
        [  5.3062,  -9.1623],
        [  5.1629,  -6.5709],
        [  5.2784,  -7.4459],
        [ -8.6839,   6.7276],
        [  4.4387,  -7.9727],
        [ -8.2852,   6.4745],
        [ -5.7545,   3.2973],
        [ -8.2790,   6.7515],
        [  5.4163,  -7.2207],
        [  5.3218,  -8.0809],
        [ -8.0870,   6.6999],
        [ -7.3710,   5.5626],
        [  4.1038,  -5.5366],
        [ -7.3107,   5.7706],
        [ -7.7589,   6.3554],
        [ -8.5545,   7.1596],
        [ -8.3128,   6.9193],
        [  5.8255,  -8.4156],
        [ -8.0577,   5.2839],
        [  5.7282,  -7.4590],
        [ -7.3931,   5.9837],
        [ -6.8570,   3.3149],
        [  5.2759,  -8.1864],
        [ -5.9829,   4.4779],
        [ -7.3043,   5.7941],
        [ -8.3894,   7.0026],
        [  4.9060,  -6.4728],
        [  4.8525,  -6.9860],
        [  5.4102,  -7.7010],
        [  4.4955,  -7.0259],
        [  5.5191,  -7.1010],
        [  5.4081,  -6.8028],
        [ -7.9555,   6.5676],
        [  5.4784,  -7.3971]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8121, 0.1879],
        [0.2861, 0.7139]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4796, 0.5204], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1930, 0.0877],
         [0.5678, 0.3960]],

        [[0.7693, 0.0889],
         [0.0351, 0.4014]],

        [[0.4550, 0.1027],
         [0.8878, 0.2082]],

        [[0.4072, 0.1204],
         [0.3969, 0.6200]],

        [[0.6588, 0.1081],
         [0.3711, 0.3423]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9919988588426287
Average Adjusted Rand Index: 0.99199877740731
Iteration 0: Loss = -27256.44514780544
Iteration 10: Loss = -11938.76430110915
Iteration 20: Loss = -11938.764301227084
1
Iteration 30: Loss = -11938.764231884725
Iteration 40: Loss = -11937.007425523048
Iteration 50: Loss = -11926.214066889555
Iteration 60: Loss = -11874.239155023582
Iteration 70: Loss = -11296.026341215631
Iteration 80: Loss = -11294.904779200775
Iteration 90: Loss = -11294.904779594055
1
Iteration 100: Loss = -11294.904779594055
2
Iteration 110: Loss = -11294.904779594055
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.7091, 0.2909],
        [0.1890, 0.8110]], dtype=torch.float64)
alpha: tensor([0.4407, 0.5593])
beta: tensor([[[0.3889, 0.0871],
         [0.0286, 0.1894]],

        [[0.1574, 0.0889],
         [0.5521, 0.0794]],

        [[0.5396, 0.1026],
         [0.1347, 0.0834]],

        [[0.8347, 0.1212],
         [0.6501, 0.3633]],

        [[0.8455, 0.1084],
         [0.1273, 0.0847]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9919988588426287
Average Adjusted Rand Index: 0.99199877740731
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27255.243892749575
Iteration 100: Loss = -11949.473555121622
Iteration 200: Loss = -11924.941632096854
Iteration 300: Loss = -11879.049376888723
Iteration 400: Loss = -11762.679302389366
Iteration 500: Loss = -11722.340436177836
Iteration 600: Loss = -11691.178359461996
Iteration 700: Loss = -11506.972320084813
Iteration 800: Loss = -11387.125095474177
Iteration 900: Loss = -11352.615964381544
Iteration 1000: Loss = -11346.377480162684
Iteration 1100: Loss = -11331.712874917517
Iteration 1200: Loss = -11331.234058365955
Iteration 1300: Loss = -11311.300877873233
Iteration 1400: Loss = -11311.07542751918
Iteration 1500: Loss = -11310.854219637391
Iteration 1600: Loss = -11308.237823650283
Iteration 1700: Loss = -11308.137118868917
Iteration 1800: Loss = -11308.064274758588
Iteration 1900: Loss = -11308.004918495008
Iteration 2000: Loss = -11307.954042059064
Iteration 2100: Loss = -11307.904793576636
Iteration 2200: Loss = -11307.753403079825
Iteration 2300: Loss = -11303.820189163122
Iteration 2400: Loss = -11303.78501712212
Iteration 2500: Loss = -11303.757937690723
Iteration 2600: Loss = -11303.7353697687
Iteration 2700: Loss = -11303.7167555842
Iteration 2800: Loss = -11303.813979912875
1
Iteration 2900: Loss = -11303.686356724687
Iteration 3000: Loss = -11303.673599930451
Iteration 3100: Loss = -11303.662153983154
Iteration 3200: Loss = -11303.651874402778
Iteration 3300: Loss = -11303.642346172937
Iteration 3400: Loss = -11303.633595045401
Iteration 3500: Loss = -11303.625365861593
Iteration 3600: Loss = -11303.616378702907
Iteration 3700: Loss = -11303.605014549092
Iteration 3800: Loss = -11303.56473760736
Iteration 3900: Loss = -11292.056223844847
Iteration 4000: Loss = -11292.047142442145
Iteration 4100: Loss = -11292.039445746348
Iteration 4200: Loss = -11292.041930300365
1
Iteration 4300: Loss = -11292.027286980236
Iteration 4400: Loss = -11292.022723623444
Iteration 4500: Loss = -11292.018636271505
Iteration 4600: Loss = -11292.014882495156
Iteration 4700: Loss = -11292.011487640577
Iteration 4800: Loss = -11292.008451944259
Iteration 4900: Loss = -11292.007781229726
Iteration 5000: Loss = -11292.003200706233
Iteration 5100: Loss = -11292.000882693126
Iteration 5200: Loss = -11292.00125535473
1
Iteration 5300: Loss = -11291.996688992176
Iteration 5400: Loss = -11291.994790200153
Iteration 5500: Loss = -11291.99306698809
Iteration 5600: Loss = -11291.992569731608
Iteration 5700: Loss = -11291.98983262442
Iteration 5800: Loss = -11291.988426039905
Iteration 5900: Loss = -11291.994584541799
1
Iteration 6000: Loss = -11291.985808044297
Iteration 6100: Loss = -11291.984655742588
Iteration 6200: Loss = -11291.983509042997
Iteration 6300: Loss = -11292.009900480793
1
Iteration 6400: Loss = -11291.981434791745
Iteration 6500: Loss = -11291.981473600343
1
Iteration 6600: Loss = -11292.085418746068
2
Iteration 6700: Loss = -11291.98091130931
Iteration 6800: Loss = -11291.978012578704
Iteration 6900: Loss = -11291.9886029886
1
Iteration 7000: Loss = -11291.976698167146
Iteration 7100: Loss = -11291.97631013797
Iteration 7200: Loss = -11291.978874597213
1
Iteration 7300: Loss = -11291.975849077855
Iteration 7400: Loss = -11291.975033251449
Iteration 7500: Loss = -11291.974274099573
Iteration 7600: Loss = -11292.058767368779
1
Iteration 7700: Loss = -11291.974730770702
2
Iteration 7800: Loss = -11291.986014913806
3
Iteration 7900: Loss = -11291.970616800765
Iteration 8000: Loss = -11291.971148756602
1
Iteration 8100: Loss = -11291.970440734573
Iteration 8200: Loss = -11291.970206762264
Iteration 8300: Loss = -11291.968906239515
Iteration 8400: Loss = -11291.969433754743
1
Iteration 8500: Loss = -11291.968765761274
Iteration 8600: Loss = -11291.971037931167
1
Iteration 8700: Loss = -11291.971802478913
2
Iteration 8800: Loss = -11291.971876966882
3
Iteration 8900: Loss = -11291.972910351096
4
Iteration 9000: Loss = -11291.967063873084
Iteration 9100: Loss = -11291.96804222379
1
Iteration 9200: Loss = -11291.974570214206
2
Iteration 9300: Loss = -11291.974112297472
3
Iteration 9400: Loss = -11291.965489952361
Iteration 9500: Loss = -11291.965909642358
1
Iteration 9600: Loss = -11291.975301773582
2
Iteration 9700: Loss = -11291.963026607713
Iteration 9800: Loss = -11291.99186237162
1
Iteration 9900: Loss = -11291.958060639256
Iteration 10000: Loss = -11291.966800403825
1
Iteration 10100: Loss = -11291.967084960199
2
Iteration 10200: Loss = -11291.987018051675
3
Iteration 10300: Loss = -11291.968084279506
4
Iteration 10400: Loss = -11291.973430899461
5
Iteration 10500: Loss = -11291.9568328586
Iteration 10600: Loss = -11291.9584806751
1
Iteration 10700: Loss = -11291.849142799132
Iteration 10800: Loss = -11291.861536529235
1
Iteration 10900: Loss = -11291.853202574746
2
Iteration 11000: Loss = -11291.847140224572
Iteration 11100: Loss = -11291.856583962923
1
Iteration 11200: Loss = -11291.85663883456
2
Iteration 11300: Loss = -11291.850616288306
3
Iteration 11400: Loss = -11291.85850294487
4
Iteration 11500: Loss = -11291.876506030432
5
Iteration 11600: Loss = -11291.935717590512
6
Iteration 11700: Loss = -11291.866392889426
7
Iteration 11800: Loss = -11291.900007112745
8
Iteration 11900: Loss = -11291.848891048225
9
Iteration 12000: Loss = -11291.852249199152
10
Stopping early at iteration 12000 due to no improvement.
tensor([[  5.9078,  -7.6007],
        [  7.3948,  -8.8107],
        [  7.0935,  -9.7026],
        [  3.8742,  -5.4399],
        [  7.7148, -11.2179],
        [ -7.5922,   6.1778],
        [ -7.6654,   6.2370],
        [ -7.4946,   6.0454],
        [  6.7075,  -8.1076],
        [  6.9808,  -8.3671],
        [  7.1023,  -8.7458],
        [  6.4272, -11.0424],
        [  5.9722,  -7.3614],
        [ -5.2851,   3.6304],
        [  6.9174,  -8.3129],
        [  6.2150,  -7.6339],
        [ -7.4767,   5.9007],
        [ -8.8325,   6.3602],
        [  7.1474,  -9.0111],
        [  7.3128,  -8.9433],
        [  6.8241,  -8.2337],
        [ -7.7025,   5.2852],
        [ -2.2861,   0.7944],
        [  2.8860,  -5.1418],
        [  7.8019,  -9.2026],
        [ -6.4684,   5.0606],
        [ -8.2360,   5.9277],
        [ -9.6810,   5.9207],
        [  6.8027,  -8.1892],
        [  5.9664,  -8.8076],
        [  7.0527,  -8.4440],
        [  6.2458,  -7.6882],
        [ -8.2678,   6.7448],
        [ -7.6374,   6.0409],
        [  6.6567,  -9.1200],
        [  6.9705,  -8.4673],
        [ -5.7547,   4.3667],
        [  6.3070,  -7.8138],
        [ -7.6552,   5.7731],
        [ -7.4082,   5.8764],
        [  6.1653,  -8.5009],
        [ -7.6416,   6.1154],
        [  7.1205,  -8.7038],
        [  6.6754,  -8.2411],
        [ -7.6658,   6.2795],
        [ -8.3346,   6.6367],
        [  6.0023,  -7.6957],
        [ -7.8415,   6.3745],
        [  7.3898, -10.0681],
        [-10.5546,   6.3936],
        [  7.5190,  -9.6214],
        [ -7.2889,   5.8821],
        [ -6.4955,   4.9700],
        [ -7.5068,   5.7744],
        [  6.9534,  -9.9888],
        [ -8.2618,   5.9848],
        [ -7.3907,   6.0009],
        [  7.1389,  -8.6639],
        [ -5.1598,   3.5474],
        [ -7.0287,   5.6424],
        [ -9.9921,   5.3768],
        [ -9.1541,   5.7750],
        [  7.3493,  -8.7372],
        [ -5.9661,   2.9395],
        [  7.0948,  -8.6574],
        [  7.1332,  -8.5483],
        [ -8.3540,   6.6428],
        [ -8.3534,   5.4295],
        [ -7.4303,   6.0405],
        [  6.7477,  -8.2121],
        [ -7.2157,   5.7872],
        [  7.3238, -10.2326],
        [  3.6749,  -5.3739],
        [  6.9647,  -8.9508],
        [ -7.4342,   6.0216],
        [ -7.7887,   5.6797],
        [  7.0533,  -8.4906],
        [  5.0106,  -8.5684],
        [ -5.5128,   4.1086],
        [  6.2502,  -7.9151],
        [  5.8689,  -8.8185],
        [  7.4200,  -8.8222],
        [  7.1804,  -8.6700],
        [ -7.5740,   6.1858],
        [  6.6033,  -8.1258],
        [ -8.7668,   6.9865],
        [  7.4697, -11.3266],
        [  3.7133,  -6.4967],
        [ -7.6875,   5.8674],
        [  4.8441,  -6.2456],
        [  6.5349,  -8.3052],
        [  6.5500, -10.0271],
        [ -9.1418,   7.4514],
        [ -7.6324,   5.1020],
        [ -7.4194,   6.0022],
        [ -6.9785,   5.0531],
        [ -7.8291,   5.9700],
        [ -7.2605,   5.7988],
        [  7.3457,  -9.1428],
        [ -7.6662,   6.2177]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7124, 0.2876],
        [0.1869, 0.8131]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5213, 0.4787], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3980, 0.0873],
         [0.0286, 0.1935]],

        [[0.1574, 0.0893],
         [0.5521, 0.0794]],

        [[0.5396, 0.1029],
         [0.1347, 0.0834]],

        [[0.8347, 0.1203],
         [0.6501, 0.3633]],

        [[0.8455, 0.1084],
         [0.1273, 0.0847]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9919988588426287
Average Adjusted Rand Index: 0.99199877740731
Iteration 0: Loss = -37788.882095754976
Iteration 10: Loss = -11927.69394512045
Iteration 20: Loss = -11911.671743591423
Iteration 30: Loss = -11757.79535357152
Iteration 40: Loss = -11295.294694950751
Iteration 50: Loss = -11294.904786433211
Iteration 60: Loss = -11294.904779594055
Iteration 70: Loss = -11294.904779594055
1
Iteration 80: Loss = -11294.904779594055
2
Iteration 90: Loss = -11294.904779594055
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7091, 0.2909],
        [0.1890, 0.8110]], dtype=torch.float64)
alpha: tensor([0.4407, 0.5593])
beta: tensor([[[0.3889, 0.0871],
         [0.1556, 0.1894]],

        [[0.0538, 0.0889],
         [0.7639, 0.9068]],

        [[0.8939, 0.1026],
         [0.9057, 0.3496]],

        [[0.5909, 0.1212],
         [0.5137, 0.5280]],

        [[0.1605, 0.1084],
         [0.1148, 0.0841]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9919988588426287
Average Adjusted Rand Index: 0.99199877740731
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37787.13226363827
Iteration 100: Loss = -12043.160386944173
Iteration 200: Loss = -12004.907718649763
Iteration 300: Loss = -11983.928027950902
Iteration 400: Loss = -11973.750612774273
Iteration 500: Loss = -11965.722535571718
Iteration 600: Loss = -11955.5428680302
Iteration 700: Loss = -11835.537248115234
Iteration 800: Loss = -11793.153475586028
Iteration 900: Loss = -11781.01098226788
Iteration 1000: Loss = -11776.122993808382
Iteration 1100: Loss = -11774.37403355746
Iteration 1200: Loss = -11774.000078317018
Iteration 1300: Loss = -11773.729298206761
Iteration 1400: Loss = -11773.522485281112
Iteration 1500: Loss = -11773.358981002893
Iteration 1600: Loss = -11773.226610740292
Iteration 1700: Loss = -11773.11736509983
Iteration 1800: Loss = -11773.025861132694
Iteration 1900: Loss = -11772.948164487836
Iteration 2000: Loss = -11772.881130657548
Iteration 2100: Loss = -11772.822379081845
Iteration 2200: Loss = -11772.771694079876
Iteration 2300: Loss = -11772.727780632931
Iteration 2400: Loss = -11772.689288016198
Iteration 2500: Loss = -11772.655212612157
Iteration 2600: Loss = -11772.624928863712
Iteration 2700: Loss = -11772.597868715975
Iteration 2800: Loss = -11772.573576957722
Iteration 2900: Loss = -11772.551737179332
Iteration 3000: Loss = -11772.531995694657
Iteration 3100: Loss = -11772.514097988169
Iteration 3200: Loss = -11772.497843807934
Iteration 3300: Loss = -11772.52041970588
1
Iteration 3400: Loss = -11772.469475380261
Iteration 3500: Loss = -11772.457164784462
Iteration 3600: Loss = -11772.446409105129
Iteration 3700: Loss = -11772.435366587175
Iteration 3800: Loss = -11772.425815462688
Iteration 3900: Loss = -11772.416947528553
Iteration 4000: Loss = -11772.408809902174
Iteration 4100: Loss = -11772.401264854218
Iteration 4200: Loss = -11772.394266669715
Iteration 4300: Loss = -11772.387864362994
Iteration 4400: Loss = -11772.381811913521
Iteration 4500: Loss = -11772.37624990054
Iteration 4600: Loss = -11772.371496855043
Iteration 4700: Loss = -11772.36624019036
Iteration 4800: Loss = -11772.36172210196
Iteration 4900: Loss = -11772.368461684271
1
Iteration 5000: Loss = -11772.35358844164
Iteration 5100: Loss = -11772.34996550156
Iteration 5200: Loss = -11772.346525224404
Iteration 5300: Loss = -11772.34336327712
Iteration 5400: Loss = -11772.34037671412
Iteration 5500: Loss = -11772.337578346682
Iteration 5600: Loss = -11772.335129224399
Iteration 5700: Loss = -11772.332464551584
Iteration 5800: Loss = -11772.331340690733
Iteration 5900: Loss = -11772.328088523065
Iteration 6000: Loss = -11772.409865112868
1
Iteration 6100: Loss = -11772.32435107901
Iteration 6200: Loss = -11772.32659198708
1
Iteration 6300: Loss = -11772.320570309977
Iteration 6400: Loss = -11772.337761356852
1
Iteration 6500: Loss = -11772.317472373936
Iteration 6600: Loss = -11772.316784954062
Iteration 6700: Loss = -11772.315973635365
Iteration 6800: Loss = -11772.315732765905
Iteration 6900: Loss = -11772.316280137606
1
Iteration 7000: Loss = -11772.311806108792
Iteration 7100: Loss = -11772.315405635363
1
Iteration 7200: Loss = -11772.309078390845
Iteration 7300: Loss = -11772.314738507488
1
Iteration 7400: Loss = -11772.307271653315
Iteration 7500: Loss = -11772.306433729178
Iteration 7600: Loss = -11772.321383342909
1
Iteration 7700: Loss = -11772.408223656781
2
Iteration 7800: Loss = -11772.30462847899
Iteration 7900: Loss = -11772.30368072442
Iteration 8000: Loss = -11772.307379679332
1
Iteration 8100: Loss = -11772.302488212783
Iteration 8200: Loss = -11772.30235100655
Iteration 8300: Loss = -11772.301417699915
Iteration 8400: Loss = -11772.305868027708
1
Iteration 8500: Loss = -11772.300581877083
Iteration 8600: Loss = -11772.30007762517
Iteration 8700: Loss = -11772.301138466453
1
Iteration 8800: Loss = -11772.299344486702
Iteration 8900: Loss = -11772.29901627356
Iteration 9000: Loss = -11772.298820038663
Iteration 9100: Loss = -11772.299688679686
1
Iteration 9200: Loss = -11772.298076412528
Iteration 9300: Loss = -11772.297798349897
Iteration 9400: Loss = -11772.297504377262
Iteration 9500: Loss = -11772.308215383835
1
Iteration 9600: Loss = -11772.297062370862
Iteration 9700: Loss = -11772.296821138156
Iteration 9800: Loss = -11772.311622817611
1
Iteration 9900: Loss = -11772.296377692117
Iteration 10000: Loss = -11772.303105678768
1
Iteration 10100: Loss = -11772.304132996254
2
Iteration 10200: Loss = -11772.305365683489
3
Iteration 10300: Loss = -11772.296335413968
Iteration 10400: Loss = -11772.296429006632
1
Iteration 10500: Loss = -11772.300776059259
2
Iteration 10600: Loss = -11772.295070860542
Iteration 10700: Loss = -11772.295681592535
1
Iteration 10800: Loss = -11770.791118318262
Iteration 10900: Loss = -11770.790711660613
Iteration 11000: Loss = -11770.792749933727
1
Iteration 11100: Loss = -11770.79047552969
Iteration 11200: Loss = -11770.7905949947
1
Iteration 11300: Loss = -11770.790109198264
Iteration 11400: Loss = -11769.913947763485
Iteration 11500: Loss = -11769.885594918474
Iteration 11600: Loss = -11769.882928698136
Iteration 11700: Loss = -11769.524475936874
Iteration 11800: Loss = -11769.290221855072
Iteration 11900: Loss = -11768.924054804318
Iteration 12000: Loss = -11768.814273492872
Iteration 12100: Loss = -11768.819638201385
1
Iteration 12200: Loss = -11768.80986698306
Iteration 12300: Loss = -11768.269415665516
Iteration 12400: Loss = -11767.895138451659
Iteration 12500: Loss = -11767.58327043533
Iteration 12600: Loss = -11765.588655110407
Iteration 12700: Loss = -11763.472265771117
Iteration 12800: Loss = -11757.466341142817
Iteration 12900: Loss = -11744.531664497565
Iteration 13000: Loss = -11742.164233995749
Iteration 13100: Loss = -11738.42822076961
Iteration 13200: Loss = -11734.607763922268
Iteration 13300: Loss = -11731.788761923328
Iteration 13400: Loss = -11719.114027227592
Iteration 13500: Loss = -11716.024930024007
Iteration 13600: Loss = -11715.935923820647
Iteration 13700: Loss = -11715.946502874322
1
Iteration 13800: Loss = -11715.936362044655
2
Iteration 13900: Loss = -11715.880757183819
Iteration 14000: Loss = -11715.877296326475
Iteration 14100: Loss = -11715.873179441363
Iteration 14200: Loss = -11715.818022598876
Iteration 14300: Loss = -11715.143993390713
Iteration 14400: Loss = -11715.161098602952
1
Iteration 14500: Loss = -11715.049811753534
Iteration 14600: Loss = -11715.087444531577
1
Iteration 14700: Loss = -11715.049568177083
Iteration 14800: Loss = -11715.048436542162
Iteration 14900: Loss = -11715.031011112076
Iteration 15000: Loss = -11715.006281302485
Iteration 15100: Loss = -11715.01780291787
1
Iteration 15200: Loss = -11715.002274976452
Iteration 15300: Loss = -11715.000846014476
Iteration 15400: Loss = -11715.0066441879
1
Iteration 15500: Loss = -11714.999991524886
Iteration 15600: Loss = -11715.009484832824
1
Iteration 15700: Loss = -11714.99827075819
Iteration 15800: Loss = -11715.22652383425
1
Iteration 15900: Loss = -11714.989898192705
Iteration 16000: Loss = -11715.001243749004
1
Iteration 16100: Loss = -11714.995640960547
2
Iteration 16200: Loss = -11714.998735764371
3
Iteration 16300: Loss = -11715.01165541777
4
Iteration 16400: Loss = -11714.987348048306
Iteration 16500: Loss = -11714.985790451374
Iteration 16600: Loss = -11714.974286885066
Iteration 16700: Loss = -11703.210134273404
Iteration 16800: Loss = -11685.07990699203
Iteration 16900: Loss = -11678.861513269292
Iteration 17000: Loss = -11671.600620124173
Iteration 17100: Loss = -11658.476161130595
Iteration 17200: Loss = -11653.297470813257
Iteration 17300: Loss = -11646.754050874717
Iteration 17400: Loss = -11644.660911676143
Iteration 17500: Loss = -11637.475717145859
Iteration 17600: Loss = -11636.874435569951
Iteration 17700: Loss = -11636.698275347093
Iteration 17800: Loss = -11620.811713043644
Iteration 17900: Loss = -11596.783026205681
Iteration 18000: Loss = -11589.95926030225
Iteration 18100: Loss = -11588.864867337643
Iteration 18200: Loss = -11583.646660938097
Iteration 18300: Loss = -11583.634899422063
Iteration 18400: Loss = -11583.631916806062
Iteration 18500: Loss = -11573.378248974881
Iteration 18600: Loss = -11560.387798802934
Iteration 18700: Loss = -11557.186295845668
Iteration 18800: Loss = -11553.387275332823
Iteration 18900: Loss = -11551.091952811308
Iteration 19000: Loss = -11551.067863683762
Iteration 19100: Loss = -11551.048422043781
Iteration 19200: Loss = -11521.500694959763
Iteration 19300: Loss = -11521.55087561079
1
Iteration 19400: Loss = -11521.242359893276
Iteration 19500: Loss = -11521.24445868959
1
Iteration 19600: Loss = -11521.2467218593
2
Iteration 19700: Loss = -11521.134776619901
Iteration 19800: Loss = -11517.455847144993
Iteration 19900: Loss = -11517.376644694294
tensor([[ -9.7897,   7.7631],
        [ -9.8719,   8.2150],
        [ -9.8462,   7.6153],
        [ -8.8867,   7.0688],
        [-11.1176,   9.1603],
        [  4.2645,  -5.7221],
        [  5.7113,  -7.2039],
        [  5.7547,  -8.5484],
        [ -9.8977,   8.5017],
        [ -9.4924,   7.7123],
        [ -8.9125,   7.3439],
        [-10.0640,   8.6772],
        [ -9.1795,   7.3276],
        [  1.6514,  -3.6681],
        [-10.0693,   8.4557],
        [-10.8719,   8.1232],
        [  4.6777,  -6.1176],
        [  5.0917,  -7.1171],
        [-10.1230,   8.1144],
        [-10.4250,   8.8265],
        [ -9.5831,   7.6519],
        [  4.9133,  -7.0776],
        [ -2.1613,   0.6340],
        [ -6.7239,   2.6429],
        [-10.0531,   8.5843],
        [  2.2874,  -3.6744],
        [  6.2582,  -7.7384],
        [  6.1723,  -8.0329],
        [-10.2017,   8.0081],
        [ -9.7931,   8.4047],
        [ -9.9673,   8.1053],
        [ -9.2674,   7.8526],
        [  6.1159,  -7.8769],
        [  6.4366,  -7.8646],
        [-10.0957,   5.8104],
        [ -9.5329,   6.9616],
        [  6.1373,  -7.5589],
        [ -9.5263,   6.6721],
        [  5.2749,  -7.1797],
        [  5.6715,  -8.4461],
        [ -6.8189,   4.5955],
        [  5.5195,  -7.5930],
        [-10.1113,   8.5663],
        [ -9.0852,   7.6936],
        [  4.9881,  -6.7588],
        [  5.8360,  -7.4189],
        [-11.4589,   7.3511],
        [  5.2037,  -7.4916],
        [ -9.5063,   8.0651],
        [  5.6161,  -7.4054],
        [-10.2218,   8.3235],
        [  6.3428,  -7.9438],
        [  3.1636,  -4.9218],
        [  6.0524,  -7.4541],
        [ -8.2912,   6.8339],
        [  5.7014,  -7.4547],
        [  4.9322,  -7.8950],
        [ -9.8445,   7.9561],
        [  2.1847,  -4.3924],
        [  6.2239,  -7.6766],
        [  5.4302,  -7.4269],
        [  3.6959,  -5.1159],
        [-10.5120,   8.8822],
        [ -9.0429,   7.3501],
        [-10.6837,   7.5700],
        [ -9.1286,   5.5536],
        [  6.2091,  -7.6155],
        [  5.9141,  -8.0538],
        [  4.7195,  -7.1024],
        [ -9.6655,   7.1317],
        [  2.9562,  -4.4127],
        [ -9.6304,   7.9974],
        [ -6.7898,   5.2824],
        [-10.2739,   8.8673],
        [  5.3750,  -6.8011],
        [  4.2795,  -7.6119],
        [ -9.1237,   7.2756],
        [ -8.7663,   6.0545],
        [  3.1685,  -4.5712],
        [ -9.9417,   8.1239],
        [ -8.9741,   7.2311],
        [-10.7073,   6.9837],
        [-10.6430,   6.0278],
        [  5.0081,  -7.3706],
        [ -9.6432,   7.0506],
        [ -7.4789,   5.3900],
        [ -9.9881,   8.5997],
        [ -9.1169,   7.7154],
        [  5.1930,  -8.6171],
        [ -7.8382,   5.6449],
        [ -9.3582,   6.4400],
        [-10.0996,   8.7117],
        [  4.7797,  -6.2676],
        [  5.1980,  -6.7381],
        [  5.6423,  -7.0998],
        [  2.5370,  -3.9239],
        [  4.8295,  -6.6584],
        [  4.6479,  -6.1936],
        [-10.0931,   8.5207],
        [  5.2731,  -8.5283]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3403, 0.6597],
        [0.8244, 0.1756]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4505, 0.5495], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2308, 0.0878],
         [0.1556, 0.3300]],

        [[0.0538, 0.0921],
         [0.7639, 0.9068]],

        [[0.8939, 0.1010],
         [0.9057, 0.3496]],

        [[0.5909, 0.1196],
         [0.5137, 0.5280]],

        [[0.1605, 0.1068],
         [0.1148, 0.0841]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 11
Adjusted Rand Index: 0.6044444444444445
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 19
Adjusted Rand Index: 0.37775517955378035
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
Global Adjusted Rand Index: 0.08105277204213411
Average Adjusted Rand Index: 0.7335691151418359
Iteration 0: Loss = -20323.179638800102
Iteration 10: Loss = -11344.822742906257
Iteration 20: Loss = -11294.9048237127
Iteration 30: Loss = -11294.90477734103
Iteration 40: Loss = -11294.904777341058
1
Iteration 50: Loss = -11294.904777341058
2
Iteration 60: Loss = -11294.904777341058
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.8110, 0.1890],
        [0.2909, 0.7091]], dtype=torch.float64)
alpha: tensor([0.5593, 0.4407])
beta: tensor([[[0.1894, 0.0871],
         [0.0721, 0.3889]],

        [[0.5710, 0.0889],
         [0.2256, 0.3580]],

        [[0.4004, 0.1026],
         [0.4233, 0.0544]],

        [[0.6881, 0.1212],
         [0.1036, 0.3747]],

        [[0.9207, 0.1084],
         [0.9009, 0.2283]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9919988588426287
Average Adjusted Rand Index: 0.99199877740731
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20322.466870879936
Iteration 100: Loss = -11974.219782244752
Iteration 200: Loss = -11930.75086086212
Iteration 300: Loss = -11906.603846645743
Iteration 400: Loss = -11886.523506787751
Iteration 500: Loss = -11872.291282842907
Iteration 600: Loss = -11831.612510845724
Iteration 700: Loss = -11753.23050735203
Iteration 800: Loss = -11715.686917467332
Iteration 900: Loss = -11708.75115292205
Iteration 1000: Loss = -11706.440041869897
Iteration 1100: Loss = -11700.13854245366
Iteration 1200: Loss = -11630.46725274918
Iteration 1300: Loss = -11453.979210272373
Iteration 1400: Loss = -11391.01350437063
Iteration 1500: Loss = -11369.379457324238
Iteration 1600: Loss = -11357.84603217266
Iteration 1700: Loss = -11321.444030838926
Iteration 1800: Loss = -11321.31255420906
Iteration 1900: Loss = -11313.739355167156
Iteration 2000: Loss = -11313.698191718919
Iteration 2100: Loss = -11313.670643364543
Iteration 2200: Loss = -11313.648948406604
Iteration 2300: Loss = -11313.628806415014
Iteration 2400: Loss = -11309.7880848169
Iteration 2500: Loss = -11306.704246943831
Iteration 2600: Loss = -11306.694177490872
Iteration 2700: Loss = -11306.686404428454
Iteration 2800: Loss = -11306.678461227983
Iteration 2900: Loss = -11306.710063339608
1
Iteration 3000: Loss = -11306.66661413433
Iteration 3100: Loss = -11306.661690959229
Iteration 3200: Loss = -11306.65858184393
Iteration 3300: Loss = -11306.653477016418
Iteration 3400: Loss = -11306.650046758326
Iteration 3500: Loss = -11306.657530529437
1
Iteration 3600: Loss = -11306.64406706244
Iteration 3700: Loss = -11306.641530433511
Iteration 3800: Loss = -11306.639230960542
Iteration 3900: Loss = -11306.637098116695
Iteration 4000: Loss = -11306.635016880287
Iteration 4100: Loss = -11306.63307438443
Iteration 4200: Loss = -11306.630985207785
Iteration 4300: Loss = -11306.627560758248
Iteration 4400: Loss = -11306.625321007632
Iteration 4500: Loss = -11306.624071196871
Iteration 4600: Loss = -11306.622738186574
Iteration 4700: Loss = -11306.621691868868
Iteration 4800: Loss = -11306.620619392457
Iteration 4900: Loss = -11306.625675158135
1
Iteration 5000: Loss = -11306.618738961039
Iteration 5100: Loss = -11306.61791020819
Iteration 5200: Loss = -11306.623571494676
1
Iteration 5300: Loss = -11306.68332312478
2
Iteration 5400: Loss = -11306.632135827727
3
Iteration 5500: Loss = -11306.690506841767
4
Iteration 5600: Loss = -11302.226284628716
Iteration 5700: Loss = -11302.251123652795
1
Iteration 5800: Loss = -11302.223841874946
Iteration 5900: Loss = -11302.227997346745
1
Iteration 6000: Loss = -11302.222374640542
Iteration 6100: Loss = -11302.251233841956
1
Iteration 6200: Loss = -11302.221261760156
Iteration 6300: Loss = -11302.221886333422
1
Iteration 6400: Loss = -11302.22199539557
2
Iteration 6500: Loss = -11302.220810040826
Iteration 6600: Loss = -11302.219745887067
Iteration 6700: Loss = -11302.222839612266
1
Iteration 6800: Loss = -11302.223246341335
2
Iteration 6900: Loss = -11302.227071739168
3
Iteration 7000: Loss = -11302.222748740971
4
Iteration 7100: Loss = -11302.229440603962
5
Iteration 7200: Loss = -11302.218538204253
Iteration 7300: Loss = -11302.223557368414
1
Iteration 7400: Loss = -11302.249623531075
2
Iteration 7500: Loss = -11302.231411760695
3
Iteration 7600: Loss = -11302.219046817701
4
Iteration 7700: Loss = -11302.251493944526
5
Iteration 7800: Loss = -11302.220647989347
6
Iteration 7900: Loss = -11302.223556344226
7
Iteration 8000: Loss = -11302.218356230713
Iteration 8100: Loss = -11302.219780087882
1
Iteration 8200: Loss = -11302.216751243559
Iteration 8300: Loss = -11302.217684836598
1
Iteration 8400: Loss = -11302.22032045055
2
Iteration 8500: Loss = -11302.220499302226
3
Iteration 8600: Loss = -11302.21616400609
Iteration 8700: Loss = -11302.216531004507
1
Iteration 8800: Loss = -11302.216232689821
2
Iteration 8900: Loss = -11302.216172952934
3
Iteration 9000: Loss = -11302.222503420891
4
Iteration 9100: Loss = -11302.222221303762
5
Iteration 9200: Loss = -11302.27320343014
6
Iteration 9300: Loss = -11302.21628028469
7
Iteration 9400: Loss = -11302.218651104364
8
Iteration 9500: Loss = -11302.216566647901
9
Iteration 9600: Loss = -11302.216278390419
10
Stopping early at iteration 9600 due to no improvement.
tensor([[  5.2861,  -7.7421],
        [  6.3529,  -7.8782],
        [  6.4869,  -8.0833],
        [  3.9498,  -5.3871],
        [  6.4934,  -8.0411],
        [ -7.4454,   5.7738],
        [ -7.3104,   5.9240],
        [ -7.5247,   5.9275],
        [  6.5775,  -8.8234],
        [  5.9293,  -7.8904],
        [  5.9144,  -7.3785],
        [  5.7996,  -7.2881],
        [  5.3142,  -7.0497],
        [ -6.2346,   2.5275],
        [  6.3983,  -7.9784],
        [  6.5888,  -8.1427],
        [ -7.1731,   5.7862],
        [ -8.6285,   4.0133],
        [  5.3555,  -7.6112],
        [  6.6474,  -8.2615],
        [  6.4320,  -8.0600],
        [ -6.8252,   5.4354],
        [ -2.2360,   0.7241],
        [  3.2964,  -4.7923],
        [  6.7991,  -8.4804],
        [ -7.0291,   5.1104],
        [ -7.9803,   5.1513],
        [ -7.0139,   5.5774],
        [  6.5332,  -7.9547],
        [  6.5744,  -8.7755],
        [  6.5881,  -8.0963],
        [  6.2189,  -7.6688],
        [ -7.8597,   6.4662],
        [ -7.6049,   5.7922],
        [  6.1172,  -7.8667],
        [  6.1205,  -7.9265],
        [ -5.8207,   4.1793],
        [  5.8952,  -7.7587],
        [ -7.4122,   4.9862],
        [ -7.6113,   6.2026],
        [  4.8198,  -6.3205],
        [ -7.1635,   5.7534],
        [  5.3142,  -7.7905],
        [  6.4057,  -8.1434],
        [ -7.5502,   6.0162],
        [ -7.5819,   5.6103],
        [  5.7009,  -7.7111],
        [ -9.7576,   6.1802],
        [  6.6364,  -8.0581],
        [ -7.2809,   5.8911],
        [  6.9349,  -8.4802],
        [ -8.6202,   5.8981],
        [ -6.3287,   4.5275],
        [ -6.3213,   4.5813],
        [  5.8907,  -7.6609],
        [ -7.3536,   5.8132],
        [ -7.0482,   5.6329],
        [  5.8158,  -8.3360],
        [ -5.0048,   3.5681],
        [ -6.7474,   5.3490],
        [ -9.4800,   6.3412],
        [ -7.1362,   5.5115],
        [  6.2735, -10.8887],
        [ -5.1408,   3.6195],
        [  6.8236,  -8.2718],
        [  5.4794,  -7.0472],
        [ -6.5120,   4.3999],
        [ -8.4207,   3.8054],
        [ -7.2575,   5.8711],
        [  6.4122,  -7.8390],
        [ -7.1463,   5.5127],
        [  5.6974,  -8.0995],
        [  3.8638,  -5.2501],
        [  6.4381,  -7.9912],
        [ -7.4325,   4.8560],
        [ -7.0158,   5.6186],
        [  6.2137,  -8.4094],
        [  6.1102,  -7.8996],
        [ -6.2054,   3.2645],
        [  6.4220,  -7.9579],
        [  6.0667,  -7.4988],
        [  6.9188,  -9.4000],
        [  6.2640,  -7.6508],
        [ -7.3488,   5.9610],
        [  5.8190,  -7.4583],
        [ -7.2589,   5.8726],
        [  5.7660,  -8.3489],
        [  4.0098,  -6.2443],
        [ -7.3387,   5.6438],
        [  4.5024,  -6.5885],
        [  5.8364,  -7.3831],
        [  7.1107,  -9.2404],
        [ -7.1637,   5.2662],
        [ -6.9700,   5.1710],
        [ -7.2711,   5.0723],
        [ -6.6981,   5.1623],
        [ -7.4654,   4.8763],
        [ -6.9470,   5.4013],
        [  6.5176,  -7.9765],
        [ -7.5668,   6.1805]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7089, 0.2911],
        [0.1922, 0.8078]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5205, 0.4795], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3958, 0.0873],
         [0.0721, 0.1927]],

        [[0.5710, 0.0889],
         [0.2256, 0.3580]],

        [[0.4004, 0.1048],
         [0.4233, 0.0544]],

        [[0.6881, 0.1202],
         [0.1036, 0.3747]],

        [[0.9207, 0.1084],
         [0.9009, 0.2283]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9840299598123581
Average Adjusted Rand Index: 0.9839975548146199
11308.031146132842
new:  [0.9919988588426287, 0.9919988588426287, 0.08105277204213411, 0.9840299598123581] [0.99199877740731, 0.99199877740731, 0.7335691151418359, 0.9839975548146199] [11292.007165973257, 11291.852249199152, 11517.372057953942, 11302.216278390419]
prior:  [0.0, 0.9919988588426287, 0.9919988588426287, 0.9919988588426287] [0.0, 0.99199877740731, 0.99199877740731, 0.99199877740731] [nan, 11294.904779594055, 11294.904779594055, 11294.904777341058]
-----------------------------------------------------------------------------------------
This iteration is 8
True Objective function: Loss = -11461.682472272678
Iteration 0: Loss = -26048.602184529293
Iteration 10: Loss = -11456.479307814647
Iteration 20: Loss = -11453.908456238616
Iteration 30: Loss = -11453.908457756834
1
Iteration 40: Loss = -11453.908457756834
2
Iteration 50: Loss = -11453.908457756834
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7666, 0.2334],
        [0.2589, 0.7411]], dtype=torch.float64)
alpha: tensor([0.5217, 0.4783])
beta: tensor([[[0.1894, 0.1040],
         [0.7794, 0.3943]],

        [[0.3443, 0.1118],
         [0.0652, 0.9155]],

        [[0.0515, 0.1001],
         [0.1918, 0.5369]],

        [[0.9674, 0.0972],
         [0.3787, 0.6721]],

        [[0.8191, 0.0949],
         [0.8274, 0.2172]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961975210904
Average Adjusted Rand Index: 0.9759983386585752
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25872.047113761182
Iteration 100: Loss = -12179.689947988983
Iteration 200: Loss = -12121.459528521078
Iteration 300: Loss = -11753.608790392873
Iteration 400: Loss = -11525.065921597314
Iteration 500: Loss = -11497.731538681302
Iteration 600: Loss = -11486.613106996223
Iteration 700: Loss = -11477.558751181647
Iteration 800: Loss = -11475.116903855735
Iteration 900: Loss = -11475.021148686457
Iteration 1000: Loss = -11474.929487257048
Iteration 1100: Loss = -11474.895955622254
Iteration 1200: Loss = -11474.861844682817
Iteration 1300: Loss = -11474.819121481645
Iteration 1400: Loss = -11474.781836841119
Iteration 1500: Loss = -11474.769395970143
Iteration 1600: Loss = -11474.75872812821
Iteration 1700: Loss = -11474.749719836558
Iteration 1800: Loss = -11474.742505743256
Iteration 1900: Loss = -11474.736427702794
Iteration 2000: Loss = -11474.731099924937
Iteration 2100: Loss = -11474.726455700762
Iteration 2200: Loss = -11474.722429961603
Iteration 2300: Loss = -11474.718846880442
Iteration 2400: Loss = -11474.71563502968
Iteration 2500: Loss = -11474.71278637043
Iteration 2600: Loss = -11474.710231564173
Iteration 2700: Loss = -11474.707875381897
Iteration 2800: Loss = -11474.705763545728
Iteration 2900: Loss = -11474.703782795523
Iteration 3000: Loss = -11474.701997966007
Iteration 3100: Loss = -11474.70022592242
Iteration 3200: Loss = -11474.698500784276
Iteration 3300: Loss = -11474.69697364868
Iteration 3400: Loss = -11474.695603788849
Iteration 3500: Loss = -11474.694384198605
Iteration 3600: Loss = -11474.692932411812
Iteration 3700: Loss = -11471.96146326287
Iteration 3800: Loss = -11471.892371348182
Iteration 3900: Loss = -11471.854810701867
Iteration 4000: Loss = -11471.854000790814
Iteration 4100: Loss = -11471.853261147775
Iteration 4200: Loss = -11471.852510747902
Iteration 4300: Loss = -11471.851892462604
Iteration 4400: Loss = -11471.851334496856
Iteration 4500: Loss = -11471.85081113912
Iteration 4600: Loss = -11471.850322920323
Iteration 4700: Loss = -11471.849845945753
Iteration 4800: Loss = -11471.84941547636
Iteration 4900: Loss = -11460.674844893485
Iteration 5000: Loss = -11460.67134032701
Iteration 5100: Loss = -11460.671184309711
Iteration 5200: Loss = -11460.670397493834
Iteration 5300: Loss = -11460.670033755528
Iteration 5400: Loss = -11460.669674230025
Iteration 5500: Loss = -11460.669363845742
Iteration 5600: Loss = -11460.670282764573
1
Iteration 5700: Loss = -11460.68304968963
2
Iteration 5800: Loss = -11460.668376243437
Iteration 5900: Loss = -11460.668399792974
1
Iteration 6000: Loss = -11460.66616108702
Iteration 6100: Loss = -11460.65377814877
Iteration 6200: Loss = -11460.649821800185
Iteration 6300: Loss = -11460.648805283368
Iteration 6400: Loss = -11460.310860582289
Iteration 6500: Loss = -11460.287462269316
Iteration 6600: Loss = -11451.673043442217
Iteration 6700: Loss = -11451.673166340976
1
Iteration 6800: Loss = -11451.673759971713
2
Iteration 6900: Loss = -11451.673546084672
3
Iteration 7000: Loss = -11451.672492156671
Iteration 7100: Loss = -11451.672542419776
1
Iteration 7200: Loss = -11451.673226215838
2
Iteration 7300: Loss = -11451.677429992098
3
Iteration 7400: Loss = -11451.672090583716
Iteration 7500: Loss = -11451.672045740748
Iteration 7600: Loss = -11451.674300453036
1
Iteration 7700: Loss = -11451.67186274978
Iteration 7800: Loss = -11451.67416284566
1
Iteration 7900: Loss = -11451.67172663262
Iteration 8000: Loss = -11451.674478355442
1
Iteration 8100: Loss = -11451.68744738367
2
Iteration 8200: Loss = -11451.672358702817
3
Iteration 8300: Loss = -11451.671842420874
4
Iteration 8400: Loss = -11451.672470716783
5
Iteration 8500: Loss = -11451.68076865487
6
Iteration 8600: Loss = -11451.671332138636
Iteration 8700: Loss = -11451.67270093788
1
Iteration 8800: Loss = -11451.67124525723
Iteration 8900: Loss = -11451.67174359743
1
Iteration 9000: Loss = -11451.670821821499
Iteration 9100: Loss = -11451.701317955976
1
Iteration 9200: Loss = -11451.670614100141
Iteration 9300: Loss = -11451.684529110404
1
Iteration 9400: Loss = -11451.670540023968
Iteration 9500: Loss = -11451.68214369965
1
Iteration 9600: Loss = -11451.70478159818
2
Iteration 9700: Loss = -11451.699024481022
3
Iteration 9800: Loss = -11451.695199247153
4
Iteration 9900: Loss = -11451.67255149965
5
Iteration 10000: Loss = -11451.670501699991
Iteration 10100: Loss = -11451.670734413616
1
Iteration 10200: Loss = -11451.899848446632
2
Iteration 10300: Loss = -11451.670841206662
3
Iteration 10400: Loss = -11451.676739050308
4
Iteration 10500: Loss = -11451.673162812265
5
Iteration 10600: Loss = -11451.683411585618
6
Iteration 10700: Loss = -11451.670226138336
Iteration 10800: Loss = -11451.688609481786
1
Iteration 10900: Loss = -11451.675110120706
2
Iteration 11000: Loss = -11451.670366167271
3
Iteration 11100: Loss = -11451.671744464049
4
Iteration 11200: Loss = -11451.67337749186
5
Iteration 11300: Loss = -11451.67946055872
6
Iteration 11400: Loss = -11451.677228993723
7
Iteration 11500: Loss = -11451.675947571359
8
Iteration 11600: Loss = -11451.670179808696
Iteration 11700: Loss = -11451.678028974171
1
Iteration 11800: Loss = -11451.68079859684
2
Iteration 11900: Loss = -11451.690787483696
3
Iteration 12000: Loss = -11451.671037332831
4
Iteration 12100: Loss = -11451.670148700774
Iteration 12200: Loss = -11451.674843975765
1
Iteration 12300: Loss = -11451.681384372492
2
Iteration 12400: Loss = -11451.670100266734
Iteration 12500: Loss = -11451.672917534966
1
Iteration 12600: Loss = -11451.675216729938
2
Iteration 12700: Loss = -11451.670066450055
Iteration 12800: Loss = -11451.682203319526
1
Iteration 12900: Loss = -11451.698373521154
2
Iteration 13000: Loss = -11451.670042049751
Iteration 13100: Loss = -11451.679246043954
1
Iteration 13200: Loss = -11451.673590829621
2
Iteration 13300: Loss = -11451.689473933344
3
Iteration 13400: Loss = -11451.69364428932
4
Iteration 13500: Loss = -11451.669726709157
Iteration 13600: Loss = -11451.678840255925
1
Iteration 13700: Loss = -11451.708405898318
2
Iteration 13800: Loss = -11451.672629698813
3
Iteration 13900: Loss = -11451.760239494855
4
Iteration 14000: Loss = -11451.669785765862
5
Iteration 14100: Loss = -11451.672714467619
6
Iteration 14200: Loss = -11451.684961457739
7
Iteration 14300: Loss = -11451.670354846383
8
Iteration 14400: Loss = -11451.671017106628
9
Iteration 14500: Loss = -11451.687701112427
10
Stopping early at iteration 14500 due to no improvement.
tensor([[  6.2700, -10.8853],
        [-10.3248,   5.7095],
        [  1.7802,  -6.3954],
        [ -1.7661,  -2.8491],
        [ -9.3863,   4.7710],
        [  6.2683, -10.8835],
        [  5.1225,  -9.7377],
        [-11.1787,   6.5634],
        [-10.1906,   5.5754],
        [  4.5605,  -9.1757],
        [  4.9648,  -9.5800],
        [-10.2622,   5.6470],
        [  5.0868,  -9.7020],
        [  6.5245, -11.1397],
        [  3.6060,  -8.2212],
        [  6.5252, -11.1405],
        [  6.1220, -10.7372],
        [-10.2116,   5.5963],
        [  6.4705, -11.0857],
        [-11.1591,   6.5439],
        [-11.1836,   6.5684],
        [-11.2911,   6.6759],
        [  6.3909, -11.0061],
        [-10.4788,   5.8636],
        [  4.7302,  -9.3454],
        [-10.8209,   6.2057],
        [-11.1230,   6.5078],
        [ -9.0104,   4.3952],
        [-11.2597,   6.6444],
        [  1.0440,  -5.6592],
        [ -9.0072,   4.3920],
        [  5.3445,  -9.9597],
        [-10.4559,   5.8407],
        [ -8.7901,   4.1749],
        [ -7.5986,   2.9834],
        [-11.4940,   6.8788],
        [-10.6984,   6.0831],
        [  4.6285,  -9.2437],
        [-10.6852,   6.0700],
        [-11.2457,   6.6305],
        [-10.6216,   6.0064],
        [ -7.4459,   2.8307],
        [  5.1871,  -9.8023],
        [  3.0695,  -7.6847],
        [  1.0663,  -5.6815],
        [  5.6752, -10.2904],
        [  2.6654,  -7.2806],
        [  5.2581,  -9.8733],
        [  6.2520, -10.8672],
        [  3.6115,  -8.2267],
        [-11.4224,   6.8071],
        [ -6.1365,   1.5213],
        [  6.2464, -10.8616],
        [ -9.2182,   4.6030],
        [-10.6513,   6.0361],
        [  4.9030,  -9.5182],
        [-10.1040,   5.4888],
        [-10.5606,   5.9454],
        [-10.7540,   6.1388],
        [-11.2124,   6.5972],
        [ -9.5278,   4.9126],
        [-10.9873,   6.3721],
        [-11.5941,   6.9789],
        [  5.6540, -10.2692],
        [  3.9958,  -8.6111],
        [  5.9272, -10.5425],
        [  6.3730, -10.9882],
        [-10.9201,   6.3048],
        [-10.1245,   5.5093],
        [-10.9402,   6.3250],
        [ -9.8031,   5.1879],
        [-11.1459,   6.5307],
        [  6.4220, -11.0372],
        [  5.7856, -10.4008],
        [-10.3412,   5.7260],
        [-10.1873,   5.5720],
        [  3.3332,  -7.9484],
        [ -0.5016,  -4.1136],
        [ -0.8247,  -3.7905],
        [-10.5562,   5.9410],
        [  6.5974, -11.2126],
        [  2.8657,  -7.4809],
        [-10.4934,   5.8781],
        [-10.2104,   5.5952],
        [-11.1413,   6.5261],
        [  6.7627, -11.3779],
        [  6.1997, -10.8149],
        [  4.9747,  -9.5899],
        [  6.1178, -10.7330],
        [  5.4075, -10.0227],
        [ -4.2649,  -0.3503],
        [-10.2852,   5.6700],
        [  5.0676,  -9.6828],
        [  5.6431, -10.2583],
        [-10.3657,   5.7505],
        [  3.7169,  -8.3321],
        [ -7.3642,   2.7490],
        [  3.9401,  -8.5553],
        [  6.4193, -11.0345],
        [-12.1403,   7.5250]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7749, 0.2251],
        [0.2527, 0.7473]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4858, 0.5142], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1934, 0.1038],
         [0.7794, 0.4019]],

        [[0.3443, 0.1120],
         [0.0652, 0.9155]],

        [[0.0515, 0.1000],
         [0.1918, 0.5369]],

        [[0.9674, 0.0968],
         [0.3787, 0.6721]],

        [[0.8191, 0.0948],
         [0.8274, 0.2172]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320634059188
Average Adjusted Rand Index: 0.9839989969312853
Iteration 0: Loss = -19743.314083105975
Iteration 10: Loss = -11454.009882199302
Iteration 20: Loss = -11453.908455291055
Iteration 30: Loss = -11453.908451115103
Iteration 40: Loss = -11453.908451115103
1
Iteration 50: Loss = -11453.908451115103
2
Iteration 60: Loss = -11453.908451115103
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7411, 0.2589],
        [0.2334, 0.7666]], dtype=torch.float64)
alpha: tensor([0.4783, 0.5217])
beta: tensor([[[0.3943, 0.1040],
         [0.1922, 0.1894]],

        [[0.7914, 0.1118],
         [0.8356, 0.0396]],

        [[0.0323, 0.1001],
         [0.4487, 0.0701]],

        [[0.6485, 0.0972],
         [0.0791, 0.3287]],

        [[0.8469, 0.0949],
         [0.7941, 0.5695]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961975210904
Average Adjusted Rand Index: 0.9759983386585752
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19743.126752068718
Iteration 100: Loss = -12200.294217513196
Iteration 200: Loss = -12167.576896926013
Iteration 300: Loss = -12056.046778250313
Iteration 400: Loss = -11793.951056494112
Iteration 500: Loss = -11605.992600562411
Iteration 600: Loss = -11547.2311866911
Iteration 700: Loss = -11546.354024501501
Iteration 800: Loss = -11530.194220890802
Iteration 900: Loss = -11529.94435932454
Iteration 1000: Loss = -11529.768523660592
Iteration 1100: Loss = -11525.66406738261
Iteration 1200: Loss = -11525.569076851436
Iteration 1300: Loss = -11525.499229286046
Iteration 1400: Loss = -11525.44323917492
Iteration 1500: Loss = -11525.395559487488
Iteration 1600: Loss = -11525.353762021605
Iteration 1700: Loss = -11514.143547281177
Iteration 1800: Loss = -11510.469943327735
Iteration 1900: Loss = -11499.50899462428
Iteration 2000: Loss = -11499.484648956224
Iteration 2100: Loss = -11499.470392068084
Iteration 2200: Loss = -11499.45110520658
Iteration 2300: Loss = -11499.457310446784
1
Iteration 2400: Loss = -11499.4273069199
Iteration 2500: Loss = -11499.417724490962
Iteration 2600: Loss = -11499.40921610355
Iteration 2700: Loss = -11499.401658603378
Iteration 2800: Loss = -11499.396536107719
Iteration 2900: Loss = -11499.388789060627
Iteration 3000: Loss = -11499.383280622844
Iteration 3100: Loss = -11499.39189575495
1
Iteration 3200: Loss = -11499.363961928782
Iteration 3300: Loss = -11499.334997023949
Iteration 3400: Loss = -11499.331266132098
Iteration 3500: Loss = -11499.327922428225
Iteration 3600: Loss = -11499.324803082005
Iteration 3700: Loss = -11499.321979665378
Iteration 3800: Loss = -11499.32242252464
1
Iteration 3900: Loss = -11499.316939832312
Iteration 4000: Loss = -11499.314731156606
Iteration 4100: Loss = -11499.31267599827
Iteration 4200: Loss = -11499.354137786153
1
Iteration 4300: Loss = -11499.308961721435
Iteration 4400: Loss = -11499.307327603869
Iteration 4500: Loss = -11499.305781844923
Iteration 4600: Loss = -11499.304347411664
Iteration 4700: Loss = -11499.30301216332
Iteration 4800: Loss = -11499.301782840053
Iteration 4900: Loss = -11499.300567578597
Iteration 5000: Loss = -11499.29939456784
Iteration 5100: Loss = -11499.300757699712
1
Iteration 5200: Loss = -11499.297457654779
Iteration 5300: Loss = -11499.297320551908
Iteration 5400: Loss = -11499.298245450998
1
Iteration 5500: Loss = -11486.349698014199
Iteration 5600: Loss = -11486.345971064395
Iteration 5700: Loss = -11486.33628571413
Iteration 5800: Loss = -11476.870530911998
Iteration 5900: Loss = -11476.858019680602
Iteration 6000: Loss = -11476.856976413239
Iteration 6100: Loss = -11476.85659185146
Iteration 6200: Loss = -11476.855515047413
Iteration 6300: Loss = -11476.854823561747
Iteration 6400: Loss = -11476.854187693232
Iteration 6500: Loss = -11461.49043466051
Iteration 6600: Loss = -11451.84681534094
Iteration 6700: Loss = -11451.800361892756
Iteration 6800: Loss = -11451.805438586849
1
Iteration 6900: Loss = -11451.79709258217
Iteration 7000: Loss = -11451.807075959414
1
Iteration 7100: Loss = -11451.79645484103
Iteration 7200: Loss = -11451.796521349126
1
Iteration 7300: Loss = -11451.795815510826
Iteration 7400: Loss = -11451.795857450474
1
Iteration 7500: Loss = -11451.809194105337
2
Iteration 7600: Loss = -11451.794853162846
Iteration 7700: Loss = -11451.801795554744
1
Iteration 7800: Loss = -11451.787246464413
Iteration 7900: Loss = -11451.788109790128
1
Iteration 8000: Loss = -11451.83336873287
2
Iteration 8100: Loss = -11451.78614462863
Iteration 8200: Loss = -11451.786079642261
Iteration 8300: Loss = -11451.790434365144
1
Iteration 8400: Loss = -11451.835725601275
2
Iteration 8500: Loss = -11451.815310932705
3
Iteration 8600: Loss = -11451.87918235579
4
Iteration 8700: Loss = -11451.785258131438
Iteration 8800: Loss = -11451.785609879174
1
Iteration 8900: Loss = -11451.804027969176
2
Iteration 9000: Loss = -11451.79151834092
3
Iteration 9100: Loss = -11451.78515397513
Iteration 9200: Loss = -11451.785021066791
Iteration 9300: Loss = -11451.786979428716
1
Iteration 9400: Loss = -11451.788008954058
2
Iteration 9500: Loss = -11451.78514350849
3
Iteration 9600: Loss = -11451.793583526603
4
Iteration 9700: Loss = -11451.786451951823
5
Iteration 9800: Loss = -11451.790389814334
6
Iteration 9900: Loss = -11451.788981679987
7
Iteration 10000: Loss = -11451.784211213115
Iteration 10100: Loss = -11451.784630153645
1
Iteration 10200: Loss = -11451.898115252767
2
Iteration 10300: Loss = -11451.788607445067
3
Iteration 10400: Loss = -11451.72721035927
Iteration 10500: Loss = -11451.7200952041
Iteration 10600: Loss = -11451.724890200143
1
Iteration 10700: Loss = -11451.718630658757
Iteration 10800: Loss = -11451.728339558753
1
Iteration 10900: Loss = -11451.71772275981
Iteration 11000: Loss = -11451.717161285651
Iteration 11100: Loss = -11451.717178698806
1
Iteration 11200: Loss = -11451.718132558333
2
Iteration 11300: Loss = -11451.760267196423
3
Iteration 11400: Loss = -11451.719643271743
4
Iteration 11500: Loss = -11451.718369026406
5
Iteration 11600: Loss = -11451.720161578038
6
Iteration 11700: Loss = -11451.738204229081
7
Iteration 11800: Loss = -11451.717463257002
8
Iteration 11900: Loss = -11451.717330207368
9
Iteration 12000: Loss = -11451.718393178344
10
Stopping early at iteration 12000 due to no improvement.
tensor([[ -8.9041,   7.0596],
        [  7.1716,  -9.0864],
        [ -5.1510,   3.0149],
        [ -1.3699,  -0.2989],
        [  5.3572,  -7.4782],
        [ -7.3035,   5.5388],
        [ -8.4494,   6.8314],
        [  6.2123,  -7.9810],
        [  7.0284,  -9.1592],
        [ -7.2921,   5.9042],
        [ -8.0528,   6.6105],
        [  6.8744,  -8.6882],
        [ -8.7177,   7.1904],
        [ -8.0202,   5.3333],
        [ -6.7851,   5.0070],
        [ -9.4761,   7.3639],
        [ -8.2322,   6.8402],
        [  7.0484,  -9.2902],
        [ -8.7254,   6.4594],
        [  6.5651, -11.1803],
        [  6.8142,  -8.9411],
        [  6.7048,  -8.0911],
        [ -7.7898,   6.2640],
        [  6.3754,  -7.9462],
        [ -7.3972,   6.0101],
        [  6.9243,  -9.9914],
        [  6.4346,  -8.3327],
        [  5.3240,  -7.7084],
        [  6.5196,  -7.9340],
        [ -4.2292,   2.4646],
        [  5.7635,  -8.4216],
        [ -7.8459,   5.7914],
        [  6.4624,  -7.9614],
        [  5.5945,  -6.9836],
        [  4.3873,  -6.2851],
        [  7.0053, -10.5150],
        [  6.4697,  -7.9051],
        [ -7.9972,   5.1269],
        [  6.7446,  -9.3483],
        [  6.7486,  -8.2467],
        [  6.0334,  -7.9305],
        [  4.1560,  -6.1236],
        [ -7.6267,   6.0382],
        [ -9.2416,   5.8313],
        [ -4.0739,   2.6669],
        [-10.0945,   6.6390],
        [ -8.1607,   5.8836],
        [ -7.9627,   6.3988],
        [ -8.9353,   6.5786],
        [-10.1958,   6.4900],
        [  6.3224,  -7.7552],
        [  3.1381,  -4.5247],
        [ -6.9906,   4.9618],
        [  5.0349,  -8.9691],
        [  6.2217,  -8.8025],
        [ -8.0505,   5.8669],
        [  6.1838,  -8.1517],
        [  6.5214,  -8.4140],
        [  6.3688,  -7.9584],
        [  6.9996,  -9.3652],
        [  5.9579,  -8.3862],
        [  6.4660,  -8.1313],
        [  6.7622,  -8.2451],
        [ -7.7126,   6.2806],
        [ -7.0165,   5.4200],
        [ -7.7059,   6.2794],
        [ -7.5847,   5.0834],
        [  6.5697,  -8.8760],
        [  6.6853,  -8.2095],
        [  6.4766,  -8.1319],
        [  6.1661,  -7.6045],
        [  6.9144,  -8.7852],
        [ -8.7038,   7.2159],
        [ -7.6397,   5.9899],
        [  6.8691,  -9.4482],
        [  5.8693,  -8.1934],
        [ -6.4862,   4.8027],
        [ -2.8608,   0.7389],
        [ -2.2329,   0.7197],
        [  6.5575,  -8.9114],
        [ -8.4609,   5.8872],
        [ -5.8663,   4.4744],
        [  5.6098, -10.2250],
        [  6.9516,  -9.1424],
        [  6.5873,  -7.9737],
        [ -7.7159,   5.6990],
        [ -7.8464,   6.1698],
        [ -7.4552,   5.9482],
        [-10.1549,   6.7649],
        [ -8.1046,   6.6486],
        [  0.5059,  -3.4172],
        [  6.3162,  -7.7180],
        [ -7.2557,   5.8667],
        [ -7.8395,   6.1833],
        [  6.3828, -10.6588],
        [ -6.7681,   5.2444],
        [  4.2416,  -5.8876],
        [ -8.0812,   6.2021],
        [ -8.1787,   6.7922],
        [  5.9169,  -7.8223]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7447, 0.2553],
        [0.2297, 0.7703]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5129, 0.4871], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4026, 0.1040],
         [0.1922, 0.1931]],

        [[0.7914, 0.1117],
         [0.8356, 0.0396]],

        [[0.0323, 0.0999],
         [0.4487, 0.0701]],

        [[0.6485, 0.0969],
         [0.0791, 0.3287]],

        [[0.8469, 0.0951],
         [0.7941, 0.5695]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320634059188
Average Adjusted Rand Index: 0.9839989969312853
Iteration 0: Loss = -29161.8546826961
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.2001,    nan]],

        [[0.6241,    nan],
         [0.2342, 0.3644]],

        [[0.7595,    nan],
         [0.1089, 0.7400]],

        [[0.8286,    nan],
         [0.4891, 0.8076]],

        [[0.5399,    nan],
         [0.8006, 0.0360]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29161.669350974727
Iteration 100: Loss = -12220.136648338934
Iteration 200: Loss = -12196.745917090566
Iteration 300: Loss = -12165.738214194436
Iteration 400: Loss = -12099.378600555723
Iteration 500: Loss = -11937.535992400668
Iteration 600: Loss = -11763.580792584973
Iteration 700: Loss = -11683.104339509582
Iteration 800: Loss = -11668.687004384918
Iteration 900: Loss = -11667.452208837858
Iteration 1000: Loss = -11666.632505927426
Iteration 1100: Loss = -11647.04392351802
Iteration 1200: Loss = -11646.63809913401
Iteration 1300: Loss = -11646.375732729697
Iteration 1400: Loss = -11646.175839974152
Iteration 1500: Loss = -11646.018899258914
Iteration 1600: Loss = -11645.900622026777
Iteration 1700: Loss = -11645.808303563983
Iteration 1800: Loss = -11645.73128907739
Iteration 1900: Loss = -11645.661584201302
Iteration 2000: Loss = -11645.544450785204
Iteration 2100: Loss = -11637.600949034153
Iteration 2200: Loss = -11637.554836874828
Iteration 2300: Loss = -11637.517044269065
Iteration 2400: Loss = -11637.48347887081
Iteration 2500: Loss = -11637.445924359716
Iteration 2600: Loss = -11621.200611196227
Iteration 2700: Loss = -11621.056275745128
Iteration 2800: Loss = -11621.025611690078
Iteration 2900: Loss = -11621.003765106345
Iteration 3000: Loss = -11620.984668616136
Iteration 3100: Loss = -11620.965323293713
Iteration 3200: Loss = -11605.28862473828
Iteration 3300: Loss = -11605.187531689939
Iteration 3400: Loss = -11605.169718761601
Iteration 3500: Loss = -11605.1510372379
Iteration 3600: Loss = -11605.123544843764
Iteration 3700: Loss = -11605.106697852096
Iteration 3800: Loss = -11605.035565572909
Iteration 3900: Loss = -11598.611314347121
Iteration 4000: Loss = -11598.60218170464
Iteration 4100: Loss = -11598.594645949255
Iteration 4200: Loss = -11598.58780027642
Iteration 4300: Loss = -11598.581882520846
Iteration 4400: Loss = -11598.575516245623
Iteration 4500: Loss = -11598.581509327118
1
Iteration 4600: Loss = -11587.762217555366
Iteration 4700: Loss = -11587.752918367336
Iteration 4800: Loss = -11587.748225780924
Iteration 4900: Loss = -11587.742901011741
Iteration 5000: Loss = -11587.735517301464
Iteration 5100: Loss = -11583.819159957686
Iteration 5200: Loss = -11583.79417707053
Iteration 5300: Loss = -11583.789107375167
Iteration 5400: Loss = -11583.796922923784
1
Iteration 5500: Loss = -11583.782664432953
Iteration 5600: Loss = -11583.779860841845
Iteration 5700: Loss = -11583.782303015838
1
Iteration 5800: Loss = -11583.768669716388
Iteration 5900: Loss = -11578.57837229747
Iteration 6000: Loss = -11578.575818262361
Iteration 6100: Loss = -11578.575895529215
1
Iteration 6200: Loss = -11578.574308106778
Iteration 6300: Loss = -11578.570867892755
Iteration 6400: Loss = -11565.245965329985
Iteration 6500: Loss = -11565.234576453848
Iteration 6600: Loss = -11565.232712016745
Iteration 6700: Loss = -11565.23190768498
Iteration 6800: Loss = -11565.236920006173
1
Iteration 6900: Loss = -11565.249918805586
2
Iteration 7000: Loss = -11565.232258031021
3
Iteration 7100: Loss = -11565.244195522824
4
Iteration 7200: Loss = -11565.23833636903
5
Iteration 7300: Loss = -11565.225085342445
Iteration 7400: Loss = -11565.223525156061
Iteration 7500: Loss = -11565.241623114098
1
Iteration 7600: Loss = -11565.243328028006
2
Iteration 7700: Loss = -11565.23376494929
3
Iteration 7800: Loss = -11565.222379926427
Iteration 7900: Loss = -11565.22039127649
Iteration 8000: Loss = -11565.219318201734
Iteration 8100: Loss = -11565.218812695743
Iteration 8200: Loss = -11565.219453652033
1
Iteration 8300: Loss = -11565.236900920085
2
Iteration 8400: Loss = -11565.216009650641
Iteration 8500: Loss = -11562.587302128168
Iteration 8600: Loss = -11562.580609506276
Iteration 8700: Loss = -11562.579981234758
Iteration 8800: Loss = -11562.57926696059
Iteration 8900: Loss = -11562.581810543248
1
Iteration 9000: Loss = -11562.57876644907
Iteration 9100: Loss = -11562.577507612394
Iteration 9200: Loss = -11562.57718147676
Iteration 9300: Loss = -11562.577491169586
1
Iteration 9400: Loss = -11562.5765000502
Iteration 9500: Loss = -11562.57694002144
1
Iteration 9600: Loss = -11554.127091626535
Iteration 9700: Loss = -11554.119809654563
Iteration 9800: Loss = -11554.115765000548
Iteration 9900: Loss = -11554.131364931152
1
Iteration 10000: Loss = -11554.101923821816
Iteration 10100: Loss = -11554.100918116206
Iteration 10200: Loss = -11554.111681059669
1
Iteration 10300: Loss = -11554.134730162428
2
Iteration 10400: Loss = -11554.108418700584
3
Iteration 10500: Loss = -11554.100228706382
Iteration 10600: Loss = -11554.128749932182
1
Iteration 10700: Loss = -11554.101246233038
2
Iteration 10800: Loss = -11554.108889517085
3
Iteration 10900: Loss = -11543.323498316626
Iteration 11000: Loss = -11543.256238704362
Iteration 11100: Loss = -11543.285137362045
1
Iteration 11200: Loss = -11543.275057220091
2
Iteration 11300: Loss = -11543.267494211175
3
Iteration 11400: Loss = -11543.309697152257
4
Iteration 11500: Loss = -11543.283759515116
5
Iteration 11600: Loss = -11543.260496733954
6
Iteration 11700: Loss = -11543.255619414173
Iteration 11800: Loss = -11543.254775239351
Iteration 11900: Loss = -11543.272595323055
1
Iteration 12000: Loss = -11543.272331308895
2
Iteration 12100: Loss = -11543.263089806196
3
Iteration 12200: Loss = -11543.261656450566
4
Iteration 12300: Loss = -11543.25205518343
Iteration 12400: Loss = -11536.528954102001
Iteration 12500: Loss = -11536.500088017392
Iteration 12600: Loss = -11536.499817396198
Iteration 12700: Loss = -11536.500060680957
1
Iteration 12800: Loss = -11536.499764608321
Iteration 12900: Loss = -11536.500960843567
1
Iteration 13000: Loss = -11536.515359908002
2
Iteration 13100: Loss = -11536.505047093706
3
Iteration 13200: Loss = -11536.507560868311
4
Iteration 13300: Loss = -11536.59213835968
5
Iteration 13400: Loss = -11536.527593605744
6
Iteration 13500: Loss = -11536.503367651141
7
Iteration 13600: Loss = -11536.655106700384
8
Iteration 13700: Loss = -11536.500306099248
9
Iteration 13800: Loss = -11536.500657821618
10
Stopping early at iteration 13800 due to no improvement.
tensor([[  3.1669,  -6.4514],
        [ -8.5710,   6.8035],
        [  2.6682,  -5.8578],
        [  0.1443,  -1.5457],
        [ -7.8156,   6.4118],
        [  5.4813,  -8.4886],
        [  5.3145,  -8.6115],
        [ -8.4431,   6.9621],
        [ -9.8827,   6.4097],
        [  5.7293, -10.3445],
        [  5.0133,  -8.8257],
        [ -8.6769,   6.1192],
        [  6.2327,  -7.6198],
        [  5.3223,  -8.3136],
        [  4.8054,  -7.1853],
        [  5.4848,  -7.7788],
        [  5.9011,  -8.0070],
        [ -7.9800,   6.5544],
        [  6.2674,  -7.9417],
        [ -8.6487,   7.2595],
        [ -9.7635,   7.3702],
        [ -9.4898,   7.1679],
        [  6.2104,  -7.5990],
        [ -8.3391,   6.9418],
        [  6.2402,  -7.7248],
        [-10.0801,   7.1468],
        [ -8.9315,   6.0661],
        [ -7.5294,   5.2013],
        [ -9.5293,   7.4541],
        [  2.7405,  -4.2048],
        [ -7.4045,   5.6016],
        [  6.1737,  -7.5984],
        [ -8.7083,   6.6748],
        [ -7.5943,   4.9107],
        [ -6.1457,   4.0563],
        [ -8.3121,   6.7909],
        [ -8.3325,   5.9942],
        [  5.7186,  -7.1832],
        [ -8.3852,   6.8839],
        [ -9.0585,   7.6491],
        [ -8.5578,   7.1698],
        [ -5.8304,   4.3949],
        [  5.9140,  -7.5436],
        [  4.7590,  -6.1518],
        [  5.6377,  -7.7474],
        [  5.9074,  -7.3667],
        [  4.4671,  -5.8559],
        [  6.1886,  -7.9702],
        [  6.1141,  -7.7436],
        [  5.0413,  -6.5687],
        [ -8.9220,   7.5091],
        [ -4.6631,   2.5636],
        [  5.3168,  -6.7416],
        [ -8.3988,   5.8123],
        [ -8.3799,   6.5507],
        [  5.9409,  -8.5177],
        [ -7.5215,   6.1351],
        [ -8.2266,   6.8279],
        [ -8.0293,   6.4784],
        [ -8.6086,   7.2033],
        [ -8.0660,   5.9595],
        [ -7.9376,   6.4469],
        [ -9.0231,   7.0115],
        [  6.5673,  -8.1763],
        [  5.6427,  -7.0738],
        [  5.3755,  -9.9908],
        [  5.4523,  -7.2485],
        [ -9.1037,   7.2524],
        [ -8.1520,   6.4231],
        [ -9.1184,   6.5326],
        [ -7.5931,   5.8966],
        [ -8.3899,   6.5792],
        [  5.7641,  -8.2342],
        [  6.2410,  -8.2131],
        [ -8.1570,   6.6792],
        [ -7.7382,   6.2293],
        [  5.7078,  -7.8419],
        [  1.0449,  -2.7838],
        [  0.9156,  -2.5194],
        [ -8.0530,   6.4632],
        [  5.9838,  -7.7930],
        [  4.3438,  -6.2842],
        [ -9.0870,   6.3812],
        [ -7.9976,   6.4687],
        [ -8.1315,   6.7137],
        [  6.2330,  -7.6636],
        [  4.7095,  -6.3192],
        [  6.0621,  -7.6490],
        [  6.2703,  -7.8265],
        [  4.1345,  -6.4752],
        [ -3.5659,   0.2702],
        [ -7.8500,   6.4586],
        [  6.0690,  -7.5941],
        [  6.4176,  -9.7621],
        [ -7.9917,   6.6006],
        [  5.3124,  -7.0863],
        [ -5.7663,   4.3111],
        [  5.0213,  -7.6892],
        [  6.6735, -11.2887],
        [ -8.6916,   6.7141]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7596, 0.2404],
        [0.3019, 0.6981]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4879, 0.5121], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1889, 0.1042],
         [0.2001, 0.4113]],

        [[0.6241, 0.1176],
         [0.2342, 0.3644]],

        [[0.7595, 0.1001],
         [0.1089, 0.7400]],

        [[0.8286, 0.1257],
         [0.4891, 0.8076]],

        [[0.5399, 0.0953],
         [0.8006, 0.0360]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080477173169247
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 91
Adjusted Rand Index: 0.6691881173576176
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8833673283455051
Average Adjusted Rand Index: 0.8874465086621985
Iteration 0: Loss = -30247.88258145565
Iteration 10: Loss = -12212.824855124229
Iteration 20: Loss = -12206.495603593166
Iteration 30: Loss = -11888.4221358697
Iteration 40: Loss = -11453.909225623845
Iteration 50: Loss = -11453.908457757043
Iteration 60: Loss = -11453.908457756834
Iteration 70: Loss = -11453.908457756834
1
Iteration 80: Loss = -11453.908457756834
2
Iteration 90: Loss = -11453.908457756834
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7666, 0.2334],
        [0.2589, 0.7411]], dtype=torch.float64)
alpha: tensor([0.5217, 0.4783])
beta: tensor([[[0.1894, 0.1040],
         [0.2601, 0.3943]],

        [[0.2596, 0.1118],
         [0.6174, 0.6050]],

        [[0.4872, 0.1001],
         [0.8208, 0.5414]],

        [[0.2258, 0.0972],
         [0.2496, 0.6522]],

        [[0.4205, 0.0949],
         [0.6689, 0.5473]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961975210904
Average Adjusted Rand Index: 0.9759983386585752
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30247.547369386448
Iteration 100: Loss = -12256.668180394365
Iteration 200: Loss = -12194.014955543462
Iteration 300: Loss = -12090.165693059751
Iteration 400: Loss = -12015.136954692527
Iteration 500: Loss = -11946.832291281786
Iteration 600: Loss = -11880.799835478841
Iteration 700: Loss = -11813.833359618873
Iteration 800: Loss = -11778.953974860087
Iteration 900: Loss = -11769.970181342856
Iteration 1000: Loss = -11750.252629942357
Iteration 1100: Loss = -11724.196421826187
Iteration 1200: Loss = -11682.465715882232
Iteration 1300: Loss = -11632.218590556315
Iteration 1400: Loss = -11599.579285871278
Iteration 1500: Loss = -11568.69273769453
Iteration 1600: Loss = -11567.780932936806
Iteration 1700: Loss = -11556.214990372078
Iteration 1800: Loss = -11555.307224963562
Iteration 1900: Loss = -11555.040999571904
Iteration 2000: Loss = -11544.011087610372
Iteration 2100: Loss = -11543.863370571222
Iteration 2200: Loss = -11538.050229073682
Iteration 2300: Loss = -11537.952964960203
Iteration 2400: Loss = -11537.912789058377
Iteration 2500: Loss = -11537.863957432273
Iteration 2600: Loss = -11520.4725868968
Iteration 2700: Loss = -11520.42968063947
Iteration 2800: Loss = -11520.373721752312
Iteration 2900: Loss = -11514.119122341692
Iteration 3000: Loss = -11514.098456356467
Iteration 3100: Loss = -11514.080218361829
Iteration 3200: Loss = -11514.062330431698
Iteration 3300: Loss = -11514.037924967473
Iteration 3400: Loss = -11513.776674944504
Iteration 3500: Loss = -11513.742103998227
Iteration 3600: Loss = -11513.731784709058
Iteration 3700: Loss = -11513.722807157645
Iteration 3800: Loss = -11513.714392238866
Iteration 3900: Loss = -11513.70385071914
Iteration 4000: Loss = -11505.00450522594
Iteration 4100: Loss = -11504.995067893944
Iteration 4200: Loss = -11491.637427655356
Iteration 4300: Loss = -11491.563064928452
Iteration 4400: Loss = -11491.556575372095
Iteration 4500: Loss = -11491.550335050133
Iteration 4600: Loss = -11491.54245772212
Iteration 4700: Loss = -11482.590332094811
Iteration 4800: Loss = -11482.563755661316
Iteration 4900: Loss = -11482.30208885702
Iteration 5000: Loss = -11479.913296560606
Iteration 5100: Loss = -11479.908626212618
Iteration 5200: Loss = -11479.904538452804
Iteration 5300: Loss = -11479.901380663492
Iteration 5400: Loss = -11479.89777614468
Iteration 5500: Loss = -11479.894822735268
Iteration 5600: Loss = -11479.90859830283
1
Iteration 5700: Loss = -11479.889771354303
Iteration 5800: Loss = -11479.887357492631
Iteration 5900: Loss = -11479.884254352359
Iteration 6000: Loss = -11479.82136385849
Iteration 6100: Loss = -11479.819488491676
Iteration 6200: Loss = -11479.813541887706
Iteration 6300: Loss = -11471.475512964325
Iteration 6400: Loss = -11471.470940494595
Iteration 6500: Loss = -11471.46427459209
Iteration 6600: Loss = -11471.464193393902
Iteration 6700: Loss = -11471.462245543973
Iteration 6800: Loss = -11471.467149561557
1
Iteration 6900: Loss = -11471.462046867855
Iteration 7000: Loss = -11471.461261904498
Iteration 7100: Loss = -11471.459013696149
Iteration 7200: Loss = -11471.54973510704
1
Iteration 7300: Loss = -11471.465919415814
2
Iteration 7400: Loss = -11471.47823277782
3
Iteration 7500: Loss = -11471.45588834585
Iteration 7600: Loss = -11471.455142251481
Iteration 7700: Loss = -11471.462490798845
1
Iteration 7800: Loss = -11471.454863934294
Iteration 7900: Loss = -11471.453569954545
Iteration 8000: Loss = -11471.554121479036
1
Iteration 8100: Loss = -11471.45258658401
Iteration 8200: Loss = -11471.471638552592
1
Iteration 8300: Loss = -11471.450213565702
Iteration 8400: Loss = -11471.454437276068
1
Iteration 8500: Loss = -11471.43613675676
Iteration 8600: Loss = -11471.401549108981
Iteration 8700: Loss = -11471.399034152466
Iteration 8800: Loss = -11471.405354241506
1
Iteration 8900: Loss = -11471.408197710043
2
Iteration 9000: Loss = -11471.398504703262
Iteration 9100: Loss = -11471.39566400629
Iteration 9200: Loss = -11471.39348096819
Iteration 9300: Loss = -11471.393248645863
Iteration 9400: Loss = -11465.302339067339
Iteration 9500: Loss = -11465.301103978274
Iteration 9600: Loss = -11465.318532708741
1
Iteration 9700: Loss = -11465.387530006004
2
Iteration 9800: Loss = -11465.300235246852
Iteration 9900: Loss = -11465.300529843395
1
Iteration 10000: Loss = -11465.30149748918
2
Iteration 10100: Loss = -11465.300456785631
3
Iteration 10200: Loss = -11465.306090058533
4
Iteration 10300: Loss = -11465.407999289859
5
Iteration 10400: Loss = -11465.29783204676
Iteration 10500: Loss = -11465.297942387646
1
Iteration 10600: Loss = -11465.326452537445
2
Iteration 10700: Loss = -11465.250328401109
Iteration 10800: Loss = -11465.2484281919
Iteration 10900: Loss = -11465.263324245872
1
Iteration 11000: Loss = -11465.361232112027
2
Iteration 11100: Loss = -11465.248872445583
3
Iteration 11200: Loss = -11465.248677179272
4
Iteration 11300: Loss = -11465.249332390427
5
Iteration 11400: Loss = -11465.256444814322
6
Iteration 11500: Loss = -11465.250797485292
7
Iteration 11600: Loss = -11465.250515792628
8
Iteration 11700: Loss = -11465.24961187693
9
Iteration 11800: Loss = -11465.247992648723
Iteration 11900: Loss = -11465.248527866912
1
Iteration 12000: Loss = -11465.247985454758
Iteration 12100: Loss = -11465.256575798632
1
Iteration 12200: Loss = -11465.253329227997
2
Iteration 12300: Loss = -11465.249495649114
3
Iteration 12400: Loss = -11465.248448118431
4
Iteration 12500: Loss = -11465.260198417365
5
Iteration 12600: Loss = -11465.295615009574
6
Iteration 12700: Loss = -11465.25215488734
7
Iteration 12800: Loss = -11465.247401190569
Iteration 12900: Loss = -11465.24801224705
1
Iteration 13000: Loss = -11465.247536786934
2
Iteration 13100: Loss = -11465.262845560324
3
Iteration 13200: Loss = -11465.24985043574
4
Iteration 13300: Loss = -11465.260381175105
5
Iteration 13400: Loss = -11465.250614619006
6
Iteration 13500: Loss = -11465.247543374599
7
Iteration 13600: Loss = -11465.255379296066
8
Iteration 13700: Loss = -11465.246630091777
Iteration 13800: Loss = -11465.249243930688
1
Iteration 13900: Loss = -11465.24769055025
2
Iteration 14000: Loss = -11465.248182933568
3
Iteration 14100: Loss = -11465.248076025668
4
Iteration 14200: Loss = -11465.2472135985
5
Iteration 14300: Loss = -11465.259674601719
6
Iteration 14400: Loss = -11465.274865013944
7
Iteration 14500: Loss = -11465.250634454273
8
Iteration 14600: Loss = -11465.249887296668
9
Iteration 14700: Loss = -11465.264330575268
10
Stopping early at iteration 14700 due to no improvement.
tensor([[  3.9867,  -5.3777],
        [ -9.1121,   7.7169],
        [  3.0890,  -5.2187],
        [  7.2110,  -9.6293],
        [ -9.3402,   7.2647],
        [  4.8574,  -9.4726],
        [  6.3365,  -7.7319],
        [ -9.5485,   8.0135],
        [-11.0004,   6.3852],
        [  7.8762, -10.3365],
        [  6.1967,  -7.6868],
        [-10.3227,   6.2739],
        [  6.2749,  -7.7989],
        [  5.7756,  -8.8603],
        [  4.7344,  -7.1132],
        [  5.7927,  -8.3204],
        [  5.8711,  -7.2830],
        [ -8.5651,   7.1056],
        [  6.1018,  -7.5441],
        [ -9.5557,   8.1308],
        [ -9.9417,   7.9590],
        [ -8.3113,   6.7146],
        [  6.3020,  -9.3578],
        [ -9.5592,   7.0473],
        [  5.9413,  -8.8157],
        [ -9.6630,   7.2965],
        [ -8.7376,   7.3250],
        [ -9.0091,   7.0979],
        [ -9.7380,   8.0485],
        [  2.7127,  -4.1015],
        [ -7.4067,   5.7677],
        [  6.2820,  -8.4153],
        [ -8.2136,   6.6783],
        [-12.4194,   8.0502],
        [ -8.5259,   6.8378],
        [ -9.3141,   7.9263],
        [ -9.3854,   7.2052],
        [  5.2066,  -8.0810],
        [-11.1003,   7.4665],
        [ -9.5471,   8.1607],
        [ -9.8145,   8.2818],
        [ -5.6627,   4.0545],
        [  6.1356,  -7.5709],
        [  4.2688,  -6.4951],
        [  7.2208,  -8.6446],
        [  5.9707,  -7.3575],
        [  4.3323,  -5.7513],
        [  6.3986, -10.8406],
        [  6.3591,  -7.8554],
        [  4.4753,  -6.9918],
        [ -9.5110,   8.1196],
        [-10.8975,   7.4277],
        [  5.2847,  -6.9398],
        [ -9.0257,   6.8767],
        [-10.4092,   7.9988],
        [  7.3447,  -8.7363],
        [ -9.2374,   7.8502],
        [ -9.2827,   7.4511],
        [ -9.6324,   8.2444],
        [ -9.5384,   7.7835],
        [ -8.4360,   7.0335],
        [-10.6899,   8.1051],
        [-10.3010,   7.9267],
        [  8.0255,  -9.4486],
        [  5.5365,  -7.2900],
        [  6.8865,  -8.2873],
        [  5.5848,  -7.0128],
        [ -9.0352,   7.2818],
        [ -9.8418,   8.3759],
        [-10.1459,   7.1007],
        [ -9.5004,   7.7899],
        [ -9.8795,   8.3092],
        [  6.3688,  -7.9118],
        [  7.0168,  -9.2599],
        [ -9.4377,   7.8249],
        [ -9.7425,   7.4170],
        [  7.6039,  -9.5085],
        [  1.1469,  -2.5378],
        [  1.0982,  -2.5865],
        [ -8.9061,   7.1812],
        [  6.4881,  -7.9476],
        [  3.1255,  -7.2976],
        [-11.3200,   6.7048],
        [ -9.0466,   7.2245],
        [ -9.0894,   7.6566],
        [  6.6279,  -8.1913],
        [  4.6809,  -6.1171],
        [  6.1056,  -7.5256],
        [  6.8886,  -8.2971],
        [  3.6174,  -6.8227],
        [ -2.8961,   1.0049],
        [ -9.1696,   7.6642],
        [  6.0480,  -7.4806],
        [  6.4311,  -7.8743],
        [ -8.6938,   6.7309],
        [  5.3085,  -6.8127],
        [ -5.7972,   4.2966],
        [  5.3263,  -7.2083],
        [  6.9669,  -8.4256],
        [ -9.8153,   7.8008]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7638, 0.2362],
        [0.2690, 0.7310]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4894, 0.5106], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1913, 0.1039],
         [0.2601, 0.4065]],

        [[0.2596, 0.1128],
         [0.6174, 0.6050]],

        [[0.4872, 0.1001],
         [0.8208, 0.5414]],

        [[0.2258, 0.1045],
         [0.2496, 0.6522]],

        [[0.4205, 0.0952],
         [0.6689, 0.5473]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524810156639772
Average Adjusted Rand Index: 0.9529686939009823
Iteration 0: Loss = -21093.697625343717
Iteration 10: Loss = -12178.445236083217
Iteration 20: Loss = -11454.019279051494
Iteration 30: Loss = -11453.908451630698
Iteration 40: Loss = -11453.908447441894
Iteration 50: Loss = -11453.908447441894
1
Iteration 60: Loss = -11453.908447441894
2
Iteration 70: Loss = -11453.908447441894
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7411, 0.2589],
        [0.2334, 0.7666]], dtype=torch.float64)
alpha: tensor([0.4783, 0.5217])
beta: tensor([[[0.3943, 0.1040],
         [0.3217, 0.1894]],

        [[0.5664, 0.1118],
         [0.6952, 0.6189]],

        [[0.7619, 0.1001],
         [0.5264, 0.7504]],

        [[0.3259, 0.0972],
         [0.7024, 0.2902]],

        [[0.1020, 0.0949],
         [0.8266, 0.6503]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961975210904
Average Adjusted Rand Index: 0.9759983386585752
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21093.589777363162
Iteration 100: Loss = -12181.601273539709
Iteration 200: Loss = -11755.9567114605
Iteration 300: Loss = -11577.319316530611
Iteration 400: Loss = -11541.789213296977
Iteration 500: Loss = -11532.35727845661
Iteration 600: Loss = -11517.745039629131
Iteration 700: Loss = -11517.44823769091
Iteration 800: Loss = -11517.26749294724
Iteration 900: Loss = -11517.142808724107
Iteration 1000: Loss = -11517.048029186448
Iteration 1100: Loss = -11516.953778890784
Iteration 1200: Loss = -11515.321660789052
Iteration 1300: Loss = -11515.17390426087
Iteration 1400: Loss = -11510.501172383209
Iteration 1500: Loss = -11510.472638259982
Iteration 1600: Loss = -11510.447425984872
Iteration 1700: Loss = -11488.931532711858
Iteration 1800: Loss = -11478.89574975125
Iteration 1900: Loss = -11478.878831165293
Iteration 2000: Loss = -11478.865044871054
Iteration 2100: Loss = -11478.853382868443
Iteration 2200: Loss = -11478.843405813432
Iteration 2300: Loss = -11478.83484092182
Iteration 2400: Loss = -11478.827304385142
Iteration 2500: Loss = -11478.820719490966
Iteration 2600: Loss = -11478.81488404166
Iteration 2700: Loss = -11478.80968694593
Iteration 2800: Loss = -11478.805060603361
Iteration 2900: Loss = -11478.800858796476
Iteration 3000: Loss = -11478.79713427542
Iteration 3100: Loss = -11478.793719371897
Iteration 3200: Loss = -11478.790610896138
Iteration 3300: Loss = -11478.787808686808
Iteration 3400: Loss = -11478.785230505726
Iteration 3500: Loss = -11478.78281825361
Iteration 3600: Loss = -11478.780603790668
Iteration 3700: Loss = -11478.778409275543
Iteration 3800: Loss = -11478.774852400802
Iteration 3900: Loss = -11461.78053507473
Iteration 4000: Loss = -11461.77984600142
Iteration 4100: Loss = -11461.772409231176
Iteration 4200: Loss = -11461.770826008402
Iteration 4300: Loss = -11461.76950487041
Iteration 4400: Loss = -11461.768062660381
Iteration 4500: Loss = -11461.771583510832
1
Iteration 4600: Loss = -11461.76571890536
Iteration 4700: Loss = -11461.764701850108
Iteration 4800: Loss = -11461.763750850316
Iteration 4900: Loss = -11461.76299902747
Iteration 5000: Loss = -11461.764364935316
1
Iteration 5100: Loss = -11461.761283532393
Iteration 5200: Loss = -11461.763174728612
1
Iteration 5300: Loss = -11461.760371453533
Iteration 5400: Loss = -11461.759377445569
Iteration 5500: Loss = -11461.770287796864
1
Iteration 5600: Loss = -11461.758430518243
Iteration 5700: Loss = -11461.757760010354
Iteration 5800: Loss = -11461.75735269308
Iteration 5900: Loss = -11461.78923981856
1
Iteration 6000: Loss = -11461.756511062747
Iteration 6100: Loss = -11461.756132900522
Iteration 6200: Loss = -11461.755829964844
Iteration 6300: Loss = -11461.755955552326
1
Iteration 6400: Loss = -11461.75512788646
Iteration 6500: Loss = -11461.754837775701
Iteration 6600: Loss = -11461.782659026809
1
Iteration 6700: Loss = -11461.75426771373
Iteration 6800: Loss = -11461.755580078134
1
Iteration 6900: Loss = -11461.753942125197
Iteration 7000: Loss = -11461.753543917142
Iteration 7100: Loss = -11461.753612458504
1
Iteration 7200: Loss = -11461.75318647312
Iteration 7300: Loss = -11461.759571727729
1
Iteration 7400: Loss = -11461.75285034461
Iteration 7500: Loss = -11461.75325162354
1
Iteration 7600: Loss = -11461.752531382204
Iteration 7700: Loss = -11461.75234902328
Iteration 7800: Loss = -11461.75235470485
1
Iteration 7900: Loss = -11461.752165065864
Iteration 8000: Loss = -11461.756726034177
1
Iteration 8100: Loss = -11461.751860906656
Iteration 8200: Loss = -11461.751821993912
Iteration 8300: Loss = -11461.766041264988
1
Iteration 8400: Loss = -11461.760359895656
2
Iteration 8500: Loss = -11461.771813833766
3
Iteration 8600: Loss = -11461.751654709793
Iteration 8700: Loss = -11461.751666904405
1
Iteration 8800: Loss = -11461.752861076684
2
Iteration 8900: Loss = -11461.758908809074
3
Iteration 9000: Loss = -11461.752537648337
4
Iteration 9100: Loss = -11461.756164526316
5
Iteration 9200: Loss = -11461.751338976432
Iteration 9300: Loss = -11461.782187920588
1
Iteration 9400: Loss = -11461.760082670338
2
Iteration 9500: Loss = -11461.754655671384
3
Iteration 9600: Loss = -11461.762337617738
4
Iteration 9700: Loss = -11461.750801274457
Iteration 9800: Loss = -11461.754075044471
1
Iteration 9900: Loss = -11461.788855631472
2
Iteration 10000: Loss = -11461.75857592963
3
Iteration 10100: Loss = -11461.753451497272
4
Iteration 10200: Loss = -11461.75267786694
5
Iteration 10300: Loss = -11461.752606533955
6
Iteration 10400: Loss = -11461.75737299257
7
Iteration 10500: Loss = -11461.754117549843
8
Iteration 10600: Loss = -11461.750605767718
Iteration 10700: Loss = -11461.75060476345
Iteration 10800: Loss = -11461.754932379768
1
Iteration 10900: Loss = -11461.750508278483
Iteration 11000: Loss = -11461.752432373552
1
Iteration 11100: Loss = -11461.753408365714
2
Iteration 11200: Loss = -11461.750219277998
Iteration 11300: Loss = -11461.76624024912
1
Iteration 11400: Loss = -11461.759335680279
2
Iteration 11500: Loss = -11461.750170365798
Iteration 11600: Loss = -11461.752070069788
1
Iteration 11700: Loss = -11461.759087551338
2
Iteration 11800: Loss = -11461.77627394302
3
Iteration 11900: Loss = -11461.764148737584
4
Iteration 12000: Loss = -11461.75892645572
5
Iteration 12100: Loss = -11461.750179861388
6
Iteration 12200: Loss = -11461.749944502846
Iteration 12300: Loss = -11461.751702669158
1
Iteration 12400: Loss = -11461.922287132951
2
Iteration 12500: Loss = -11461.751007175326
3
Iteration 12600: Loss = -11461.753786717292
4
Iteration 12700: Loss = -11461.770301250703
5
Iteration 12800: Loss = -11461.755302421227
6
Iteration 12900: Loss = -11461.752147722378
7
Iteration 13000: Loss = -11461.75108734532
8
Iteration 13100: Loss = -11461.749706650113
Iteration 13200: Loss = -11461.754827863566
1
Iteration 13300: Loss = -11461.78360291468
2
Iteration 13400: Loss = -11461.766465876968
3
Iteration 13500: Loss = -11461.750128716165
4
Iteration 13600: Loss = -11461.751773009912
5
Iteration 13700: Loss = -11461.808147321233
6
Iteration 13800: Loss = -11461.758535290872
7
Iteration 13900: Loss = -11461.745121500262
Iteration 14000: Loss = -11461.744684065878
Iteration 14100: Loss = -11461.740687901767
Iteration 14200: Loss = -11461.740467541253
Iteration 14300: Loss = -11461.74073702193
1
Iteration 14400: Loss = -11461.750178284106
2
Iteration 14500: Loss = -11461.740914929464
3
Iteration 14600: Loss = -11461.742255684458
4
Iteration 14700: Loss = -11461.741713003881
5
Iteration 14800: Loss = -11461.743563954647
6
Iteration 14900: Loss = -11461.745596075129
7
Iteration 15000: Loss = -11461.742289009058
8
Iteration 15100: Loss = -11461.756058606525
9
Iteration 15200: Loss = -11461.740251654728
Iteration 15300: Loss = -11461.741051770938
1
Iteration 15400: Loss = -11461.745022614361
2
Iteration 15500: Loss = -11461.743877394103
3
Iteration 15600: Loss = -11461.750809725132
4
Iteration 15700: Loss = -11461.739736712483
Iteration 15800: Loss = -11461.73939843336
Iteration 15900: Loss = -11461.73924070013
Iteration 16000: Loss = -11461.74261031306
1
Iteration 16100: Loss = -11461.741402289748
2
Iteration 16200: Loss = -11461.740200469376
3
Iteration 16300: Loss = -11461.739603469476
4
Iteration 16400: Loss = -11461.740717625364
5
Iteration 16500: Loss = -11461.746541393113
6
Iteration 16600: Loss = -11461.770024679281
7
Iteration 16700: Loss = -11461.740473358768
8
Iteration 16800: Loss = -11461.739454740962
9
Iteration 16900: Loss = -11461.740431829317
10
Stopping early at iteration 16900 due to no improvement.
tensor([[ -5.3260,   3.8178],
        [  7.1364, -11.3852],
        [ -4.7647,   3.3563],
        [-10.2159,   8.8121],
        [  6.1598,  -7.8081],
        [ -8.0589,   6.1209],
        [ -8.5107,   6.6683],
        [  8.4447, -10.6644],
        [  8.1366,  -9.5290],
        [ -8.1604,   6.5589],
        [ -8.2965,   6.2415],
        [  6.8295, -10.0719],
        [ -8.6838,   6.4923],
        [ -8.7567,   5.7438],
        [ -6.5561,   5.1366],
        [ -8.0801,   6.0479],
        [ -7.9753,   5.8619],
        [  7.7132,  -9.5175],
        [ -8.9371,   7.1206],
        [  8.1418,  -9.8065],
        [  8.4871, -10.2292],
        [  7.6677,  -9.1685],
        [ -8.6925,   6.8801],
        [  8.0379,  -9.4432],
        [ -9.2636,   5.1687],
        [  8.3030, -10.3985],
        [  7.0668, -10.1343],
        [  5.9605,  -7.3472],
        [  8.1777,  -9.7695],
        [ -4.1837,   2.4824],
        [  5.9651,  -7.5669],
        [ -9.5062,   7.8475],
        [  7.7221,  -9.4691],
        [  5.0739,  -7.8203],
        [  4.5550,  -5.9673],
        [  8.7151, -10.3679],
        [  6.6424, -11.0971],
        [ -7.8058,   5.9768],
        [  8.0541,  -9.5106],
        [  8.5760, -10.8842],
        [  7.4289,  -9.2795],
        [  4.0736,  -5.7113],
        [ -9.0571,   6.7713],
        [ -6.0766,   4.5584],
        [ -4.8759,   2.5101],
        [ -7.8239,   6.3020],
        [ -6.0445,   3.8387],
        [ -8.7850,   7.3641],
        [ -9.2727,   7.3589],
        [ -7.5533,   3.7979],
        [  8.0882, -10.7626],
        [  3.1041,  -4.5332],
        [ -6.7431,   5.2748],
        [  8.1564, -10.7486],
        [  7.6421,  -9.6498],
        [ -8.5203,   6.2637],
        [  7.6102,  -9.2057],
        [  8.0746,  -9.4627],
        [  7.6198,  -9.6942],
        [  8.0072,  -9.8103],
        [  6.7878,  -8.1820],
        [  7.7750,  -9.2041],
        [  8.8988, -10.8957],
        [ -8.5476,   7.1586],
        [-10.6373,   8.5607],
        [ -9.1415,   7.4114],
        [ -7.1188,   5.4950],
        [  7.6416,  -9.2385],
        [  6.4363,  -9.5788],
        [  7.8970, -10.7600],
        [  6.7348,  -8.2912],
        [  8.9048, -10.3497],
        [ -8.5615,   6.9806],
        [ -9.7435,   7.2614],
        [  7.4196,  -9.5376],
        [  7.1349,  -8.7128],
        [ -9.6541,   5.9452],
        [ -2.8629,   0.6716],
        [ -2.5002,   1.0314],
        [  8.1377,  -9.5243],
        [ -9.1059,   6.7189],
        [ -6.1866,   4.0740],
        [  8.4578, -10.7077],
        [  7.5384,  -8.9371],
        [  8.0895, -10.7623],
        [ -9.4075,   7.5254],
        [ -7.0091,   3.5863],
        [ -7.9836,   6.5439],
        [ -9.2237,   7.7188],
        [ -5.8596,   4.4003],
        [  1.3040,  -2.6959],
        [  7.4196,  -8.8183],
        [ -8.4665,   6.7957],
        [ -8.7264,   7.3400],
        [  7.6846, -10.0699],
        [ -6.7032,   5.2503],
        [  4.0245,  -6.1221],
        [ -6.8899,   5.4846],
        [ -9.5701,   8.1449],
        [  7.0789,  -8.4962]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7366, 0.2634],
        [0.2342, 0.7658]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5101, 0.4899], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4043, 0.1045],
         [0.3217, 0.1922]],

        [[0.5664, 0.1155],
         [0.6952, 0.6189]],

        [[0.7619, 0.1000],
         [0.5264, 0.7504]],

        [[0.3259, 0.0971],
         [0.7024, 0.2902]],

        [[0.1020, 0.0949],
         [0.8266, 0.6503]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 2
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961975210904
Average Adjusted Rand Index: 0.9761598485910703
11461.682472272678
new:  [0.9840320634059188, 0.8833673283455051, 0.9524810156639772, 0.9760961975210904] [0.9839989969312853, 0.8874465086621985, 0.9529686939009823, 0.9761598485910703] [11451.718393178344, 11536.500657821618, 11465.264330575268, 11461.740431829317]
prior:  [0.9760961975210904, 0.0, 0.9760961975210904, 0.9760961975210904] [0.9759983386585752, 0.0, 0.9759983386585752, 0.9759983386585752] [11453.908451115103, nan, 11453.908457756834, 11453.908447441894]
-----------------------------------------------------------------------------------------
This iteration is 9
True Objective function: Loss = -11797.278072778518
Iteration 0: Loss = -21809.29814353723
Iteration 10: Loss = -11786.100126843283
Iteration 20: Loss = -11786.10006974292
Iteration 30: Loss = -11786.10006974293
1
Iteration 40: Loss = -11786.10006974293
2
Iteration 50: Loss = -11786.10006974293
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7329, 0.2671],
        [0.2382, 0.7618]], dtype=torch.float64)
alpha: tensor([0.4798, 0.5202])
beta: tensor([[[0.2076, 0.1102],
         [0.2545, 0.3938]],

        [[0.9711, 0.0885],
         [0.6287, 0.6486]],

        [[0.7804, 0.0984],
         [0.3038, 0.0679]],

        [[0.3485, 0.1007],
         [0.4751, 0.3629]],

        [[0.1070, 0.1098],
         [0.0161, 0.3839]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961908856516
Average Adjusted Rand Index: 0.9759992163675584
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21734.165522406613
Iteration 100: Loss = -12184.035185878023
Iteration 200: Loss = -11830.355126539605
Iteration 300: Loss = -11820.46132374961
Iteration 400: Loss = -11820.059976319004
Iteration 500: Loss = -11819.861434258097
Iteration 600: Loss = -11819.744364765449
Iteration 700: Loss = -11819.668569916654
Iteration 800: Loss = -11819.61043169281
Iteration 900: Loss = -11815.976203546537
Iteration 1000: Loss = -11803.820760407278
Iteration 1100: Loss = -11803.797917108395
Iteration 1200: Loss = -11803.780080125554
Iteration 1300: Loss = -11803.765728848579
Iteration 1400: Loss = -11803.753843585326
Iteration 1500: Loss = -11803.744033310979
Iteration 1600: Loss = -11803.735892466708
Iteration 1700: Loss = -11803.729012685417
Iteration 1800: Loss = -11803.723158206029
Iteration 1900: Loss = -11803.718046230853
Iteration 2000: Loss = -11803.713622896654
Iteration 2100: Loss = -11803.709672655501
Iteration 2200: Loss = -11803.706126318415
Iteration 2300: Loss = -11803.702820496042
Iteration 2400: Loss = -11803.699044154393
Iteration 2500: Loss = -11803.68565439377
Iteration 2600: Loss = -11803.674314240821
Iteration 2700: Loss = -11803.672444784272
Iteration 2800: Loss = -11803.670705943216
Iteration 2900: Loss = -11803.669091534768
Iteration 3000: Loss = -11803.667564676245
Iteration 3100: Loss = -11803.666223254902
Iteration 3200: Loss = -11803.664997280488
Iteration 3300: Loss = -11803.663968876574
Iteration 3400: Loss = -11803.662985283883
Iteration 3500: Loss = -11803.662399461105
Iteration 3600: Loss = -11803.661258458873
Iteration 3700: Loss = -11803.660754892842
Iteration 3800: Loss = -11803.659814052833
Iteration 3900: Loss = -11803.659180551584
Iteration 4000: Loss = -11803.65855726784
Iteration 4100: Loss = -11803.658108453674
Iteration 4200: Loss = -11803.657457489637
Iteration 4300: Loss = -11803.703012970798
1
Iteration 4400: Loss = -11803.656509199727
Iteration 4500: Loss = -11803.656093293042
Iteration 4600: Loss = -11803.663354043696
1
Iteration 4700: Loss = -11803.655358599151
Iteration 4800: Loss = -11803.655025970107
Iteration 4900: Loss = -11803.795878840225
1
Iteration 5000: Loss = -11803.654436776205
Iteration 5100: Loss = -11803.654139037937
Iteration 5200: Loss = -11803.653885280708
Iteration 5300: Loss = -11803.657483840088
1
Iteration 5400: Loss = -11803.653382810542
Iteration 5500: Loss = -11803.653192479476
Iteration 5600: Loss = -11803.654649609774
1
Iteration 5700: Loss = -11803.652761158479
Iteration 5800: Loss = -11803.652587567834
Iteration 5900: Loss = -11803.652439227784
Iteration 6000: Loss = -11803.652758875965
1
Iteration 6100: Loss = -11803.652130462904
Iteration 6200: Loss = -11803.652017937993
Iteration 6300: Loss = -11803.651860755897
Iteration 6400: Loss = -11803.678846573723
1
Iteration 6500: Loss = -11803.651618413835
Iteration 6600: Loss = -11803.651517549548
Iteration 6700: Loss = -11803.651405315375
Iteration 6800: Loss = -11803.687906588313
1
Iteration 6900: Loss = -11803.651198300702
Iteration 7000: Loss = -11803.651170550222
Iteration 7100: Loss = -11803.722224776699
1
Iteration 7200: Loss = -11803.651006414892
Iteration 7300: Loss = -11803.650930225578
Iteration 7400: Loss = -11803.659843099844
1
Iteration 7500: Loss = -11803.650783077075
Iteration 7600: Loss = -11803.65079057367
1
Iteration 7700: Loss = -11803.653515440063
2
Iteration 7800: Loss = -11803.650598504079
Iteration 7900: Loss = -11803.650704062577
1
Iteration 8000: Loss = -11803.651548225376
2
Iteration 8100: Loss = -11803.650485956772
Iteration 8200: Loss = -11803.650506957847
1
Iteration 8300: Loss = -11803.650569484711
2
Iteration 8400: Loss = -11803.650595853127
3
Iteration 8500: Loss = -11803.650569231235
4
Iteration 8600: Loss = -11803.658901128796
5
Iteration 8700: Loss = -11803.65030036293
Iteration 8800: Loss = -11803.687360148546
1
Iteration 8900: Loss = -11803.650240608904
Iteration 9000: Loss = -11803.650208897592
Iteration 9100: Loss = -11803.65821129868
1
Iteration 9200: Loss = -11803.650086501122
Iteration 9300: Loss = -11803.650096304678
1
Iteration 9400: Loss = -11803.650341689881
2
Iteration 9500: Loss = -11803.650635530128
3
Iteration 9600: Loss = -11803.649988059044
Iteration 9700: Loss = -11803.665195527108
1
Iteration 9800: Loss = -11803.649998140176
2
Iteration 9900: Loss = -11803.666433794864
3
Iteration 10000: Loss = -11803.650355144115
4
Iteration 10100: Loss = -11803.651682755055
5
Iteration 10200: Loss = -11803.65487144563
6
Iteration 10300: Loss = -11803.650035177589
7
Iteration 10400: Loss = -11803.66091265543
8
Iteration 10500: Loss = -11803.655266935217
9
Iteration 10600: Loss = -11803.709999977616
10
Stopping early at iteration 10600 due to no improvement.
tensor([[  5.9872, -10.6024],
        [  3.5047,  -8.1199],
        [  3.5234,  -8.1386],
        [ -9.7605,   5.1453],
        [ -9.7631,   5.1478],
        [  3.8059,  -8.4211],
        [  5.4013, -10.0166],
        [  3.1511,  -7.7664],
        [  3.7246,  -8.3398],
        [-10.0256,   5.4104],
        [  7.3486, -11.9639],
        [ -9.9987,   5.3835],
        [ -9.9265,   5.3112],
        [ -9.6866,   5.0714],
        [ -6.4649,   1.8497],
        [  5.4566, -10.0719],
        [ -6.3629,   1.7477],
        [-10.1201,   5.5048],
        [ -6.1695,   1.5543],
        [  6.0661, -10.6813],
        [  5.6114, -10.2267],
        [  4.8787,  -9.4939],
        [ -9.5792,   4.9640],
        [  5.5729, -10.1881],
        [  4.3708,  -8.9861],
        [ -9.5250,   4.9098],
        [  4.5468,  -9.1620],
        [ -5.8474,   1.2322],
        [  6.2697, -10.8850],
        [ -9.0234,   4.4082],
        [  4.5825,  -9.1977],
        [ -6.2977,   1.6825],
        [ -5.8556,   1.2403],
        [  6.2799, -10.8951],
        [ -9.8946,   5.2794],
        [ -1.9538,  -2.6614],
        [ -8.9974,   4.3822],
        [-10.0522,   5.4370],
        [  6.5276, -11.1428],
        [  6.3111, -10.9263],
        [ -8.7154,   4.1001],
        [  4.8671,  -9.4823],
        [ -9.7275,   5.1123],
        [  5.1425,  -9.7577],
        [ -9.6303,   5.0151],
        [ -0.7294,  -3.8858],
        [ -7.9666,   3.3514],
        [ -9.0105,   4.3953],
        [  4.9474,  -9.5626],
        [  3.3953,  -8.0105],
        [ -7.9533,   3.3381],
        [  5.4905, -10.1058],
        [ -0.7806,  -3.8346],
        [  6.3100, -10.9252],
        [  5.7511, -10.3663],
        [  6.7242, -11.3394],
        [ -8.9784,   4.3632],
        [ -9.7144,   5.0992],
        [  5.7670, -10.3823],
        [ -8.7994,   4.1841],
        [  6.0178, -10.6331],
        [ -9.5588,   4.9436],
        [ -6.4281,   1.8129],
        [ -6.6320,   2.0168],
        [  3.8548,  -8.4700],
        [  4.6090,  -9.2243],
        [ -9.6543,   5.0391],
        [ -9.1482,   4.5330],
        [ -9.3544,   4.7392],
        [ -7.6319,   3.0167],
        [  5.2141,  -9.8293],
        [ -7.1422,   2.5270],
        [  3.0147,  -7.6300],
        [  5.9893, -10.6045],
        [ -9.9786,   5.3634],
        [  6.0110, -10.6262],
        [  1.7422,  -6.3574],
        [  4.9788,  -9.5941],
        [ -7.1319,   2.5167],
        [ -4.2989,  -0.3164],
        [-10.2844,   5.6692],
        [  5.5767, -10.1919],
        [-10.1115,   5.4962],
        [  6.1116, -10.7268],
        [  5.6101, -10.2253],
        [  7.3358, -11.9510],
        [ -7.0868,   2.4715],
        [  4.7465,  -9.3617],
        [ -9.8666,   5.2513],
        [  4.8067,  -9.4219],
        [ -9.1522,   4.5370],
        [ -7.9699,   3.3547],
        [  4.2343,  -8.8496],
        [ -7.5933,   2.9780],
        [  5.5288, -10.1440],
        [  5.9332, -10.5484],
        [  4.5805,  -9.1957],
        [  5.5194, -10.1346],
        [ -8.0209,   3.4057],
        [  1.2894,  -5.9046]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7776, 0.2224],
        [0.2578, 0.7422]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5285, 0.4715], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3951, 0.1099],
         [0.2545, 0.2139]],

        [[0.9711, 0.0883],
         [0.6287, 0.6486]],

        [[0.7804, 0.1014],
         [0.3038, 0.0679]],

        [[0.3485, 0.1005],
         [0.4751, 0.3629]],

        [[0.1070, 0.1092],
         [0.0161, 0.3839]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9603206686557758
Average Adjusted Rand Index: 0.9604840648524068
Iteration 0: Loss = -25357.585763031788
Iteration 10: Loss = -11786.10009482729
Iteration 20: Loss = -11786.100069742954
Iteration 30: Loss = -11786.10006974293
Iteration 40: Loss = -11786.10006974293
1
Iteration 50: Loss = -11786.10006974293
2
Iteration 60: Loss = -11786.10006974293
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7329, 0.2671],
        [0.2382, 0.7618]], dtype=torch.float64)
alpha: tensor([0.4798, 0.5202])
beta: tensor([[[0.2076, 0.1102],
         [0.6628, 0.3938]],

        [[0.7952, 0.0885],
         [0.9372, 0.8629]],

        [[0.7063, 0.0984],
         [0.7144, 0.7848]],

        [[0.3849, 0.1007],
         [0.8960, 0.9499]],

        [[0.5797, 0.1098],
         [0.6925, 0.1917]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961908856516
Average Adjusted Rand Index: 0.9759992163675584
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25357.437536165733
Iteration 100: Loss = -12631.860507432402
Iteration 200: Loss = -12511.500514462996
Iteration 300: Loss = -12417.196785155666
Iteration 400: Loss = -12257.664184386587
Iteration 500: Loss = -12190.08004063205
Iteration 600: Loss = -12147.790628837707
Iteration 700: Loss = -12127.816057228702
Iteration 800: Loss = -12114.740741673873
Iteration 900: Loss = -12104.174769696909
Iteration 1000: Loss = -12080.951690122822
Iteration 1100: Loss = -12079.583826252516
Iteration 1200: Loss = -12057.743561280511
Iteration 1300: Loss = -12044.92516287631
Iteration 1400: Loss = -12042.277175422683
Iteration 1500: Loss = -12042.018217329894
Iteration 1600: Loss = -12040.164905553082
Iteration 1700: Loss = -12038.281039503669
Iteration 1800: Loss = -12032.890401705974
Iteration 1900: Loss = -12030.412577633257
Iteration 2000: Loss = -12009.630700765352
Iteration 2100: Loss = -12002.37031940532
Iteration 2200: Loss = -11995.51071159515
Iteration 2300: Loss = -11984.470409452742
Iteration 2400: Loss = -11984.425888529227
Iteration 2500: Loss = -11984.24049638827
Iteration 2600: Loss = -11983.10082332666
Iteration 2700: Loss = -11968.864838918904
Iteration 2800: Loss = -11944.978007173742
Iteration 2900: Loss = -11933.0317281121
Iteration 3000: Loss = -11920.611428363263
Iteration 3100: Loss = -11920.326722413329
Iteration 3200: Loss = -11898.029247373795
Iteration 3300: Loss = -11887.566842383689
Iteration 3400: Loss = -11887.540899796768
Iteration 3500: Loss = -11876.833722144094
Iteration 3600: Loss = -11876.808026368422
Iteration 3700: Loss = -11876.706622775637
Iteration 3800: Loss = -11872.232967599357
Iteration 3900: Loss = -11858.567873247614
Iteration 4000: Loss = -11818.626593000712
Iteration 4100: Loss = -11818.5756761824
Iteration 4200: Loss = -11818.564510721162
Iteration 4300: Loss = -11818.557104656536
Iteration 4400: Loss = -11818.5509857506
Iteration 4500: Loss = -11818.543957181098
Iteration 4600: Loss = -11800.376488391601
Iteration 4700: Loss = -11800.172494813063
Iteration 4800: Loss = -11800.168948845432
Iteration 4900: Loss = -11800.166098695943
Iteration 5000: Loss = -11800.163506367104
Iteration 5100: Loss = -11800.161275568915
Iteration 5200: Loss = -11800.159225955782
Iteration 5300: Loss = -11800.158723130293
Iteration 5400: Loss = -11800.15543040022
Iteration 5500: Loss = -11800.153561442092
Iteration 5600: Loss = -11800.15158604133
Iteration 5700: Loss = -11800.147787252805
Iteration 5800: Loss = -11784.589015320926
Iteration 5900: Loss = -11784.584139534987
Iteration 6000: Loss = -11784.64160778153
1
Iteration 6100: Loss = -11784.58137700816
Iteration 6200: Loss = -11784.580371358918
Iteration 6300: Loss = -11784.584280011983
1
Iteration 6400: Loss = -11784.583072397798
2
Iteration 6500: Loss = -11784.577911975703
Iteration 6600: Loss = -11784.577274292844
Iteration 6700: Loss = -11784.585944781187
1
Iteration 6800: Loss = -11784.576933405826
Iteration 6900: Loss = -11784.5778784828
1
Iteration 7000: Loss = -11784.577316177152
2
Iteration 7100: Loss = -11784.575674321524
Iteration 7200: Loss = -11784.574351712256
Iteration 7300: Loss = -11784.574310539185
Iteration 7400: Loss = -11784.577840259324
1
Iteration 7500: Loss = -11784.573202374191
Iteration 7600: Loss = -11784.573930703458
1
Iteration 7700: Loss = -11784.572695995837
Iteration 7800: Loss = -11784.572327914988
Iteration 7900: Loss = -11784.656242465684
1
Iteration 8000: Loss = -11784.573559534718
2
Iteration 8100: Loss = -11784.571452147378
Iteration 8200: Loss = -11784.571756462217
1
Iteration 8300: Loss = -11784.638293726297
2
Iteration 8400: Loss = -11784.597802561238
3
Iteration 8500: Loss = -11784.5716143394
4
Iteration 8600: Loss = -11784.57170095398
5
Iteration 8700: Loss = -11784.564844012182
Iteration 8800: Loss = -11784.564101162498
Iteration 8900: Loss = -11784.568319739361
1
Iteration 9000: Loss = -11784.696931930637
2
Iteration 9100: Loss = -11784.564480433473
3
Iteration 9200: Loss = -11784.570966570758
4
Iteration 9300: Loss = -11784.65486048755
5
Iteration 9400: Loss = -11784.56257783398
Iteration 9500: Loss = -11784.563340059187
1
Iteration 9600: Loss = -11784.562827868422
2
Iteration 9700: Loss = -11784.580474083461
3
Iteration 9800: Loss = -11784.581344558113
4
Iteration 9900: Loss = -11784.567633453029
5
Iteration 10000: Loss = -11784.567088530259
6
Iteration 10100: Loss = -11784.570324384986
7
Iteration 10200: Loss = -11784.578067063152
8
Iteration 10300: Loss = -11784.566372259711
9
Iteration 10400: Loss = -11784.562472202659
Iteration 10500: Loss = -11784.562591737968
1
Iteration 10600: Loss = -11784.646141290645
2
Iteration 10700: Loss = -11784.57769797985
3
Iteration 10800: Loss = -11784.564788710537
4
Iteration 10900: Loss = -11784.584262886117
5
Iteration 11000: Loss = -11784.561475942308
Iteration 11100: Loss = -11784.561419203466
Iteration 11200: Loss = -11784.579236588665
1
Iteration 11300: Loss = -11784.562532611908
2
Iteration 11400: Loss = -11784.562219042085
3
Iteration 11500: Loss = -11784.57112360337
4
Iteration 11600: Loss = -11784.562195891893
5
Iteration 11700: Loss = -11784.563753748764
6
Iteration 11800: Loss = -11784.607950336222
7
Iteration 11900: Loss = -11784.563518251554
8
Iteration 12000: Loss = -11784.571732973216
9
Iteration 12100: Loss = -11784.565897655086
10
Stopping early at iteration 12100 due to no improvement.
tensor([[ -8.8754,   7.4607],
        [ -9.4960,   7.0279],
        [ -6.6921,   5.0530],
        [  5.0731,  -9.0137],
        [  6.0860,  -7.6372],
        [-10.2679,   8.2637],
        [-10.7334,   7.1783],
        [ -6.3113,   4.5880],
        [ -8.5477,   3.9589],
        [  6.1215,  -7.5544],
        [ -9.9220,   8.4065],
        [  6.6339,  -8.0283],
        [  5.7797,  -8.0197],
        [  6.3173,  -8.2092],
        [  3.3108,  -5.0713],
        [ -8.7268,   7.3295],
        [  6.8606,  -8.3502],
        [  6.8677,  -9.9104],
        [  3.2060,  -4.6304],
        [ -9.7707,   6.8908],
        [ -9.3217,   7.2036],
        [ -8.8652,   7.4625],
        [  7.6454,  -9.1961],
        [ -8.3449,   6.9189],
        [ -7.1591,   5.4902],
        [  5.8955,  -8.9891],
        [ -9.6551,   7.6282],
        [  2.7609,  -4.4503],
        [ -7.2757,   5.8691],
        [  7.7467,  -9.1536],
        [ -8.3354,   4.8635],
        [  2.8712,  -5.2767],
        [  2.9182,  -4.3202],
        [ -7.7988,   6.2806],
        [  6.1912,  -7.9432],
        [ -1.3032,  -0.6544],
        [  7.4212, -11.5192],
        [  6.3560,  -7.7720],
        [ -9.4585,   8.0655],
        [-10.2990,   8.5994],
        [  5.6543,  -7.2594],
        [-10.2361,   6.1229],
        [  6.7318,  -8.6810],
        [ -8.9491,   6.4737],
        [  6.8049,  -8.2185],
        [ -3.7385,  -0.7028],
        [  4.6041,  -6.8468],
        [  5.6481,  -7.3307],
        [ -8.2634,   6.7481],
        [ -9.9232,   7.7597],
        [  7.5537,  -9.0062],
        [ -9.3086,   5.0901],
        [ -2.2900,   0.5653],
        [ -9.2509,   7.5780],
        [ -9.1473,   7.7584],
        [ -9.2810,   7.7921],
        [  7.1285,  -9.0277],
        [  7.4177,  -9.5481],
        [ -9.6796,   7.8934],
        [  5.4983,  -7.3909],
        [ -9.3884,   7.6589],
        [  6.0451,  -8.3940],
        [  7.4005,  -9.4933],
        [  3.7008,  -5.0873],
        [ -8.2597,   5.7882],
        [ -7.6656,   6.2131],
        [  7.2628,  -9.7977],
        [  5.8712,  -7.4177],
        [  6.5455,  -8.0102],
        [  3.9080,  -6.9777],
        [-10.1239,   7.8592],
        [  3.8632,  -6.0391],
        [ -8.5440,   7.1577],
        [ -9.6354,   7.0816],
        [  6.8774,  -8.8533],
        [ -9.8092,   7.5158],
        [ -8.3227,   6.9094],
        [ -8.6934,   6.5994],
        [  3.9725,  -5.8673],
        [  1.3364,  -2.7964],
        [  6.8092,  -9.2546],
        [ -9.7124,   8.1069],
        [  6.8108,  -8.2807],
        [ -9.3526,   7.9584],
        [ -9.1833,   7.7924],
        [ -9.7352,   7.6766],
        [  3.9301,  -5.8858],
        [ -7.7279,   6.3033],
        [  6.0954,  -7.6059],
        [ -8.2106,   6.7667],
        [  5.7760,  -7.1763],
        [  7.7041,  -9.1157],
        [ -8.4343,   6.6170],
        [  7.6859,  -9.0753],
        [ -8.6115,   7.0644],
        [ -9.8337,   7.8855],
        [ -8.9445,   7.5527],
        [-10.2047,   8.0365],
        [  4.1319,  -7.2618],
        [ -8.2775,   6.8592]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7341, 0.2659],
        [0.2355, 0.7645]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4737, 0.5263], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2122, 0.1103],
         [0.6628, 0.4010]],

        [[0.7952, 0.0883],
         [0.9372, 0.8629]],

        [[0.7063, 0.0983],
         [0.7144, 0.7848]],

        [[0.3849, 0.1006],
         [0.8960, 0.9499]],

        [[0.5797, 0.1101],
         [0.6925, 0.1917]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961908856516
Average Adjusted Rand Index: 0.9759992163675584
Iteration 0: Loss = -40124.16420223035
Iteration 10: Loss = -11786.100060098534
Iteration 20: Loss = -11786.100069742957
1
Iteration 30: Loss = -11786.10006974293
2
Iteration 40: Loss = -11786.10006974293
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7329, 0.2671],
        [0.2382, 0.7618]], dtype=torch.float64)
alpha: tensor([0.4798, 0.5202])
beta: tensor([[[0.2076, 0.1102],
         [0.5920, 0.3938]],

        [[0.6035, 0.0885],
         [0.4210, 0.2332]],

        [[0.1894, 0.0984],
         [0.8475, 0.3674]],

        [[0.5690, 0.1007],
         [0.5964, 0.7719]],

        [[0.9343, 0.1098],
         [0.2045, 0.3224]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961908856516
Average Adjusted Rand Index: 0.9759992163675584
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40123.86421324773
Iteration 100: Loss = -12500.197164056917
Iteration 200: Loss = -12283.898437364727
Iteration 300: Loss = -12274.42976635187
Iteration 400: Loss = -12272.798390515632
Iteration 500: Loss = -12271.991810199655
Iteration 600: Loss = -12271.552683751837
Iteration 700: Loss = -12271.27141417998
Iteration 800: Loss = -12271.073088066196
Iteration 900: Loss = -12270.922668212934
Iteration 1000: Loss = -12270.734007884936
Iteration 1100: Loss = -12270.455813890034
Iteration 1200: Loss = -12269.070235078781
Iteration 1300: Loss = -12268.841016457513
Iteration 1400: Loss = -12264.251595360734
Iteration 1500: Loss = -12264.103064238536
Iteration 1600: Loss = -12264.056570392113
Iteration 1700: Loss = -12264.009868536428
Iteration 1800: Loss = -12263.948475590918
Iteration 1900: Loss = -12263.92256699121
Iteration 2000: Loss = -12263.890473066302
Iteration 2100: Loss = -12263.874623919755
Iteration 2200: Loss = -12263.861292215806
Iteration 2300: Loss = -12263.849573674162
Iteration 2400: Loss = -12263.839271423063
Iteration 2500: Loss = -12263.830036280593
Iteration 2600: Loss = -12263.821797479637
Iteration 2700: Loss = -12263.814412136759
Iteration 2800: Loss = -12263.807690065309
Iteration 2900: Loss = -12263.801662565222
Iteration 3000: Loss = -12263.796144981225
Iteration 3100: Loss = -12263.791148300837
Iteration 3200: Loss = -12263.786597442164
Iteration 3300: Loss = -12263.782388223593
Iteration 3400: Loss = -12263.77859008027
Iteration 3500: Loss = -12263.775064976096
Iteration 3600: Loss = -12263.77184744658
Iteration 3700: Loss = -12263.7688353792
Iteration 3800: Loss = -12263.766038046268
Iteration 3900: Loss = -12263.763504396253
Iteration 4000: Loss = -12263.76116638327
Iteration 4100: Loss = -12263.758961490526
Iteration 4200: Loss = -12263.75693835633
Iteration 4300: Loss = -12263.755039511627
Iteration 4400: Loss = -12263.753275995768
Iteration 4500: Loss = -12263.751630608569
Iteration 4600: Loss = -12263.750056915705
Iteration 4700: Loss = -12263.748606496543
Iteration 4800: Loss = -12263.74728129922
Iteration 4900: Loss = -12263.745987159751
Iteration 5000: Loss = -12263.744675902919
Iteration 5100: Loss = -12263.743600256914
Iteration 5200: Loss = -12263.74254975294
Iteration 5300: Loss = -12263.741896428839
Iteration 5400: Loss = -12263.740685794488
Iteration 5500: Loss = -12263.739820819743
Iteration 5600: Loss = -12263.743384130465
1
Iteration 5700: Loss = -12263.739051559256
Iteration 5800: Loss = -12263.737440129924
Iteration 5900: Loss = -12263.73681990801
Iteration 6000: Loss = -12263.736108936897
Iteration 6100: Loss = -12263.742152166518
1
Iteration 6200: Loss = -12263.73480327879
Iteration 6300: Loss = -12263.737089189894
1
Iteration 6400: Loss = -12263.736285626585
2
Iteration 6500: Loss = -12263.732842542662
Iteration 6600: Loss = -12263.732193371216
Iteration 6700: Loss = -12263.730652384706
Iteration 6800: Loss = -12263.736406683123
1
Iteration 6900: Loss = -12257.598258217544
Iteration 7000: Loss = -12257.595931699308
Iteration 7100: Loss = -12256.335171606748
Iteration 7200: Loss = -12255.059290913026
Iteration 7300: Loss = -12254.787783956624
Iteration 7400: Loss = -12253.795294290856
Iteration 7500: Loss = -12253.789465747857
Iteration 7600: Loss = -12253.785373066905
Iteration 7700: Loss = -12253.048649951228
Iteration 7800: Loss = -12252.079755323404
Iteration 7900: Loss = -12251.489853161976
Iteration 8000: Loss = -12250.60405166269
Iteration 8100: Loss = -12250.14655110152
Iteration 8200: Loss = -12249.14650238949
Iteration 8300: Loss = -12248.526420114684
Iteration 8400: Loss = -12247.907300011422
Iteration 8500: Loss = -12247.262018012052
Iteration 8600: Loss = -12246.889423290895
Iteration 8700: Loss = -12243.880480943437
Iteration 8800: Loss = -12237.714442856548
Iteration 8900: Loss = -12215.425122824983
Iteration 9000: Loss = -12198.10699905516
Iteration 9100: Loss = -12187.081813214278
Iteration 9200: Loss = -12157.777052862662
Iteration 9300: Loss = -12147.10302403412
Iteration 9400: Loss = -12118.850317451952
Iteration 9500: Loss = -12100.475659612093
Iteration 9600: Loss = -12086.094046445633
Iteration 9700: Loss = -12060.912745058482
Iteration 9800: Loss = -12060.887068490474
Iteration 9900: Loss = -12060.8803605194
Iteration 10000: Loss = -12060.825716360123
Iteration 10100: Loss = -12060.794701232078
Iteration 10200: Loss = -12060.767216776534
Iteration 10300: Loss = -12060.769495490182
1
Iteration 10400: Loss = -12060.767257363019
2
Iteration 10500: Loss = -12060.81301209044
3
Iteration 10600: Loss = -12060.766610765273
Iteration 10700: Loss = -12060.762982651782
Iteration 10800: Loss = -12060.908028167143
1
Iteration 10900: Loss = -12060.765067665458
2
Iteration 11000: Loss = -12043.99314283186
Iteration 11100: Loss = -12043.989086501957
Iteration 11200: Loss = -12043.987926655811
Iteration 11300: Loss = -12043.988231517285
1
Iteration 11400: Loss = -12044.040941045076
2
Iteration 11500: Loss = -12043.987681301009
Iteration 11600: Loss = -12043.992003680205
1
Iteration 11700: Loss = -12043.994372982514
2
Iteration 11800: Loss = -12043.994662636864
3
Iteration 11900: Loss = -12043.987271001186
Iteration 12000: Loss = -12043.989583782186
1
Iteration 12100: Loss = -12043.987214978148
Iteration 12200: Loss = -12043.99880095837
1
Iteration 12300: Loss = -12043.996616411512
2
Iteration 12400: Loss = -12043.989590531784
3
Iteration 12500: Loss = -12043.990293346145
4
Iteration 12600: Loss = -12044.005987075465
5
Iteration 12700: Loss = -12044.00698651655
6
Iteration 12800: Loss = -12043.98906803243
7
Iteration 12900: Loss = -12043.98723680781
8
Iteration 13000: Loss = -12043.987620562857
9
Iteration 13100: Loss = -12043.988275328446
10
Stopping early at iteration 13100 due to no improvement.
tensor([[ -8.8592,   7.1721],
        [ -9.4291,   7.2280],
        [ -6.9830,   5.5851],
        [  0.6583,  -2.4043],
        [  3.5548,  -4.9429],
        [ -8.6015,   5.4846],
        [ -8.6459,   6.0374],
        [ -7.9426,   6.3451],
        [ -8.2243,   6.7955],
        [  1.5573,  -2.9575],
        [ -8.5304,   6.9141],
        [  5.1657,  -9.7809],
        [  4.4021,  -6.6243],
        [  3.9243,  -5.4568],
        [ -4.4597,   2.2913],
        [ -9.1308,   6.8734],
        [ -2.1553,   0.3342],
        [  5.6105,  -7.0616],
        [ -2.2599,   0.7366],
        [ -8.5863,   7.1984],
        [ -8.7445,   6.9393],
        [ -9.2811,   7.4915],
        [  3.4528,  -5.0284],
        [ -8.9119,   7.5199],
        [ -9.0998,   7.6378],
        [  1.8480,  -4.0219],
        [ -8.3871,   6.9371],
        [ -0.5847,  -1.6077],
        [ -8.2837,   6.5455],
        [ -3.0270,   1.3562],
        [ -8.5735,   6.1132],
        [ -2.2670,   0.3853],
        [  0.9264,  -3.3630],
        [ -6.2732,   4.8695],
        [  5.1916,  -7.9224],
        [ -4.7282,   3.2601],
        [  1.7222,  -3.1343],
        [  1.4073,  -2.7943],
        [ -8.6034,   7.2135],
        [-10.2572,   7.4061],
        [ -5.2903,   3.8872],
        [ -7.5725,   4.8840],
        [ -4.6344,   2.9731],
        [ -8.6315,   6.0414],
        [  2.0346,  -3.9238],
        [ -5.4878,   4.0988],
        [ -0.5376,  -0.8495],
        [  3.1697,  -4.9438],
        [ -8.5554,   6.6620],
        [ -8.4787,   7.0906],
        [  1.7178,  -4.6907],
        [ -9.2525,   7.2470],
        [ -4.6630,   3.2420],
        [ -9.6986,   7.9818],
        [ -9.4964,   4.8812],
        [ -8.5869,   7.1884],
        [ -1.5214,  -0.0818],
        [  2.4283,  -4.5130],
        [ -8.8063,   7.4149],
        [ -0.2359,  -1.9339],
        [ -9.2795,   7.8663],
        [  2.5503,  -4.2922],
        [ -5.0412,   3.6549],
        [ -4.1757,  -0.4395],
        [ -9.3111,   7.8547],
        [ -8.9132,   7.4796],
        [  5.2403,  -6.6753],
        [  0.7564,  -2.8922],
        [  1.3997,  -2.9268],
        [  1.2254,  -3.0401],
        [ -8.8808,   7.4493],
        [ -3.6814,   2.0590],
        [ -7.4787,   6.0292],
        [ -9.3871,   7.4785],
        [  3.7133,  -5.8009],
        [ -8.6950,   7.1696],
        [ -7.3274,   4.5867],
        [ -9.1801,   7.5991],
        [ -7.1978,   5.3597],
        [ -3.4167,   1.7963],
        [  3.5658,  -5.1073],
        [ -8.2162,   6.8287],
        [  1.2738,  -2.7216],
        [ -8.7628,   7.2412],
        [ -8.5097,   6.9938],
        [ -8.7699,   7.3827],
        [  0.1216,  -2.3425],
        [ -8.6433,   7.1042],
        [  3.0560,  -4.5729],
        [ -8.3234,   6.8983],
        [ -3.1001,   1.2868],
        [  2.8484,  -4.3907],
        [ -9.2289,   7.1162],
        [  0.4603,  -2.9574],
        [ -8.6213,   5.7527],
        [-10.2407,   7.9987],
        [ -9.1809,   7.4238],
        [ -8.9459,   7.3865],
        [ -2.1326,   0.6662],
        [ -6.2651,   4.6866]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4404, 0.5596],
        [0.5300, 0.4700]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3137, 0.6863], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3759, 0.1070],
         [0.5920, 0.2544]],

        [[0.6035, 0.0885],
         [0.4210, 0.2332]],

        [[0.1894, 0.1034],
         [0.8475, 0.3674]],

        [[0.5690, 0.1006],
         [0.5964, 0.7719]],

        [[0.9343, 0.1090],
         [0.2045, 0.3224]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 84
Adjusted Rand Index: 0.4575257538278486
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 80
Adjusted Rand Index: 0.35353535353535354
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.11383204578898504
Average Adjusted Rand Index: 0.7542118766766358
Iteration 0: Loss = -31132.246158833364
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.2065,    nan]],

        [[0.8464,    nan],
         [0.7189, 0.9012]],

        [[0.7315,    nan],
         [0.1489, 0.0118]],

        [[0.2521,    nan],
         [0.2111, 0.7388]],

        [[0.2747,    nan],
         [0.2445, 0.5394]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31131.05070270681
Iteration 100: Loss = -12649.33264513416
Iteration 200: Loss = -12637.907744854041
Iteration 300: Loss = -12614.215600636058
Iteration 400: Loss = -12597.136491607527
Iteration 500: Loss = -12537.550224380015
Iteration 600: Loss = -12392.307283600583
Iteration 700: Loss = -12229.618717941144
Iteration 800: Loss = -12173.774485562606
Iteration 900: Loss = -12144.801632081222
Iteration 1000: Loss = -12137.14437546426
Iteration 1100: Loss = -12129.437738555995
Iteration 1200: Loss = -12123.56041267473
Iteration 1300: Loss = -12118.948072373496
Iteration 1400: Loss = -12100.32306742507
Iteration 1500: Loss = -12092.378130896523
Iteration 1600: Loss = -12088.513012390193
Iteration 1700: Loss = -12069.135117645725
Iteration 1800: Loss = -12068.975545913774
Iteration 1900: Loss = -12062.041290683588
Iteration 2000: Loss = -12061.67601305873
Iteration 2100: Loss = -12061.212448312417
Iteration 2200: Loss = -12055.887789106018
Iteration 2300: Loss = -12055.807238838604
Iteration 2400: Loss = -12055.744403839537
Iteration 2500: Loss = -12041.22932383025
Iteration 2600: Loss = -12031.328809676785
Iteration 2700: Loss = -12031.285006828743
Iteration 2800: Loss = -12031.253190428724
Iteration 2900: Loss = -12031.22659002927
Iteration 3000: Loss = -12031.202653132308
Iteration 3100: Loss = -12031.177846606868
Iteration 3200: Loss = -12031.11420942103
Iteration 3300: Loss = -12025.317158584998
Iteration 3400: Loss = -12025.31467857755
Iteration 3500: Loss = -12025.286853020854
Iteration 3600: Loss = -12025.274557157298
Iteration 3700: Loss = -12025.262932087931
Iteration 3800: Loss = -12025.250330548652
Iteration 3900: Loss = -12025.222367850563
Iteration 4000: Loss = -12018.64446584382
Iteration 4100: Loss = -12018.632259937756
Iteration 4200: Loss = -12018.624420400303
Iteration 4300: Loss = -12018.617341797759
Iteration 4400: Loss = -12018.610666912089
Iteration 4500: Loss = -12018.603365825134
Iteration 4600: Loss = -12018.591466845015
Iteration 4700: Loss = -12011.335781418136
Iteration 4800: Loss = -12011.327356325695
Iteration 4900: Loss = -12011.321571453464
Iteration 5000: Loss = -12011.317430453
Iteration 5100: Loss = -12011.316017794132
Iteration 5200: Loss = -12011.310219604202
Iteration 5300: Loss = -12011.306863987633
Iteration 5400: Loss = -12011.303527385491
Iteration 5500: Loss = -12011.299980432032
Iteration 5600: Loss = -12011.293636926624
Iteration 5700: Loss = -11999.68155915258
Iteration 5800: Loss = -11986.499707481687
Iteration 5900: Loss = -11986.489246490266
Iteration 6000: Loss = -11986.492674084391
1
Iteration 6100: Loss = -11986.49479361364
2
Iteration 6200: Loss = -11986.476635162662
Iteration 6300: Loss = -11986.48438481235
1
Iteration 6400: Loss = -11986.475542034534
Iteration 6500: Loss = -11986.471912228248
Iteration 6600: Loss = -11986.477275064544
1
Iteration 6700: Loss = -11986.46878894178
Iteration 6800: Loss = -11986.480600496394
1
Iteration 6900: Loss = -11986.467077651097
Iteration 7000: Loss = -11986.472991420022
1
Iteration 7100: Loss = -11979.589333299315
Iteration 7200: Loss = -11979.558252367879
Iteration 7300: Loss = -11976.694306917278
Iteration 7400: Loss = -11976.700289669094
1
Iteration 7500: Loss = -11976.69767033295
2
Iteration 7600: Loss = -11951.159384906148
Iteration 7700: Loss = -11942.97888413653
Iteration 7800: Loss = -11942.972119925278
Iteration 7900: Loss = -11942.974765738329
1
Iteration 8000: Loss = -11942.968827947934
Iteration 8100: Loss = -11942.96868002909
Iteration 8200: Loss = -11942.966645912886
Iteration 8300: Loss = -11942.96393038641
Iteration 8400: Loss = -11942.968919348585
1
Iteration 8500: Loss = -11931.133927189036
Iteration 8600: Loss = -11919.729573786195
Iteration 8700: Loss = -11911.082696878491
Iteration 8800: Loss = -11911.07320224582
Iteration 8900: Loss = -11911.075087384776
1
Iteration 9000: Loss = -11911.091425176744
2
Iteration 9100: Loss = -11911.077200036661
3
Iteration 9200: Loss = -11898.916593944652
Iteration 9300: Loss = -11877.718527462936
Iteration 9400: Loss = -11877.750247903037
1
Iteration 9500: Loss = -11869.231595937359
Iteration 9600: Loss = -11859.795071038012
Iteration 9700: Loss = -11859.769709936332
Iteration 9800: Loss = -11859.762845377169
Iteration 9900: Loss = -11859.773881353558
1
Iteration 10000: Loss = -11859.78959837607
2
Iteration 10100: Loss = -11859.76224872114
Iteration 10200: Loss = -11859.759674906745
Iteration 10300: Loss = -11859.760090471658
1
Iteration 10400: Loss = -11859.80597525392
2
Iteration 10500: Loss = -11859.78675467996
3
Iteration 10600: Loss = -11859.778601315225
4
Iteration 10700: Loss = -11859.763244795437
5
Iteration 10800: Loss = -11859.762103154191
6
Iteration 10900: Loss = -11859.772420634154
7
Iteration 11000: Loss = -11844.409484487764
Iteration 11100: Loss = -11844.410701017083
1
Iteration 11200: Loss = -11844.418513189452
2
Iteration 11300: Loss = -11844.429483852931
3
Iteration 11400: Loss = -11844.411312093867
4
Iteration 11500: Loss = -11844.413646993655
5
Iteration 11600: Loss = -11844.409347139373
Iteration 11700: Loss = -11844.414030361633
1
Iteration 11800: Loss = -11844.433556315138
2
Iteration 11900: Loss = -11844.414961051465
3
Iteration 12000: Loss = -11844.413877303188
4
Iteration 12100: Loss = -11844.410912761028
5
Iteration 12200: Loss = -11844.40955885189
6
Iteration 12300: Loss = -11844.413803987043
7
Iteration 12400: Loss = -11844.428375744752
8
Iteration 12500: Loss = -11844.413857317646
9
Iteration 12600: Loss = -11844.40820264434
Iteration 12700: Loss = -11844.409074121944
1
Iteration 12800: Loss = -11844.40806353237
Iteration 12900: Loss = -11844.41658661931
1
Iteration 13000: Loss = -11844.025901465335
Iteration 13100: Loss = -11844.023015230012
Iteration 13200: Loss = -11844.021476805658
Iteration 13300: Loss = -11844.02095215412
Iteration 13400: Loss = -11844.020089248439
Iteration 13500: Loss = -11844.020421988836
1
Iteration 13600: Loss = -11844.021271216734
2
Iteration 13700: Loss = -11844.031246789762
3
Iteration 13800: Loss = -11844.08380315053
4
Iteration 13900: Loss = -11844.048362178757
5
Iteration 14000: Loss = -11844.023581123343
6
Iteration 14100: Loss = -11844.117319999976
7
Iteration 14200: Loss = -11844.03239986348
8
Iteration 14300: Loss = -11844.020749844865
9
Iteration 14400: Loss = -11844.020576092425
10
Stopping early at iteration 14400 due to no improvement.
tensor([[-11.5220,   6.9068],
        [ -4.8207,   3.4165],
        [ -7.4762,   4.0360],
        [  5.4281,  -7.0297],
        [  6.0827, -10.1873],
        [ -4.7537,   3.2946],
        [ -8.9236,   6.5855],
        [ -6.0711,   3.5454],
        [ -6.8050,   4.9021],
        [  5.6828,  -7.4157],
        [  6.1534,  -9.9625],
        [  6.2178,  -7.7759],
        [  5.9519,  -7.7188],
        [  6.3911,  -7.7797],
        [  2.5585,  -5.3974],
        [ -8.1117,   6.7196],
        [  1.5435,  -5.2485],
        [  6.4902,  -7.8772],
        [  1.8117,  -4.0470],
        [ -7.2756,   5.8668],
        [ -8.0983,   6.1797],
        [ -6.9066,   4.8891],
        [  5.3696,  -6.8837],
        [ -7.5926,   5.9872],
        [ -7.4143,   4.7121],
        [  5.4508,  -6.8536],
        [ -8.3300,   5.8919],
        [  2.7689,  -4.3689],
        [ -5.7313,   4.2602],
        [  6.4830,  -8.2794],
        [ -6.1934,   4.7393],
        [  3.3917,  -5.4261],
        [  1.4447,  -4.2701],
        [ -8.6385,   6.9293],
        [  6.2700,  -7.6608],
        [  0.2135,  -2.2150],
        [  7.1639,  -8.5652],
        [  6.2381,  -7.7512],
        [ -8.3873,   6.2910],
        [ -9.0203,   7.5089],
        [  5.6124,  -7.0488],
        [ -7.2098,   5.7677],
        [  5.3671,  -8.2477],
        [ -7.8796,   5.7418],
        [  6.2431,  -7.6526],
        [ -0.4227,  -2.0352],
        [  2.9850,  -6.7917],
        [  5.2715,  -6.6627],
        [ -7.8345,   6.2863],
        [ -5.2481,   3.3519],
        [  3.9728,  -5.5157],
        [ -7.1813,   5.7592],
        [ -0.3728,  -1.0240],
        [ -8.8815,   7.4952],
        [  6.6327,  -9.1532],
        [ -8.3980,   6.2480],
        [  3.9651,  -7.8601],
        [  6.1610, -10.2837],
        [ -8.0043,   6.3535],
        [  4.8818,  -6.3375],
        [ -7.4548,   6.0617],
        [  5.4057,  -7.2198],
        [  3.4184,  -5.1127],
        [  2.6026,  -4.2902],
        [ -5.7110,   4.1239],
        [ -8.1224,   5.8532],
        [  6.7508,  -8.1423],
        [  5.9583,  -7.4674],
        [  7.0928,  -8.6099],
        [  4.4866,  -6.1767],
        [ -9.4457,   7.6631],
        [  4.6713,  -6.0862],
        [ -8.2523,   6.8312],
        [ -8.4104,   6.9366],
        [  5.1419,  -8.6504],
        [ -9.5547,   7.5037],
        [ -3.5571,   1.8875],
        [ -7.3067,   5.8281],
        [  3.4193,  -4.8056],
        [  0.9566,  -2.3430],
        [  6.2326,  -7.6456],
        [ -9.1860,   7.7795],
        [  5.6454,  -8.5283],
        [-10.6336,   8.2915],
        [ -6.5193,   5.0891],
        [ -8.5696,   6.9322],
        [  4.4743,  -5.8744],
        [ -6.6681,   5.0796],
        [  6.2632,  -7.6838],
        [ -6.2321,   4.7964],
        [  5.3541,  -7.8383],
        [  7.2245,  -8.6151],
        [ -5.9877,   4.4472],
        [  3.7255,  -5.2103],
        [ -7.3334,   5.7286],
        [ -8.1963,   6.0746],
        [ -7.9123,   6.2564],
        [ -7.9684,   6.4237],
        [  2.7820,  -7.3972],
        [ -4.1743,   2.7855]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7172, 0.2828],
        [0.2508, 0.7492]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5148, 0.4852], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2071, 0.1260],
         [0.2065, 0.4058]],

        [[0.8464, 0.0886],
         [0.7189, 0.9012]],

        [[0.7315, 0.1074],
         [0.1489, 0.0118]],

        [[0.2521, 0.1007],
         [0.2111, 0.7388]],

        [[0.2747, 0.1099],
         [0.2445, 0.5394]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9446732460693126
Average Adjusted Rand Index: 0.945128734220605
Iteration 0: Loss = -24764.998182699746
Iteration 10: Loss = -11786.100563278333
Iteration 20: Loss = -11786.100070133814
Iteration 30: Loss = -11786.100070133256
Iteration 40: Loss = -11786.100070133256
1
Iteration 50: Loss = -11786.100070133256
2
Iteration 60: Loss = -11786.100070133256
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7618, 0.2382],
        [0.2671, 0.7329]], dtype=torch.float64)
alpha: tensor([0.5202, 0.4798])
beta: tensor([[[0.3938, 0.1102],
         [0.5631, 0.2076]],

        [[0.4592, 0.0885],
         [0.1645, 0.3793]],

        [[0.3839, 0.0984],
         [0.2577, 0.8270]],

        [[0.8143, 0.1007],
         [0.2128, 0.2347]],

        [[0.4384, 0.1098],
         [0.6449, 0.4521]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961908856516
Average Adjusted Rand Index: 0.9759992163675584
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24764.580276074415
Iteration 100: Loss = -12607.733036878748
Iteration 200: Loss = -12366.050613006384
Iteration 300: Loss = -12275.402976194784
Iteration 400: Loss = -12210.78392271759
Iteration 500: Loss = -12185.836855149133
Iteration 600: Loss = -12152.131852534716
Iteration 700: Loss = -12132.226609402771
Iteration 800: Loss = -12130.806401437147
Iteration 900: Loss = -12099.424465622575
Iteration 1000: Loss = -12087.813201407158
Iteration 1100: Loss = -12080.605745847919
Iteration 1200: Loss = -12062.327596194733
Iteration 1300: Loss = -12058.818106265353
Iteration 1400: Loss = -12050.63233532151
Iteration 1500: Loss = -12041.630036230947
Iteration 1600: Loss = -12011.64715723074
Iteration 1700: Loss = -11989.522957831981
Iteration 1800: Loss = -11967.620138318689
Iteration 1900: Loss = -11931.071773065385
Iteration 2000: Loss = -11895.001803282727
Iteration 2100: Loss = -11893.11486319765
Iteration 2200: Loss = -11840.750659427556
Iteration 2300: Loss = -11839.582986370426
Iteration 2400: Loss = -11820.683055116917
Iteration 2500: Loss = -11808.102205117677
Iteration 2600: Loss = -11807.994466256028
Iteration 2700: Loss = -11807.972789861273
Iteration 2800: Loss = -11807.956167317425
Iteration 2900: Loss = -11807.942759557189
Iteration 3000: Loss = -11807.931595153472
Iteration 3100: Loss = -11807.922148353697
Iteration 3200: Loss = -11807.913943012509
Iteration 3300: Loss = -11807.906819619579
Iteration 3400: Loss = -11807.900531536947
Iteration 3500: Loss = -11807.89493877718
Iteration 3600: Loss = -11807.889896501121
Iteration 3700: Loss = -11807.885378068962
Iteration 3800: Loss = -11807.881305954623
Iteration 3900: Loss = -11807.877726643786
Iteration 4000: Loss = -11807.874073394061
Iteration 4100: Loss = -11807.870514686043
Iteration 4200: Loss = -11807.79062249567
Iteration 4300: Loss = -11797.835358670252
Iteration 4400: Loss = -11797.832467261793
Iteration 4500: Loss = -11797.830344075905
Iteration 4600: Loss = -11797.828059849982
Iteration 4700: Loss = -11797.8261647741
Iteration 4800: Loss = -11797.824131727815
Iteration 4900: Loss = -11797.82250963248
Iteration 5000: Loss = -11797.825513603017
1
Iteration 5100: Loss = -11797.821198369358
Iteration 5200: Loss = -11797.81828806093
Iteration 5300: Loss = -11797.81707934214
Iteration 5400: Loss = -11797.816684152329
Iteration 5500: Loss = -11797.814858834736
Iteration 5600: Loss = -11797.813923054484
Iteration 5700: Loss = -11797.82156511083
1
Iteration 5800: Loss = -11797.812092303058
Iteration 5900: Loss = -11797.811283012852
Iteration 6000: Loss = -11797.81056200078
Iteration 6100: Loss = -11797.824205365063
1
Iteration 6200: Loss = -11797.809524690732
Iteration 6300: Loss = -11797.818715194115
1
Iteration 6400: Loss = -11797.807991628246
Iteration 6500: Loss = -11797.834520217639
1
Iteration 6600: Loss = -11797.816022094714
2
Iteration 6700: Loss = -11797.815211846653
3
Iteration 6800: Loss = -11797.806423000506
Iteration 6900: Loss = -11797.868965941605
1
Iteration 7000: Loss = -11797.866548007612
2
Iteration 7100: Loss = -11797.88938159974
3
Iteration 7200: Loss = -11797.834121908933
4
Iteration 7300: Loss = -11797.894568845139
5
Iteration 7400: Loss = -11797.804057962276
Iteration 7500: Loss = -11797.803347416258
Iteration 7600: Loss = -11797.805331503809
1
Iteration 7700: Loss = -11797.808280493544
2
Iteration 7800: Loss = -11797.803453436678
3
Iteration 7900: Loss = -11797.804384274043
4
Iteration 8000: Loss = -11797.804494333894
5
Iteration 8100: Loss = -11797.80432717135
6
Iteration 8200: Loss = -11797.80210315001
Iteration 8300: Loss = -11797.836072447039
1
Iteration 8400: Loss = -11797.804112187565
2
Iteration 8500: Loss = -11797.809273094383
3
Iteration 8600: Loss = -11797.824318150308
4
Iteration 8700: Loss = -11797.801065832711
Iteration 8800: Loss = -11797.808805836188
1
Iteration 8900: Loss = -11796.960559114454
Iteration 9000: Loss = -11784.860206125912
Iteration 9100: Loss = -11784.87799691945
1
Iteration 9200: Loss = -11784.859654133028
Iteration 9300: Loss = -11784.86939241766
1
Iteration 9400: Loss = -11784.859171380604
Iteration 9500: Loss = -11784.856545998859
Iteration 9600: Loss = -11784.85836358308
1
Iteration 9700: Loss = -11784.85932024542
2
Iteration 9800: Loss = -11784.868072635116
3
Iteration 9900: Loss = -11784.864951470396
4
Iteration 10000: Loss = -11784.863399331265
5
Iteration 10100: Loss = -11784.867341968362
6
Iteration 10200: Loss = -11784.893256038515
7
Iteration 10300: Loss = -11784.860790091776
8
Iteration 10400: Loss = -11784.878327624927
9
Iteration 10500: Loss = -11784.874833462221
10
Stopping early at iteration 10500 due to no improvement.
tensor([[  7.4940,  -8.8809],
        [  5.1576,  -6.5451],
        [  4.3100,  -7.4085],
        [ -7.6413,   5.6856],
        [ -7.7980,   6.3925],
        [  7.5610,  -9.3023],
        [  6.5363,  -9.9352],
        [  4.7767,  -6.1684],
        [  5.0621,  -6.8539],
        [ -7.9877,   6.2506],
        [  7.4436, -10.1185],
        [ -8.5813,   6.3806],
        [ -9.3565,   5.6894],
        [ -8.8974,   5.9415],
        [ -5.0986,   3.2774],
        [  6.3244,  -8.9462],
        [ -9.1216,   6.6743],
        [ -9.0026,   7.1299],
        [ -4.6071,   3.2208],
        [  7.5508,  -9.8582],
        [  5.8822,  -8.0188],
        [  6.8011,  -8.3768],
        [ -7.6246,   6.2377],
        [  6.4197,  -8.2755],
        [  5.3587,  -8.6349],
        [ -7.5612,   6.1635],
        [  5.7369,  -7.3694],
        [ -4.4261,   2.7810],
        [  4.4128,  -7.5223],
        [ -8.1623,   6.7760],
        [  6.5970,  -7.9906],
        [ -4.7626,   3.3739],
        [ -4.5553,   2.6753],
        [  6.7583,  -8.8641],
        [ -8.1986,   6.4513],
        [ -0.3785,  -1.0292],
        [ -8.2804,   6.8379],
        [ -8.9343,   5.8982],
        [  7.1117, -10.2785],
        [  7.8813,  -9.7224],
        [ -7.0021,   5.5995],
        [  7.6968,  -9.1299],
        [ -8.3648,   5.8643],
        [  7.2188,  -9.5890],
        [ -8.7408,   6.9724],
        [  0.6744,  -2.3728],
        [ -6.5640,   4.9345],
        [ -7.7795,   5.7354],
        [  6.4512,  -7.8474],
        [  4.8233,  -6.4017],
        [ -7.3634,   4.4115],
        [  6.7547,  -8.3274],
        [  0.5863,  -2.2906],
        [  8.0257,  -9.5371],
        [  8.0499,  -9.4379],
        [  7.9289,  -9.3423],
        [ -7.2150,   5.2279],
        [ -8.7932,   7.3699],
        [  5.9242,  -9.1801],
        [ -7.1780,   5.7365],
        [  6.9454,  -8.6997],
        [ -8.3078,   5.8592],
        [ -5.0377,   3.5499],
        [ -5.0875,   3.7007],
        [  6.3892,  -8.0385],
        [  6.7627,  -8.1545],
        [ -8.6733,   5.9196],
        [ -7.4252,   5.5298],
        [ -8.7768,   7.1954],
        [ -6.3842,   4.5272],
        [  7.2616,  -8.6568],
        [ -5.6619,   4.2677],
        [  6.7496,  -9.9260],
        [  6.7561,  -9.4063],
        [ -8.3514,   6.8987],
        [  6.9916,  -8.3862],
        [  7.0507,  -8.6109],
        [  6.5332,  -8.0779],
        [ -6.5526,   3.2674],
        [ -2.7630,   1.3510],
        [ -7.6367,   6.2240],
        [  7.8664,  -9.3012],
        [ -7.9420,   6.5239],
        [  8.1793, -10.2443],
        [  7.4000,  -9.1641],
        [  7.8708,  -9.3129],
        [ -5.5817,   4.1875],
        [  6.0465,  -8.0654],
        [ -8.5786,   6.5725],
        [  6.3283,  -8.1791],
        [ -8.4478,   5.2030],
        [ -7.1633,   4.4432],
        [  7.1359,  -8.5222],
        [ -8.7318,   7.3360],
        [  7.4721,  -8.8592],
        [  7.5005,  -9.7295],
        [  7.2724,  -9.8351],
        [  7.5127,  -8.9777],
        [ -6.4816,   5.0932],
        [  7.8782,  -9.3225]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7654, 0.2346],
        [0.2653, 0.7347]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5278, 0.4722], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4006, 0.1100],
         [0.5631, 0.2118]],

        [[0.4592, 0.0884],
         [0.1645, 0.3793]],

        [[0.3839, 0.0978],
         [0.2577, 0.8270]],

        [[0.8143, 0.1006],
         [0.2128, 0.2347]],

        [[0.4384, 0.1095],
         [0.6449, 0.4521]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961908856516
Average Adjusted Rand Index: 0.9759992163675584
11797.278072778518
new:  [0.9760961908856516, 0.11383204578898504, 0.9446732460693126, 0.9760961908856516] [0.9759992163675584, 0.7542118766766358, 0.945128734220605, 0.9759992163675584] [11784.565897655086, 12043.988275328446, 11844.020576092425, 11784.874833462221]
prior:  [0.9760961908856516, 0.9760961908856516, 0.0, 0.9760961908856516] [0.9759992163675584, 0.9759992163675584, 0.0, 0.9759992163675584] [11786.10006974293, 11786.10006974293, nan, 11786.100070133256]
-----------------------------------------------------------------------------------------
This iteration is 10
True Objective function: Loss = -11439.518184010527
Iteration 0: Loss = -33025.623550817734
Iteration 10: Loss = -12238.302367236596
Iteration 20: Loss = -12238.116481743175
Iteration 30: Loss = -12085.963744033665
Iteration 40: Loss = -11428.467399403396
Iteration 50: Loss = -11428.467409664923
1
Iteration 60: Loss = -11428.467409664923
2
Iteration 70: Loss = -11428.467409664923
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.6925, 0.3075],
        [0.2279, 0.7721]], dtype=torch.float64)
alpha: tensor([0.4630, 0.5370])
beta: tensor([[[0.4035, 0.0809],
         [0.6860, 0.2014]],

        [[0.4460, 0.1035],
         [0.6914, 0.4976]],

        [[0.1784, 0.1026],
         [0.1500, 0.4566]],

        [[0.7758, 0.0909],
         [0.9035, 0.3298]],

        [[0.3612, 0.1086],
         [0.7604, 0.7266]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33035.197085237975
Iteration 100: Loss = -12242.708722485297
Iteration 200: Loss = -12239.215718721223
Iteration 300: Loss = -12237.745067586491
Iteration 400: Loss = -12235.203275596845
Iteration 500: Loss = -12232.58629428906
Iteration 600: Loss = -12223.13412704463
Iteration 700: Loss = -12149.966667492794
Iteration 800: Loss = -12117.349278561991
Iteration 900: Loss = -12083.46147192322
Iteration 1000: Loss = -12080.028061985697
Iteration 1100: Loss = -12076.868793856314
Iteration 1200: Loss = -12065.927355274647
Iteration 1300: Loss = -11931.643885040243
Iteration 1400: Loss = -11857.495610914862
Iteration 1500: Loss = -11770.565796906822
Iteration 1600: Loss = -11723.74947535565
Iteration 1700: Loss = -11721.927099436603
Iteration 1800: Loss = -11714.665297867823
Iteration 1900: Loss = -11714.495574497028
Iteration 2000: Loss = -11714.397080503757
Iteration 2100: Loss = -11714.328151083002
Iteration 2200: Loss = -11714.276928018522
Iteration 2300: Loss = -11714.237187721235
Iteration 2400: Loss = -11714.205467399172
Iteration 2500: Loss = -11714.180187889713
Iteration 2600: Loss = -11714.157386253954
Iteration 2700: Loss = -11700.63730354665
Iteration 2800: Loss = -11700.618391447922
Iteration 2900: Loss = -11700.602863306445
Iteration 3000: Loss = -11700.620139694664
1
Iteration 3100: Loss = -11700.578222577682
Iteration 3200: Loss = -11700.568637519083
Iteration 3300: Loss = -11700.55983204932
Iteration 3400: Loss = -11700.552277341376
Iteration 3500: Loss = -11700.549584337261
Iteration 3600: Loss = -11700.539564471786
Iteration 3700: Loss = -11700.534156843514
Iteration 3800: Loss = -11700.529261319554
Iteration 3900: Loss = -11700.524926411978
Iteration 4000: Loss = -11700.527562120571
1
Iteration 4100: Loss = -11700.517278139509
Iteration 4200: Loss = -11700.514119036385
Iteration 4300: Loss = -11700.51097263869
Iteration 4400: Loss = -11700.508173516875
Iteration 4500: Loss = -11700.511547715754
1
Iteration 4600: Loss = -11700.503629531477
Iteration 4700: Loss = -11700.501008565938
Iteration 4800: Loss = -11700.499015923371
Iteration 4900: Loss = -11700.497145322894
Iteration 5000: Loss = -11700.498430606065
1
Iteration 5100: Loss = -11700.49375640552
Iteration 5200: Loss = -11700.49223524258
Iteration 5300: Loss = -11700.491382808228
Iteration 5400: Loss = -11700.489580150614
Iteration 5500: Loss = -11700.489196505603
Iteration 5600: Loss = -11700.48760298655
Iteration 5700: Loss = -11700.487405454103
Iteration 5800: Loss = -11700.485126253221
Iteration 5900: Loss = -11700.48521311338
1
Iteration 6000: Loss = -11700.483300408545
Iteration 6100: Loss = -11700.482530693893
Iteration 6200: Loss = -11700.481725600475
Iteration 6300: Loss = -11700.481121071562
Iteration 6400: Loss = -11700.480316150788
Iteration 6500: Loss = -11700.479722454353
Iteration 6600: Loss = -11700.479991947384
1
Iteration 6700: Loss = -11700.478535344846
Iteration 6800: Loss = -11700.482314711027
1
Iteration 6900: Loss = -11700.477487165814
Iteration 7000: Loss = -11700.497196615037
1
Iteration 7100: Loss = -11700.476626680063
Iteration 7200: Loss = -11700.476363756969
Iteration 7300: Loss = -11700.47580861404
Iteration 7400: Loss = -11700.475730980454
Iteration 7500: Loss = -11700.475222103201
Iteration 7600: Loss = -11700.474788420319
Iteration 7700: Loss = -11700.474519976122
Iteration 7800: Loss = -11700.478113253046
1
Iteration 7900: Loss = -11700.474057124848
Iteration 8000: Loss = -11700.48451387222
1
Iteration 8100: Loss = -11700.473488388494
Iteration 8200: Loss = -11700.484444976057
1
Iteration 8300: Loss = -11700.472964234314
Iteration 8400: Loss = -11700.47285741524
Iteration 8500: Loss = -11700.472693652828
Iteration 8600: Loss = -11700.4759723805
1
Iteration 8700: Loss = -11700.47212210765
Iteration 8800: Loss = -11700.473503128256
1
Iteration 8900: Loss = -11700.471836053588
Iteration 9000: Loss = -11700.471786606375
Iteration 9100: Loss = -11700.471489490386
Iteration 9200: Loss = -11700.487334452113
1
Iteration 9300: Loss = -11700.471226698717
Iteration 9400: Loss = -11700.490342855437
1
Iteration 9500: Loss = -11700.470946605723
Iteration 9600: Loss = -11696.215229278072
Iteration 9700: Loss = -11696.21292265126
Iteration 9800: Loss = -11696.212540429719
Iteration 9900: Loss = -11696.217510439747
1
Iteration 10000: Loss = -11696.21233193967
Iteration 10100: Loss = -11696.212224166722
Iteration 10200: Loss = -11696.212189136188
Iteration 10300: Loss = -11696.212083646544
Iteration 10400: Loss = -11696.276798511783
1
Iteration 10500: Loss = -11696.211901974259
Iteration 10600: Loss = -11696.211826362261
Iteration 10700: Loss = -11696.331153628917
1
Iteration 10800: Loss = -11696.21172165678
Iteration 10900: Loss = -11696.211634404359
Iteration 11000: Loss = -11696.21796831055
1
Iteration 11100: Loss = -11696.21155682649
Iteration 11200: Loss = -11696.215243560942
1
Iteration 11300: Loss = -11696.211500489975
Iteration 11400: Loss = -11696.211368888522
Iteration 11500: Loss = -11696.21874205454
1
Iteration 11600: Loss = -11696.216892544388
2
Iteration 11700: Loss = -11696.21500775077
3
Iteration 11800: Loss = -11696.213988385472
4
Iteration 11900: Loss = -11696.211313392663
Iteration 12000: Loss = -11696.217645375833
1
Iteration 12100: Loss = -11696.21114469241
Iteration 12200: Loss = -11696.211434160894
1
Iteration 12300: Loss = -11696.211507134758
2
Iteration 12400: Loss = -11696.211251352062
3
Iteration 12500: Loss = -11696.327844178484
4
Iteration 12600: Loss = -11696.21100255007
Iteration 12700: Loss = -11696.286411466335
1
Iteration 12800: Loss = -11696.211013202696
2
Iteration 12900: Loss = -11696.220736143177
3
Iteration 13000: Loss = -11696.211943049013
4
Iteration 13100: Loss = -11696.221817706943
5
Iteration 13200: Loss = -11696.210938141188
Iteration 13300: Loss = -11696.2117333915
1
Iteration 13400: Loss = -11696.22095933966
2
Iteration 13500: Loss = -11696.211285146936
3
Iteration 13600: Loss = -11696.210920733025
Iteration 13700: Loss = -11696.212821057536
1
Iteration 13800: Loss = -11696.22700308752
2
Iteration 13900: Loss = -11696.215449125319
3
Iteration 14000: Loss = -11696.210707564198
Iteration 14100: Loss = -11696.215996381085
1
Iteration 14200: Loss = -11696.21141074822
2
Iteration 14300: Loss = -11696.214864340138
3
Iteration 14400: Loss = -11696.21510819424
4
Iteration 14500: Loss = -11696.209218554493
Iteration 14600: Loss = -11696.199637815638
Iteration 14700: Loss = -11696.199025221616
Iteration 14800: Loss = -11696.199067389114
1
Iteration 14900: Loss = -11696.199167817269
2
Iteration 15000: Loss = -11696.199044925594
3
Iteration 15100: Loss = -11696.202895008302
4
Iteration 15200: Loss = -11696.200679304353
5
Iteration 15300: Loss = -11696.219886250045
6
Iteration 15400: Loss = -11696.198921841155
Iteration 15500: Loss = -11696.199919292738
1
Iteration 15600: Loss = -11696.198986908488
2
Iteration 15700: Loss = -11696.199398386383
3
Iteration 15800: Loss = -11696.199000957557
4
Iteration 15900: Loss = -11696.379780118941
5
Iteration 16000: Loss = -11696.198921119014
Iteration 16100: Loss = -11696.19906449762
1
Iteration 16200: Loss = -11696.202185598764
2
Iteration 16300: Loss = -11696.325772979075
3
Iteration 16400: Loss = -11696.198919724446
Iteration 16500: Loss = -11696.20123185025
1
Iteration 16600: Loss = -11696.238796743606
2
Iteration 16700: Loss = -11696.206055162711
3
Iteration 16800: Loss = -11696.205185744224
4
Iteration 16900: Loss = -11696.199521489412
5
Iteration 17000: Loss = -11696.222054310756
6
Iteration 17100: Loss = -11696.199082182538
7
Iteration 17200: Loss = -11696.200293604285
8
Iteration 17300: Loss = -11696.210497964106
9
Iteration 17400: Loss = -11696.199743676132
10
Stopping early at iteration 17400 due to no improvement.
tensor([[ -8.6046,   3.9894],
        [  6.6885, -11.3038],
        [-10.7430,   6.1278],
        [ -9.7732,   5.1579],
        [-10.7785,   6.1633],
        [  5.9198, -10.5351],
        [  6.7267, -11.3419],
        [-11.3888,   6.7736],
        [-11.2057,   6.5904],
        [  6.5858, -11.2010],
        [ -9.2541,   4.6389],
        [-10.2415,   5.6262],
        [  6.3294, -10.9446],
        [  6.2301, -10.8454],
        [  6.4764, -11.0916],
        [ -8.8433,   4.2280],
        [-10.7571,   6.1419],
        [  5.4454, -10.0606],
        [-10.8762,   6.2610],
        [  6.7296, -11.3448],
        [  5.9449, -10.5601],
        [  5.3626,  -9.9778],
        [-11.4841,   6.8689],
        [ -8.0211,   3.4059],
        [  6.7284, -11.3436],
        [-11.1632,   6.5479],
        [-11.6740,   7.0588],
        [  5.7997, -10.4150],
        [  6.9607, -11.5759],
        [  6.2650, -10.8802],
        [ -9.4273,   4.8121],
        [ -7.6734,   3.0582],
        [-11.5227,   6.9075],
        [  6.0713, -10.6866],
        [  6.4861, -11.1013],
        [ -6.8534,   2.2382],
        [  6.4100, -11.0252],
        [ -9.3750,   4.7597],
        [  6.1458, -10.7610],
        [  6.1344, -10.7496],
        [-11.5177,   6.9025],
        [  5.3071,  -9.9223],
        [ -6.4798,   1.8646],
        [-10.6363,   6.0211],
        [ -6.4797,   1.8645],
        [  6.3993, -11.0146],
        [ -8.0862,   3.4710],
        [  6.4937, -11.1089],
        [-11.1925,   6.5773],
        [ -9.8369,   5.2217],
        [  6.5939, -11.2091],
        [  6.1313, -10.7465],
        [  2.9354,  -7.5507],
        [  6.6303, -11.2455],
        [  6.1862, -10.8014],
        [ -9.8372,   5.2220],
        [  6.4790, -11.0943],
        [ -8.8144,   4.1992],
        [  1.9404,  -6.5557],
        [  5.5253, -10.1405],
        [-11.7776,   7.1624],
        [ -7.2648,   2.6496],
        [  5.7264, -10.3416],
        [  6.3791, -10.9943],
        [-10.2369,   5.6217],
        [ -7.6758,   3.0605],
        [-10.5519,   5.9367],
        [  5.7162, -10.3314],
        [-11.8314,   7.2162],
        [ -8.8433,   4.2281],
        [-10.7615,   6.1462],
        [  6.5271, -11.1423],
        [ -7.6757,   3.0605],
        [  6.0643, -10.6795],
        [  6.5582, -11.1735],
        [  6.5917, -11.2070],
        [-11.5787,   6.9635],
        [  6.8409, -11.4562],
        [  5.9856, -10.6008],
        [  6.2336, -10.8488],
        [ -9.6655,   5.0503],
        [-11.3921,   6.7769],
        [  5.6050, -10.2203],
        [  5.9803, -10.5955],
        [  5.7584, -10.3736],
        [  6.4062, -11.0214],
        [  6.5430, -11.1583],
        [ -6.4204,   1.8052],
        [-10.7623,   6.1471],
        [-11.6872,   7.0720],
        [  5.8102, -10.4255],
        [-11.6432,   7.0280],
        [-10.0454,   5.4301],
        [-11.1006,   6.4854],
        [  5.8576, -10.4728],
        [  5.9575, -10.5727],
        [  6.6329, -11.2481],
        [-11.1964,   6.5812],
        [  6.2608, -10.8760],
        [-11.3669,   6.7517]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5668, 0.4332],
        [0.2139, 0.7861]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3912, 0.0808],
         [0.6860, 0.2205]],

        [[0.4460, 0.1030],
         [0.6914, 0.4976]],

        [[0.1784, 0.1024],
         [0.1500, 0.4566]],

        [[0.7758, 0.1001],
         [0.9035, 0.3298]],

        [[0.3612, 0.1203],
         [0.7604, 0.7266]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.042748091603053436
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.03746111985706271
Global Adjusted Rand Index: 0.2523793703573193
Average Adjusted Rand Index: 0.6160418422920233
Iteration 0: Loss = -21386.59427510289
Iteration 10: Loss = -11429.056938149557
Iteration 20: Loss = -11428.467415582421
Iteration 30: Loss = -11428.467408912717
Iteration 40: Loss = -11428.467408912717
1
Iteration 50: Loss = -11428.467408912717
2
Iteration 60: Loss = -11428.467408912717
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7721, 0.2279],
        [0.3075, 0.6925]], dtype=torch.float64)
alpha: tensor([0.5370, 0.4630])
beta: tensor([[[0.2014, 0.0809],
         [0.8687, 0.4035]],

        [[0.3736, 0.1035],
         [0.5286, 0.9634]],

        [[0.3715, 0.1026],
         [0.5504, 0.1905]],

        [[0.9751, 0.0909],
         [0.8002, 0.2922]],

        [[0.8861, 0.1086],
         [0.6900, 0.9149]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21386.704538654143
Iteration 100: Loss = -12135.144220094382
Iteration 200: Loss = -11970.545051457057
Iteration 300: Loss = -11915.506641315042
Iteration 400: Loss = -11893.663352007987
Iteration 500: Loss = -11852.744435473753
Iteration 600: Loss = -11838.696062403124
Iteration 700: Loss = -11833.421371820761
Iteration 800: Loss = -11815.22749535752
Iteration 900: Loss = -11807.44316942961
Iteration 1000: Loss = -11805.701468181252
Iteration 1100: Loss = -11803.125520227673
Iteration 1200: Loss = -11802.011130448318
Iteration 1300: Loss = -11787.652086197972
Iteration 1400: Loss = -11777.632059391732
Iteration 1500: Loss = -11775.317900228929
Iteration 1600: Loss = -11761.847881284208
Iteration 1700: Loss = -11761.273449590668
Iteration 1800: Loss = -11742.508598718303
Iteration 1900: Loss = -11738.976872052068
Iteration 2000: Loss = -11738.87831116435
Iteration 2100: Loss = -11738.380027383953
Iteration 2200: Loss = -11734.21780304353
Iteration 2300: Loss = -11734.1809493329
Iteration 2400: Loss = -11726.00383089326
Iteration 2500: Loss = -11725.700425427924
Iteration 2600: Loss = -11717.546259612554
Iteration 2700: Loss = -11717.150172458178
Iteration 2800: Loss = -11717.125656009504
Iteration 2900: Loss = -11717.152898964745
1
Iteration 3000: Loss = -11711.24519597401
Iteration 3100: Loss = -11711.21232826832
Iteration 3200: Loss = -11711.231945614349
1
Iteration 3300: Loss = -11710.166226109872
Iteration 3400: Loss = -11696.282517833664
Iteration 3500: Loss = -11696.245920343648
Iteration 3600: Loss = -11696.21138071503
Iteration 3700: Loss = -11688.417008032971
Iteration 3800: Loss = -11672.657753797148
Iteration 3900: Loss = -11668.703886767431
Iteration 4000: Loss = -11668.79048054591
1
Iteration 4100: Loss = -11668.688161955459
Iteration 4200: Loss = -11668.683563448682
Iteration 4300: Loss = -11668.679683919287
Iteration 4400: Loss = -11668.675948509544
Iteration 4500: Loss = -11648.587326159648
Iteration 4600: Loss = -11648.504855003786
Iteration 4700: Loss = -11648.569697148396
1
Iteration 4800: Loss = -11648.498429589932
Iteration 4900: Loss = -11648.496213125585
Iteration 5000: Loss = -11648.49434846291
Iteration 5100: Loss = -11648.492727805544
Iteration 5200: Loss = -11648.491191878367
Iteration 5300: Loss = -11648.489830396056
Iteration 5400: Loss = -11648.489554836935
Iteration 5500: Loss = -11648.487471452812
Iteration 5600: Loss = -11648.486448254173
Iteration 5700: Loss = -11648.485605528316
Iteration 5800: Loss = -11648.484664791578
Iteration 5900: Loss = -11648.48387434991
Iteration 6000: Loss = -11648.600716637962
1
Iteration 6100: Loss = -11648.48252275613
Iteration 6200: Loss = -11648.481951910673
Iteration 6300: Loss = -11648.482342426385
1
Iteration 6400: Loss = -11648.48091751789
Iteration 6500: Loss = -11648.480466004854
Iteration 6600: Loss = -11648.480044240243
Iteration 6700: Loss = -11648.479665865485
Iteration 6800: Loss = -11648.529181763972
1
Iteration 6900: Loss = -11648.478881919586
Iteration 7000: Loss = -11648.482630056635
1
Iteration 7100: Loss = -11648.47816315123
Iteration 7200: Loss = -11648.487002691023
1
Iteration 7300: Loss = -11648.475999119757
Iteration 7400: Loss = -11648.469109555595
Iteration 7500: Loss = -11648.47768896639
1
Iteration 7600: Loss = -11648.468213651679
Iteration 7700: Loss = -11648.468066166044
Iteration 7800: Loss = -11648.467821362912
Iteration 7900: Loss = -11648.467618105746
Iteration 8000: Loss = -11648.468383774278
1
Iteration 8100: Loss = -11648.46732451373
Iteration 8200: Loss = -11648.467129790815
Iteration 8300: Loss = -11648.467074598433
Iteration 8400: Loss = -11648.466867359128
Iteration 8500: Loss = -11648.473704885653
1
Iteration 8600: Loss = -11648.466554167817
Iteration 8700: Loss = -11648.466425814539
Iteration 8800: Loss = -11648.466325623012
Iteration 8900: Loss = -11648.416313155916
Iteration 9000: Loss = -11648.419796678063
1
Iteration 9100: Loss = -11648.413032069679
Iteration 9200: Loss = -11648.462184217384
1
Iteration 9300: Loss = -11648.412814900046
Iteration 9400: Loss = -11648.41277291345
Iteration 9500: Loss = -11648.413467681203
1
Iteration 9600: Loss = -11648.412651440918
Iteration 9700: Loss = -11648.412540198462
Iteration 9800: Loss = -11648.412604676247
1
Iteration 9900: Loss = -11648.412444642576
Iteration 10000: Loss = -11648.487167028197
1
Iteration 10100: Loss = -11648.412340010882
Iteration 10200: Loss = -11648.440966605616
1
Iteration 10300: Loss = -11648.412857834239
2
Iteration 10400: Loss = -11648.417178811156
3
Iteration 10500: Loss = -11648.41408481252
4
Iteration 10600: Loss = -11648.412232962552
Iteration 10700: Loss = -11648.412239388595
1
Iteration 10800: Loss = -11648.414679764122
2
Iteration 10900: Loss = -11648.412018543493
Iteration 11000: Loss = -11648.412029389869
1
Iteration 11100: Loss = -11648.41197421011
Iteration 11200: Loss = -11648.412029153276
1
Iteration 11300: Loss = -11648.418816697658
2
Iteration 11400: Loss = -11648.41834454199
3
Iteration 11500: Loss = -11648.41423291494
4
Iteration 11600: Loss = -11648.412069293381
5
Iteration 11700: Loss = -11648.412968983803
6
Iteration 11800: Loss = -11648.420179239454
7
Iteration 11900: Loss = -11648.411967057255
Iteration 12000: Loss = -11648.475332260126
1
Iteration 12100: Loss = -11648.41278437694
2
Iteration 12200: Loss = -11648.443214193654
3
Iteration 12300: Loss = -11648.416081432757
4
Iteration 12400: Loss = -11648.413641973755
5
Iteration 12500: Loss = -11648.438538475224
6
Iteration 12600: Loss = -11648.47761863024
7
Iteration 12700: Loss = -11648.41168378346
Iteration 12800: Loss = -11648.411799019725
1
Iteration 12900: Loss = -11648.411727645846
2
Iteration 13000: Loss = -11648.411684664761
3
Iteration 13100: Loss = -11648.411626101044
Iteration 13200: Loss = -11648.414549248004
1
Iteration 13300: Loss = -11648.411636667479
2
Iteration 13400: Loss = -11648.41163530624
3
Iteration 13500: Loss = -11648.411649761509
4
Iteration 13600: Loss = -11648.411600012589
Iteration 13700: Loss = -11648.412778597232
1
Iteration 13800: Loss = -11648.421002288535
2
Iteration 13900: Loss = -11648.41249297009
3
Iteration 14000: Loss = -11648.408571286423
Iteration 14100: Loss = -11648.40874209895
1
Iteration 14200: Loss = -11648.40977818939
2
Iteration 14300: Loss = -11648.664687721008
3
Iteration 14400: Loss = -11648.404261142972
Iteration 14500: Loss = -11648.405469374678
1
Iteration 14600: Loss = -11648.405023058358
2
Iteration 14700: Loss = -11648.41258222715
3
Iteration 14800: Loss = -11648.404254851903
Iteration 14900: Loss = -11648.407008416905
1
Iteration 15000: Loss = -11648.404271097095
2
Iteration 15100: Loss = -11648.695476046216
3
Iteration 15200: Loss = -11648.404274996577
4
Iteration 15300: Loss = -11648.404259994255
5
Iteration 15400: Loss = -11648.406198827588
6
Iteration 15500: Loss = -11648.404241892933
Iteration 15600: Loss = -11648.445765985329
1
Iteration 15700: Loss = -11648.685797555125
2
Iteration 15800: Loss = -11648.404302794805
3
Iteration 15900: Loss = -11648.420233935762
4
Iteration 16000: Loss = -11648.407522416532
5
Iteration 16100: Loss = -11648.404503514314
6
Iteration 16200: Loss = -11648.404640390085
7
Iteration 16300: Loss = -11648.411583008598
8
Iteration 16400: Loss = -11648.40430747699
9
Iteration 16500: Loss = -11648.404373901363
10
Stopping early at iteration 16500 due to no improvement.
tensor([[ -2.5161,   0.8816],
        [  8.3653,  -9.9636],
        [  2.3121,  -3.6989],
        [  3.7730,  -5.1721],
        [ -7.1509,   5.4567],
        [  7.7034,  -9.5121],
        [  8.4519,  -9.9305],
        [ -0.5203,  -0.8714],
        [  3.1515,  -5.8682],
        [  8.3834,  -9.8843],
        [  0.1126,  -2.0245],
        [  8.0997,  -9.7440],
        [  7.7979,  -9.1916],
        [  8.9685, -10.4192],
        [  8.6541, -10.0449],
        [  1.9733,  -3.5839],
        [ -1.2199,  -0.3914],
        [  7.2881,  -8.7479],
        [  6.7206,  -8.1417],
        [  8.6905, -10.3859],
        [  7.1599,  -8.9626],
        [  7.8538,  -9.2508],
        [  6.9489,  -8.3413],
        [  3.5735,  -4.9604],
        [  8.0697, -10.3050],
        [ -4.1641,   2.7694],
        [ -1.5832,  -0.0365],
        [  6.5356,  -7.9285],
        [  5.7638,  -7.4485],
        [  8.4810, -10.0713],
        [  1.1206,  -3.0158],
        [ -2.9331,   1.5095],
        [  1.4269,  -2.9142],
        [  8.2083,  -9.6613],
        [  8.5893, -10.2777],
        [  2.0860,  -3.7717],
        [  7.7117, -11.8680],
        [  0.5087,  -2.3405],
        [  7.5251, -10.0505],
        [  7.9269,  -9.3322],
        [ -2.5556,   1.1431],
        [  6.0975,  -8.8950],
        [ -3.3720,   1.6933],
        [  2.5987,  -3.9850],
        [ -0.9138,  -1.1353],
        [  7.7637,  -9.6112],
        [ -0.6701,  -0.8684],
        [  8.2702,  -9.8906],
        [ -4.9318,   3.0155],
        [ -3.5735,   1.1743],
        [  8.2114, -10.3949],
        [  5.2511,  -6.6398],
        [  7.8699,  -9.5222],
        [  7.9963,  -9.3963],
        [  6.8380,  -8.3587],
        [  1.6047,  -3.1174],
        [  8.6771, -10.1865],
        [  6.8797,  -8.6423],
        [  8.0478, -10.9396],
        [  6.9517,  -8.6263],
        [  0.6679,  -2.1139],
        [  3.5512,  -5.6483],
        [  6.8666,  -8.2568],
        [  8.1861,  -9.8171],
        [ -1.4163,   0.0217],
        [  1.1379,  -2.6992],
        [  0.2482,  -1.7801],
        [  7.8119, -10.0525],
        [  0.9332,  -3.3572],
        [ -0.9062,  -2.0730],
        [  0.7820,  -2.6822],
        [  8.4369,  -9.8823],
        [  1.5516,  -2.9836],
        [  8.3702, -10.7453],
        [  7.8864,  -9.2816],
        [  7.1794,  -8.5830],
        [ -3.8347,   2.2388],
        [  6.2248,  -8.7332],
        [  6.9202,  -8.4655],
        [  7.3424,  -8.7300],
        [ -4.7355,   3.0066],
        [ -1.7398,   0.2479],
        [  7.7245,  -9.6515],
        [  7.8367, -10.4046],
        [  6.2468,  -8.6000],
        [  7.8847,  -9.4385],
        [  8.7137, -10.2506],
        [  1.7202,  -3.1752],
        [  1.4573,  -3.0549],
        [  1.4516,  -2.9044],
        [  6.1102,  -8.1365],
        [  1.3914,  -4.0600],
        [  7.6939,  -9.8672],
        [ -2.6596,   1.0970],
        [  7.9889, -10.1288],
        [  8.4346, -10.3513],
        [  8.4683,  -9.8610],
        [  8.0514,  -9.9893],
        [  8.3913,  -9.7776],
        [ -0.0307,  -2.0662]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6698, 0.3302],
        [0.3607, 0.6393]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8375, 0.1625], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2143, 0.0992],
         [0.8687, 0.4086]],

        [[0.3736, 0.1032],
         [0.5286, 0.9634]],

        [[0.3715, 0.1025],
         [0.5504, 0.1905]],

        [[0.9751, 0.0910],
         [0.8002, 0.2922]],

        [[0.8861, 0.1082],
         [0.6900, 0.9149]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 34
Adjusted Rand Index: 0.09757613424487259
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5406967593831985
Average Adjusted Rand Index: 0.8195152268489746
Iteration 0: Loss = -20843.316330501984
Iteration 10: Loss = -12237.356557677287
Iteration 20: Loss = -11701.310557950075
Iteration 30: Loss = -11428.467409144867
Iteration 40: Loss = -11428.467409664923
1
Iteration 50: Loss = -11428.467409664923
2
Iteration 60: Loss = -11428.467409664923
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6925, 0.3075],
        [0.2279, 0.7721]], dtype=torch.float64)
alpha: tensor([0.4630, 0.5370])
beta: tensor([[[0.4035, 0.0809],
         [0.1640, 0.2014]],

        [[0.7236, 0.1035],
         [0.0969, 0.4173]],

        [[0.6261, 0.1026],
         [0.8414, 0.8072]],

        [[0.3439, 0.0909],
         [0.1473, 0.9351]],

        [[0.2934, 0.1086],
         [0.8400, 0.1279]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20843.248479785212
Iteration 100: Loss = -12235.56079828072
Iteration 200: Loss = -12181.784830350334
Iteration 300: Loss = -11576.669447119653
Iteration 400: Loss = -11510.638196890568
Iteration 500: Loss = -11488.874618213256
Iteration 600: Loss = -11487.813125954752
Iteration 700: Loss = -11473.472648566183
Iteration 800: Loss = -11462.605425756836
Iteration 900: Loss = -11462.44845486555
Iteration 1000: Loss = -11462.339390591858
Iteration 1100: Loss = -11462.258999796612
Iteration 1200: Loss = -11462.197616409938
Iteration 1300: Loss = -11462.14941054282
Iteration 1400: Loss = -11462.110726502586
Iteration 1500: Loss = -11462.079029434099
Iteration 1600: Loss = -11462.052716042312
Iteration 1700: Loss = -11462.03074586347
Iteration 1800: Loss = -11462.011678125258
Iteration 1900: Loss = -11461.995425161833
Iteration 2000: Loss = -11461.982405982732
Iteration 2100: Loss = -11461.9688426219
Iteration 2200: Loss = -11461.95764084007
Iteration 2300: Loss = -11461.961091922974
1
Iteration 2400: Loss = -11461.936661678641
Iteration 2500: Loss = -11461.927104937278
Iteration 2600: Loss = -11461.919818982675
Iteration 2700: Loss = -11461.913700386129
Iteration 2800: Loss = -11461.90833841206
Iteration 2900: Loss = -11461.903611823614
Iteration 3000: Loss = -11461.899292050644
Iteration 3100: Loss = -11461.895475580572
Iteration 3200: Loss = -11461.891940281335
Iteration 3300: Loss = -11461.889955298051
Iteration 3400: Loss = -11461.885759414732
Iteration 3500: Loss = -11461.883102426867
Iteration 3600: Loss = -11461.880662206198
Iteration 3700: Loss = -11461.878532822433
Iteration 3800: Loss = -11461.876315409876
Iteration 3900: Loss = -11461.874391909902
Iteration 4000: Loss = -11461.874661088386
1
Iteration 4100: Loss = -11461.870969815442
Iteration 4200: Loss = -11461.86944888283
Iteration 4300: Loss = -11461.868056334635
Iteration 4400: Loss = -11461.86689033244
Iteration 4500: Loss = -11461.865496095294
Iteration 4600: Loss = -11461.864360111278
Iteration 4700: Loss = -11461.876149940374
1
Iteration 4800: Loss = -11461.862308353891
Iteration 4900: Loss = -11461.861421672887
Iteration 5000: Loss = -11461.860518735186
Iteration 5100: Loss = -11461.859738553965
Iteration 5200: Loss = -11461.858987169879
Iteration 5300: Loss = -11461.858262553458
Iteration 5400: Loss = -11461.85868194998
1
Iteration 5500: Loss = -11461.8569839353
Iteration 5600: Loss = -11461.856372789154
Iteration 5700: Loss = -11461.858626795154
1
Iteration 5800: Loss = -11461.855319225708
Iteration 5900: Loss = -11461.855035087723
Iteration 6000: Loss = -11461.856010968208
1
Iteration 6100: Loss = -11461.853978946832
Iteration 6200: Loss = -11461.855485124088
1
Iteration 6300: Loss = -11461.853417436616
Iteration 6400: Loss = -11461.852833043195
Iteration 6500: Loss = -11461.852854009596
1
Iteration 6600: Loss = -11461.854296031375
2
Iteration 6700: Loss = -11461.854702468894
3
Iteration 6800: Loss = -11451.127618268683
Iteration 6900: Loss = -11451.124572236744
Iteration 7000: Loss = -11451.127147418507
1
Iteration 7100: Loss = -11451.125018882143
2
Iteration 7200: Loss = -11451.123378934137
Iteration 7300: Loss = -11451.1337450961
1
Iteration 7400: Loss = -11451.123212916304
Iteration 7500: Loss = -11451.122823954942
Iteration 7600: Loss = -11451.126888330178
1
Iteration 7700: Loss = -11451.131421863725
2
Iteration 7800: Loss = -11451.129657627507
3
Iteration 7900: Loss = -11451.126633239688
4
Iteration 8000: Loss = -11451.122894824202
5
Iteration 8100: Loss = -11451.12750516025
6
Iteration 8200: Loss = -11451.138487071412
7
Iteration 8300: Loss = -11451.129558467766
8
Iteration 8400: Loss = -11451.12147897851
Iteration 8500: Loss = -11451.130358832776
1
Iteration 8600: Loss = -11451.156420854855
2
Iteration 8700: Loss = -11451.13292745546
3
Iteration 8800: Loss = -11451.125541252917
4
Iteration 8900: Loss = -11451.123439898822
5
Iteration 9000: Loss = -11451.121553925997
6
Iteration 9100: Loss = -11451.185621415485
7
Iteration 9200: Loss = -11451.124849022328
8
Iteration 9300: Loss = -11451.133074264411
9
Iteration 9400: Loss = -11451.137235862536
10
Stopping early at iteration 9400 due to no improvement.
tensor([[ -7.5107,   5.9564],
        [  7.3034,  -8.8098],
        [ -7.5602,   6.1551],
        [ -7.0600,   5.6710],
        [ -7.5527,   5.8350],
        [  5.9358,  -7.9774],
        [  6.5492,  -7.9491],
        [ -8.0041,   6.2165],
        [ -7.5726,   6.1211],
        [  7.0411,  -9.1094],
        [ -7.3247,   5.8590],
        [ -6.9628,   5.2705],
        [  6.2334,  -7.6379],
        [  2.8963,  -4.3173],
        [  6.4497,  -8.5882],
        [ -8.3517,   5.3208],
        [ -7.5062,   6.1182],
        [  5.8204,  -7.3694],
        [ -7.7288,   6.2932],
        [  6.9220,  -9.6222],
        [  6.1919,  -7.6993],
        [  5.8565,  -7.9668],
        [ -7.9313,   6.2534],
        [ -6.8443,   5.4571],
        [  6.1003,  -9.8809],
        [ -7.6570,   6.2707],
        [ -7.8861,   6.2877],
        [  6.1888,  -7.5767],
        [  6.4834,  -8.1987],
        [  5.9430,  -8.1283],
        [ -7.8151,   5.7576],
        [ -8.4920,   4.5747],
        [ -9.7168,   5.3039],
        [  6.0434,  -7.8085],
        [  6.9302,  -8.3373],
        [ -6.2030,   4.5999],
        [  5.9635,  -8.3187],
        [ -6.6489,   5.1061],
        [  5.9198,  -7.3636],
        [  6.1882,  -8.3388],
        [ -7.8615,   6.2481],
        [  5.2580,  -6.8763],
        [ -7.1586,   5.6903],
        [ -8.0192,   5.8730],
        [ -8.5784,   6.6869],
        [  6.9688,  -8.4029],
        [ -7.5773,   6.0530],
        [  6.1229,  -7.7622],
        [ -9.0778,   5.8306],
        [ -7.8882,   6.1080],
        [  6.5594,  -8.0418],
        [  5.5482,  -7.2443],
        [  3.8280,  -5.5577],
        [  7.2227,  -8.6610],
        [  6.3712,  -8.1568],
        [ -7.5164,   5.8618],
        [  6.6690,  -8.1974],
        [ -7.5237,   5.5680],
        [  2.8726,  -4.3933],
        [  3.5446,  -4.9427],
        [ -9.4626,   6.7939],
        [ -6.8175,   4.7641],
        [  5.6280,  -8.3811],
        [  6.4948,  -8.0598],
        [ -7.2430,   5.7874],
        [ -7.0062,   5.3688],
        [ -7.4035,   5.8889],
        [  5.9235,  -7.3452],
        [-10.0040,   5.4030],
        [ -7.0037,   5.5240],
        [ -7.9738,   5.8144],
        [  7.2265,  -8.6191],
        [ -6.9837,   5.1930],
        [  6.7366,  -8.6682],
        [  6.1794,  -7.6148],
        [  6.0436,  -7.4475],
        [ -8.3655,   5.8869],
        [  5.7382,  -7.6153],
        [  6.6030,  -8.1799],
        [  6.5330,  -8.1507],
        [ -7.5902,   6.0236],
        [ -8.0017,   6.4726],
        [  1.4704,  -3.6658],
        [  7.4109, -10.7908],
        [  6.4315,  -7.9638],
        [  5.9094,  -8.3800],
        [  6.5080,  -8.0472],
        [ -8.0403,   5.6645],
        [ -7.5762,   5.8405],
        [ -7.7850,   6.2822],
        [  6.0016,  -7.8359],
        [ -8.0511,   6.5506],
        [ -9.1955,   5.9901],
        [ -7.7112,   6.0767],
        [  5.9632,  -7.4150],
        [  6.2304,  -7.6270],
        [  6.3261,  -7.8348],
        [ -7.6921,   6.0315],
        [  6.4318, -11.0470],
        [ -7.4929,   5.9988]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6759, 0.3241],
        [0.2316, 0.7684]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5099, 0.4901], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4172, 0.0809],
         [0.1640, 0.2016]],

        [[0.7236, 0.1149],
         [0.0969, 0.4173]],

        [[0.6261, 0.1028],
         [0.8414, 0.8072]],

        [[0.3439, 0.0919],
         [0.1473, 0.9351]],

        [[0.2934, 0.1092],
         [0.8400, 0.1279]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760956718354579
Average Adjusted Rand Index: 0.9764840192712029
Iteration 0: Loss = -24158.69182679647
Iteration 10: Loss = -12238.299003131762
Iteration 20: Loss = -12216.751248183116
Iteration 30: Loss = -11428.46768473587
Iteration 40: Loss = -11428.467409959803
Iteration 50: Loss = -11428.467409664923
Iteration 60: Loss = -11428.467409664923
1
Iteration 70: Loss = -11428.467409664923
2
Iteration 80: Loss = -11428.467409664923
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.6925, 0.3075],
        [0.2279, 0.7721]], dtype=torch.float64)
alpha: tensor([0.4630, 0.5370])
beta: tensor([[[0.4035, 0.0809],
         [0.2844, 0.2014]],

        [[0.6276, 0.1035],
         [0.7018, 0.6898]],

        [[0.4543, 0.1026],
         [0.1258, 0.6697]],

        [[0.2172, 0.0909],
         [0.9880, 0.8388]],

        [[0.1767, 0.1086],
         [0.5362, 0.9364]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24158.53392432698
Iteration 100: Loss = -12242.781773942539
Iteration 200: Loss = -12235.674658529155
Iteration 300: Loss = -12213.734385138567
Iteration 400: Loss = -11902.639622629236
Iteration 500: Loss = -11891.155210580868
Iteration 600: Loss = -11890.265769086627
Iteration 700: Loss = -11889.803779700647
Iteration 800: Loss = -11877.449343818458
Iteration 900: Loss = -11877.178750255636
Iteration 1000: Loss = -11876.928082323448
Iteration 1100: Loss = -11867.903306907363
Iteration 1200: Loss = -11867.583223064184
Iteration 1300: Loss = -11867.0929203904
Iteration 1400: Loss = -11866.68604855395
Iteration 1500: Loss = -11866.279567770847
Iteration 1600: Loss = -11865.95860628522
Iteration 1700: Loss = -11864.437701601359
Iteration 1800: Loss = -11849.843477295983
Iteration 1900: Loss = -11834.939677710034
Iteration 2000: Loss = -11797.585160439723
Iteration 2100: Loss = -11797.239540106248
Iteration 2200: Loss = -11777.604683017214
Iteration 2300: Loss = -11777.32742931497
Iteration 2400: Loss = -11737.818255226664
Iteration 2500: Loss = -11723.56756518212
Iteration 2600: Loss = -11703.635940059146
Iteration 2700: Loss = -11696.936962629532
Iteration 2800: Loss = -11696.904037419776
Iteration 2900: Loss = -11696.880465525452
Iteration 3000: Loss = -11696.86157928204
Iteration 3100: Loss = -11696.842084339272
Iteration 3200: Loss = -11696.773491675416
Iteration 3300: Loss = -11662.432411182563
Iteration 3400: Loss = -11662.35884298049
Iteration 3500: Loss = -11662.348456575588
Iteration 3600: Loss = -11662.339800504513
Iteration 3700: Loss = -11662.32982444688
Iteration 3800: Loss = -11662.31603259277
Iteration 3900: Loss = -11662.289240956181
Iteration 4000: Loss = -11636.03420127167
Iteration 4100: Loss = -11635.947433489911
Iteration 4200: Loss = -11635.780772066602
Iteration 4300: Loss = -11635.765122199275
Iteration 4400: Loss = -11635.761590415306
Iteration 4500: Loss = -11635.765838865049
1
Iteration 4600: Loss = -11635.755624287907
Iteration 4700: Loss = -11635.75452783974
Iteration 4800: Loss = -11635.75089239063
Iteration 4900: Loss = -11635.748668832517
Iteration 5000: Loss = -11635.764113432286
1
Iteration 5100: Loss = -11635.752511770768
2
Iteration 5200: Loss = -11635.742003563493
Iteration 5300: Loss = -11635.740525877954
Iteration 5400: Loss = -11635.738986320745
Iteration 5500: Loss = -11635.738539197631
Iteration 5600: Loss = -11635.736691998507
Iteration 5700: Loss = -11635.746837611217
1
Iteration 5800: Loss = -11635.734762353142
Iteration 5900: Loss = -11635.733821670887
Iteration 6000: Loss = -11635.737315528411
1
Iteration 6100: Loss = -11635.732267635614
Iteration 6200: Loss = -11635.734082989167
1
Iteration 6300: Loss = -11635.731019163546
Iteration 6400: Loss = -11635.730431507556
Iteration 6500: Loss = -11635.729763959514
Iteration 6600: Loss = -11635.729604940825
Iteration 6700: Loss = -11635.732843675913
1
Iteration 6800: Loss = -11635.728381744384
Iteration 6900: Loss = -11635.730120554848
1
Iteration 7000: Loss = -11635.731191501302
2
Iteration 7100: Loss = -11635.72689754239
Iteration 7200: Loss = -11635.726561915553
Iteration 7300: Loss = -11635.747596546695
1
Iteration 7400: Loss = -11635.7322770294
2
Iteration 7500: Loss = -11635.732824848214
3
Iteration 7600: Loss = -11635.734069655984
4
Iteration 7700: Loss = -11635.727278928096
5
Iteration 7800: Loss = -11635.723310612544
Iteration 7900: Loss = -11635.72375517651
1
Iteration 8000: Loss = -11635.730757372176
2
Iteration 8100: Loss = -11635.731385496872
3
Iteration 8200: Loss = -11635.723854584936
4
Iteration 8300: Loss = -11635.72248141821
Iteration 8400: Loss = -11635.722071832053
Iteration 8500: Loss = -11635.72176845723
Iteration 8600: Loss = -11635.721913497402
1
Iteration 8700: Loss = -11635.722051380548
2
Iteration 8800: Loss = -11635.721192626424
Iteration 8900: Loss = -11635.720944902407
Iteration 9000: Loss = -11635.720847082666
Iteration 9100: Loss = -11635.720461694262
Iteration 9200: Loss = -11635.689583800166
Iteration 9300: Loss = -11635.690882646868
1
Iteration 9400: Loss = -11635.686748898826
Iteration 9500: Loss = -11635.687943845329
1
Iteration 9600: Loss = -11635.688852358284
2
Iteration 9700: Loss = -11631.826788656375
Iteration 9800: Loss = -11631.819983672081
Iteration 9900: Loss = -11631.918530794273
1
Iteration 10000: Loss = -11631.818166075513
Iteration 10100: Loss = -11631.818502405631
1
Iteration 10200: Loss = -11631.822086597414
2
Iteration 10300: Loss = -11631.898660205165
3
Iteration 10400: Loss = -11631.815942999012
Iteration 10500: Loss = -11631.82991659216
1
Iteration 10600: Loss = -11631.815869715594
Iteration 10700: Loss = -11631.81933695832
1
Iteration 10800: Loss = -11631.815920668922
2
Iteration 10900: Loss = -11631.81629865052
3
Iteration 11000: Loss = -11631.946482533953
4
Iteration 11100: Loss = -11631.815903277704
5
Iteration 11200: Loss = -11631.817930438496
6
Iteration 11300: Loss = -11631.820367354676
7
Iteration 11400: Loss = -11631.815620450985
Iteration 11500: Loss = -11631.81768954599
1
Iteration 11600: Loss = -11631.832457373825
2
Iteration 11700: Loss = -11631.869120587002
3
Iteration 11800: Loss = -11631.815913520359
4
Iteration 11900: Loss = -11631.818176067412
5
Iteration 12000: Loss = -11631.941975774254
6
Iteration 12100: Loss = -11631.816320730257
7
Iteration 12200: Loss = -11631.816049467605
8
Iteration 12300: Loss = -11631.828694714399
9
Iteration 12400: Loss = -11631.970039925462
10
Stopping early at iteration 12400 due to no improvement.
tensor([[ -7.2075,   4.8911],
        [  6.7778,  -8.1784],
        [ -7.6270,   6.2345],
        [ -7.3544,   5.9610],
        [ -8.5376,   6.3700],
        [  6.1562,  -7.9498],
        [  5.6573,  -9.0308],
        [ -7.7984,   6.4109],
        [ -8.7270,   4.9715],
        [  7.1386,  -8.5480],
        [ -7.1697,   5.5392],
        [ -6.6912,   5.2187],
        [  5.7783,  -7.2263],
        [  2.8886,  -4.2955],
        [  7.3223,  -9.1540],
        [ -7.0121,   5.3842],
        [ -7.9650,   6.3056],
        [  4.7778,  -8.6407],
        [ -7.8247,   6.3939],
        [  7.0357,  -8.4706],
        [  6.8737, -10.3672],
        [  6.1504,  -7.6015],
        [ -7.9788,   6.5617],
        [ -7.2002,   4.4833],
        [  6.6022,  -8.4004],
        [ -9.1538,   4.6003],
        [ -8.0572,   6.0750],
        [  6.4400,  -7.8494],
        [  4.9555,  -9.5708],
        [  6.1776,  -8.6723],
        [ -7.3539,   5.8861],
        [ -6.4169,   4.6542],
        [ -8.8793,   6.6928],
        [  6.5923,  -7.9952],
        [  7.1280, -10.0494],
        [ -5.4748,   3.9299],
        [  7.1020,  -8.4884],
        [ -7.1432,   5.3348],
        [  6.2734,  -7.6813],
        [  6.6594,  -8.1854],
        [ -8.0577,   6.5843],
        [  5.9464,  -7.4105],
        [ -5.6086,   3.7450],
        [ -7.5815,   6.1545],
        [ -5.4443,   3.8963],
        [  7.0398,  -8.4350],
        [ -7.0699,   4.9943],
        [  6.1385,  -8.3507],
        [ -7.8637,   6.4154],
        [ -7.2590,   5.7772],
        [  6.6340,  -8.0806],
        [  5.7553,  -7.6417],
        [  3.9545,  -5.7541],
        [  6.6532,  -8.0927],
        [  7.2153,  -9.2430],
        [ -7.7975,   5.2603],
        [  7.2054,  -8.6556],
        [ -7.1858,   5.4937],
        [  1.9207,  -5.7696],
        [  5.6608,  -7.7653],
        [ -8.0993,   6.6586],
        [ -5.8485,   4.4494],
        [  6.5525,  -8.4365],
        [  6.9757,  -9.1837],
        [ -7.3348,   5.8114],
        [ -6.2993,   4.8671],
        [ -7.4738,   6.0586],
        [  6.3841,  -7.8958],
        [ -8.1035,   6.6959],
        [ -7.5897,   5.5838],
        [ -7.6214,   6.2342],
        [  6.7487,  -8.9403],
        [ -6.2839,   4.8937],
        [  6.5896,  -7.9838],
        [  6.6128,  -8.8738],
        [  6.3335,  -8.2529],
        [ -8.5024,   6.7445],
        [  6.5858, -10.5564],
        [  6.1328,  -8.2571],
        [  7.0875,  -8.9683],
        [ -7.2106,   5.8071],
        [ -9.3733,   4.7581],
        [  1.4634,  -3.9538],
        [  6.7160,  -9.1213],
        [  6.4706,  -8.6493],
        [  6.6164,  -8.2488],
        [  7.3497, -10.1272],
        [ -5.1786,   3.7877],
        [ -8.3408,   6.2882],
        [ -8.0093,   6.6187],
        [  6.0957,  -8.0916],
        [ -7.9530,   6.4519],
        [ -7.7394,   6.1176],
        [ -7.9878,   6.4166],
        [  6.1442,  -7.6722],
        [  6.4879,  -7.9810],
        [  6.4386,  -7.9289],
        [ -8.9185,   6.5604],
        [  6.4211,  -8.3041],
        [ -7.9491,   6.0037]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5192, 0.4808],
        [0.2921, 0.7079]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5099, 0.4901], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4021, 0.0809],
         [0.2844, 0.2135]],

        [[0.6276, 0.1037],
         [0.7018, 0.6898]],

        [[0.4543, 0.1030],
         [0.1258, 0.6697]],

        [[0.2172, 0.0965],
         [0.9880, 0.8388]],

        [[0.1767, 0.1091],
         [0.5362, 0.9364]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.024095957982895065
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
Global Adjusted Rand Index: 0.5706346675933257
Average Adjusted Rand Index: 0.7968097774091043
Iteration 0: Loss = -26273.350842552336
Iteration 10: Loss = -12238.302308403974
Iteration 20: Loss = -12238.302352452332
1
Iteration 30: Loss = -12237.297437431527
Iteration 40: Loss = -11993.737892294594
Iteration 50: Loss = -11428.467389114929
Iteration 60: Loss = -11428.467409664923
1
Iteration 70: Loss = -11428.467409664923
2
Iteration 80: Loss = -11428.467409664923
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.6925, 0.3075],
        [0.2279, 0.7721]], dtype=torch.float64)
alpha: tensor([0.4630, 0.5370])
beta: tensor([[[0.4035, 0.0809],
         [0.1225, 0.2014]],

        [[0.4813, 0.1035],
         [0.8698, 0.1587]],

        [[0.1633, 0.1026],
         [0.3993, 0.4983]],

        [[0.2235, 0.0909],
         [0.9087, 0.9729]],

        [[0.9620, 0.1086],
         [0.1909, 0.5548]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26274.92768173011
Iteration 100: Loss = -12241.55398718804
Iteration 200: Loss = -12191.504807841668
Iteration 300: Loss = -12013.235943745494
Iteration 400: Loss = -11720.000782785104
Iteration 500: Loss = -11597.069207195966
Iteration 600: Loss = -11578.32936865652
Iteration 700: Loss = -11575.384596833921
Iteration 800: Loss = -11574.33889911649
Iteration 900: Loss = -11568.282282448461
Iteration 1000: Loss = -11563.667112540183
Iteration 1100: Loss = -11549.151306606485
Iteration 1200: Loss = -11535.431611747827
Iteration 1300: Loss = -11524.656262822225
Iteration 1400: Loss = -11513.29332855223
Iteration 1500: Loss = -11508.768420778975
Iteration 1600: Loss = -11494.994636566813
Iteration 1700: Loss = -11478.851371048155
Iteration 1800: Loss = -11478.776067015073
Iteration 1900: Loss = -11478.717874962098
Iteration 2000: Loss = -11478.648457349442
Iteration 2100: Loss = -11470.167215788306
Iteration 2200: Loss = -11470.13173918741
Iteration 2300: Loss = -11470.102111511165
Iteration 2400: Loss = -11470.076664768274
Iteration 2500: Loss = -11470.054683289096
Iteration 2600: Loss = -11470.035438225415
Iteration 2700: Loss = -11470.018376269354
Iteration 2800: Loss = -11470.003176847742
Iteration 2900: Loss = -11469.989632106559
Iteration 3000: Loss = -11469.977706755893
Iteration 3100: Loss = -11469.966360692748
Iteration 3200: Loss = -11469.956279613703
Iteration 3300: Loss = -11469.947442568704
Iteration 3400: Loss = -11469.93838290702
Iteration 3500: Loss = -11469.933596232862
Iteration 3600: Loss = -11469.923491187401
Iteration 3700: Loss = -11469.917624880647
Iteration 3800: Loss = -11469.910188394566
Iteration 3900: Loss = -11469.90218299751
Iteration 4000: Loss = -11469.881739465798
Iteration 4100: Loss = -11466.552098779885
Iteration 4200: Loss = -11466.548824113483
Iteration 4300: Loss = -11466.542103752132
Iteration 4400: Loss = -11466.536959058516
Iteration 4500: Loss = -11466.529778837494
Iteration 4600: Loss = -11466.51322188996
Iteration 4700: Loss = -11466.50388327675
Iteration 4800: Loss = -11466.500816809026
Iteration 4900: Loss = -11466.498589908486
Iteration 5000: Loss = -11466.495814643957
Iteration 5100: Loss = -11466.493396301596
Iteration 5200: Loss = -11466.490122472704
Iteration 5300: Loss = -11466.45464240154
Iteration 5400: Loss = -11457.388123953935
Iteration 5500: Loss = -11457.386180239173
Iteration 5600: Loss = -11457.384114154098
Iteration 5700: Loss = -11457.383021599382
Iteration 5800: Loss = -11457.403897059736
1
Iteration 5900: Loss = -11457.381802648993
Iteration 6000: Loss = -11457.38098493846
Iteration 6100: Loss = -11457.37727403887
Iteration 6200: Loss = -11457.376897967366
Iteration 6300: Loss = -11457.391330673023
1
Iteration 6400: Loss = -11457.374146387083
Iteration 6500: Loss = -11457.38725291566
1
Iteration 6600: Loss = -11457.372278664607
Iteration 6700: Loss = -11457.383976731384
1
Iteration 6800: Loss = -11457.376507038702
2
Iteration 6900: Loss = -11457.369925886147
Iteration 7000: Loss = -11457.369266204301
Iteration 7100: Loss = -11457.371289839703
1
Iteration 7200: Loss = -11457.368870173534
Iteration 7300: Loss = -11457.374542377966
1
Iteration 7400: Loss = -11457.371316270915
2
Iteration 7500: Loss = -11457.368925617064
3
Iteration 7600: Loss = -11457.365973993094
Iteration 7700: Loss = -11457.370554073672
1
Iteration 7800: Loss = -11457.370560880558
2
Iteration 7900: Loss = -11457.364992723496
Iteration 8000: Loss = -11449.699085224967
Iteration 8100: Loss = -11449.684188243937
Iteration 8200: Loss = -11449.680553679764
Iteration 8300: Loss = -11449.678678416036
Iteration 8400: Loss = -11449.683177574088
1
Iteration 8500: Loss = -11449.680103364439
2
Iteration 8600: Loss = -11449.6786973946
3
Iteration 8700: Loss = -11449.677186015166
Iteration 8800: Loss = -11449.679971881244
1
Iteration 8900: Loss = -11449.675848120296
Iteration 9000: Loss = -11449.701178035391
1
Iteration 9100: Loss = -11449.679891161957
2
Iteration 9200: Loss = -11449.67246500257
Iteration 9300: Loss = -11449.672196995014
Iteration 9400: Loss = -11449.681839437953
1
Iteration 9500: Loss = -11449.670996524334
Iteration 9600: Loss = -11449.670176941501
Iteration 9700: Loss = -11449.672095244263
1
Iteration 9800: Loss = -11449.670446227929
2
Iteration 9900: Loss = -11449.709394742627
3
Iteration 10000: Loss = -11449.671248220402
4
Iteration 10100: Loss = -11449.671260169967
5
Iteration 10200: Loss = -11449.691662397021
6
Iteration 10300: Loss = -11449.718699698467
7
Iteration 10400: Loss = -11449.695145562298
8
Iteration 10500: Loss = -11449.718492497926
9
Iteration 10600: Loss = -11449.758177714833
10
Stopping early at iteration 10600 due to no improvement.
tensor([[-6.6235,  5.1938],
        [ 6.5354, -8.0595],
        [-8.1144,  5.3306],
        [-7.0682,  5.6544],
        [-8.2373,  5.8042],
        [ 5.9374, -8.4405],
        [ 6.6319, -8.0950],
        [-7.1909,  5.7282],
        [-7.5104,  5.8826],
        [ 7.0047, -8.4209],
        [-7.7936,  5.8765],
        [-8.5608,  6.9831],
        [ 5.6285, -7.0476],
        [ 2.7494, -4.5093],
        [ 7.5034, -9.0306],
        [-7.9877,  5.7472],
        [-7.1785,  5.7794],
        [ 5.7636, -7.2589],
        [-7.2928,  5.8015],
        [ 7.0173, -8.5736],
        [ 6.2788, -7.6670],
        [ 6.4847, -8.2864],
        [-8.9129,  6.6773],
        [-6.6732,  5.2840],
        [ 6.7016, -9.0652],
        [-8.6511,  5.2846],
        [-8.0957,  5.6608],
        [ 5.9212, -7.5308],
        [ 6.4955, -8.8249],
        [ 6.2911, -8.7057],
        [-8.2984,  5.6346],
        [-6.6342,  5.2240],
        [-7.7121,  6.2251],
        [ 7.0533, -9.1348],
        [ 6.6924, -8.1720],
        [-6.1696,  4.5397],
        [ 6.4881, -8.6091],
        [-6.9979,  5.6104],
        [ 5.9020, -7.3185],
        [ 5.8979, -9.4009],
        [-7.7688,  5.9235],
        [ 5.0613, -7.7164],
        [-5.8036,  4.2043],
        [-8.0540,  6.4059],
        [-5.7051,  4.3167],
        [ 6.8942, -8.3039],
        [-6.7489,  5.3028],
        [ 7.0969, -8.6439],
        [-7.3013,  5.9065],
        [-7.0223,  5.5738],
        [ 6.2406, -7.7562],
        [ 5.8876, -7.2790],
        [ 5.9853, -8.7861],
        [ 6.4047, -8.1842],
        [ 6.2542, -9.1366],
        [-7.1807,  5.6875],
        [ 6.6338, -8.0201],
        [-6.8091,  5.1463],
        [ 2.4837, -4.8619],
        [ 4.5084, -5.9615],
        [-7.9199,  6.4720],
        [-6.5976,  5.1997],
        [ 6.0932, -7.9768],
        [ 7.1228, -9.2102],
        [-7.9477,  5.7813],
        [-7.0492,  5.1091],
        [-7.5919,  5.9619],
        [ 5.8558, -7.6505],
        [-7.7999,  6.3297],
        [-7.7890,  5.2008],
        [-8.0971,  6.4237],
        [ 6.6385, -8.0453],
        [-6.8580,  4.8589],
        [ 6.2623, -7.6667],
        [ 6.4311, -7.8977],
        [ 6.7257, -9.1943],
        [-7.7124,  6.2454],
        [ 6.7982, -8.6392],
        [ 6.2979, -8.0393],
        [ 7.1104, -9.5564],
        [-7.1355,  5.5716],
        [-7.8919,  6.4090],
        [ 5.9694, -7.8645],
        [ 6.4996, -7.9282],
        [ 5.5313, -9.9862],
        [ 6.2015, -8.2686],
        [ 6.9234, -8.5083],
        [-6.8167,  5.1168],
        [-7.1123,  5.7255],
        [-7.8351,  6.1381],
        [ 5.9870, -8.2147],
        [-8.0194,  6.1206],
        [-7.6715,  5.7710],
        [-8.6669,  6.1061],
        [ 5.9748, -7.4509],
        [ 6.1381, -8.5659],
        [ 6.3180, -7.8170],
        [-7.8949,  6.4758],
        [ 6.8111, -9.3248],
        [-7.9652,  5.9595]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6779, 0.3221],
        [0.2312, 0.7688]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4166, 0.0808],
         [0.1225, 0.2021]],

        [[0.4813, 0.1089],
         [0.8698, 0.1587]],

        [[0.1633, 0.1070],
         [0.3993, 0.4983]],

        [[0.2235, 0.0922],
         [0.9087, 0.9729]],

        [[0.9620, 0.1092],
         [0.1909, 0.5548]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840317580481719
Average Adjusted Rand Index: 0.9839985580570193
11439.518184010527
new:  [0.5406967593831985, 0.9760956718354579, 0.5706346675933257, 0.9840317580481719] [0.8195152268489746, 0.9764840192712029, 0.7968097774091043, 0.9839985580570193] [11648.404373901363, 11451.137235862536, 11631.970039925462, 11449.758177714833]
prior:  [1.0, 1.0, 1.0, 1.0] [1.0, 1.0, 1.0, 1.0] [11428.467408912717, 11428.467409664923, 11428.467409664923, 11428.467409664923]
-----------------------------------------------------------------------------------------
This iteration is 11
True Objective function: Loss = -11413.67858654651
Iteration 0: Loss = -20584.768132719888
Iteration 10: Loss = -11575.398459803242
Iteration 20: Loss = -11575.648027969897
1
Iteration 30: Loss = -11575.465767267418
2
Iteration 40: Loss = -11575.315952418547
Iteration 50: Loss = -11575.274298548396
Iteration 60: Loss = -11575.26576531127
Iteration 70: Loss = -11575.264192868022
Iteration 80: Loss = -11575.263899394251
Iteration 90: Loss = -11575.26385076112
Iteration 100: Loss = -11575.263845701353
Iteration 110: Loss = -11575.263840667536
Iteration 120: Loss = -11575.26384253011
1
Iteration 130: Loss = -11575.263842530487
2
Iteration 140: Loss = -11575.263842530487
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.7666, 0.2334],
        [0.3774, 0.6226]], dtype=torch.float64)
alpha: tensor([0.5928, 0.4072])
beta: tensor([[[0.2111, 0.0995],
         [0.4383, 0.3886]],

        [[0.2698, 0.0998],
         [0.0533, 0.0661]],

        [[0.8428, 0.1003],
         [0.8752, 0.7678]],

        [[0.8813, 0.1035],
         [0.7300, 0.2774]],

        [[0.0068, 0.1169],
         [0.0484, 0.4310]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 20
Adjusted Rand Index: 0.354551002206262
Global Adjusted Rand Index: 0.4612060500902698
Average Adjusted Rand Index: 0.8709102004412523
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20484.210096202725
Iteration 100: Loss = -11848.141145034246
Iteration 200: Loss = -11484.242511860171
Iteration 300: Loss = -11463.164141769144
Iteration 400: Loss = -11462.809045515098
Iteration 500: Loss = -11462.65109899293
Iteration 600: Loss = -11462.563923338143
Iteration 700: Loss = -11462.508666587284
Iteration 800: Loss = -11462.470217596141
Iteration 900: Loss = -11462.433686330614
Iteration 1000: Loss = -11443.071916554924
Iteration 1100: Loss = -11443.049721460888
Iteration 1200: Loss = -11443.034572329816
Iteration 1300: Loss = -11443.02296695331
Iteration 1400: Loss = -11443.013603698164
Iteration 1500: Loss = -11443.005735601733
Iteration 1600: Loss = -11442.938439832693
Iteration 1700: Loss = -11442.891166543035
Iteration 1800: Loss = -11442.88681343763
Iteration 1900: Loss = -11442.883056832932
Iteration 2000: Loss = -11442.87987049205
Iteration 2100: Loss = -11442.877086944562
Iteration 2200: Loss = -11442.874666026511
Iteration 2300: Loss = -11442.872524576484
Iteration 2400: Loss = -11442.87060844462
Iteration 2500: Loss = -11442.868912914004
Iteration 2600: Loss = -11442.867398302888
Iteration 2700: Loss = -11442.86608050628
Iteration 2800: Loss = -11442.864803805483
Iteration 2900: Loss = -11442.86369696764
Iteration 3000: Loss = -11442.862703962992
Iteration 3100: Loss = -11442.86180114728
Iteration 3200: Loss = -11442.86094083284
Iteration 3300: Loss = -11442.860193800127
Iteration 3400: Loss = -11442.859494866032
Iteration 3500: Loss = -11442.85885527603
Iteration 3600: Loss = -11427.359503320242
Iteration 3700: Loss = -11427.357783675292
Iteration 3800: Loss = -11427.357130377233
Iteration 3900: Loss = -11427.356503195268
Iteration 4000: Loss = -11427.356001625063
Iteration 4100: Loss = -11427.355535572902
Iteration 4200: Loss = -11427.355070951515
Iteration 4300: Loss = -11427.353426505935
Iteration 4400: Loss = -11417.847733291714
Iteration 4500: Loss = -11417.869464368481
1
Iteration 4600: Loss = -11417.847410060833
Iteration 4700: Loss = -11417.846652431857
Iteration 4800: Loss = -11417.846544866965
Iteration 4900: Loss = -11417.856766726525
1
Iteration 5000: Loss = -11417.846123933905
Iteration 5100: Loss = -11407.79815721
Iteration 5200: Loss = -11407.79026657897
Iteration 5300: Loss = -11407.789476531103
Iteration 5400: Loss = -11407.810748770738
1
Iteration 5500: Loss = -11407.788763996252
Iteration 5600: Loss = -11407.8001932925
1
Iteration 5700: Loss = -11407.792477989973
2
Iteration 5800: Loss = -11407.78993694306
3
Iteration 5900: Loss = -11407.792541802417
4
Iteration 6000: Loss = -11407.805777075215
5
Iteration 6100: Loss = -11407.804680212155
6
Iteration 6200: Loss = -11407.789325811395
7
Iteration 6300: Loss = -11407.787953438836
Iteration 6400: Loss = -11407.788749656283
1
Iteration 6500: Loss = -11407.79034783184
2
Iteration 6600: Loss = -11407.822537582551
3
Iteration 6700: Loss = -11407.806168800922
4
Iteration 6800: Loss = -11407.790333373909
5
Iteration 6900: Loss = -11407.787180972467
Iteration 7000: Loss = -11407.787754939092
1
Iteration 7100: Loss = -11407.87528875281
2
Iteration 7200: Loss = -11407.786992454914
Iteration 7300: Loss = -11407.78788175436
1
Iteration 7400: Loss = -11407.78967574753
2
Iteration 7500: Loss = -11407.790018878772
3
Iteration 7600: Loss = -11407.800711648348
4
Iteration 7700: Loss = -11407.790099727954
5
Iteration 7800: Loss = -11407.790758010771
6
Iteration 7900: Loss = -11407.79160941944
7
Iteration 8000: Loss = -11407.812515799042
8
Iteration 8100: Loss = -11407.79083394902
9
Iteration 8200: Loss = -11407.807996868009
10
Stopping early at iteration 8200 due to no improvement.
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|        | 12/100 [7:15:38<51:31:30, 2107.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|        | 13/100 [7:51:18<51:10:14, 2117.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|        | 14/100 [8:29:47<51:57:51, 2175.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|        | 15/100 [9:10:08<53:06:54, 2249.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|        | 16/100 [9:51:37<54:10:06, 2321.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|        | 17/100 [10:30:04<53:25:36, 2317.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|        | 18/100 [11:00:48<49:32:27, 2174.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|        | 19/100 [11:41:34<50:46:09, 2256.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|        | 20/100 [12:19:12<50:09:14, 2256.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|        | 21/100 [12:57:17<49:42:31, 2265.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|       | 22/100 [13:39:32<50:50:12, 2346.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
tensor([[  3.3441,  -7.9593],
        [ -7.0065,   2.3913],
        [ -8.7461,   4.1308],
        [ -7.2934,   2.6781],
        [-10.5263,   5.9111],
        [  3.9714,  -8.5867],
        [ -8.2496,   3.6344],
        [ -8.8621,   4.2469],
        [ -9.6509,   5.0357],
        [  4.7450,  -9.3603],
        [ -7.7842,   3.1690],
        [-10.6277,   6.0125],
        [ -8.7036,   4.0884],
        [  4.9497,  -9.5649],
        [  4.9029,  -9.5181],
        [  2.5061,  -7.1213],
        [  4.6151,  -9.2303],
        [  4.9928,  -9.6080],
        [ -1.1334,  -3.4818],
        [  4.7891,  -9.4043],
        [ -9.1486,   4.5334],
        [ -9.0911,   4.4759],
        [  1.6061,  -6.2213],
        [  0.4341,  -5.0493],
        [  3.8699,  -8.4852],
        [ -9.6500,   5.0348],
        [ -8.7020,   4.0868],
        [ -9.9778,   5.3626],
        [  3.6987,  -8.3140],
        [  4.4437,  -9.0589],
        [  5.3912, -10.0065],
        [ -9.2838,   4.6685],
        [ -9.0820,   4.4667],
        [ -5.5658,   0.9506],
        [  4.4772,  -9.0924],
        [  4.7569,  -9.3721],
        [ -6.6343,   2.0191],
        [ -8.9914,   4.3762],
        [  4.9269,  -9.5421],
        [ -8.8430,   4.2278],
        [  3.0709,  -7.6862],
        [ -0.0615,  -4.5538],
        [ -9.2655,   4.6503],
        [ -8.5041,   3.8888],
        [  3.4553,  -8.0705],
        [ -5.1738,   0.5586],
        [ -8.1634,   3.5482],
        [ -8.2717,   3.6564],
        [ -7.7452,   3.1300],
        [  2.4008,  -7.0160],
        [ -8.4599,   3.8447],
        [ -9.3489,   4.7336],
        [  4.0152,  -8.6304],
        [  3.8886,  -8.5038],
        [ -9.0967,   4.4815],
        [ -9.5183,   4.9030],
        [  4.0069,  -8.6221],
        [ -6.9581,   2.3429],
        [ -8.9341,   4.3189],
        [ -9.0447,   4.4295],
        [ -4.6472,   0.0320],
        [  5.2453,  -9.8606],
        [ -9.2723,   4.6571],
        [ -8.2075,   3.5923],
        [ -9.1720,   4.5568],
        [  4.5013,  -9.1165],
        [  4.4052,  -9.0204],
        [ -9.0357,   4.4205],
        [  1.2822,  -5.8974],
        [  6.1569, -10.7721],
        [ -7.5291,   2.9139],
        [  1.6955,  -6.3107],
        [ -9.6634,   5.0482],
        [ -7.0072,   2.3920],
        [  2.7302,  -7.3454],
        [  5.8694, -10.4846],
        [  4.6429,  -9.2581],
        [  6.3186, -10.9338],
        [ -8.0285,   3.4133],
        [ -9.3836,   4.7683],
        [  5.1809,  -9.7962],
        [ -8.0251,   3.4099],
        [ -9.8214,   5.2062],
        [ -9.5828,   4.9676],
        [ -7.2685,   2.6533],
        [ -8.7794,   4.1642],
        [ -6.7692,   2.1540],
        [  4.0835,  -8.6987],
        [ -8.9349,   4.3197],
        [ -9.5671,   4.9519],
        [ -8.5596,   3.9444],
        [ -9.0256,   4.4104],
        [ -8.9686,   4.3534],
        [ -7.0444,   2.4292],
        [  2.9692,  -7.5844],
        [  2.9728,  -7.5880],
        [ -9.0514,   4.4362],
        [  4.8322,  -9.4475],
        [ -9.3738,   4.7586],
        [  2.4382,  -7.0534]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7596, 0.2404],
        [0.2280, 0.7720]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4187, 0.5813], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4048, 0.0991],
         [0.4383, 0.2020]],

        [[0.2698, 0.0998],
         [0.0533, 0.0661]],

        [[0.8428, 0.1012],
         [0.8752, 0.7678]],

        [[0.8813, 0.1042],
         [0.7300, 0.2774]],

        [[0.0068, 0.0984],
         [0.0484, 0.4310]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919996253315568
Average Adjusted Rand Index: 0.9919993417272899
Iteration 0: Loss = -16611.16149558889
Iteration 10: Loss = -11602.555924671382
Iteration 20: Loss = -11409.819079129926
Iteration 30: Loss = -11409.819077550324
Iteration 40: Loss = -11409.819077550324
1
Iteration 50: Loss = -11409.819077550324
2
Iteration 60: Loss = -11409.819077550324
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7561, 0.2439],
        [0.2324, 0.7676]], dtype=torch.float64)
alpha: tensor([0.4534, 0.5466])
beta: tensor([[[0.3940, 0.0996],
         [0.2991, 0.1981]],

        [[0.9745, 0.0999],
         [0.7862, 0.5439]],

        [[0.0473, 0.1004],
         [0.0625, 0.2540]],

        [[0.1409, 0.1037],
         [0.9648, 0.4783]],

        [[0.0070, 0.0980],
         [0.7590, 0.7686]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919996253315568
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16610.83143377511
Iteration 100: Loss = -12110.009084809046
Iteration 200: Loss = -12087.421001165587
Iteration 300: Loss = -11562.48645302467
Iteration 400: Loss = -11480.61864539231
Iteration 500: Loss = -11472.050470533557
Iteration 600: Loss = -11468.829300051344
Iteration 700: Loss = -11455.21640061045
Iteration 800: Loss = -11455.034812944104
Iteration 900: Loss = -11439.806076872568
Iteration 1000: Loss = -11439.740105190844
Iteration 1100: Loss = -11439.693820652677
Iteration 1200: Loss = -11439.657710766165
Iteration 1300: Loss = -11439.625612119558
Iteration 1400: Loss = -11432.174226421805
Iteration 1500: Loss = -11432.139502043598
Iteration 1600: Loss = -11432.113846989123
Iteration 1700: Loss = -11432.082122993821
Iteration 1800: Loss = -11432.069397552781
Iteration 1900: Loss = -11432.059234942386
Iteration 2000: Loss = -11432.050311266297
Iteration 2100: Loss = -11432.042099062475
Iteration 2200: Loss = -11432.034910501527
Iteration 2300: Loss = -11432.029282007643
Iteration 2400: Loss = -11432.025002533204
Iteration 2500: Loss = -11432.020538077963
Iteration 2600: Loss = -11432.016865151101
Iteration 2700: Loss = -11432.013649983517
Iteration 2800: Loss = -11432.010586430459
Iteration 2900: Loss = -11432.007877869659
Iteration 3000: Loss = -11432.00923338928
1
Iteration 3100: Loss = -11432.007084391953
Iteration 3200: Loss = -11432.000343532076
Iteration 3300: Loss = -11431.995083833193
Iteration 3400: Loss = -11419.774983568715
Iteration 3500: Loss = -11419.713081649401
Iteration 3600: Loss = -11419.710819270978
Iteration 3700: Loss = -11419.71193935298
1
Iteration 3800: Loss = -11419.707595378248
Iteration 3900: Loss = -11419.706338662569
Iteration 4000: Loss = -11419.706668770143
1
Iteration 4100: Loss = -11419.83864163826
2
Iteration 4200: Loss = -11419.707622889762
3
Iteration 4300: Loss = -11419.70260662612
Iteration 4400: Loss = -11419.706383197119
1
Iteration 4500: Loss = -11419.70063399793
Iteration 4600: Loss = -11419.70019338794
Iteration 4700: Loss = -11419.714513664876
1
Iteration 4800: Loss = -11419.698849047267
Iteration 4900: Loss = -11419.70774883533
1
Iteration 5000: Loss = -11419.69789252489
Iteration 5100: Loss = -11419.695700695529
Iteration 5200: Loss = -11407.716254864226
Iteration 5300: Loss = -11407.725698074797
1
Iteration 5400: Loss = -11407.71416987593
Iteration 5500: Loss = -11407.71354340401
Iteration 5600: Loss = -11407.713150993313
Iteration 5700: Loss = -11407.712741579768
Iteration 5800: Loss = -11407.725523087154
1
Iteration 5900: Loss = -11407.712578958337
Iteration 6000: Loss = -11407.712748309781
1
Iteration 6100: Loss = -11407.711539366446
Iteration 6200: Loss = -11407.72440851478
1
Iteration 6300: Loss = -11407.718923122764
2
Iteration 6400: Loss = -11407.710955313816
Iteration 6500: Loss = -11407.71132166952
1
Iteration 6600: Loss = -11407.710418310155
Iteration 6700: Loss = -11407.711090044595
1
Iteration 6800: Loss = -11407.71008796499
Iteration 6900: Loss = -11407.71076933029
1
Iteration 7000: Loss = -11407.710127250257
2
Iteration 7100: Loss = -11407.710257940325
3
Iteration 7200: Loss = -11407.712415534246
4
Iteration 7300: Loss = -11407.710425965694
5
Iteration 7400: Loss = -11407.709676783252
Iteration 7500: Loss = -11407.711365387227
1
Iteration 7600: Loss = -11407.716590786838
2
Iteration 7700: Loss = -11407.788113632474
3
Iteration 7800: Loss = -11407.70893792698
Iteration 7900: Loss = -11407.710978469915
1
Iteration 8000: Loss = -11407.711422087857
2
Iteration 8100: Loss = -11407.73218117739
3
Iteration 8200: Loss = -11407.709160350672
4
Iteration 8300: Loss = -11407.710496764435
5
Iteration 8400: Loss = -11407.722664384088
6
Iteration 8500: Loss = -11407.743424768158
7
Iteration 8600: Loss = -11407.757222151742
8
Iteration 8700: Loss = -11407.734163659621
9
Iteration 8800: Loss = -11407.711802153352
10
Stopping early at iteration 8800 due to no improvement.
tensor([[  4.7604,  -6.2252],
        [ -6.7507,   2.7956],
        [ -7.0219,   5.6291],
        [ -5.6706,   4.2574],
        [ -9.7002,   5.0850],
        [  4.7920,  -8.5167],
        [ -7.7569,   5.2648],
        [ -8.2770,   5.9128],
        [ -9.5883,   4.9731],
        [  6.9063,  -9.2014],
        [ -6.2902,   4.6021],
        [ -7.6068,   5.6762],
        [ -7.5125,   5.3867],
        [  5.9442,  -8.4520],
        [  6.2733,  -7.7527],
        [  5.3518,  -8.0706],
        [  5.8199,  -8.7056],
        [  6.6275,  -8.9114],
        [  0.4123,  -1.9528],
        [  6.0610,  -7.4486],
        [ -7.7524,   5.3629],
        [ -8.4721,   5.2619],
        [  3.0111,  -4.8327],
        [  1.8501,  -3.4811],
        [  5.2174,  -7.1389],
        [ -7.7981,   5.6176],
        [ -6.8606,   5.4730],
        [ -6.2288,   3.4578],
        [  4.6890,  -7.2477],
        [  5.4271,  -7.2369],
        [  6.9411,  -8.6807],
        [ -7.6786,   6.2692],
        [ -8.2467,   5.2430],
        [ -4.2541,   2.2271],
        [  5.7745,  -7.6698],
        [  0.4289,  -2.1980],
        [ -5.0242,   3.5949],
        [ -8.4432,   5.5375],
        [  6.7380,  -8.3439],
        [ -7.3401,   5.9330],
        [  4.6033,  -6.0817],
        [  1.3920,  -3.1224],
        [ -7.7372,   6.2983],
        [ -7.8238,   6.3224],
        [  6.2377,  -8.0040],
        [ -4.0009,   1.6955],
        [ -6.7703,   5.2928],
        [ -7.7727,   6.2382],
        [ -6.9101,   4.0075],
        [  3.5661,  -5.8763],
        [ -6.9567,   5.4323],
        [ -6.6100,   3.7792],
        [  5.1579,  -6.5539],
        [  5.3039,  -6.8635],
        [ -7.7103,   6.1983],
        [ -5.9804,   3.4780],
        [  5.5048,  -7.6091],
        [ -5.4035,   3.8833],
        [ -7.1433,   5.6837],
        [ -7.8223,   5.6514],
        [ -3.0519,   1.5994],
        [  6.2145,  -8.1681],
        [ -7.3985,   5.8660],
        [ -6.4945,   5.1082],
        [ -7.8679,   5.4565],
        [  6.0677,  -7.5129],
        [  5.9914,  -7.4600],
        [ -7.0453,   5.6060],
        [  2.8041,  -4.2186],
        [  5.7466,  -7.1813],
        [ -5.9349,   4.4728],
        [  5.4169,  -7.7347],
        [ -8.2882,   5.8686],
        [ -6.9403,   2.4155],
        [  4.3526,  -5.7390],
        [  5.8886,  -7.5939],
        [  5.5179,  -7.1438],
        [  6.0939,  -7.4802],
        [ -7.2096,   4.2453],
        [ -7.7839,   6.2122],
        [  5.9556, -10.5708],
        [ -7.7343,   6.1164],
        [ -4.9051,   3.3390],
        [ -9.2077,   6.1753],
        [ -6.4201,   3.4635],
        [ -8.2680,   5.7877],
        [ -5.3144,   3.5919],
        [  5.7606,  -7.2523],
        [ -6.7290,   5.3426],
        [ -5.5296,   3.9129],
        [ -6.6627,   5.2704],
        [ -9.7756,   6.3403],
        [ -7.2968,   5.8099],
        [ -7.5770,   5.8646],
        [  4.2931,  -6.3174],
        [  3.9776,  -6.6920],
        [ -5.8603,   2.7520],
        [  5.9629,  -7.5694],
        [ -8.3919,   7.0054],
        [  2.4523,  -7.0508]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7591, 0.2409],
        [0.2286, 0.7714]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4183, 0.5817], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4037, 0.0993],
         [0.2991, 0.2016]],

        [[0.9745, 0.1005],
         [0.7862, 0.5439]],

        [[0.0473, 0.1009],
         [0.0625, 0.2540]],

        [[0.1409, 0.1042],
         [0.9648, 0.4783]],

        [[0.0070, 0.0980],
         [0.7590, 0.7686]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919996253315568
Average Adjusted Rand Index: 0.9919993417272899
Iteration 0: Loss = -17183.24986187687
Iteration 10: Loss = -11998.105902943751
Iteration 20: Loss = -11409.8190788318
Iteration 30: Loss = -11409.819074870531
Iteration 40: Loss = -11409.81907487053
Iteration 50: Loss = -11409.81907487053
1
Iteration 60: Loss = -11409.81907487053
2
Iteration 70: Loss = -11409.81907487053
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7561, 0.2439],
        [0.2324, 0.7676]], dtype=torch.float64)
alpha: tensor([0.4534, 0.5466])
beta: tensor([[[0.3940, 0.0996],
         [0.1347, 0.1981]],

        [[0.2875, 0.0999],
         [0.4184, 0.6046]],

        [[0.5897, 0.1004],
         [0.6915, 0.2981]],

        [[0.9167, 0.1037],
         [0.0970, 0.0381]],

        [[0.2205, 0.0980],
         [0.9433, 0.6026]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919996253315568
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17182.80328622368
Iteration 100: Loss = -12108.683003552682
Iteration 200: Loss = -12070.82440068194
Iteration 300: Loss = -11571.578976939436
Iteration 400: Loss = -11544.772293389815
Iteration 500: Loss = -11543.977801487701
Iteration 600: Loss = -11543.620733263167
Iteration 700: Loss = -11543.417546415387
Iteration 800: Loss = -11543.287708886182
Iteration 900: Loss = -11543.198939415272
Iteration 1000: Loss = -11543.135185973848
Iteration 1100: Loss = -11543.087237188733
Iteration 1200: Loss = -11543.049826880171
Iteration 1300: Loss = -11543.019376297181
Iteration 1400: Loss = -11542.992008869844
Iteration 1500: Loss = -11536.71885186727
Iteration 1600: Loss = -11536.649182331977
Iteration 1700: Loss = -11530.230938834924
Iteration 1800: Loss = -11530.149729865749
Iteration 1900: Loss = -11529.391829171927
Iteration 2000: Loss = -11529.34820263277
Iteration 2100: Loss = -11529.32697934148
Iteration 2200: Loss = -11529.312581274506
Iteration 2300: Loss = -11529.301860952652
Iteration 2400: Loss = -11529.293475015345
Iteration 2500: Loss = -11529.286619961797
Iteration 2600: Loss = -11529.28090546104
Iteration 2700: Loss = -11529.27606432885
Iteration 2800: Loss = -11529.27185641483
Iteration 2900: Loss = -11529.268220157885
Iteration 3000: Loss = -11529.265023563537
Iteration 3100: Loss = -11529.262163444739
Iteration 3200: Loss = -11529.259678748418
Iteration 3300: Loss = -11529.257359482115
Iteration 3400: Loss = -11529.255317701616
Iteration 3500: Loss = -11529.253510249631
Iteration 3600: Loss = -11529.251823067518
Iteration 3700: Loss = -11529.250279538528
Iteration 3800: Loss = -11529.251053345302
1
Iteration 3900: Loss = -11529.24756187941
Iteration 4000: Loss = -11529.246404082818
Iteration 4100: Loss = -11529.256373862301
1
Iteration 4200: Loss = -11529.244275835725
Iteration 4300: Loss = -11529.243347723583
Iteration 4400: Loss = -11529.277513076151
1
Iteration 4500: Loss = -11529.241411229596
Iteration 4600: Loss = -11529.240020595726
Iteration 4700: Loss = -11529.238293012711
Iteration 4800: Loss = -11529.237466248933
Iteration 4900: Loss = -11529.236742338753
Iteration 5000: Loss = -11529.236096334449
Iteration 5100: Loss = -11529.235559053825
Iteration 5200: Loss = -11529.234964872807
Iteration 5300: Loss = -11529.234444451504
Iteration 5400: Loss = -11529.234030944104
Iteration 5500: Loss = -11529.233438001944
Iteration 5600: Loss = -11529.23287946676
Iteration 5700: Loss = -11529.233593117775
1
Iteration 5800: Loss = -11529.231236789825
Iteration 5900: Loss = -11529.229231629542
Iteration 6000: Loss = -11529.228350304891
Iteration 6100: Loss = -11529.227863412741
Iteration 6200: Loss = -11529.238940102707
1
Iteration 6300: Loss = -11529.225986512245
Iteration 6400: Loss = -11529.229299718296
1
Iteration 6500: Loss = -11529.225484568551
Iteration 6600: Loss = -11529.225816641863
1
Iteration 6700: Loss = -11529.260260637042
2
Iteration 6800: Loss = -11529.22401148492
Iteration 6900: Loss = -11529.225227547604
1
Iteration 7000: Loss = -11529.241974123199
2
Iteration 7100: Loss = -11529.223303869081
Iteration 7200: Loss = -11529.225854117312
1
Iteration 7300: Loss = -11529.223038811198
Iteration 7400: Loss = -11529.236495379102
1
Iteration 7500: Loss = -11529.230200905924
2
Iteration 7600: Loss = -11529.223006031902
Iteration 7700: Loss = -11529.222726529742
Iteration 7800: Loss = -11529.22381624184
1
Iteration 7900: Loss = -11529.253711496898
2
Iteration 8000: Loss = -11529.224902174123
3
Iteration 8100: Loss = -11529.223634718055
4
Iteration 8200: Loss = -11529.225360361736
5
Iteration 8300: Loss = -11529.22748888404
6
Iteration 8400: Loss = -11529.232199129488
7
Iteration 8500: Loss = -11529.253250746151
8
Iteration 8600: Loss = -11529.228436382225
9
Iteration 8700: Loss = -11529.24372393339
10
Stopping early at iteration 8700 due to no improvement.
tensor([[ -7.8479,   6.4460],
        [ -4.5340,   2.8266],
        [ -6.6765,   5.2505],
        [ -7.6796,   6.1546],
        [ -7.6172,   5.2602],
        [ -7.6361,   5.8769],
        [ -8.6887,   7.0858],
        [ -3.2353,   1.6811],
        [ -3.8565,   1.7779],
        [ -8.2679,   6.3829],
        [ -7.5554,   5.0347],
        [ -7.3773,   4.2002],
        [ -5.0241,   3.5968],
        [ -7.6693,   6.2450],
        [ -8.4232,   6.8372],
        [ -7.6731,   6.0384],
        [ -9.6875,   6.9743],
        [ -7.8975,   6.4962],
        [ -4.7283,   3.2747],
        [ -9.0868,   4.8953],
        [ -8.2915,   5.7472],
        [ -5.8022,   4.0477],
        [ -8.1062,   5.8368],
        [ -7.5208,   6.0832],
        [ -8.9162,   7.5028],
        [ -5.3856,   0.7704],
        [ -6.6901,   5.2792],
        [ -4.5527,   2.8607],
        [ -7.7263,   6.3119],
        [ -7.8680,   6.4807],
        [ -8.1218,   6.5232],
        [ -7.7275,   4.9541],
        [ -5.5810,   4.1876],
        [ -2.9301,   1.4302],
        [ -7.3282,   5.9279],
        [ -5.6493,   4.1884],
        [ -4.5356,   1.6751],
        [ -8.0681,   5.1978],
        [ -8.1143,   6.2043],
        [ -5.8195,   3.9214],
        [ -9.8668,   7.7461],
        [ -8.9067,   6.0834],
        [ -5.4441,   3.0683],
        [ -7.9700,   6.5233],
        [ -7.4834,   5.1581],
        [ -7.3889,   5.6907],
        [ -4.5149,   2.8911],
        [ -3.8296,   1.7469],
        [ -6.9616,   5.4739],
        [ -6.5708,   5.1769],
        [ -5.6846,   4.1940],
        [ -8.4703,   4.6112],
        [ -9.2135,   6.1685],
        [ -6.9815,   5.2832],
        [ -3.1945,   1.8023],
        [ -6.0035,   1.3882],
        [ -8.0000,   6.4768],
        [ -6.2691,   4.7473],
        [ -8.6404,   4.3350],
        [ -4.8150,   2.6027],
        [ -7.3595,   5.6034],
        [ -7.9582,   6.1569],
        [ -2.7203,   1.0318],
        [ -5.3270,   2.0732],
        [ -6.3908,   4.3121],
        [ -7.3311,   5.5501],
        [ -8.5413,   6.3663],
        [-10.4061,   5.7909],
        [ -7.8567,   6.4610],
        [ -8.7665,   6.6804],
        [ -3.1661,   1.7654],
        [ -8.0046,   5.3191],
        [ -5.6152,   4.1894],
        [ -8.6360,   6.4340],
        [ -7.7896,   6.3794],
        [ -8.2255,   5.9364],
        [ -9.3196,   6.1455],
        [ -7.8652,   6.4327],
        [ -9.5525,   4.9373],
        [ -4.1670,   1.4831],
        [ -7.6864,   6.2497],
        [ -4.8581,   3.0305],
        [ -8.9619,   4.5129],
        [ -7.8788,   6.2376],
        [ -7.7462,   5.4817],
        [ -4.0739,   0.9258],
        [  0.0837,  -1.4705],
        [ -8.4523,   6.7244],
        [ -3.3865,   1.6179],
        [ -5.2091,   2.1433],
        [ -6.1451,   3.7859],
        [ -8.4815,   6.7425],
        [ -2.6637,   1.0885],
        [ -6.8957,   5.2956],
        [ -8.5091,   6.8366],
        [ -7.5274,   4.8417],
        [ -4.9123,   1.3088],
        [-10.4767,   5.8615],
        [ -8.0557,   5.9331],
        [ -5.3642,   3.8769]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7435, 0.2565],
        [0.3189, 0.6811]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0094, 0.9906], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4063, 0.0670],
         [0.1347, 0.1942]],

        [[0.2875, 0.0999],
         [0.4184, 0.6046]],

        [[0.5897, 0.1006],
         [0.6915, 0.2981]],

        [[0.9167, 0.1040],
         [0.0970, 0.0381]],

        [[0.2205, 0.0980],
         [0.9433, 0.6026]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.6781956604619552
Average Adjusted Rand Index: 0.7908965996409413
Iteration 0: Loss = -33986.02984359367
Iteration 10: Loss = -12114.386681159529
Iteration 20: Loss = -12097.873043325731
Iteration 30: Loss = -11409.806936615469
Iteration 40: Loss = -11409.819075070704
1
Iteration 50: Loss = -11409.819075070785
2
Iteration 60: Loss = -11409.819075070785
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7676, 0.2324],
        [0.2439, 0.7561]], dtype=torch.float64)
alpha: tensor([0.5466, 0.4534])
beta: tensor([[[0.1981, 0.0996],
         [0.8451, 0.3940]],

        [[0.8556, 0.0999],
         [0.1519, 0.9846]],

        [[0.6002, 0.1004],
         [0.3289, 0.7583]],

        [[0.7227, 0.1037],
         [0.0154, 0.0369]],

        [[0.6112, 0.0980],
         [0.1221, 0.8927]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919996253315568
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33986.16166607178
Iteration 100: Loss = -12122.637170617158
Iteration 200: Loss = -12115.174719490527
Iteration 300: Loss = -12111.029686945642
Iteration 400: Loss = -12109.625439959933
Iteration 500: Loss = -12108.390120476755
Iteration 600: Loss = -12105.922272933136
Iteration 700: Loss = -12101.217849599445
Iteration 800: Loss = -12096.68226757766
Iteration 900: Loss = -12083.869881993405
Iteration 1000: Loss = -12043.92886317871
Iteration 1100: Loss = -11946.689807503144
Iteration 1200: Loss = -11741.32019007736
Iteration 1300: Loss = -11593.597285303285
Iteration 1400: Loss = -11518.019531169082
Iteration 1500: Loss = -11500.464939480149
Iteration 1600: Loss = -11490.407249553917
Iteration 1700: Loss = -11490.186041702687
Iteration 1800: Loss = -11489.527724454083
Iteration 1900: Loss = -11489.410412630634
Iteration 2000: Loss = -11487.382592973025
Iteration 2100: Loss = -11469.331274212585
Iteration 2200: Loss = -11469.188394191397
Iteration 2300: Loss = -11464.101332135302
Iteration 2400: Loss = -11464.073536445187
Iteration 2500: Loss = -11464.050384680315
Iteration 2600: Loss = -11464.029767346237
Iteration 2700: Loss = -11464.006324930644
Iteration 2800: Loss = -11455.366384152177
Iteration 2900: Loss = -11455.339754086483
Iteration 3000: Loss = -11449.039206859075
Iteration 3100: Loss = -11448.658972271462
Iteration 3200: Loss = -11445.787907984542
Iteration 3300: Loss = -11445.751135346365
Iteration 3400: Loss = -11444.36890039886
Iteration 3500: Loss = -11434.553461800482
Iteration 3600: Loss = -11434.533899733806
Iteration 3700: Loss = -11434.526129926922
Iteration 3800: Loss = -11434.524213684748
Iteration 3900: Loss = -11434.512284361032
Iteration 4000: Loss = -11434.501379070341
Iteration 4100: Loss = -11432.908976099587
Iteration 4200: Loss = -11432.731575935235
Iteration 4300: Loss = -11432.725648221021
Iteration 4400: Loss = -11432.721975623139
Iteration 4500: Loss = -11432.735645967778
1
Iteration 4600: Loss = -11432.71127412755
Iteration 4700: Loss = -11421.741959385356
Iteration 4800: Loss = -11421.726418208638
Iteration 4900: Loss = -11421.729215430714
1
Iteration 5000: Loss = -11421.72227115542
Iteration 5100: Loss = -11421.720158042626
Iteration 5200: Loss = -11421.724785598943
1
Iteration 5300: Loss = -11421.717647544623
Iteration 5400: Loss = -11421.71934409021
1
Iteration 5500: Loss = -11421.714312508193
Iteration 5600: Loss = -11421.713192512245
Iteration 5700: Loss = -11421.712067204782
Iteration 5800: Loss = -11421.710939275403
Iteration 5900: Loss = -11421.710758214203
Iteration 6000: Loss = -11421.710333616182
Iteration 6100: Loss = -11421.708651134693
Iteration 6200: Loss = -11421.710999079756
1
Iteration 6300: Loss = -11421.707103827408
Iteration 6400: Loss = -11421.706854166263
Iteration 6500: Loss = -11421.70512368374
Iteration 6600: Loss = -11421.706606376256
1
Iteration 6700: Loss = -11421.70393975879
Iteration 6800: Loss = -11421.709658961914
1
Iteration 6900: Loss = -11421.703056679133
Iteration 7000: Loss = -11421.702824659753
Iteration 7100: Loss = -11421.705957387245
1
Iteration 7200: Loss = -11421.716503745127
2
Iteration 7300: Loss = -11421.702193559484
Iteration 7400: Loss = -11421.700993056875
Iteration 7500: Loss = -11421.700820646718
Iteration 7600: Loss = -11421.701420015044
1
Iteration 7700: Loss = -11421.700081815861
Iteration 7800: Loss = -11421.699829915253
Iteration 7900: Loss = -11421.723194732265
1
Iteration 8000: Loss = -11421.699568126305
Iteration 8100: Loss = -11421.708665500124
1
Iteration 8200: Loss = -11421.708433588183
2
Iteration 8300: Loss = -11421.706734048877
3
Iteration 8400: Loss = -11421.69886735797
Iteration 8500: Loss = -11421.703134655067
1
Iteration 8600: Loss = -11421.708252613766
2
Iteration 8700: Loss = -11421.698472373882
Iteration 8800: Loss = -11421.698092968165
Iteration 8900: Loss = -11421.699238446497
1
Iteration 9000: Loss = -11417.574575460487
Iteration 9100: Loss = -11417.573480875055
Iteration 9200: Loss = -11417.569624218302
Iteration 9300: Loss = -11417.566725767994
Iteration 9400: Loss = -11417.570913084299
1
Iteration 9500: Loss = -11417.571251546917
2
Iteration 9600: Loss = -11417.572872114231
3
Iteration 9700: Loss = -11417.57604580559
4
Iteration 9800: Loss = -11417.5675838974
5
Iteration 9900: Loss = -11417.567147738435
6
Iteration 10000: Loss = -11417.634486472054
7
Iteration 10100: Loss = -11417.571312297705
8
Iteration 10200: Loss = -11417.563682840006
Iteration 10300: Loss = -11417.616935876928
1
Iteration 10400: Loss = -11417.56780148813
2
Iteration 10500: Loss = -11417.568326886472
3
Iteration 10600: Loss = -11417.570172838225
4
Iteration 10700: Loss = -11417.56174157986
Iteration 10800: Loss = -11417.564079252616
1
Iteration 10900: Loss = -11417.56342610965
2
Iteration 11000: Loss = -11417.56134793024
Iteration 11100: Loss = -11417.561759639799
1
Iteration 11200: Loss = -11417.637479128842
2
Iteration 11300: Loss = -11417.563575013848
3
Iteration 11400: Loss = -11417.568446232252
4
Iteration 11500: Loss = -11417.560492378465
Iteration 11600: Loss = -11417.560239471868
Iteration 11700: Loss = -11417.562372697734
1
Iteration 11800: Loss = -11417.657313492773
2
Iteration 11900: Loss = -11417.56017215447
Iteration 12000: Loss = -11417.560833203504
1
Iteration 12100: Loss = -11417.64103088389
2
Iteration 12200: Loss = -11417.561129104135
3
Iteration 12300: Loss = -11417.561835374505
4
Iteration 12400: Loss = -11417.561633427003
5
Iteration 12500: Loss = -11417.560022451573
Iteration 12600: Loss = -11417.561389338329
1
Iteration 12700: Loss = -11417.585249686397
2
Iteration 12800: Loss = -11417.562528535313
3
Iteration 12900: Loss = -11417.560327481777
4
Iteration 13000: Loss = -11417.560619274496
5
Iteration 13100: Loss = -11417.560415710977
6
Iteration 13200: Loss = -11417.560296111631
7
Iteration 13300: Loss = -11417.563407042377
8
Iteration 13400: Loss = -11417.560012752023
Iteration 13500: Loss = -11417.560985218934
1
Iteration 13600: Loss = -11417.668508797587
2
Iteration 13700: Loss = -11417.565730310838
3
Iteration 13800: Loss = -11417.54718679537
Iteration 13900: Loss = -11417.59119522229
1
Iteration 14000: Loss = -11417.555339928473
2
Iteration 14100: Loss = -11417.573112765378
3
Iteration 14200: Loss = -11417.555648699818
4
Iteration 14300: Loss = -11417.592945015173
5
Iteration 14400: Loss = -11417.5627961325
6
Iteration 14500: Loss = -11417.555835415482
7
Iteration 14600: Loss = -11417.546020081283
Iteration 14700: Loss = -11417.558206329259
1
Iteration 14800: Loss = -11417.656566003614
2
Iteration 14900: Loss = -11417.54499174514
Iteration 15000: Loss = -11417.545274793181
1
Iteration 15100: Loss = -11417.54825479405
2
Iteration 15200: Loss = -11417.548214959044
3
Iteration 15300: Loss = -11417.548285682273
4
Iteration 15400: Loss = -11417.690691746873
5
Iteration 15500: Loss = -11417.545181552869
6
Iteration 15600: Loss = -11417.548575849385
7
Iteration 15700: Loss = -11417.54798609091
8
Iteration 15800: Loss = -11417.560391096642
9
Iteration 15900: Loss = -11417.545350123019
10
Stopping early at iteration 15900 due to no improvement.
tensor([[  4.7915,  -6.3216],
        [ -8.2186,   5.3587],
        [ -9.9447,   5.3295],
        [ -8.6814,   6.8709],
        [ -8.0395,   6.4528],
        [  5.9529,  -7.3420],
        [ -7.3132,   5.6373],
        [ -7.9373,   6.5510],
        [ -9.3631,   7.6033],
        [  7.6004,  -9.0473],
        [ -9.3219,   7.8246],
        [ -8.4268,   6.3966],
        [ -7.3013,   5.8590],
        [  6.5765,  -9.8561],
        [  7.0449,  -8.4935],
        [  3.6976,  -5.7386],
        [  7.1749,  -8.5616],
        [  6.9011,  -8.6231],
        [  0.3480,  -2.0245],
        [  6.6484,  -8.6586],
        [ -7.4565,   6.0207],
        [ -8.2703,   6.6793],
        [  3.1970,  -4.6273],
        [  1.9629,  -3.3708],
        [  5.5989,  -7.4680],
        [-10.2970,   6.1100],
        [ -7.5133,   6.0477],
        [ -5.4993,   4.0447],
        [  5.1529,  -7.1196],
        [  6.1477,  -7.6730],
        [  7.5000,  -8.8913],
        [-10.3098,   7.2635],
        [ -7.5192,   5.9685],
        [ -4.0325,   2.4031],
        [  6.6658,  -8.1242],
        [  0.4176,  -2.2618],
        [ -8.1245,   6.0807],
        [ -9.0839,   7.6967],
        [  7.5323,  -9.9280],
        [ -8.6394,   4.9525],
        [  4.6311,  -6.1289],
        [  0.9042,  -3.6053],
        [-11.2980,   7.8630],
        [ -7.4291,   5.7838],
        [  5.1624,  -7.1282],
        [ -3.7724,   1.8374],
        [ -5.7049,   3.9514],
        [ -7.2766,   4.9998],
        [-10.5255,   7.7198],
        [  3.7076,  -5.7131],
        [ -9.1211,   4.5059],
        [ -6.1088,   4.2145],
        [  5.5656,  -7.0140],
        [  5.9896,  -7.4101],
        [ -8.0686,   6.4667],
        [ -5.5770,   3.7805],
        [  6.3866,  -8.1805],
        [ -6.2761,   2.8949],
        [ -8.2861,   6.4269],
        [ -8.0639,   6.4808],
        [ -3.1795,   1.3921],
        [  6.5094,  -8.9043],
        [-10.4857,   8.1316],
        [ -6.7963,   5.1592],
        [-10.1922,   7.7851],
        [  7.0496,  -8.6832],
        [  7.4886, -10.1373],
        [-10.7663,   6.9260],
        [  2.8037,  -4.2133],
        [  6.4341,  -7.8754],
        [ -9.6861,   7.4465],
        [  3.1846,  -4.8228],
        [-11.3708,   8.4376],
        [ -5.4949,   3.8057],
        [  3.4391,  -6.6624],
        [  6.0493,  -9.1696],
        [  6.2339,  -7.6218],
        [  6.3386, -10.9538],
        [ -7.4193,   4.1188],
        [ -9.5095,   7.0308],
        [  7.8882,  -9.3931],
        [ -6.3474,   4.9470],
        [ -4.9955,   3.1455],
        [ -9.3266,   7.3591],
        [ -9.6876,   7.8166],
        [ -8.9509,   6.1003],
        [ -8.9701,   7.4919],
        [  6.1840,  -8.8838],
        [ -9.0841,   7.5629],
        [ -5.7550,   3.5625],
        [ -7.4241,   6.0100],
        [ -8.4718,   7.0359],
        [ -9.4300,   7.9029],
        [ -5.3699,   3.9570],
        [  4.5991,  -5.9888],
        [  3.9713,  -6.6657],
        [ -6.0508,   2.4402],
        [  7.0521,  -8.4384],
        [ -9.6267,   6.9237],
        [  3.7103,  -5.7679]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7541, 0.2459],
        [0.2341, 0.7659]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4185, 0.5815], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4019, 0.0993],
         [0.8451, 0.2010]],

        [[0.8556, 0.1018],
         [0.1519, 0.9846]],

        [[0.6002, 0.1005],
         [0.3289, 0.7583]],

        [[0.7227, 0.1041],
         [0.0154, 0.0369]],

        [[0.6112, 0.0981],
         [0.1221, 0.8927]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9840313942303629
Average Adjusted Rand Index: 0.9839993417272901
Iteration 0: Loss = -26062.25233950897
Iteration 10: Loss = -11409.89427918307
Iteration 20: Loss = -11409.819077550954
Iteration 30: Loss = -11409.819077550324
Iteration 40: Loss = -11409.819077550324
1
Iteration 50: Loss = -11409.819077550324
2
Iteration 60: Loss = -11409.819077550324
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7561, 0.2439],
        [0.2324, 0.7676]], dtype=torch.float64)
alpha: tensor([0.4534, 0.5466])
beta: tensor([[[0.3940, 0.0996],
         [0.6919, 0.1981]],

        [[0.6240, 0.0999],
         [0.4370, 0.2827]],

        [[0.4429, 0.1004],
         [0.6683, 0.9020]],

        [[0.7952, 0.1037],
         [0.6676, 0.2264]],

        [[0.7828, 0.0980],
         [0.9121, 0.0134]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919996253315568
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26062.033280967804
Iteration 100: Loss = -12123.096334214018
Iteration 200: Loss = -12110.154468048453
Iteration 300: Loss = -11768.382431674869
Iteration 400: Loss = -11724.838014166831
Iteration 500: Loss = -11719.629126682796
Iteration 600: Loss = -11710.505739284688
Iteration 700: Loss = -11704.632773301337
Iteration 800: Loss = -11701.809744700779
Iteration 900: Loss = -11695.869447226709
Iteration 1000: Loss = -11690.608283531
Iteration 1100: Loss = -11680.203379827573
Iteration 1200: Loss = -11651.617902505917
Iteration 1300: Loss = -11616.885187164962
Iteration 1400: Loss = -11587.2417029824
Iteration 1500: Loss = -11583.410751713513
Iteration 1600: Loss = -11553.368278493312
Iteration 1700: Loss = -11551.078999399757
Iteration 1800: Loss = -11536.956148014922
Iteration 1900: Loss = -11533.51312407192
Iteration 2000: Loss = -11533.284892853668
Iteration 2100: Loss = -11529.520165187876
Iteration 2200: Loss = -11523.483186923913
Iteration 2300: Loss = -11523.446206589759
Iteration 2400: Loss = -11523.420884366898
Iteration 2500: Loss = -11523.400275547925
Iteration 2600: Loss = -11523.379032559382
Iteration 2700: Loss = -11521.1822618766
Iteration 2800: Loss = -11507.592083387544
Iteration 2900: Loss = -11507.486732938763
Iteration 3000: Loss = -11507.463170370276
Iteration 3100: Loss = -11507.444892876489
Iteration 3200: Loss = -11507.410219316898
Iteration 3300: Loss = -11502.044533178663
Iteration 3400: Loss = -11501.183120654026
Iteration 3500: Loss = -11501.139994225532
Iteration 3600: Loss = -11501.131832485531
Iteration 3700: Loss = -11501.125614595072
Iteration 3800: Loss = -11501.120410993457
Iteration 3900: Loss = -11501.114158917797
Iteration 4000: Loss = -11497.956430881493
Iteration 4100: Loss = -11497.94469228606
Iteration 4200: Loss = -11497.940120403573
Iteration 4300: Loss = -11497.93647233118
Iteration 4400: Loss = -11497.933203528612
Iteration 4500: Loss = -11497.929943047828
Iteration 4600: Loss = -11497.927510222718
Iteration 4700: Loss = -11497.925405744369
Iteration 4800: Loss = -11497.923525459051
Iteration 4900: Loss = -11497.921731162765
Iteration 5000: Loss = -11497.919939915111
Iteration 5100: Loss = -11497.918128105273
Iteration 5200: Loss = -11497.915793237857
Iteration 5300: Loss = -11497.910137738267
Iteration 5400: Loss = -11497.880896773195
Iteration 5500: Loss = -11497.879390724303
Iteration 5600: Loss = -11497.877871513807
Iteration 5700: Loss = -11497.876354560874
Iteration 5800: Loss = -11497.87498001014
Iteration 5900: Loss = -11497.872899032509
Iteration 6000: Loss = -11484.899230406574
Iteration 6100: Loss = -11484.825877032888
Iteration 6200: Loss = -11484.825218999033
Iteration 6300: Loss = -11484.822884900468
Iteration 6400: Loss = -11484.82178673457
Iteration 6500: Loss = -11484.81582401498
Iteration 6600: Loss = -11482.830789083382
Iteration 6700: Loss = -11482.829819792702
Iteration 6800: Loss = -11482.829014515779
Iteration 6900: Loss = -11482.828240302711
Iteration 7000: Loss = -11482.82748856641
Iteration 7100: Loss = -11482.826911639144
Iteration 7200: Loss = -11482.826510043926
Iteration 7300: Loss = -11482.826049168432
Iteration 7400: Loss = -11482.825425838173
Iteration 7500: Loss = -11470.88344282014
Iteration 7600: Loss = -11470.86042476836
Iteration 7700: Loss = -11470.858401596752
Iteration 7800: Loss = -11470.84073390231
Iteration 7900: Loss = -11470.840656768767
Iteration 8000: Loss = -11470.840080041982
Iteration 8100: Loss = -11470.839990284214
Iteration 8200: Loss = -11470.839669230565
Iteration 8300: Loss = -11470.83985194996
1
Iteration 8400: Loss = -11470.842839765048
2
Iteration 8500: Loss = -11470.84180585921
3
Iteration 8600: Loss = -11470.840547795211
4
Iteration 8700: Loss = -11470.842553788001
5
Iteration 8800: Loss = -11470.852104313899
6
Iteration 8900: Loss = -11470.860988761055
7
Iteration 9000: Loss = -11470.838613935774
Iteration 9100: Loss = -11470.838721674869
1
Iteration 9200: Loss = -11470.845027845968
2
Iteration 9300: Loss = -11470.863954196779
3
Iteration 9400: Loss = -11470.842474232186
4
Iteration 9500: Loss = -11470.829237468006
Iteration 9600: Loss = -11455.36395075846
Iteration 9700: Loss = -11455.360543704968
Iteration 9800: Loss = -11455.37190341477
1
Iteration 9900: Loss = -11436.297433863747
Iteration 10000: Loss = -11436.293620454353
Iteration 10100: Loss = -11436.249602274982
Iteration 10200: Loss = -11436.249464088987
Iteration 10300: Loss = -11424.040817843805
Iteration 10400: Loss = -11423.913910581256
Iteration 10500: Loss = -11423.903279059297
Iteration 10600: Loss = -11423.901261198316
Iteration 10700: Loss = -11423.895221286582
Iteration 10800: Loss = -11423.893621237989
Iteration 10900: Loss = -11423.896141907593
1
Iteration 11000: Loss = -11423.991266973617
2
Iteration 11100: Loss = -11423.89350842207
Iteration 11200: Loss = -11423.907951950005
1
Iteration 11300: Loss = -11423.893134169684
Iteration 11400: Loss = -11423.893522149867
1
Iteration 11500: Loss = -11423.90919851623
2
Iteration 11600: Loss = -11423.894998215783
3
Iteration 11700: Loss = -11423.896434135731
4
Iteration 11800: Loss = -11423.906773969095
5
Iteration 11900: Loss = -11423.931259180828
6
Iteration 12000: Loss = -11423.964586919841
7
Iteration 12100: Loss = -11423.906844664336
8
Iteration 12200: Loss = -11423.89289399435
Iteration 12300: Loss = -11423.894440647286
1
Iteration 12400: Loss = -11423.928832861637
2
Iteration 12500: Loss = -11423.900125399981
3
Iteration 12600: Loss = -11423.892973364385
4
Iteration 12700: Loss = -11423.914237017918
5
Iteration 12800: Loss = -11423.893306968355
6
Iteration 12900: Loss = -11423.900084850773
7
Iteration 13000: Loss = -11423.906582612086
8
Iteration 13100: Loss = -11423.936833678412
9
Iteration 13200: Loss = -11423.893915793942
10
Stopping early at iteration 13200 due to no improvement.
tensor([[  4.9362,  -6.4979],
        [ -6.2896,   4.8093],
        [ -7.8786,   6.1736],
        [ -6.0791,   4.6738],
        [ -7.7244,   5.5383],
        [  5.4437,  -8.2127],
        [ -8.1244,   6.1420],
        [ -7.4038,   5.9419],
        [ -9.8912,   6.5596],
        [  7.0222,  -8.4967],
        [ -6.5818,   4.9865],
        [ -7.8286,   5.7867],
        [ -6.8491,   4.8909],
        [  6.1529,  -7.8540],
        [  6.6833,  -8.1195],
        [  1.6113,  -3.2287],
        [  6.4454,  -7.9396],
        [  6.5478,  -8.1781],
        [  1.0661,  -2.4653],
        [  5.5106,  -6.8998],
        [ -6.5947,   5.1093],
        [ -9.0537,   5.3842],
        [  2.3987,  -3.8717],
        [ -8.9443,   6.6675],
        [  4.0932,  -7.1979],
        [ -7.7067,   6.0525],
        [ -9.0825,   6.2615],
        [ -4.7679,   3.2228],
        [  6.2765,  -7.7482],
        [  6.3513,  -7.9201],
        [  7.2810,  -9.5835],
        [ -8.8469,   6.9217],
        [ -8.2774,   5.0468],
        [ -8.9440,   6.3558],
        [  5.6304,  -7.2218],
        [ -0.3361,  -1.3901],
        [ -5.7721,   3.9631],
        [ -8.0091,   5.9251],
        [  7.3783,  -8.8617],
        [ -7.7689,   4.1748],
        [ -8.0812,   5.3951],
        [ -8.1554,   6.7687],
        [ -8.0625,   6.3176],
        [ -6.5314,   5.1034],
        [  4.2938,  -6.2561],
        [ -2.7082,   1.2734],
        [ -8.2515,   5.1727],
        [ -8.3801,   6.8496],
        [ -5.3725,   3.8520],
        [  2.7716,  -5.2426],
        [ -7.4059,   6.0038],
        [ -5.2819,   3.8742],
        [  5.6215,  -7.0352],
        [  4.6572,  -6.7362],
        [ -7.5570,   5.9415],
        [ -6.9293,   5.5104],
        [  6.8042,  -8.2496],
        [ -6.0374,   4.2497],
        [ -8.8478,   5.4390],
        [ -7.2239,   5.7353],
        [ -3.7238,   1.9597],
        [  5.7755,  -9.1818],
        [ -7.5526,   5.9791],
        [ -6.4418,   4.0115],
        [ -8.0492,   6.1559],
        [  6.4726,  -7.8911],
        [  6.8058,  -8.3737],
        [ -7.8802,   6.4623],
        [  7.0436, -10.3567],
        [  5.2050,  -8.0517],
        [ -7.5185,   5.7951],
        [  3.4602,  -5.5585],
        [ -8.7443,   6.3827],
        [ -4.6166,   3.0778],
        [  1.0339,  -4.9405],
        [  5.5843,  -8.7854],
        [  5.7134,  -8.5337],
        [  5.5254,  -8.1384],
        [ -7.9243,   6.1104],
        [ -8.1034,   6.7152],
        [  6.6741,  -8.5587],
        [ -7.8339,   6.4077],
        [ -4.1132,   2.5716],
        [ -8.1870,   6.7623],
        [ -6.6508,   4.0419],
        [ -7.4041,   5.1094],
        [ -4.5340,   3.0825],
        [  4.6753,  -6.1208],
        [ -7.4912,   5.9489],
        [ -4.7158,   3.2563],
        [ -6.6204,   5.2340],
        [ -9.0033,   7.5655],
        [ -7.9787,   6.3124],
        [ -4.7036,   2.9926],
        [  0.4361,  -3.5224],
        [  4.8269,  -6.7462],
        [ -5.7181,   4.0181],
        [  5.8998,  -9.3175],
        [ -8.2766,   6.8899],
        [  6.2604,  -7.7524]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7556, 0.2444],
        [0.2385, 0.7615]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3870, 0.6130], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4060, 0.1045],
         [0.6919, 0.2010]],

        [[0.6240, 0.0998],
         [0.4370, 0.2827]],

        [[0.4429, 0.1006],
         [0.6683, 0.9020]],

        [[0.7952, 0.1038],
         [0.6676, 0.2264]],

        [[0.7828, 0.0981],
         [0.9121, 0.0134]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8823523358261945
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.968190389711498
Average Adjusted Rand Index: 0.9684698088925288
11413.67858654651
new:  [0.9919996253315568, 0.6781956604619552, 0.9840313942303629, 0.968190389711498] [0.9919993417272899, 0.7908965996409413, 0.9839993417272901, 0.9684698088925288] [11407.711802153352, 11529.24372393339, 11417.545350123019, 11423.893915793942]
prior:  [0.9919996253315568, 0.9919996253315568, 0.9919996253315568, 0.9919996253315568] [0.9919993417272899, 0.9919993417272899, 0.9919993417272899, 0.9919993417272899] [11409.819077550324, 11409.81907487053, 11409.819075070785, 11409.819077550324]
-----------------------------------------------------------------------------------------
This iteration is 12
True Objective function: Loss = -11600.475379451182
Iteration 0: Loss = -18323.92949027498
Iteration 10: Loss = -11596.04189879396
Iteration 20: Loss = -11596.041691231778
Iteration 30: Loss = -11596.041690843662
Iteration 40: Loss = -11596.041690843662
1
Iteration 50: Loss = -11596.041690843662
2
Iteration 60: Loss = -11596.041690843662
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7671, 0.2329],
        [0.2659, 0.7341]], dtype=torch.float64)
alpha: tensor([0.4987, 0.5013])
beta: tensor([[[0.3929, 0.0959],
         [0.8350, 0.1956]],

        [[0.4178, 0.1036],
         [0.3637, 0.8197]],

        [[0.0193, 0.0917],
         [0.9927, 0.2920]],

        [[0.2613, 0.1145],
         [0.7425, 0.5226]],

        [[0.1517, 0.1000],
         [0.4035, 0.8683]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.984032001021952
Average Adjusted Rand Index: 0.9839995611635629
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18368.420516701593
Iteration 100: Loss = -11679.912193745264
Iteration 200: Loss = -11594.88541195074
Iteration 300: Loss = -11594.293521208247
Iteration 400: Loss = -11594.057048870593
Iteration 500: Loss = -11593.932876827563
Iteration 600: Loss = -11593.857826828937
Iteration 700: Loss = -11593.808292675501
Iteration 800: Loss = -11593.77353869078
Iteration 900: Loss = -11593.747996292263
Iteration 1000: Loss = -11593.728655215262
Iteration 1100: Loss = -11593.71363677467
Iteration 1200: Loss = -11593.701822139894
Iteration 1300: Loss = -11593.692314885418
Iteration 1400: Loss = -11593.68462172798
Iteration 1500: Loss = -11593.678240912812
Iteration 1600: Loss = -11593.672885140466
Iteration 1700: Loss = -11593.668373947172
Iteration 1800: Loss = -11593.664434493976
Iteration 1900: Loss = -11593.661117871585
Iteration 2000: Loss = -11593.658241459376
Iteration 2100: Loss = -11593.655700438805
Iteration 2200: Loss = -11593.653493061456
Iteration 2300: Loss = -11593.651519610345
Iteration 2400: Loss = -11593.649786237056
Iteration 2500: Loss = -11593.648234585618
Iteration 2600: Loss = -11593.646814832195
Iteration 2700: Loss = -11593.645578202624
Iteration 2800: Loss = -11593.644956444654
Iteration 2900: Loss = -11593.643424621974
Iteration 3000: Loss = -11593.64249685395
Iteration 3100: Loss = -11593.64166946302
Iteration 3200: Loss = -11593.640892887974
Iteration 3300: Loss = -11593.64018175249
Iteration 3400: Loss = -11593.63956537968
Iteration 3500: Loss = -11593.638972786342
Iteration 3600: Loss = -11593.6384558503
Iteration 3700: Loss = -11593.63795060011
Iteration 3800: Loss = -11593.637503316264
Iteration 3900: Loss = -11593.637055094632
Iteration 4000: Loss = -11593.636623636437
Iteration 4100: Loss = -11593.636286093719
Iteration 4200: Loss = -11593.635946530072
Iteration 4300: Loss = -11593.63576505526
Iteration 4400: Loss = -11593.635330069712
Iteration 4500: Loss = -11593.635044415982
Iteration 4600: Loss = -11593.634765241979
Iteration 4700: Loss = -11593.634528163511
Iteration 4800: Loss = -11593.636303540368
1
Iteration 4900: Loss = -11593.634127161182
Iteration 5000: Loss = -11593.633886032063
Iteration 5100: Loss = -11593.633700765635
Iteration 5200: Loss = -11593.633549084047
Iteration 5300: Loss = -11593.633417120278
Iteration 5400: Loss = -11593.633277176237
Iteration 5500: Loss = -11593.633187286441
Iteration 5600: Loss = -11593.632939797011
Iteration 5700: Loss = -11593.633185301158
1
Iteration 5800: Loss = -11593.632781794075
Iteration 5900: Loss = -11593.632643121915
Iteration 6000: Loss = -11593.632563856416
Iteration 6100: Loss = -11593.632461871928
Iteration 6200: Loss = -11593.632513863542
1
Iteration 6300: Loss = -11593.632468871992
2
Iteration 6400: Loss = -11593.632531226722
3
Iteration 6500: Loss = -11593.636848652368
4
Iteration 6600: Loss = -11593.634259314647
5
Iteration 6700: Loss = -11593.649593533815
6
Iteration 6800: Loss = -11593.631983566058
Iteration 6900: Loss = -11593.6327428948
1
Iteration 7000: Loss = -11593.633105385872
2
Iteration 7100: Loss = -11593.667293457498
3
Iteration 7200: Loss = -11593.631777376957
Iteration 7300: Loss = -11593.631928798226
1
Iteration 7400: Loss = -11593.637828610877
2
Iteration 7500: Loss = -11593.68436745569
3
Iteration 7600: Loss = -11593.632631114888
4
Iteration 7700: Loss = -11593.710874978577
5
Iteration 7800: Loss = -11593.632092554106
6
Iteration 7900: Loss = -11593.63527731147
7
Iteration 8000: Loss = -11593.636570370174
8
Iteration 8100: Loss = -11593.632481413617
9
Iteration 8200: Loss = -11593.645189927525
10
Stopping early at iteration 8200 due to no improvement.
tensor([[ -6.2999,   1.6847],
        [-10.0695,   5.4543],
        [-10.3394,   5.7242],
        [ -8.6711,   4.0559],
        [  5.8031, -10.4183],
        [ -8.7319,   4.1167],
        [ -9.5885,   4.9733],
        [ -6.4555,   1.8403],
        [ -9.9885,   5.3733],
        [ -7.7162,   3.1010],
        [ -9.8459,   5.2307],
        [  4.5447,  -9.1600],
        [ -2.6945,  -1.9208],
        [  3.1524,  -7.7676],
        [-10.0105,   5.3953],
        [  2.4530,  -7.0682],
        [  4.0018,  -8.6170],
        [  6.3283, -10.9435],
        [  2.2420,  -6.8572],
        [ -6.8079,   2.1926],
        [  1.9640,  -6.5792],
        [ -9.6755,   5.0603],
        [ -8.9611,   4.3459],
        [  2.6650,  -7.2802],
        [ -9.4544,   4.8391],
        [ -7.9386,   3.3234],
        [  3.1593,  -7.7745],
        [-11.1392,   6.5240],
        [ -9.6039,   4.9886],
        [  4.7762,  -9.3914],
        [  4.4633,  -9.0785],
        [  3.4203,  -8.0355],
        [  4.6012,  -9.2164],
        [  1.1785,  -5.7937],
        [  5.0209,  -9.6362],
        [ -5.4547,   0.8395],
        [  2.1680,  -6.7832],
        [ -7.6473,   3.0321],
        [  5.1070,  -9.7222],
        [  2.3043,  -6.9195],
        [  4.6304,  -9.2456],
        [  6.1093, -10.7245],
        [  4.9166,  -9.5318],
        [  1.6776,  -6.2928],
        [ -0.0895,  -4.5257],
        [  5.9288, -10.5440],
        [  4.1348,  -8.7500],
        [ -5.3984,   0.7832],
        [  5.5988, -10.2140],
        [-10.1765,   5.5613],
        [  1.4633,  -6.0785],
        [ -9.9919,   5.3767],
        [ -8.8583,   4.2430],
        [ -6.2556,   1.6404],
        [  5.9882, -10.6035],
        [ -9.2579,   4.6427],
        [ -7.1498,   2.5346],
        [  4.5058,  -9.1211],
        [  5.3116,  -9.9268],
        [-10.1009,   5.4857],
        [  5.2274,  -9.8426],
        [ -6.7276,   2.1124],
        [  3.5821,  -8.1973],
        [ -9.5189,   4.9037],
        [ -9.7763,   5.1611],
        [ -9.5105,   4.8953],
        [  3.5227,  -8.1379],
        [ -6.4395,   1.8243],
        [-10.4295,   5.8143],
        [  5.4211, -10.0363],
        [ -6.7289,   2.1137],
        [  5.6725, -10.2877],
        [ -8.2122,   3.5970],
        [-10.1179,   5.5027],
        [  4.6012,  -9.2164],
        [ -9.6359,   5.0207],
        [  5.3557,  -9.9709],
        [  4.2489,  -8.8641],
        [ -6.9530,   2.3378],
        [ -9.8477,   5.2325],
        [ -9.9467,   5.3315],
        [  1.7491,  -6.3643],
        [ -5.1144,   0.4992],
        [  7.0160, -11.6312],
        [ -9.7190,   5.1038],
        [-10.0096,   5.3944],
        [ -9.5332,   4.9180],
        [  2.3190,  -6.9343],
        [-10.2785,   5.6633],
        [  4.0673,  -8.6826],
        [ -8.7173,   4.1021],
        [ -8.8637,   4.2485],
        [  5.4659, -10.0811],
        [ -8.6869,   4.0717],
        [-10.0208,   5.4055],
        [-10.5667,   5.9515],
        [ -6.0869,   1.4716],
        [ -6.4408,   1.8256],
        [ -9.6308,   5.0156],
        [  5.3361,  -9.9513]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7713, 0.2287],
        [0.2623, 0.7377]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4513, 0.5487], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4014, 0.0967],
         [0.8350, 0.1995]],

        [[0.4178, 0.1035],
         [0.3637, 0.8197]],

        [[0.0193, 0.0923],
         [0.9927, 0.2920]],

        [[0.2613, 0.1142],
         [0.7425, 0.5226]],

        [[0.1517, 0.1000],
         [0.4035, 0.8683]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -20065.948038520277
Iteration 10: Loss = -11608.580800914777
Iteration 20: Loss = -11596.041693668622
Iteration 30: Loss = -11596.041692336563
Iteration 40: Loss = -11596.041692336561
Iteration 50: Loss = -11596.041692336561
1
Iteration 60: Loss = -11596.041692336561
2
Iteration 70: Loss = -11596.041692336561
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7341, 0.2659],
        [0.2329, 0.7671]], dtype=torch.float64)
alpha: tensor([0.5013, 0.4987])
beta: tensor([[[0.1956, 0.0959],
         [0.3644, 0.3929]],

        [[0.2368, 0.1036],
         [0.6751, 0.6437]],

        [[0.3552, 0.0917],
         [0.9109, 0.4842]],

        [[0.1692, 0.1145],
         [0.6382, 0.5543]],

        [[0.7332, 0.1000],
         [0.1953, 0.3093]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.984032001021952
Average Adjusted Rand Index: 0.9839995611635629
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20065.634139796155
Iteration 100: Loss = -12390.9496963011
Iteration 200: Loss = -12273.379116519827
Iteration 300: Loss = -11973.185242500835
Iteration 400: Loss = -11941.365564697622
Iteration 500: Loss = -11919.399179392463
Iteration 600: Loss = -11915.821151908789
Iteration 700: Loss = -11908.747378301816
Iteration 800: Loss = -11892.985533125931
Iteration 900: Loss = -11884.281016564495
Iteration 1000: Loss = -11882.080477332773
Iteration 1100: Loss = -11881.837244898285
Iteration 1200: Loss = -11881.591811957505
Iteration 1300: Loss = -11881.476329843103
Iteration 1400: Loss = -11881.435124144566
Iteration 1500: Loss = -11881.405691403152
Iteration 1600: Loss = -11881.382288979365
Iteration 1700: Loss = -11881.363846034366
Iteration 1800: Loss = -11881.349023556357
Iteration 1900: Loss = -11881.336527525245
Iteration 2000: Loss = -11881.325775190131
Iteration 2100: Loss = -11881.316290830457
Iteration 2200: Loss = -11881.307600139191
Iteration 2300: Loss = -11881.299637260725
Iteration 2400: Loss = -11881.293201541339
Iteration 2500: Loss = -11881.28756861483
Iteration 2600: Loss = -11881.282450310016
Iteration 2700: Loss = -11881.277581695645
Iteration 2800: Loss = -11881.272178524543
Iteration 2900: Loss = -11881.256104964772
Iteration 3000: Loss = -11879.31720229532
Iteration 3100: Loss = -11879.21367842858
Iteration 3200: Loss = -11879.21047880903
Iteration 3300: Loss = -11879.206953610852
Iteration 3400: Loss = -11879.20045632391
Iteration 3500: Loss = -11879.1873405193
Iteration 3600: Loss = -11879.184861378408
Iteration 3700: Loss = -11879.182728954856
Iteration 3800: Loss = -11879.180737637917
Iteration 3900: Loss = -11879.178750170766
Iteration 4000: Loss = -11879.176580231278
Iteration 4100: Loss = -11879.17327736466
Iteration 4200: Loss = -11879.168127687226
Iteration 4300: Loss = -11879.16621584496
Iteration 4400: Loss = -11879.1651097803
Iteration 4500: Loss = -11879.16413413425
Iteration 4600: Loss = -11879.163287322164
Iteration 4700: Loss = -11879.169481477762
1
Iteration 4800: Loss = -11879.161635157869
Iteration 4900: Loss = -11879.160846765706
Iteration 5000: Loss = -11879.178361373042
1
Iteration 5100: Loss = -11879.158759877608
Iteration 5200: Loss = -11879.154839141243
Iteration 5300: Loss = -11879.143253690443
Iteration 5400: Loss = -11879.138833821586
Iteration 5500: Loss = -11879.147188171411
1
Iteration 5600: Loss = -11879.13792836345
Iteration 5700: Loss = -11879.13852209105
1
Iteration 5800: Loss = -11879.137173215973
Iteration 5900: Loss = -11879.13684439955
Iteration 6000: Loss = -11879.141351180768
1
Iteration 6100: Loss = -11879.136202700185
Iteration 6200: Loss = -11879.137161709783
1
Iteration 6300: Loss = -11879.135989446377
Iteration 6400: Loss = -11879.13948006249
1
Iteration 6500: Loss = -11879.137811129805
2
Iteration 6600: Loss = -11879.136118294395
3
Iteration 6700: Loss = -11879.136071024866
4
Iteration 6800: Loss = -11879.158783750163
5
Iteration 6900: Loss = -11879.13357660187
Iteration 7000: Loss = -11879.135431564566
1
Iteration 7100: Loss = -11879.134236204678
2
Iteration 7200: Loss = -11879.155913495651
3
Iteration 7300: Loss = -11879.118102190198
Iteration 7400: Loss = -11879.116403671722
Iteration 7500: Loss = -11879.112614743814
Iteration 7600: Loss = -11879.116977203803
1
Iteration 7700: Loss = -11879.111549502604
Iteration 7800: Loss = -11879.11232112712
1
Iteration 7900: Loss = -11879.106565306294
Iteration 8000: Loss = -11879.094087297479
Iteration 8100: Loss = -11879.09156953
Iteration 8200: Loss = -11879.091545971587
Iteration 8300: Loss = -11879.14184473689
1
Iteration 8400: Loss = -11879.091189898345
Iteration 8500: Loss = -11879.091541030391
1
Iteration 8600: Loss = -11879.112880267943
2
Iteration 8700: Loss = -11879.080415340843
Iteration 8800: Loss = -11879.084498011325
1
Iteration 8900: Loss = -11879.080050318595
Iteration 9000: Loss = -11879.079881280122
Iteration 9100: Loss = -11879.109289094673
1
Iteration 9200: Loss = -11879.079757618392
Iteration 9300: Loss = -11879.079788103496
1
Iteration 9400: Loss = -11879.07975485226
Iteration 9500: Loss = -11879.079627914001
Iteration 9600: Loss = -11879.338200581491
1
Iteration 9700: Loss = -11879.079580113657
Iteration 9800: Loss = -11879.079517969523
Iteration 9900: Loss = -11879.08137812245
1
Iteration 10000: Loss = -11879.079460642688
Iteration 10100: Loss = -11879.079593106459
1
Iteration 10200: Loss = -11879.080981749963
2
Iteration 10300: Loss = -11879.082582915902
3
Iteration 10400: Loss = -11879.087455050927
4
Iteration 10500: Loss = -11879.30430578306
5
Iteration 10600: Loss = -11879.079318036906
Iteration 10700: Loss = -11879.080643502302
1
Iteration 10800: Loss = -11879.079267030733
Iteration 10900: Loss = -11879.079670611336
1
Iteration 11000: Loss = -11879.079148400353
Iteration 11100: Loss = -11879.078843635441
Iteration 11200: Loss = -11879.078279063187
Iteration 11300: Loss = -11879.088249793645
1
Iteration 11400: Loss = -11879.07826695299
Iteration 11500: Loss = -11879.174609796197
1
Iteration 11600: Loss = -11879.078214725785
Iteration 11700: Loss = -11879.07865362427
1
Iteration 11800: Loss = -11879.085394271116
2
Iteration 11900: Loss = -11879.078201360986
Iteration 12000: Loss = -11879.078316688388
1
Iteration 12100: Loss = -11879.113949523726
2
Iteration 12200: Loss = -11879.078120822469
Iteration 12300: Loss = -11879.07955579654
1
Iteration 12400: Loss = -11879.078127310146
2
Iteration 12500: Loss = -11879.07908464195
3
Iteration 12600: Loss = -11879.10634113897
4
Iteration 12700: Loss = -11879.078217425893
5
Iteration 12800: Loss = -11879.07839622457
6
Iteration 12900: Loss = -11879.18409486563
7
Iteration 13000: Loss = -11879.078111299632
Iteration 13100: Loss = -11879.078546600826
1
Iteration 13200: Loss = -11879.080180100824
2
Iteration 13300: Loss = -11879.093035372656
3
Iteration 13400: Loss = -11879.099350842325
4
Iteration 13500: Loss = -11879.16960701635
5
Iteration 13600: Loss = -11879.078437654918
6
Iteration 13700: Loss = -11879.078369636441
7
Iteration 13800: Loss = -11879.079517352118
8
Iteration 13900: Loss = -11879.081813851548
9
Iteration 14000: Loss = -11879.08012515561
10
Stopping early at iteration 14000 due to no improvement.
tensor([[ -0.4257,  -1.9577],
        [  2.6668,  -4.1394],
        [ -5.1347,   3.7336],
        [ -6.4188,   4.8538],
        [  7.9394,  -9.4766],
        [ -3.8761,   2.1411],
        [ -2.4237,   0.2046],
        [  3.6741,  -5.0717],
        [ -0.6818,  -0.9322],
        [  5.6046,  -7.4377],
        [ -4.9049,   3.5180],
        [  6.8116,  -8.2137],
        [  6.3873,  -8.2977],
        [  5.2260,  -6.6375],
        [  0.9723,  -2.3628],
        [  2.0059,  -5.5296],
        [  6.6790,  -8.0695],
        [  7.6379,  -9.5251],
        [  3.1063,  -5.1298],
        [ -2.7982,   1.1744],
        [  4.8566,  -7.1294],
        [  0.2489,  -3.9222],
        [  2.0697,  -3.7736],
        [  6.5719,  -8.2886],
        [  3.5936,  -5.3146],
        [  0.9192,  -4.6752],
        [  6.7825,  -9.5888],
        [ -4.5441,   0.1866],
        [ -3.3448,   1.7317],
        [  7.5914,  -8.9797],
        [  4.1619,  -5.5520],
        [  5.7938,  -7.5462],
        [  6.8017,  -8.2564],
        [  7.9162,  -9.3868],
        [  8.1250, -10.7772],
        [  1.5666,  -3.9925],
        [  6.1091,  -7.5483],
        [ -4.9743,   1.1774],
        [  8.7403, -10.1410],
        [  7.4438,  -8.8603],
        [  7.3484,  -8.7581],
        [  7.4415,  -8.8750],
        [  7.6309,  -9.1812],
        [  4.9353,  -6.3978],
        [  4.5138,  -6.2355],
        [  7.8776,  -9.3313],
        [  7.0192,  -8.4162],
        [  2.9419,  -5.2914],
        [  7.0921,  -9.0504],
        [ -6.2672,   4.7384],
        [  6.4733,  -7.9399],
        [  0.5323,  -1.9741],
        [ -0.9047,  -1.0881],
        [  2.5281,  -5.7745],
        [  7.4302,  -9.8434],
        [  1.4906,  -2.9052],
        [  3.2252,  -4.8600],
        [  7.6877,  -9.3998],
        [  7.2041,  -9.4265],
        [  0.6228,  -2.0625],
        [  7.3246,  -9.9659],
        [  2.7778,  -4.1945],
        [  7.2208,  -9.2312],
        [  1.5050,  -5.3255],
        [  3.6753,  -5.1680],
        [ -4.5810,   3.0937],
        [  5.8767,  -7.7944],
        [  3.9850,  -5.5726],
        [ -5.3576,   3.8273],
        [  6.9407,  -8.6935],
        [ -3.1332,   1.7289],
        [  5.3556,  -8.5275],
        [ -0.4498,  -1.0553],
        [ -5.6109,   4.0084],
        [  6.9852,  -8.8268],
        [ -3.3606,   1.1475],
        [  7.5507,  -8.9844],
        [  6.7359,  -8.6677],
        [ -3.3643,   1.7616],
        [  2.5908,  -5.3136],
        [ -0.2553,  -1.1546],
        [  4.9980,  -6.6367],
        [  3.2772,  -4.9037],
        [  7.8449,  -9.2333],
        [ -3.0434,   1.6066],
        [  2.0306,  -3.7248],
        [ -2.3077,   0.3234],
        [  4.5557,  -5.9787],
        [  2.3252,  -3.7167],
        [  6.5458,  -8.5007],
        [  2.7527,  -4.1835],
        [ -4.1411,   1.7201],
        [  4.7536,  -6.1438],
        [ -5.5107,   3.8766],
        [  0.4265,  -2.5057],
        [ -5.9635,   2.5287],
        [  0.0817,  -2.4696],
        [ -4.7554,   1.1501],
        [  1.6305,  -3.2060],
        [  7.2136,  -8.6886]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5685, 0.4315],
        [0.5213, 0.4787]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7614, 0.2386], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2384, 0.0983],
         [0.3644, 0.3800]],

        [[0.2368, 0.1036],
         [0.6751, 0.6437]],

        [[0.3552, 0.0908],
         [0.9109, 0.4842]],

        [[0.1692, 0.1142],
         [0.6382, 0.5543]],

        [[0.7332, 0.0948],
         [0.1953, 0.3093]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 33
Adjusted Rand Index: 0.10666666666666667
time is 1
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 15
Adjusted Rand Index: 0.48484848484848486
Global Adjusted Rand Index: 0.14890610647228408
Average Adjusted Rand Index: 0.7103025914665932
Iteration 0: Loss = -20324.48965291581
Iteration 10: Loss = -11596.06002614257
Iteration 20: Loss = -11596.041692321432
Iteration 30: Loss = -11596.041687788
Iteration 40: Loss = -11596.041687788
1
Iteration 50: Loss = -11596.041687788
2
Iteration 60: Loss = -11596.041687788
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7671, 0.2329],
        [0.2659, 0.7341]], dtype=torch.float64)
alpha: tensor([0.4987, 0.5013])
beta: tensor([[[0.3929, 0.0959],
         [0.2946, 0.1956]],

        [[0.3832, 0.1036],
         [0.4945, 0.3775]],

        [[0.1880, 0.0917],
         [0.1093, 0.1476]],

        [[0.8464, 0.1145],
         [0.5799, 0.2382]],

        [[0.8774, 0.1000],
         [0.3101, 0.8253]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.984032001021952
Average Adjusted Rand Index: 0.9839995611635629
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20324.063358374995
Iteration 100: Loss = -12401.360477713879
Iteration 200: Loss = -11962.252904431276
Iteration 300: Loss = -11903.356650083651
Iteration 400: Loss = -11860.048394309344
Iteration 500: Loss = -11836.992713263846
Iteration 600: Loss = -11833.378635322964
Iteration 700: Loss = -11770.107344883909
Iteration 800: Loss = -11769.615840253586
Iteration 900: Loss = -11748.58563973045
Iteration 1000: Loss = -11737.25809105576
Iteration 1100: Loss = -11737.152874887672
Iteration 1200: Loss = -11737.06475574505
Iteration 1300: Loss = -11736.991218723944
Iteration 1400: Loss = -11727.609245930966
Iteration 1500: Loss = -11727.562329241126
Iteration 1600: Loss = -11727.396415945004
Iteration 1700: Loss = -11721.119274475599
Iteration 1800: Loss = -11721.054672307167
Iteration 1900: Loss = -11720.773789537956
Iteration 2000: Loss = -11716.78367517
Iteration 2100: Loss = -11714.493580598244
Iteration 2200: Loss = -11712.328116240737
Iteration 2300: Loss = -11711.881528284968
Iteration 2400: Loss = -11711.801406221486
Iteration 2500: Loss = -11711.802943500554
1
Iteration 2600: Loss = -11711.772154387028
Iteration 2700: Loss = -11711.79348337765
1
Iteration 2800: Loss = -11711.755201078084
Iteration 2900: Loss = -11711.74902142813
Iteration 3000: Loss = -11711.744802167705
Iteration 3100: Loss = -11711.739339989883
Iteration 3200: Loss = -11711.735342733316
Iteration 3300: Loss = -11711.7324332631
Iteration 3400: Loss = -11711.728380934515
Iteration 3500: Loss = -11711.725386750555
Iteration 3600: Loss = -11711.734128370896
1
Iteration 3700: Loss = -11711.72052705562
Iteration 3800: Loss = -11711.726015759283
1
Iteration 3900: Loss = -11711.716790095981
Iteration 4000: Loss = -11711.71519316913
Iteration 4100: Loss = -11711.713643156789
Iteration 4200: Loss = -11711.712318410302
Iteration 4300: Loss = -11711.711479322594
Iteration 4400: Loss = -11711.709806051995
Iteration 4500: Loss = -11711.708735294904
Iteration 4600: Loss = -11711.708444833414
Iteration 4700: Loss = -11711.706113365033
Iteration 4800: Loss = -11711.524825510276
Iteration 4900: Loss = -11711.518832424943
Iteration 5000: Loss = -11711.517956551557
Iteration 5100: Loss = -11711.517729137826
Iteration 5200: Loss = -11711.516066669072
Iteration 5300: Loss = -11711.51540157613
Iteration 5400: Loss = -11711.514891322206
Iteration 5500: Loss = -11711.572938744797
1
Iteration 5600: Loss = -11711.514130619373
Iteration 5700: Loss = -11711.513702307775
Iteration 5800: Loss = -11711.627207096895
1
Iteration 5900: Loss = -11711.513273383445
Iteration 6000: Loss = -11711.555505254883
1
Iteration 6100: Loss = -11711.51361709897
2
Iteration 6200: Loss = -11711.538610656258
3
Iteration 6300: Loss = -11711.511569371887
Iteration 6400: Loss = -11711.515919012734
1
Iteration 6500: Loss = -11711.510745264979
Iteration 6600: Loss = -11711.51059038733
Iteration 6700: Loss = -11711.510276400579
Iteration 6800: Loss = -11711.512015907298
1
Iteration 6900: Loss = -11711.509775416442
Iteration 7000: Loss = -11711.509745779294
Iteration 7100: Loss = -11711.509372352266
Iteration 7200: Loss = -11711.509507841049
1
Iteration 7300: Loss = -11711.509359001837
Iteration 7400: Loss = -11711.508848982729
Iteration 7500: Loss = -11711.509504195752
1
Iteration 7600: Loss = -11711.519583180849
2
Iteration 7700: Loss = -11711.509742167773
3
Iteration 7800: Loss = -11711.508606160864
Iteration 7900: Loss = -11711.508952060609
1
Iteration 8000: Loss = -11711.508626278975
2
Iteration 8100: Loss = -11711.507883886556
Iteration 8200: Loss = -11711.512757516177
1
Iteration 8300: Loss = -11711.512211856956
2
Iteration 8400: Loss = -11711.513222844851
3
Iteration 8500: Loss = -11711.50946868118
4
Iteration 8600: Loss = -11711.506783704745
Iteration 8700: Loss = -11711.506919953266
1
Iteration 8800: Loss = -11711.516469342263
2
Iteration 8900: Loss = -11711.506120520588
Iteration 9000: Loss = -11711.509946940752
1
Iteration 9100: Loss = -11711.506236134212
2
Iteration 9200: Loss = -11711.513108649473
3
Iteration 9300: Loss = -11711.506580588824
4
Iteration 9400: Loss = -11711.506262346105
5
Iteration 9500: Loss = -11711.51920516617
6
Iteration 9600: Loss = -11711.514971915229
7
Iteration 9700: Loss = -11711.505705728143
Iteration 9800: Loss = -11711.506622574345
1
Iteration 9900: Loss = -11711.52515850903
2
Iteration 10000: Loss = -11711.505565375397
Iteration 10100: Loss = -11711.507286172147
1
Iteration 10200: Loss = -11711.570779717058
2
Iteration 10300: Loss = -11711.510428439595
3
Iteration 10400: Loss = -11711.505556623135
Iteration 10500: Loss = -11711.505522198602
Iteration 10600: Loss = -11711.541411518354
1
Iteration 10700: Loss = -11711.505258648926
Iteration 10800: Loss = -11711.520510032025
1
Iteration 10900: Loss = -11711.336376863432
Iteration 11000: Loss = -11711.336729525181
1
Iteration 11100: Loss = -11711.36302021101
2
Iteration 11200: Loss = -11711.344805546434
3
Iteration 11300: Loss = -11711.33578227521
Iteration 11400: Loss = -11711.33547956514
Iteration 11500: Loss = -11711.337277752791
1
Iteration 11600: Loss = -11711.334766919827
Iteration 11700: Loss = -11711.336046757051
1
Iteration 11800: Loss = -11711.456287153478
2
Iteration 11900: Loss = -11711.334639032077
Iteration 12000: Loss = -11711.333913443666
Iteration 12100: Loss = -11711.33523215571
1
Iteration 12200: Loss = -11711.334298365931
2
Iteration 12300: Loss = -11711.335096567995
3
Iteration 12400: Loss = -11711.33754589078
4
Iteration 12500: Loss = -11711.388816809722
5
Iteration 12600: Loss = -11711.359490300038
6
Iteration 12700: Loss = -11711.33498147408
7
Iteration 12800: Loss = -11711.34070297137
8
Iteration 12900: Loss = -11711.336335794329
9
Iteration 13000: Loss = -11711.335016629506
10
Stopping early at iteration 13000 due to no improvement.
tensor([[-4.5477e+00,  8.1485e-01],
        [-6.3070e+00,  3.2067e+00],
        [-4.1384e+00,  1.5870e+00],
        [ 2.8594e+00, -4.9953e+00],
        [-7.6904e+00,  6.2913e+00],
        [-2.1774e+00,  2.0539e-01],
        [-7.9001e-01, -8.1061e-01],
        [-4.7220e+00,  3.2925e+00],
        [-2.7982e+00,  7.3542e-01],
        [-7.5720e+00,  6.0230e+00],
        [ 1.3337e+00, -2.7531e+00],
        [-8.3587e+00,  6.9516e+00],
        [-7.6311e+00,  6.1470e+00],
        [-7.9150e+00,  6.4540e+00],
        [-5.0332e+00,  2.7584e+00],
        [-6.5320e+00,  5.1206e+00],
        [-6.6373e+00,  5.1304e+00],
        [-8.7651e+00,  7.2421e+00],
        [-5.8014e+00,  4.2325e+00],
        [ 5.4368e-01, -1.9410e+00],
        [-4.9396e+00,  3.5530e+00],
        [-4.7052e+00,  2.2211e+00],
        [-7.4614e+00,  4.7146e+00],
        [-6.9111e+00,  3.5160e+00],
        [-6.2929e+00,  2.9826e+00],
        [-3.0545e+00,  1.5187e+00],
        [-9.6810e+00,  6.8460e+00],
        [ 2.3666e+00, -3.7566e+00],
        [ 9.7072e-02, -1.6154e+00],
        [-8.2829e+00,  6.8248e+00],
        [-3.4970e+00,  1.4575e+00],
        [-6.3823e+00,  4.9608e+00],
        [-7.8691e+00,  5.8681e+00],
        [-4.5841e+00,  3.0051e+00],
        [-9.1891e+00,  6.7402e+00],
        [-6.5230e+00,  4.9093e+00],
        [-6.2012e+00,  3.7011e+00],
        [-5.7328e+00,  3.3632e+00],
        [-7.4707e+00,  5.6771e+00],
        [-8.6763e+00,  6.6221e+00],
        [-7.8114e+00,  6.4173e+00],
        [-8.9350e+00,  7.1636e+00],
        [-8.1157e+00,  6.5398e+00],
        [-9.9453e+00,  7.0421e+00],
        [-4.3951e+00,  3.0027e+00],
        [-7.2898e+00,  5.6521e+00],
        [-1.0243e+01,  8.6055e+00],
        [-7.2798e+00,  5.6359e+00],
        [-8.2386e+00,  5.9654e+00],
        [ 1.9237e+00, -5.2310e+00],
        [-6.8028e+00,  5.3738e+00],
        [-4.5104e+00,  2.9580e+00],
        [-2.3126e+00, -7.1725e-01],
        [-5.0006e+00,  1.4421e+00],
        [-8.0201e+00,  6.3215e+00],
        [-3.7751e+00,  2.0344e+00],
        [-4.7561e+00,  2.7702e+00],
        [-9.2175e+00,  7.5226e+00],
        [-7.5238e+00,  5.1755e+00],
        [-3.4396e+00,  8.0460e-01],
        [-5.9223e+00,  4.5192e+00],
        [-4.3295e+00,  1.8248e+00],
        [-6.3903e+00,  4.9248e+00],
        [-3.8517e+00,  2.4404e+00],
        [-6.2917e+00,  3.7780e+00],
        [-1.4325e+00, -1.4522e-02],
        [-7.7475e+00,  6.3437e+00],
        [-6.8781e+00,  3.7707e+00],
        [-2.8275e+00,  1.2712e+00],
        [-9.2664e+00,  7.4684e+00],
        [-2.2178e+00,  8.1084e-01],
        [-8.9530e+00,  6.8071e+00],
        [-2.3534e+00, -2.2619e+00],
        [ 2.0537e+00, -3.4858e+00],
        [-9.0356e+00,  7.6447e+00],
        [-5.1078e+00,  3.4667e+00],
        [-6.8912e+00,  4.9942e+00],
        [-7.1994e+00,  5.7855e+00],
        [-8.9043e-01, -4.9685e-01],
        [-4.7325e+00,  2.1391e+00],
        [-5.2756e+00,  3.3898e+00],
        [-5.3962e+00,  3.8156e+00],
        [-4.9525e+00,  1.8728e+00],
        [-1.0086e+01,  7.0942e+00],
        [-5.9062e+00,  3.8973e+00],
        [-5.2204e+00,  3.7916e+00],
        [-7.2905e+00,  3.1078e+00],
        [-6.4844e+00,  3.7707e+00],
        [-7.0151e+00,  5.5711e+00],
        [-7.4830e+00,  5.6070e+00],
        [-6.2081e+00,  1.5929e+00],
        [-2.2539e+00,  8.6748e-01],
        [-5.8991e+00,  4.4409e+00],
        [ 1.0808e+00, -2.6188e+00],
        [-3.4513e+00,  1.5426e+00],
        [ 1.4769e+00, -4.2006e+00],
        [-2.9253e+00,  1.4126e+00],
        [-2.5545e+00, -6.4042e-03],
        [-5.8377e+00,  4.2257e+00],
        [-8.3831e+00,  6.5438e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7474, 0.2526],
        [0.3502, 0.6498]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1084, 0.8916], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4052, 0.0894],
         [0.2946, 0.2020]],

        [[0.3832, 0.1035],
         [0.4945, 0.3775]],

        [[0.1880, 0.0917],
         [0.1093, 0.1476]],

        [[0.8464, 0.1143],
         [0.5799, 0.2382]],

        [[0.8774, 0.1003],
         [0.3101, 0.8253]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6076298742476992
Average Adjusted Rand Index: 0.8
Iteration 0: Loss = -17073.35024647661
Iteration 10: Loss = -11596.044184253034
Iteration 20: Loss = -11596.041693504649
Iteration 30: Loss = -11596.041690843662
Iteration 40: Loss = -11596.041690843662
1
Iteration 50: Loss = -11596.041690843662
2
Iteration 60: Loss = -11596.041690843662
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7671, 0.2329],
        [0.2659, 0.7341]], dtype=torch.float64)
alpha: tensor([0.4987, 0.5013])
beta: tensor([[[0.3929, 0.0959],
         [0.0240, 0.1956]],

        [[0.3324, 0.1036],
         [0.6450, 0.4254]],

        [[0.6239, 0.0917],
         [0.8863, 0.1240]],

        [[0.0879, 0.1145],
         [0.9220, 0.0135]],

        [[0.1815, 0.1000],
         [0.7025, 0.1336]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.984032001021952
Average Adjusted Rand Index: 0.9839995611635629
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17073.10637266006
Iteration 100: Loss = -12337.000286239841
Iteration 200: Loss = -11850.414872041527
Iteration 300: Loss = -11799.11245393595
Iteration 400: Loss = -11794.478824316451
Iteration 500: Loss = -11789.020155867442
Iteration 600: Loss = -11763.314770484576
Iteration 700: Loss = -11756.10819178924
Iteration 800: Loss = -11658.960079333307
Iteration 900: Loss = -11632.05846397056
Iteration 1000: Loss = -11622.502829569185
Iteration 1100: Loss = -11622.451781766498
Iteration 1200: Loss = -11622.421686609496
Iteration 1300: Loss = -11622.39319719122
Iteration 1400: Loss = -11609.877362035222
Iteration 1500: Loss = -11593.795290380782
Iteration 1600: Loss = -11593.776685522513
Iteration 1700: Loss = -11593.765593639979
Iteration 1800: Loss = -11593.757005527512
Iteration 1900: Loss = -11593.75006922578
Iteration 2000: Loss = -11593.744293407906
Iteration 2100: Loss = -11593.739471643414
Iteration 2200: Loss = -11593.735362102587
Iteration 2300: Loss = -11593.731825675732
Iteration 2400: Loss = -11593.728725732508
Iteration 2500: Loss = -11593.726018399724
Iteration 2600: Loss = -11593.72368286158
Iteration 2700: Loss = -11593.721580903775
Iteration 2800: Loss = -11593.719643573044
Iteration 2900: Loss = -11593.717974399393
Iteration 3000: Loss = -11593.71647161279
Iteration 3100: Loss = -11593.715054570588
Iteration 3200: Loss = -11593.713675182553
Iteration 3300: Loss = -11593.709957071665
Iteration 3400: Loss = -11593.712523581537
1
Iteration 3500: Loss = -11593.706714910706
Iteration 3600: Loss = -11593.705820798497
Iteration 3700: Loss = -11593.70503539992
Iteration 3800: Loss = -11593.704326780688
Iteration 3900: Loss = -11593.703662516442
Iteration 4000: Loss = -11593.703056982466
Iteration 4100: Loss = -11593.709227643732
1
Iteration 4200: Loss = -11593.701949453623
Iteration 4300: Loss = -11593.701466789895
Iteration 4400: Loss = -11593.701041968512
Iteration 4500: Loss = -11593.700681250046
Iteration 4600: Loss = -11593.700212423391
Iteration 4700: Loss = -11593.699871514465
Iteration 4800: Loss = -11593.69973434268
Iteration 4900: Loss = -11593.699201096759
Iteration 5000: Loss = -11593.698903665492
Iteration 5100: Loss = -11593.70047659623
1
Iteration 5200: Loss = -11593.698397745986
Iteration 5300: Loss = -11593.698151011895
Iteration 5400: Loss = -11593.697920090426
Iteration 5500: Loss = -11593.69781324683
Iteration 5600: Loss = -11593.697514034995
Iteration 5700: Loss = -11593.697347974707
Iteration 5800: Loss = -11593.697481331861
1
Iteration 5900: Loss = -11593.697188030143
Iteration 6000: Loss = -11593.696805718791
Iteration 6100: Loss = -11593.696671882799
Iteration 6200: Loss = -11593.696651847727
Iteration 6300: Loss = -11593.696614025566
Iteration 6400: Loss = -11593.69636239123
Iteration 6500: Loss = -11593.696271796241
Iteration 6600: Loss = -11593.700073432079
1
Iteration 6700: Loss = -11593.70179165817
2
Iteration 6800: Loss = -11593.698293318148
3
Iteration 6900: Loss = -11593.755708841512
4
Iteration 7000: Loss = -11593.696648109746
5
Iteration 7100: Loss = -11593.696149814572
Iteration 7200: Loss = -11593.695541787989
Iteration 7300: Loss = -11593.695580248095
1
Iteration 7400: Loss = -11593.69778949679
2
Iteration 7500: Loss = -11593.733309548237
3
Iteration 7600: Loss = -11593.704310838806
4
Iteration 7700: Loss = -11593.69729007098
5
Iteration 7800: Loss = -11593.704198730888
6
Iteration 7900: Loss = -11593.696186065272
7
Iteration 8000: Loss = -11593.695376365202
Iteration 8100: Loss = -11593.695768246245
1
Iteration 8200: Loss = -11593.696483919168
2
Iteration 8300: Loss = -11593.70508849565
3
Iteration 8400: Loss = -11593.694858207211
Iteration 8500: Loss = -11593.695332765861
1
Iteration 8600: Loss = -11593.694892887295
2
Iteration 8700: Loss = -11593.696290140582
3
Iteration 8800: Loss = -11593.816318269206
4
Iteration 8900: Loss = -11593.696952127704
5
Iteration 9000: Loss = -11593.694997361621
6
Iteration 9100: Loss = -11593.69546750278
7
Iteration 9200: Loss = -11593.697265105442
8
Iteration 9300: Loss = -11593.701499690078
9
Iteration 9400: Loss = -11593.69512149443
10
Stopping early at iteration 9400 due to no improvement.
tensor([[-5.3314,  2.6456],
        [-7.7649,  6.2916],
        [-8.1108,  6.6516],
        [-8.9048,  5.8686],
        [ 6.9514, -8.4007],
        [-7.2503,  5.4152],
        [-8.9598,  5.9274],
        [-5.2724,  3.0201],
        [-9.2438,  5.6356],
        [-6.4943,  4.3215],
        [-8.1652,  5.9749],
        [ 6.3394, -8.1015],
        [-1.1972, -0.4182],
        [ 4.4404, -6.4912],
        [-9.1377,  6.3421],
        [ 3.8701, -5.6512],
        [ 5.2639, -7.3673],
        [ 6.4704, -8.0383],
        [ 3.7047, -5.3981],
        [-5.2013,  3.7900],
        [ 3.4884, -5.0551],
        [-9.4105,  6.2583],
        [-9.0545,  4.4393],
        [ 4.2632, -5.6861],
        [-7.8025,  5.7510],
        [-6.6077,  4.6510],
        [ 4.7154, -6.2223],
        [-8.1358,  6.5706],
        [-8.9438,  5.4980],
        [ 6.9195, -8.4842],
        [ 3.1220, -6.6833],
        [ 4.7530, -6.8886],
        [ 6.4795, -8.0677],
        [ 2.3138, -4.6632],
        [ 6.2282, -7.6618],
        [-4.3678,  1.9251],
        [ 3.5999, -5.3542],
        [-7.6491,  3.0339],
        [ 6.1656, -8.7947],
        [ 3.9206, -5.3073],
        [ 5.2232, -7.9622],
        [ 6.8906, -8.5644],
        [ 5.6322, -8.0740],
        [ 3.2841, -4.6909],
        [ 1.4220, -3.0108],
        [ 7.1650, -8.6234],
        [ 5.5857, -7.0808],
        [-4.4964,  1.6842],
        [ 5.9381, -7.4588],
        [-8.1897,  6.7962],
        [ 3.0371, -4.5058],
        [-7.9745,  6.3595],
        [-8.0127,  6.4452],
        [-4.6382,  3.2498],
        [ 6.9327, -8.3838],
        [-7.6432,  5.9096],
        [-5.5333,  4.1466],
        [ 5.8300, -7.2416],
        [ 6.8804, -8.3982],
        [-8.3230,  6.2645],
        [ 5.1755, -8.2235],
        [-5.5308,  3.3022],
        [ 5.0299, -6.7441],
        [-8.0083,  6.1462],
        [-8.3219,  6.9120],
        [-9.7806,  5.1654],
        [ 5.0825, -6.9133],
        [-4.8662,  3.3953],
        [-8.4624,  7.0146],
        [ 6.6992, -8.3583],
        [-8.0229,  6.4932],
        [ 6.9017, -8.3134],
        [-7.3401,  4.9571],
        [-9.9077,  6.9906],
        [ 5.7932, -8.7911],
        [-8.6731,  6.7157],
        [ 5.9763, -7.6590],
        [ 6.1276, -7.5191],
        [-5.6459,  3.6430],
        [-8.8871,  5.9510],
        [-8.6900,  6.1862],
        [ 3.3620, -4.7561],
        [-3.5063,  2.1042],
        [ 6.8011, -8.4029],
        [-8.6696,  5.7925],
        [-8.8317,  6.8530],
        [-8.1128,  6.0144],
        [ 3.0665, -6.1883],
        [-8.2076,  6.7807],
        [ 5.8669, -7.8417],
        [-7.2466,  5.5982],
        [-8.8656,  5.7928],
        [ 6.5049, -8.7965],
        [-7.5419,  5.2851],
        [-7.9042,  6.0624],
        [-7.7824,  6.2476],
        [-4.6676,  2.8876],
        [-4.8251,  3.4388],
        [-8.2616,  6.3313],
        [ 6.1925, -8.1017]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7726, 0.2274],
        [0.2620, 0.7380]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4530, 0.5470], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4010, 0.0962],
         [0.0240, 0.1995]],

        [[0.3324, 0.1034],
         [0.6450, 0.4254]],

        [[0.6239, 0.0920],
         [0.8863, 0.1240]],

        [[0.0879, 0.1141],
         [0.9220, 0.0135]],

        [[0.1815, 0.0999],
         [0.7025, 0.1336]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -48134.9410487622
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.9543,    nan]],

        [[0.0695,    nan],
         [0.0285, 0.2352]],

        [[0.2589,    nan],
         [0.7371, 0.5420]],

        [[0.3779,    nan],
         [0.2059, 0.2711]],

        [[0.7545,    nan],
         [0.4848, 0.9648]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -48132.76523326043
Iteration 100: Loss = -12434.799603200618
Iteration 200: Loss = -12415.512347334503
Iteration 300: Loss = -12409.949071721372
Iteration 400: Loss = -12407.068213892591
Iteration 500: Loss = -12405.342735440629
Iteration 600: Loss = -12404.185820747382
Iteration 700: Loss = -12402.739822271058
Iteration 800: Loss = -12401.755391597484
Iteration 900: Loss = -12400.243746450362
Iteration 1000: Loss = -12398.106465482704
Iteration 1100: Loss = -12395.945178828833
Iteration 1200: Loss = -12393.295479945622
Iteration 1300: Loss = -12391.042191263181
Iteration 1400: Loss = -12388.932257074845
Iteration 1500: Loss = -12385.532051100341
Iteration 1600: Loss = -12377.584913814888
Iteration 1700: Loss = -12366.562696300089
Iteration 1800: Loss = -12341.800479403513
Iteration 1900: Loss = -12304.482856043152
Iteration 2000: Loss = -12264.709796215277
Iteration 2100: Loss = -12196.462917948962
Iteration 2200: Loss = -12094.484288536296
Iteration 2300: Loss = -11999.200480550538
Iteration 2400: Loss = -11964.776997916582
Iteration 2500: Loss = -11949.99830089284
Iteration 2600: Loss = -11949.297230754615
Iteration 2700: Loss = -11947.315103038167
Iteration 2800: Loss = -11940.503195868376
Iteration 2900: Loss = -11940.333919252402
Iteration 3000: Loss = -11940.215596793694
Iteration 3100: Loss = -11940.122727979937
Iteration 3200: Loss = -11940.046237946566
Iteration 3300: Loss = -11939.976441129797
Iteration 3400: Loss = -11931.99190721923
Iteration 3500: Loss = -11931.840087734245
Iteration 3600: Loss = -11931.78876760618
Iteration 3700: Loss = -11931.742814766889
Iteration 3800: Loss = -11931.683352441683
Iteration 3900: Loss = -11917.419219700709
Iteration 4000: Loss = -11917.353034131545
Iteration 4100: Loss = -11917.172784412658
Iteration 4200: Loss = -11917.12356782632
Iteration 4300: Loss = -11917.081387590657
Iteration 4400: Loss = -11916.9046261551
Iteration 4500: Loss = -11915.715186280424
Iteration 4600: Loss = -11915.138527389818
Iteration 4700: Loss = -11905.449645071314
Iteration 4800: Loss = -11905.431925836365
Iteration 4900: Loss = -11905.416673383594
Iteration 5000: Loss = -11905.401562416926
Iteration 5100: Loss = -11905.379713548316
Iteration 5200: Loss = -11905.232309941313
Iteration 5300: Loss = -11905.206148378034
Iteration 5400: Loss = -11895.740532831083
Iteration 5500: Loss = -11895.726674200187
Iteration 5600: Loss = -11895.693829375723
Iteration 5700: Loss = -11895.529449393493
Iteration 5800: Loss = -11895.52170596046
Iteration 5900: Loss = -11895.512635552128
Iteration 6000: Loss = -11886.871715254882
Iteration 6100: Loss = -11886.822523595314
Iteration 6200: Loss = -11886.814733597863
Iteration 6300: Loss = -11886.808619237687
Iteration 6400: Loss = -11886.803614973722
Iteration 6500: Loss = -11886.796831533688
Iteration 6600: Loss = -11886.790919328778
Iteration 6700: Loss = -11883.587065459164
Iteration 6800: Loss = -11883.578728148126
Iteration 6900: Loss = -11883.582313007371
1
Iteration 7000: Loss = -11883.561090613195
Iteration 7100: Loss = -11883.560428563147
Iteration 7200: Loss = -11883.554263730748
Iteration 7300: Loss = -11883.53331081488
Iteration 7400: Loss = -11883.536396185385
1
Iteration 7500: Loss = -11883.517791411428
Iteration 7600: Loss = -11883.510321976295
Iteration 7700: Loss = -11883.425107570334
Iteration 7800: Loss = -11881.677918335607
Iteration 7900: Loss = -11881.12123638597
Iteration 8000: Loss = -11879.840535410229
Iteration 8100: Loss = -11879.689978294913
Iteration 8200: Loss = -11865.073608858384
Iteration 8300: Loss = -11863.589818842462
Iteration 8400: Loss = -11863.583694871388
Iteration 8500: Loss = -11863.58186354421
Iteration 8600: Loss = -11851.985774344546
Iteration 8700: Loss = -11851.973788507446
Iteration 8800: Loss = -11851.9852930391
1
Iteration 8900: Loss = -11851.965904103732
Iteration 9000: Loss = -11851.963735560827
Iteration 9100: Loss = -11851.970360570616
1
Iteration 9200: Loss = -11851.960638746574
Iteration 9300: Loss = -11851.962488692556
1
Iteration 9400: Loss = -11851.96762163161
2
Iteration 9500: Loss = -11851.974805909975
3
Iteration 9600: Loss = -11851.930411715406
Iteration 9700: Loss = -11847.524763783524
Iteration 9800: Loss = -11832.523798660943
Iteration 9900: Loss = -11832.513448033169
Iteration 10000: Loss = -11832.512020647335
Iteration 10100: Loss = -11832.62876908864
1
Iteration 10200: Loss = -11832.509656412183
Iteration 10300: Loss = -11832.53037811831
1
Iteration 10400: Loss = -11832.516909701313
2
Iteration 10500: Loss = -11832.512361287561
3
Iteration 10600: Loss = -11832.508561304181
Iteration 10700: Loss = -11832.50643721956
Iteration 10800: Loss = -11828.351209557144
Iteration 10900: Loss = -11828.35907558901
1
Iteration 11000: Loss = -11828.342549529485
Iteration 11100: Loss = -11828.347465339759
1
Iteration 11200: Loss = -11828.33857006854
Iteration 11300: Loss = -11828.333625293524
Iteration 11400: Loss = -11828.350320181724
1
Iteration 11500: Loss = -11828.333053596394
Iteration 11600: Loss = -11828.336763364221
1
Iteration 11700: Loss = -11828.332880219446
Iteration 11800: Loss = -11828.334563527545
1
Iteration 11900: Loss = -11828.334271831449
2
Iteration 12000: Loss = -11828.33313301023
3
Iteration 12100: Loss = -11828.33214315004
Iteration 12200: Loss = -11828.333199115526
1
Iteration 12300: Loss = -11828.447750669795
2
Iteration 12400: Loss = -11828.335878620403
3
Iteration 12500: Loss = -11821.873981384733
Iteration 12600: Loss = -11821.87145585241
Iteration 12700: Loss = -11821.870923535249
Iteration 12800: Loss = -11821.873119219752
1
Iteration 12900: Loss = -11821.934294688856
2
Iteration 13000: Loss = -11815.15292076692
Iteration 13100: Loss = -11815.245832704588
1
Iteration 13200: Loss = -11815.145688128163
Iteration 13300: Loss = -11815.138990299667
Iteration 13400: Loss = -11815.1406803625
1
Iteration 13500: Loss = -11815.138423161363
Iteration 13600: Loss = -11815.139324146066
1
Iteration 13700: Loss = -11815.142941465585
2
Iteration 13800: Loss = -11815.221614310702
3
Iteration 13900: Loss = -11810.984540050988
Iteration 14000: Loss = -11810.978465775865
Iteration 14100: Loss = -11810.977813340402
Iteration 14200: Loss = -11811.046817004957
1
Iteration 14300: Loss = -11810.973484090257
Iteration 14400: Loss = -11810.973543725127
1
Iteration 14500: Loss = -11810.987543406274
2
Iteration 14600: Loss = -11810.974528893603
3
Iteration 14700: Loss = -11810.973881053516
4
Iteration 14800: Loss = -11810.976411056603
5
Iteration 14900: Loss = -11810.97600204618
6
Iteration 15000: Loss = -11811.038742524463
7
Iteration 15100: Loss = -11810.97012250056
Iteration 15200: Loss = -11810.966766554555
Iteration 15300: Loss = -11810.964084198913
Iteration 15400: Loss = -11810.965661788794
1
Iteration 15500: Loss = -11810.968195903844
2
Iteration 15600: Loss = -11810.968778494374
3
Iteration 15700: Loss = -11810.965996345962
4
Iteration 15800: Loss = -11810.855293501141
Iteration 15900: Loss = -11810.855558981608
1
Iteration 16000: Loss = -11810.85533554095
2
Iteration 16100: Loss = -11810.891864019302
3
Iteration 16200: Loss = -11810.85892314161
4
Iteration 16300: Loss = -11810.85181832354
Iteration 16400: Loss = -11810.853353601926
1
Iteration 16500: Loss = -11810.851069355278
Iteration 16600: Loss = -11810.849123478622
Iteration 16700: Loss = -11810.848751730482
Iteration 16800: Loss = -11810.848072594528
Iteration 16900: Loss = -11810.816899491367
Iteration 17000: Loss = -11810.840928406466
1
Iteration 17100: Loss = -11810.827675534047
2
Iteration 17200: Loss = -11810.824107294278
3
Iteration 17300: Loss = -11810.819733156417
4
Iteration 17400: Loss = -11810.88390946536
5
Iteration 17500: Loss = -11810.845354335479
6
Iteration 17600: Loss = -11810.826881612042
7
Iteration 17700: Loss = -11810.815996962252
Iteration 17800: Loss = -11810.813684816903
Iteration 17900: Loss = -11810.81357046391
Iteration 18000: Loss = -11810.8180400371
1
Iteration 18100: Loss = -11810.825115592967
2
Iteration 18200: Loss = -11810.81515156561
3
Iteration 18300: Loss = -11810.817286554968
4
Iteration 18400: Loss = -11810.889974998503
5
Iteration 18500: Loss = -11810.815243441244
6
Iteration 18600: Loss = -11810.813856260897
7
Iteration 18700: Loss = -11810.818169632861
8
Iteration 18800: Loss = -11810.817707152044
9
Iteration 18900: Loss = -11810.818153251063
10
Stopping early at iteration 18900 due to no improvement.
tensor([[ -3.6740,   1.7815],
        [ -6.0866,   4.6793],
        [ -3.8707,   1.4782],
        [  2.7006,  -5.5748],
        [ -9.0618,   5.9112],
        [ -1.8823,   0.3540],
        [ -2.4723,  -0.5447],
        [ -4.6389,   3.1849],
        [ -3.2219,   0.7046],
        [ -7.5065,   6.0951],
        [ -0.5557,  -2.1945],
        [ -8.8072,   6.6807],
        [ -7.8500,   6.0593],
        [ -8.5871,   6.5897],
        [ -4.9635,   2.7143],
        [ -7.0414,   5.6542],
        [ -7.3168,   5.8177],
        [ -9.1778,   7.5457],
        [ -5.9149,   3.7326],
        [  2.0224,  -3.6759],
        [ -4.7606,   3.2673],
        [ -4.0154,   2.6025],
        [ -6.5384,   5.0884],
        [ -5.8377,   4.4286],
        [ -5.3112,   3.7531],
        [ -3.9086,   0.8919],
        [ -9.5289,   7.4924],
        [  1.8559,  -3.2451],
        [  0.1102,  -1.6222],
        [ -9.0325,   7.6374],
        [ -3.0747,   1.5258],
        [ -7.2898,   3.9549],
        [ -7.4299,   6.0435],
        [ -9.9681,   6.7106],
        [-10.0502,   8.1600],
        [ -7.5435,   6.1566],
        [ -6.3379,   4.5951],
        [ -5.8772,   3.9874],
        [ -8.7362,   6.8674],
        [ -8.7898,   7.3851],
        [ -9.3046,   6.5259],
        [ -9.9967,   8.2679],
        [ -8.7550,   7.3648],
        [ -9.5064,   8.1110],
        [ -9.2564,   7.7102],
        [ -7.8755,   6.4221],
        [ -8.5154,   6.9243],
        [ -7.6598,   6.1036],
        [ -8.1237,   6.5290],
        [  2.4102,  -3.8280],
        [ -7.2987,   4.6623],
        [ -4.2148,   2.8282],
        [ -1.7664,  -0.1192],
        [ -4.0134,   2.3827],
        [ -8.6351,   7.1364],
        [ -5.1791,   0.5639],
        [ -4.6030,   2.9301],
        [ -6.2775,   4.2762],
        [ -7.4002,   5.0778],
        [ -3.5406,   0.3605],
        [ -9.2456,   6.3951],
        [ -4.1408,   2.0586],
        [ -7.0757,   5.6875],
        [ -3.7245,   2.3109],
        [ -5.7132,   4.2752],
        [ -1.7211,  -0.5066],
        [ -7.9524,   6.5273],
        [ -6.2314,   4.2943],
        [ -3.2168,   0.1353],
        [ -9.7080,   7.2547],
        [ -2.2324,   0.7709],
        [ -9.1672,   7.2718],
        [ -8.3933,   6.4792],
        [  2.0704,  -3.5660],
        [ -8.9290,   7.2780],
        [ -4.7684,   3.2618],
        [ -8.3759,   6.1581],
        [-10.1386,   8.1816],
        [ -0.8257,  -0.7639],
        [ -5.5318,   2.6598],
        [ -5.1219,   3.2857],
        [ -5.3485,   3.8948],
        [ -4.5809,   3.1720],
        [ -8.7433,   7.2708],
        [ -6.7779,   5.2296],
        [ -5.5778,   3.3059],
        [ -5.8204,   4.0219],
        [ -6.7468,   3.3834],
        [ -7.6172,   5.9592],
        [ -7.6185,   5.3183],
        [ -4.5785,   3.1909],
        [ -2.3995,   0.8916],
        [ -5.7219,   4.3187],
        [  1.2857,  -3.1977],
        [ -3.8221,   2.0824],
        [  1.6074,  -3.6796],
        [ -4.1903,   2.5789],
        [ -2.6105,   0.0648],
        [ -5.5960,   4.2090],
        [ -8.3546,   6.8551]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7191, 0.2809],
        [0.3444, 0.6556]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1001, 0.8999], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4122, 0.0878],
         [0.9543, 0.1963]],

        [[0.0695, 0.1238],
         [0.0285, 0.2352]],

        [[0.2589, 0.1146],
         [0.7371, 0.5420]],

        [[0.3779, 0.1143],
         [0.2059, 0.2711]],

        [[0.7545, 0.0999],
         [0.4848, 0.9648]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080477173169247
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080965973782139
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5467248656840907
Average Adjusted Rand Index: 0.7145011513753181
11600.475379451182
new:  [0.14890610647228408, 0.6076298742476992, 1.0, 0.5467248656840907] [0.7103025914665932, 0.8, 1.0, 0.7145011513753181] [11879.08012515561, 11711.335016629506, 11593.69512149443, 11810.818153251063]
prior:  [0.984032001021952, 0.984032001021952, 0.984032001021952, 0.0] [0.9839995611635629, 0.9839995611635629, 0.9839995611635629, 0.0] [11596.041692336561, 11596.041687788, 11596.041690843662, nan]
-----------------------------------------------------------------------------------------
This iteration is 13
True Objective function: Loss = -11540.416477843703
Iteration 0: Loss = -19847.281532842197
Iteration 10: Loss = -11679.369493604174
Iteration 20: Loss = -11683.529277414906
1
Iteration 30: Loss = -11683.930177619144
2
Iteration 40: Loss = -11684.137657093826
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.6066, 0.3934],
        [0.3949, 0.6051]], dtype=torch.float64)
alpha: tensor([0.4733, 0.5267])
beta: tensor([[[0.3734, 0.1025],
         [0.8900, 0.2201]],

        [[0.3593, 0.1075],
         [0.3637, 0.4545]],

        [[0.9497, 0.0976],
         [0.7213, 0.0780]],

        [[0.7601, 0.0917],
         [0.0740, 0.6093]],

        [[0.3302, 0.1003],
         [0.6559, 0.5995]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 87
Adjusted Rand Index: 0.5432432604425143
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.41356436766345334
Average Adjusted Rand Index: 0.8926469906593759
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19838.661864254052
Iteration 100: Loss = -12311.864805196014
Iteration 200: Loss = -12307.638565188476
Iteration 300: Loss = -12296.318728947123
Iteration 400: Loss = -12144.59849449874
Iteration 500: Loss = -11811.272623705292
Iteration 600: Loss = -11677.274926486096
Iteration 700: Loss = -11644.480966048397
Iteration 800: Loss = -11620.03663564557
Iteration 900: Loss = -11612.611502497813
Iteration 1000: Loss = -11594.53062253623
Iteration 1100: Loss = -11594.39207470436
Iteration 1200: Loss = -11594.310582920918
Iteration 1300: Loss = -11589.056946230388
Iteration 1400: Loss = -11588.990614163233
Iteration 1500: Loss = -11588.962459081731
Iteration 1600: Loss = -11588.940972712007
Iteration 1700: Loss = -11588.923399431596
Iteration 1800: Loss = -11588.908829937733
Iteration 1900: Loss = -11588.916538551364
1
Iteration 2000: Loss = -11588.886043446519
Iteration 2100: Loss = -11588.87650441599
Iteration 2200: Loss = -11588.867501330082
Iteration 2300: Loss = -11588.85697486064
Iteration 2400: Loss = -11588.820711415487
Iteration 2500: Loss = -11587.94965972092
Iteration 2600: Loss = -11587.942845080652
Iteration 2700: Loss = -11587.93736526786
Iteration 2800: Loss = -11587.93272608186
Iteration 2900: Loss = -11587.926780489439
Iteration 3000: Loss = -11587.661750874086
Iteration 3100: Loss = -11576.881528790962
Iteration 3200: Loss = -11576.876912703996
Iteration 3300: Loss = -11576.873858282124
Iteration 3400: Loss = -11576.871314760494
Iteration 3500: Loss = -11576.869139185543
Iteration 3600: Loss = -11576.889576524358
1
Iteration 3700: Loss = -11576.865320457815
Iteration 3800: Loss = -11576.863541587152
Iteration 3900: Loss = -11576.861597604742
Iteration 4000: Loss = -11576.858655060863
Iteration 4100: Loss = -11576.83105510393
Iteration 4200: Loss = -11574.971582021657
Iteration 4300: Loss = -11574.85819643179
Iteration 4400: Loss = -11574.848939849322
Iteration 4500: Loss = -11566.72053848463
Iteration 4600: Loss = -11566.708869992495
Iteration 4700: Loss = -11566.827948193513
1
Iteration 4800: Loss = -11566.707186434656
Iteration 4900: Loss = -11566.706454383402
Iteration 5000: Loss = -11566.706383887231
Iteration 5100: Loss = -11566.705156966293
Iteration 5200: Loss = -11566.704624493183
Iteration 5300: Loss = -11566.704321096426
Iteration 5400: Loss = -11566.70381869141
Iteration 5500: Loss = -11566.703169066654
Iteration 5600: Loss = -11566.711621106972
1
Iteration 5700: Loss = -11566.702363366854
Iteration 5800: Loss = -11566.70201468505
Iteration 5900: Loss = -11566.710478058563
1
Iteration 6000: Loss = -11566.701325513814
Iteration 6100: Loss = -11566.701631757358
1
Iteration 6200: Loss = -11566.701501553998
2
Iteration 6300: Loss = -11566.701287729638
Iteration 6400: Loss = -11566.70024020793
Iteration 6500: Loss = -11566.732378076646
1
Iteration 6600: Loss = -11566.699788024469
Iteration 6700: Loss = -11566.7035476959
1
Iteration 6800: Loss = -11566.699410339706
Iteration 6900: Loss = -11566.700818322373
1
Iteration 7000: Loss = -11559.93379788541
Iteration 7100: Loss = -11559.923690673675
Iteration 7200: Loss = -11559.923874328588
1
Iteration 7300: Loss = -11559.935079071724
2
Iteration 7400: Loss = -11559.927463733358
3
Iteration 7500: Loss = -11559.92257097512
Iteration 7600: Loss = -11559.923020432454
1
Iteration 7700: Loss = -11559.923308639422
2
Iteration 7800: Loss = -11559.922684741667
3
Iteration 7900: Loss = -11559.925760539358
4
Iteration 8000: Loss = -11559.930203604552
5
Iteration 8100: Loss = -11559.926994527377
6
Iteration 8200: Loss = -11559.921918781805
Iteration 8300: Loss = -11559.922378691655
1
Iteration 8400: Loss = -11559.926141309661
2
Iteration 8500: Loss = -11559.921465352421
Iteration 8600: Loss = -11559.92288786211
1
Iteration 8700: Loss = -11559.922640723466
2
Iteration 8800: Loss = -11559.921281728675
Iteration 8900: Loss = -11559.933770408436
1
Iteration 9000: Loss = -11559.921361224126
2
Iteration 9100: Loss = -11559.923922976348
3
Iteration 9200: Loss = -11559.92132500019
4
Iteration 9300: Loss = -11559.921517926752
5
Iteration 9400: Loss = -11559.945674308205
6
Iteration 9500: Loss = -11559.945051534847
7
Iteration 9600: Loss = -11559.92238643285
8
Iteration 9700: Loss = -11559.924944240507
9
Iteration 9800: Loss = -11559.9458805061
10
Stopping early at iteration 9800 due to no improvement.
tensor([[ -7.7505,   3.1353],
        [ -8.6230,   4.0078],
        [ -9.9864,   5.3712],
        [  4.8743,  -9.4895],
        [  3.5291,  -8.1443],
        [-10.1807,   5.5655],
        [ -5.9519,   1.3367],
        [  5.9082, -10.5234],
        [-10.6806,   6.0654],
        [  4.5635,  -9.1788],
        [  4.4289,  -9.0441],
        [ -8.2303,   3.6151],
        [  4.8297,  -9.4449],
        [  6.3816, -10.9968],
        [-10.2751,   5.6599],
        [  5.3770,  -9.9922],
        [ -9.2526,   4.6374],
        [ -4.5602,  -0.0550],
        [  3.0781,  -7.6933],
        [  4.7860,  -9.4012],
        [  4.5393,  -9.1545],
        [-10.4765,   5.8613],
        [  1.6016,  -6.2168],
        [  4.3578,  -8.9731],
        [  4.6973,  -9.3125],
        [ -9.5892,   4.9740],
        [-10.1666,   5.5514],
        [  4.1535,  -8.7687],
        [  5.3831,  -9.9983],
        [  4.6882,  -9.3034],
        [  2.9246,  -7.5398],
        [-10.7307,   6.1154],
        [  4.3025,  -8.9178],
        [ -9.4650,   4.8498],
        [  4.4310,  -9.0462],
        [ -5.9821,   1.3668],
        [ -7.3192,   2.7040],
        [  6.6442, -11.2594],
        [  6.0080, -10.6232],
        [ -7.3113,   2.6960],
        [ -8.5556,   3.9403],
        [-10.0327,   5.4175],
        [ -9.8388,   5.2236],
        [  4.4582,  -9.0734],
        [  2.0314,  -6.6466],
        [  2.4460,  -7.0612],
        [  4.7834,  -9.3986],
        [ -8.6111,   3.9959],
        [  5.1370,  -9.7523],
        [  4.3565,  -8.9718],
        [ -8.6105,   3.9952],
        [  3.6580,  -8.2733],
        [  4.0285,  -8.6437],
        [  4.7812,  -9.3964],
        [  2.8644,  -7.4796],
        [-10.1564,   5.5412],
        [-10.8005,   6.1853],
        [-10.3071,   5.6919],
        [  2.4616,  -7.0768],
        [  3.9917,  -8.6069],
        [ -8.7089,   4.0937],
        [ -9.0718,   4.4566],
        [  1.7081,  -6.3233],
        [  4.7374,  -9.3526],
        [-10.3020,   5.6867],
        [-10.6819,   6.0666],
        [  1.2048,  -5.8200],
        [ -8.6106,   3.9954],
        [ -6.7183,   2.1031],
        [  4.5577,  -9.1729],
        [ -8.4480,   3.8328],
        [-10.8604,   6.2452],
        [ -9.3464,   4.7312],
        [  4.9758,  -9.5910],
        [ -9.0385,   4.4233],
        [ -8.9658,   4.3506],
        [  4.8721,  -9.4874],
        [ -9.7321,   5.1168],
        [  5.2431,  -9.8583],
        [ -5.9329,   1.3176],
        [ -9.2430,   4.6278],
        [-10.4731,   5.8579],
        [ -9.9178,   5.3026],
        [  6.4105, -11.0257],
        [  4.7545,  -9.3697],
        [ -4.9144,   0.2992],
        [  6.1651, -10.7803],
        [ -9.3039,   4.6887],
        [  4.1139,  -8.7291],
        [ -9.7532,   5.1380],
        [-10.1767,   5.5615],
        [-11.0125,   6.3972],
        [  2.4153,  -7.0305],
        [ -1.7273,  -2.8879],
        [  2.0176,  -6.6328],
        [  4.8229,  -9.4381],
        [ -6.7456,   2.1304],
        [-11.0518,   6.4366],
        [  4.6323,  -9.2475],
        [ -9.0474,   4.4322]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7089, 0.2911],
        [0.2687, 0.7313]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4999, 0.5001], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3945, 0.0959],
         [0.8900, 0.2017]],

        [[0.3593, 0.1184],
         [0.3637, 0.4545]],

        [[0.9497, 0.0977],
         [0.7213, 0.0780]],

        [[0.7601, 0.0917],
         [0.0740, 0.6093]],

        [[0.3302, 0.1009],
         [0.6559, 0.5995]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961147007183
Average Adjusted Rand Index: 0.9761598395607173
Iteration 0: Loss = -25494.413109716246
Iteration 10: Loss = -12310.037528547811
Iteration 20: Loss = -12147.019974654118
Iteration 30: Loss = -11535.827533417329
Iteration 40: Loss = -11535.827546845368
1
Iteration 50: Loss = -11535.827546845343
2
Iteration 60: Loss = -11535.827546845343
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7079, 0.2921],
        [0.2734, 0.7266]], dtype=torch.float64)
alpha: tensor([0.4962, 0.5038])
beta: tensor([[[0.3876, 0.0967],
         [0.6137, 0.1990]],

        [[0.5970, 0.1073],
         [0.5942, 0.0370]],

        [[0.4664, 0.0977],
         [0.5754, 0.7812]],

        [[0.3098, 0.0917],
         [0.9285, 0.9195]],

        [[0.7404, 0.1008],
         [0.5439, 0.8505]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25494.14271771753
Iteration 100: Loss = -12317.042625317103
Iteration 200: Loss = -12302.641530876796
Iteration 300: Loss = -12291.508508384613
Iteration 400: Loss = -12242.510946698014
Iteration 500: Loss = -12158.173117155948
Iteration 600: Loss = -11985.664525825288
Iteration 700: Loss = -11975.278698935983
Iteration 800: Loss = -11973.548217397576
Iteration 900: Loss = -11971.731309589053
Iteration 1000: Loss = -11966.471380016295
Iteration 1100: Loss = -11966.197611366311
Iteration 1200: Loss = -11965.93965015185
Iteration 1300: Loss = -11965.76417754244
Iteration 1400: Loss = -11963.176882360305
Iteration 1500: Loss = -11962.482137412715
Iteration 1600: Loss = -11962.343459789296
Iteration 1700: Loss = -11962.263866095242
Iteration 1800: Loss = -11962.160851893344
Iteration 1900: Loss = -11961.905168869213
Iteration 2000: Loss = -11960.571472120515
Iteration 2100: Loss = -11960.438456469525
Iteration 2200: Loss = -11959.248354981011
Iteration 2300: Loss = -11955.663689243755
Iteration 2400: Loss = -11954.391798795055
Iteration 2500: Loss = -11954.096508372788
Iteration 2600: Loss = -11953.9986424254
Iteration 2700: Loss = -11951.801670203498
Iteration 2800: Loss = -11947.085179407897
Iteration 2900: Loss = -11944.106201158264
Iteration 3000: Loss = -11935.943670763425
Iteration 3100: Loss = -11935.775174832057
Iteration 3200: Loss = -11935.642952486809
Iteration 3300: Loss = -11935.359893943758
Iteration 3400: Loss = -11929.814907594828
Iteration 3500: Loss = -11920.971623857053
Iteration 3600: Loss = -11920.860810933922
Iteration 3700: Loss = -11920.806229303367
Iteration 3800: Loss = -11911.49667428238
Iteration 3900: Loss = -11911.489174078393
Iteration 4000: Loss = -11911.475017041572
Iteration 4100: Loss = -11911.47540539246
1
Iteration 4200: Loss = -11911.456249282675
Iteration 4300: Loss = -11911.12871919167
Iteration 4400: Loss = -11907.98776931641
Iteration 4500: Loss = -11907.980230480749
Iteration 4600: Loss = -11907.97367941371
Iteration 4700: Loss = -11907.937154358768
Iteration 4800: Loss = -11903.968047822153
Iteration 4900: Loss = -11903.96041093378
Iteration 5000: Loss = -11897.574070748398
Iteration 5100: Loss = -11897.152329856106
Iteration 5200: Loss = -11876.832520025815
Iteration 5300: Loss = -11876.819829030952
Iteration 5400: Loss = -11876.814909270266
Iteration 5500: Loss = -11876.81157193149
Iteration 5600: Loss = -11876.809343031999
Iteration 5700: Loss = -11876.80708000899
Iteration 5800: Loss = -11876.805667698249
Iteration 5900: Loss = -11876.803805870864
Iteration 6000: Loss = -11876.80715363123
1
Iteration 6100: Loss = -11876.804079499627
2
Iteration 6200: Loss = -11876.799860910289
Iteration 6300: Loss = -11876.798871455661
Iteration 6400: Loss = -11876.797202800311
Iteration 6500: Loss = -11876.312354760526
Iteration 6600: Loss = -11868.714540159008
Iteration 6700: Loss = -11868.712555272592
Iteration 6800: Loss = -11868.71241863622
Iteration 6900: Loss = -11868.710526033918
Iteration 7000: Loss = -11868.708764502582
Iteration 7100: Loss = -11868.707319459032
Iteration 7200: Loss = -11868.701678300145
Iteration 7300: Loss = -11868.70021532967
Iteration 7400: Loss = -11868.717015153436
1
Iteration 7500: Loss = -11866.701169679169
Iteration 7600: Loss = -11866.696953377215
Iteration 7700: Loss = -11866.712448656133
1
Iteration 7800: Loss = -11866.73749749764
2
Iteration 7900: Loss = -11866.723009781836
3
Iteration 8000: Loss = -11866.701508745191
4
Iteration 8100: Loss = -11866.693004620038
Iteration 8200: Loss = -11866.72180968797
1
Iteration 8300: Loss = -11863.153376774457
Iteration 8400: Loss = -11848.379692132345
Iteration 8500: Loss = -11848.37792889847
Iteration 8600: Loss = -11848.513356101475
1
Iteration 8700: Loss = -11848.375344439028
Iteration 8800: Loss = -11848.230066894152
Iteration 8900: Loss = -11848.22620981068
Iteration 9000: Loss = -11848.225884439455
Iteration 9100: Loss = -11848.22311491529
Iteration 9200: Loss = -11848.239671960884
1
Iteration 9300: Loss = -11848.232438589019
2
Iteration 9400: Loss = -11848.197666021331
Iteration 9500: Loss = -11848.194382786247
Iteration 9600: Loss = -11848.20195540882
1
Iteration 9700: Loss = -11848.19573478727
2
Iteration 9800: Loss = -11848.195904438306
3
Iteration 9900: Loss = -11848.193804347975
Iteration 10000: Loss = -11848.219987245753
1
Iteration 10100: Loss = -11848.19298060948
Iteration 10200: Loss = -11832.950607322739
Iteration 10300: Loss = -11832.930739141802
Iteration 10400: Loss = -11831.154253415909
Iteration 10500: Loss = -11824.02878332001
Iteration 10600: Loss = -11823.85616007695
Iteration 10700: Loss = -11822.701479392868
Iteration 10800: Loss = -11822.697744685202
Iteration 10900: Loss = -11822.687992676558
Iteration 11000: Loss = -11822.68749254497
Iteration 11100: Loss = -11822.706472379612
1
Iteration 11200: Loss = -11822.691192414983
2
Iteration 11300: Loss = -11822.69349796469
3
Iteration 11400: Loss = -11822.686560155622
Iteration 11500: Loss = -11822.685644594922
Iteration 11600: Loss = -11822.71717425644
1
Iteration 11700: Loss = -11821.323269019655
Iteration 11800: Loss = -11821.205735388367
Iteration 11900: Loss = -11821.270343187449
1
Iteration 12000: Loss = -11821.188683670947
Iteration 12100: Loss = -11821.19489740696
1
Iteration 12200: Loss = -11821.188259345987
Iteration 12300: Loss = -11821.188699649972
1
Iteration 12400: Loss = -11821.31518916103
2
Iteration 12500: Loss = -11821.186956177646
Iteration 12600: Loss = -11821.187759523964
1
Iteration 12700: Loss = -11821.203815187815
2
Iteration 12800: Loss = -11821.210998598703
3
Iteration 12900: Loss = -11821.225380573103
4
Iteration 13000: Loss = -11821.18761376034
5
Iteration 13100: Loss = -11821.18719305934
6
Iteration 13200: Loss = -11821.189222093875
7
Iteration 13300: Loss = -11799.647823322752
Iteration 13400: Loss = -11799.617368073272
Iteration 13500: Loss = -11799.777544148395
1
Iteration 13600: Loss = -11799.604114573613
Iteration 13700: Loss = -11799.608297030836
1
Iteration 13800: Loss = -11799.606790803622
2
Iteration 13900: Loss = -11799.60917348148
3
Iteration 14000: Loss = -11799.64427036991
4
Iteration 14100: Loss = -11799.613441481575
5
Iteration 14200: Loss = -11799.600408801822
Iteration 14300: Loss = -11799.606429292042
1
Iteration 14400: Loss = -11799.600430749915
2
Iteration 14500: Loss = -11799.60202745003
3
Iteration 14600: Loss = -11799.632931072101
4
Iteration 14700: Loss = -11799.674211520492
5
Iteration 14800: Loss = -11788.07810052228
Iteration 14900: Loss = -11788.078719091238
1
Iteration 15000: Loss = -11788.103094994762
2
Iteration 15100: Loss = -11788.079682289304
3
Iteration 15200: Loss = -11788.077038252273
Iteration 15300: Loss = -11788.132749206452
1
Iteration 15400: Loss = -11788.07826723702
2
Iteration 15500: Loss = -11788.078395857585
3
Iteration 15600: Loss = -11787.629309050746
Iteration 15700: Loss = -11787.551354613795
Iteration 15800: Loss = -11787.54987542954
Iteration 15900: Loss = -11787.553478925849
1
Iteration 16000: Loss = -11787.612142037526
2
Iteration 16100: Loss = -11787.547352468393
Iteration 16200: Loss = -11787.54817842649
1
Iteration 16300: Loss = -11787.581497670719
2
Iteration 16400: Loss = -11787.551482724797
3
Iteration 16500: Loss = -11787.548535941165
4
Iteration 16600: Loss = -11787.57812989496
5
Iteration 16700: Loss = -11787.546890453688
Iteration 16800: Loss = -11787.549021027351
1
Iteration 16900: Loss = -11787.552633972386
2
Iteration 17000: Loss = -11787.54712666101
3
Iteration 17100: Loss = -11787.548294911116
4
Iteration 17200: Loss = -11787.56745352289
5
Iteration 17300: Loss = -11787.54696077549
6
Iteration 17400: Loss = -11787.554765377838
7
Iteration 17500: Loss = -11787.548542894127
8
Iteration 17600: Loss = -11787.55079533877
9
Iteration 17700: Loss = -11787.566504520057
10
Stopping early at iteration 17700 due to no improvement.
tensor([[ 2.2698e+00, -4.0032e+00],
        [-4.0703e+00, -3.4040e-01],
        [ 1.2274e+00, -2.7160e+00],
        [-8.7168e+00,  7.2371e+00],
        [-8.1661e+00,  6.7503e+00],
        [ 3.1318e+00, -4.5432e+00],
        [-5.7084e+00,  2.1029e+00],
        [-9.2990e+00,  7.3288e+00],
        [ 9.5709e-01, -5.5723e+00],
        [-8.2422e+00,  6.8491e+00],
        [-1.1605e+01,  6.9899e+00],
        [ 2.0272e+00, -3.6419e+00],
        [-8.5044e+00,  7.0573e+00],
        [-8.8742e+00,  7.3869e+00],
        [-5.0313e+00,  3.1292e+00],
        [-8.9287e+00,  7.2905e+00],
        [-4.5770e+00, -3.8221e-02],
        [-5.6232e+00,  4.0024e+00],
        [-8.3798e+00,  6.2622e+00],
        [-1.0644e+01,  7.2062e+00],
        [-9.2851e+00,  7.0083e+00],
        [ 1.8604e+00, -3.4584e+00],
        [-5.9473e+00,  4.5309e+00],
        [-6.6936e+00,  5.1180e+00],
        [-9.3474e+00,  7.4169e+00],
        [ 3.4000e+00, -4.9160e+00],
        [ 4.7722e+00, -6.3353e+00],
        [-8.3641e+00,  6.9735e+00],
        [-9.2088e+00,  7.7784e+00],
        [-8.2959e+00,  6.9041e+00],
        [-7.8896e+00,  6.4907e+00],
        [ 1.7714e+00, -3.4640e+00],
        [-8.9493e+00,  7.0390e+00],
        [-1.3425e+00, -7.0481e-01],
        [-8.6861e+00,  5.9537e+00],
        [ 1.0208e+00, -2.6246e+00],
        [-1.2610e+00, -1.7287e+00],
        [-8.9778e+00,  7.5771e+00],
        [-8.6486e+00,  7.2588e+00],
        [-2.3999e+00,  4.4739e-01],
        [ 1.8038e+00, -3.3310e+00],
        [-3.2403e+00,  1.4795e+00],
        [ 1.1098e+00, -3.3506e+00],
        [-8.7444e+00,  6.8577e+00],
        [-8.8034e+00,  6.4354e+00],
        [-6.4793e+00,  4.9031e+00],
        [-9.6126e+00,  7.6727e+00],
        [-2.3673e+00,  9.5900e-01],
        [-9.0592e+00,  7.6729e+00],
        [-9.7605e+00,  5.8143e+00],
        [-2.0098e+00,  6.1298e-01],
        [-8.1792e+00,  6.6688e+00],
        [-8.3458e+00,  5.9292e+00],
        [-9.0510e+00,  7.6566e+00],
        [-6.2138e+00,  4.5678e+00],
        [-2.0290e+00,  2.4657e-01],
        [ 2.4793e-01, -2.8428e+00],
        [ 2.4261e+00, -3.8354e+00],
        [-8.7072e+00,  4.3930e+00],
        [-9.2057e+00,  7.6898e+00],
        [ 1.1669e+00, -2.5939e+00],
        [-5.2667e+00,  2.7832e+00],
        [-7.2801e+00,  5.8487e+00],
        [-9.1918e+00,  7.2484e+00],
        [-3.2860e+00,  7.3029e-01],
        [ 9.5873e-01, -2.6793e+00],
        [-5.2639e+00,  2.7268e+00],
        [ 3.0000e-01, -1.6916e+00],
        [-2.8864e+00, -1.1031e+00],
        [-9.8265e+00,  7.5637e+00],
        [-5.0570e+00,  2.2954e+00],
        [ 1.0356e+00, -2.6226e+00],
        [-4.3153e+00,  2.9082e+00],
        [-8.9129e+00,  7.2266e+00],
        [-1.7738e+00,  3.6488e-01],
        [ 1.2402e+00, -3.8946e+00],
        [-1.0405e+01,  8.5241e+00],
        [-3.9974e+00, -4.6006e-01],
        [-8.7336e+00,  7.3216e+00],
        [-4.6586e+00,  2.9629e+00],
        [ 3.0996e+00, -6.7546e+00],
        [-4.0780e+00,  1.1645e+00],
        [ 1.1930e+00, -4.2388e+00],
        [-8.4841e+00,  7.0844e+00],
        [-9.2946e+00,  7.7074e+00],
        [-1.3566e+00, -1.3391e-01],
        [-9.8353e+00,  7.6106e+00],
        [-3.3546e+00,  1.5764e+00],
        [-7.5655e+00,  6.0741e+00],
        [ 2.6569e+00, -7.2722e+00],
        [-1.4056e+00, -1.6664e-03],
        [ 4.7023e+00, -7.4598e+00],
        [-8.3365e+00,  6.8907e+00],
        [-7.2284e+00,  4.2100e+00],
        [-6.9643e+00,  5.5578e+00],
        [-8.9913e+00,  7.5792e+00],
        [ 1.7330e+00, -3.8899e+00],
        [ 4.4928e+00, -5.9479e+00],
        [-9.3838e+00,  7.4162e+00],
        [-1.0317e-01, -1.6520e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6332, 0.3668],
        [0.2952, 0.7048]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2749, 0.7251], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3795, 0.1038],
         [0.6137, 0.2360]],

        [[0.5970, 0.1045],
         [0.5942, 0.0370]],

        [[0.4664, 0.0975],
         [0.5754, 0.7812]],

        [[0.3098, 0.0911],
         [0.9285, 0.9195]],

        [[0.7404, 0.1002],
         [0.5439, 0.8505]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 76
Adjusted Rand Index: 0.26448269642755023
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 78
Adjusted Rand Index: 0.3077515494541765
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.14275128738056142
Average Adjusted Rand Index: 0.7064456265836553
Iteration 0: Loss = -17292.500476068995
Iteration 10: Loss = -11721.17969828229
Iteration 20: Loss = -11727.898292338223
1
Iteration 30: Loss = -11728.067419545616
2
Iteration 40: Loss = -11728.090828269675
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.5801, 0.4199],
        [0.2992, 0.7008]], dtype=torch.float64)
alpha: tensor([0.4545, 0.5455])
beta: tensor([[[0.3784, 0.0961],
         [0.9726, 0.2033]],

        [[0.8156, 0.1072],
         [0.1620, 0.8485]],

        [[0.5748, 0.0976],
         [0.4773, 0.5189]],

        [[0.7652, 0.0917],
         [0.8740, 0.1189]],

        [[0.6661, 0.1423],
         [0.5606, 0.6958]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 63
Adjusted Rand Index: 0.05326330807323705
Global Adjusted Rand Index: 0.5526466710778271
Average Adjusted Rand Index: 0.8026518779443768
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17292.32085746589
Iteration 100: Loss = -12180.879398885556
Iteration 200: Loss = -11892.150561305623
Iteration 300: Loss = -11621.291143261366
Iteration 400: Loss = -11535.852660541519
Iteration 500: Loss = -11535.256694936135
Iteration 600: Loss = -11535.000054206439
Iteration 700: Loss = -11534.855996184373
Iteration 800: Loss = -11534.764169377862
Iteration 900: Loss = -11534.700938647737
Iteration 1000: Loss = -11534.655163557125
Iteration 1100: Loss = -11534.620769954621
Iteration 1200: Loss = -11534.594119216741
Iteration 1300: Loss = -11534.572878075913
Iteration 1400: Loss = -11534.555626612342
Iteration 1500: Loss = -11534.541277031196
Iteration 1600: Loss = -11534.529168045075
Iteration 1700: Loss = -11534.518725562099
Iteration 1800: Loss = -11534.509696835024
Iteration 1900: Loss = -11534.501985701449
Iteration 2000: Loss = -11534.495582604372
Iteration 2100: Loss = -11534.490140437752
Iteration 2200: Loss = -11534.485380497174
Iteration 2300: Loss = -11534.481203810737
Iteration 2400: Loss = -11534.477496579797
Iteration 2500: Loss = -11534.47425344255
Iteration 2600: Loss = -11534.471276664503
Iteration 2700: Loss = -11534.468621397202
Iteration 2800: Loss = -11534.466254557336
Iteration 2900: Loss = -11534.464089046309
Iteration 3000: Loss = -11534.462171961586
Iteration 3100: Loss = -11534.460372128213
Iteration 3200: Loss = -11534.458761787399
Iteration 3300: Loss = -11534.457320066016
Iteration 3400: Loss = -11534.45595461701
Iteration 3500: Loss = -11534.457834530842
1
Iteration 3600: Loss = -11534.453588566766
Iteration 3700: Loss = -11534.452619581309
Iteration 3800: Loss = -11534.452375412648
Iteration 3900: Loss = -11534.450705422909
Iteration 4000: Loss = -11534.457235486694
1
Iteration 4100: Loss = -11534.44913133536
Iteration 4200: Loss = -11534.448430343413
Iteration 4300: Loss = -11534.612982298077
1
Iteration 4400: Loss = -11534.447629824333
Iteration 4500: Loss = -11534.453740884092
1
Iteration 4600: Loss = -11534.538272812642
2
Iteration 4700: Loss = -11534.445524121296
Iteration 4800: Loss = -11534.445869101346
1
Iteration 4900: Loss = -11534.445214186742
Iteration 5000: Loss = -11534.44428616792
Iteration 5100: Loss = -11534.443894860204
Iteration 5200: Loss = -11534.443976399156
1
Iteration 5300: Loss = -11534.444523470425
2
Iteration 5400: Loss = -11534.455233241499
3
Iteration 5500: Loss = -11534.445688468237
4
Iteration 5600: Loss = -11534.452930415251
5
Iteration 5700: Loss = -11534.450442634377
6
Iteration 5800: Loss = -11534.441881275778
Iteration 5900: Loss = -11534.44254075523
1
Iteration 6000: Loss = -11534.45300768265
2
Iteration 6100: Loss = -11534.441257262168
Iteration 6200: Loss = -11534.470215928051
1
Iteration 6300: Loss = -11534.441133913335
Iteration 6400: Loss = -11534.440736594877
Iteration 6500: Loss = -11534.441883535097
1
Iteration 6600: Loss = -11534.442250358852
2
Iteration 6700: Loss = -11534.440305713273
Iteration 6800: Loss = -11534.440575007602
1
Iteration 6900: Loss = -11534.572219639696
2
Iteration 7000: Loss = -11534.451531372866
3
Iteration 7100: Loss = -11534.445026631663
4
Iteration 7200: Loss = -11534.455125294944
5
Iteration 7300: Loss = -11534.440192546483
Iteration 7400: Loss = -11534.439610667901
Iteration 7500: Loss = -11534.439680875
1
Iteration 7600: Loss = -11534.49778849845
2
Iteration 7700: Loss = -11534.439264473673
Iteration 7800: Loss = -11534.440709994353
1
Iteration 7900: Loss = -11534.46290546378
2
Iteration 8000: Loss = -11534.43905162033
Iteration 8100: Loss = -11534.462134118261
1
Iteration 8200: Loss = -11534.438912892008
Iteration 8300: Loss = -11534.45407847193
1
Iteration 8400: Loss = -11534.438824810324
Iteration 8500: Loss = -11534.453836756646
1
Iteration 8600: Loss = -11534.43871104862
Iteration 8700: Loss = -11534.444760764247
1
Iteration 8800: Loss = -11534.438687357851
Iteration 8900: Loss = -11534.469055212074
1
Iteration 9000: Loss = -11534.490307028198
2
Iteration 9100: Loss = -11534.440437941439
3
Iteration 9200: Loss = -11534.438731349439
4
Iteration 9300: Loss = -11534.441197641989
5
Iteration 9400: Loss = -11534.445186402312
6
Iteration 9500: Loss = -11534.4384455527
Iteration 9600: Loss = -11534.452954664714
1
Iteration 9700: Loss = -11534.54585036473
2
Iteration 9800: Loss = -11534.438331488458
Iteration 9900: Loss = -11534.438338502912
1
Iteration 10000: Loss = -11534.465975805351
2
Iteration 10100: Loss = -11534.441585086233
3
Iteration 10200: Loss = -11534.438668874767
4
Iteration 10300: Loss = -11534.438409433693
5
Iteration 10400: Loss = -11534.448195977657
6
Iteration 10500: Loss = -11534.440562844344
7
Iteration 10600: Loss = -11534.439823925573
8
Iteration 10700: Loss = -11534.44365146467
9
Iteration 10800: Loss = -11534.463637811892
10
Stopping early at iteration 10800 due to no improvement.
tensor([[  4.7537,  -6.1439],
        [  4.9761,  -6.5027],
        [  6.3319,  -7.7257],
        [ -9.4133,   7.1282],
        [ -6.8712,   5.2743],
        [  7.0072,  -9.3300],
        [  1.5376,  -5.6658],
        [ -9.0354,   7.2043],
        [  7.9520,  -9.3901],
        [ -8.2417,   6.6453],
        [ -8.4728,   6.2019],
        [  7.5136,  -8.9831],
        [ -8.0817,   5.6231],
        [ -9.6617,   6.8387],
        [  7.1779,  -9.8760],
        [ -9.0047,   7.3124],
        [  1.3988,  -5.5744],
        [  1.4105,  -3.0292],
        [ -6.6872,   4.4220],
        [ -9.2501,   6.0613],
        [ -9.3861,   7.3882],
        [  7.4333,  -8.8238],
        [ -8.8632,   7.1468],
        [ -8.1914,   6.4427],
        [ -8.5552,   7.0963],
        [  7.4700,  -9.1053],
        [  7.2579,  -8.7966],
        [ -8.1898,   6.0129],
        [ -8.6066,   6.5563],
        [ -8.7327,   6.9777],
        [ -6.0615,   4.5021],
        [  6.4942,  -7.8998],
        [ -8.8131,   7.3278],
        [  5.9477,  -7.6265],
        [ -6.9507,   5.3408],
        [  6.9227,  -8.3163],
        [  6.1662,  -7.5666],
        [ -8.1431,   6.3824],
        [ -7.8093,   6.1641],
        [  6.7036,  -8.1541],
        [  6.8227,  -8.2934],
        [  7.3189,  -8.7177],
        [  7.2973,  -9.8601],
        [ -8.3554,   6.2469],
        [ -5.1052,   3.6545],
        [ -5.5110,   4.0969],
        [ -8.9365,   7.5468],
        [  6.6011,  -8.6458],
        [ -8.4878,   6.7190],
        [ -8.8472,   6.5617],
        [  6.8276,  -8.2141],
        [ -7.4496,   5.6480],
        [ -7.6991,   6.2257],
        [ -8.1040,   6.4125],
        [ -6.6151,   3.8602],
        [  6.7818, -10.2818],
        [  7.6465, -10.0695],
        [  7.5335,  -8.9452],
        [ -5.5710,   4.0872],
        [ -8.0001,   5.8692],
        [  7.3112,  -9.0963],
        [  7.3860,  -8.8647],
        [ -5.1400,   3.0066],
        [ -8.5243,   6.6227],
        [  6.3623,  -8.1238],
        [  7.3247,  -8.7438],
        [ -4.4350,   2.6071],
        [  6.8699, -10.7720],
        [  4.5618,  -7.3683],
        [ -8.3793,   6.9049],
        [  6.9061,  -8.8277],
        [  7.5998,  -9.9721],
        [  6.5035,  -9.5853],
        [ -8.4867,   6.8906],
        [  6.6524,  -9.1869],
        [  5.8391,  -7.2726],
        [ -9.3929,   6.7234],
        [  5.0652,  -7.3797],
        [ -9.4570,   6.9276],
        [  4.9276,  -7.7750],
        [  6.7295,  -8.3637],
        [  7.0655,  -8.5403],
        [  7.5293,  -9.3082],
        [ -8.1428,   6.5544],
        [ -9.1835,   7.7123],
        [  5.9007,  -8.6100],
        [ -9.7847,   7.8602],
        [  6.2972,  -8.7273],
        [ -8.1646,   5.8647],
        [  7.1564,  -8.6025],
        [  6.5856,  -8.3764],
        [  7.0997,  -9.1590],
        [ -8.3200,   6.8696],
        [ -1.6653,  -0.4011],
        [ -5.1515,   3.5760],
        [ -8.7592,   7.1535],
        [  7.0133,  -8.4199],
        [  7.1589,  -9.2042],
        [ -9.7219,   6.6060],
        [  2.6534,  -7.2687]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7278, 0.2722],
        [0.2881, 0.7119]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4986, 0.5014], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2039, 0.0956],
         [0.9726, 0.3936]],

        [[0.8156, 0.1071],
         [0.1620, 0.8485]],

        [[0.5748, 0.0978],
         [0.4773, 0.5189]],

        [[0.7652, 0.0917],
         [0.8740, 0.1189]],

        [[0.6661, 0.1003],
         [0.5606, 0.6958]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
Iteration 0: Loss = -40243.481229097684
Iteration 10: Loss = -12270.43669893242
Iteration 20: Loss = -11535.8278066277
Iteration 30: Loss = -11535.827543663676
Iteration 40: Loss = -11535.827543663789
1
Iteration 50: Loss = -11535.827543663789
2
Iteration 60: Loss = -11535.827543663789
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7266, 0.2734],
        [0.2921, 0.7079]], dtype=torch.float64)
alpha: tensor([0.5038, 0.4962])
beta: tensor([[[0.1990, 0.0967],
         [0.9349, 0.3876]],

        [[0.5146, 0.1073],
         [0.1207, 0.0337]],

        [[0.7501, 0.0977],
         [0.3612, 0.5017]],

        [[0.1912, 0.0917],
         [0.1828, 0.5264]],

        [[0.9907, 0.1008],
         [0.3467, 0.7791]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40242.665669449445
Iteration 100: Loss = -12322.923768274424
Iteration 200: Loss = -12311.292021753017
Iteration 300: Loss = -12299.045725449832
Iteration 400: Loss = -12275.537422711197
Iteration 500: Loss = -12199.348038559945
Iteration 600: Loss = -11952.283419987838
Iteration 700: Loss = -11894.477259290754
Iteration 800: Loss = -11874.073421413774
Iteration 900: Loss = -11833.069804714682
Iteration 1000: Loss = -11811.387849194163
Iteration 1100: Loss = -11793.322692008038
Iteration 1200: Loss = -11788.341667607026
Iteration 1300: Loss = -11779.276501002812
Iteration 1400: Loss = -11768.830661619004
Iteration 1500: Loss = -11756.877532393444
Iteration 1600: Loss = -11752.93965587684
Iteration 1700: Loss = -11748.144429165879
Iteration 1800: Loss = -11743.392714800142
Iteration 1900: Loss = -11715.602640327417
Iteration 2000: Loss = -11714.575792854794
Iteration 2100: Loss = -11710.349758734032
Iteration 2200: Loss = -11702.334232871046
Iteration 2300: Loss = -11699.651941953596
Iteration 2400: Loss = -11699.577050878815
Iteration 2500: Loss = -11684.008833716802
Iteration 2600: Loss = -11675.669162593997
Iteration 2700: Loss = -11675.647611657798
Iteration 2800: Loss = -11675.63000088552
Iteration 2900: Loss = -11675.614432401995
Iteration 3000: Loss = -11675.601285356404
Iteration 3100: Loss = -11675.58306969396
Iteration 3200: Loss = -11675.55166385935
Iteration 3300: Loss = -11673.040028176483
Iteration 3400: Loss = -11672.98892628927
Iteration 3500: Loss = -11672.979960775143
Iteration 3600: Loss = -11672.972952363476
Iteration 3700: Loss = -11672.96937141745
Iteration 3800: Loss = -11672.961705810425
Iteration 3900: Loss = -11672.956903435605
Iteration 4000: Loss = -11672.952614495513
Iteration 4100: Loss = -11672.948601141568
Iteration 4200: Loss = -11672.94441949272
Iteration 4300: Loss = -11665.331334216533
Iteration 4400: Loss = -11660.904195764757
Iteration 4500: Loss = -11660.90054022188
Iteration 4600: Loss = -11646.032144621899
Iteration 4700: Loss = -11644.81982054931
Iteration 4800: Loss = -11644.81671556154
Iteration 4900: Loss = -11644.814407284357
Iteration 5000: Loss = -11644.811410676051
Iteration 5100: Loss = -11644.808976969252
Iteration 5200: Loss = -11644.806610173171
Iteration 5300: Loss = -11644.804101347301
Iteration 5400: Loss = -11644.799170586031
Iteration 5500: Loss = -11640.216399166198
Iteration 5600: Loss = -11638.338662333917
Iteration 5700: Loss = -11638.33861909189
Iteration 5800: Loss = -11638.33437293332
Iteration 5900: Loss = -11638.334479921159
1
Iteration 6000: Loss = -11638.331529986639
Iteration 6100: Loss = -11638.33111983272
Iteration 6200: Loss = -11638.328932303782
Iteration 6300: Loss = -11638.336881831052
1
Iteration 6400: Loss = -11638.326058320605
Iteration 6500: Loss = -11638.323712866166
Iteration 6600: Loss = -11638.318107871943
Iteration 6700: Loss = -11638.287893055225
Iteration 6800: Loss = -11638.279283696174
Iteration 6900: Loss = -11638.251703946124
Iteration 7000: Loss = -11638.245734192828
Iteration 7100: Loss = -11638.114599423661
Iteration 7200: Loss = -11632.679655694496
Iteration 7300: Loss = -11632.681969316593
1
Iteration 7400: Loss = -11632.678460625615
Iteration 7500: Loss = -11632.678100380388
Iteration 7600: Loss = -11632.677931684928
Iteration 7700: Loss = -11632.686514224595
1
Iteration 7800: Loss = -11632.680048030317
2
Iteration 7900: Loss = -11632.676721030532
Iteration 8000: Loss = -11632.676152824057
Iteration 8100: Loss = -11632.675901713175
Iteration 8200: Loss = -11632.680743990692
1
Iteration 8300: Loss = -11632.68913884998
2
Iteration 8400: Loss = -11632.674396360748
Iteration 8500: Loss = -11632.594042669845
Iteration 8600: Loss = -11621.050017370235
Iteration 8700: Loss = -11621.049635933143
Iteration 8800: Loss = -11621.048999822819
Iteration 8900: Loss = -11621.048725305984
Iteration 9000: Loss = -11621.05478952953
1
Iteration 9100: Loss = -11621.048367708037
Iteration 9200: Loss = -11621.048378646596
1
Iteration 9300: Loss = -11621.058140678186
2
Iteration 9400: Loss = -11621.047771462458
Iteration 9500: Loss = -11621.06206559741
1
Iteration 9600: Loss = -11621.050863792412
2
Iteration 9700: Loss = -11610.231101203557
Iteration 9800: Loss = -11610.258325001221
1
Iteration 9900: Loss = -11610.235094342406
2
Iteration 10000: Loss = -11610.22989200345
Iteration 10100: Loss = -11610.229913481535
1
Iteration 10200: Loss = -11610.243336090218
2
Iteration 10300: Loss = -11610.230152947493
3
Iteration 10400: Loss = -11610.232784595724
4
Iteration 10500: Loss = -11610.23866631059
5
Iteration 10600: Loss = -11590.90006660686
Iteration 10700: Loss = -11589.734666597806
Iteration 10800: Loss = -11589.726519208598
Iteration 10900: Loss = -11589.721195315338
Iteration 11000: Loss = -11584.10784048893
Iteration 11100: Loss = -11584.081168463927
Iteration 11200: Loss = -11584.075731505494
Iteration 11300: Loss = -11584.073913158061
Iteration 11400: Loss = -11584.078125055443
1
Iteration 11500: Loss = -11584.073718813152
Iteration 11600: Loss = -11584.080846494258
1
Iteration 11700: Loss = -11584.072558317885
Iteration 11800: Loss = -11584.072848906771
1
Iteration 11900: Loss = -11584.079165252138
2
Iteration 12000: Loss = -11584.085057588063
3
Iteration 12100: Loss = -11584.086266147193
4
Iteration 12200: Loss = -11584.072946724384
5
Iteration 12300: Loss = -11584.072163357077
Iteration 12400: Loss = -11584.171742432867
1
Iteration 12500: Loss = -11584.07039819872
Iteration 12600: Loss = -11584.069952601843
Iteration 12700: Loss = -11575.231001137701
Iteration 12800: Loss = -11575.213601968077
Iteration 12900: Loss = -11575.220233241269
1
Iteration 13000: Loss = -11565.329155019384
Iteration 13100: Loss = -11565.377982562572
1
Iteration 13200: Loss = -11565.340788021507
2
Iteration 13300: Loss = -11565.313453149314
Iteration 13400: Loss = -11565.311332960697
Iteration 13500: Loss = -11565.309572887201
Iteration 13600: Loss = -11565.309316924026
Iteration 13700: Loss = -11565.310107509453
1
Iteration 13800: Loss = -11565.309542009192
2
Iteration 13900: Loss = -11565.305583310052
Iteration 14000: Loss = -11565.316872868278
1
Iteration 14100: Loss = -11565.30525274796
Iteration 14200: Loss = -11565.306774317534
1
Iteration 14300: Loss = -11565.3525105155
2
Iteration 14400: Loss = -11565.417590405628
3
Iteration 14500: Loss = -11565.30591777745
4
Iteration 14600: Loss = -11561.385660304037
Iteration 14700: Loss = -11561.384503735217
Iteration 14800: Loss = -11561.40567062457
1
Iteration 14900: Loss = -11561.38397504629
Iteration 15000: Loss = -11561.386095844293
1
Iteration 15100: Loss = -11561.384666296726
2
Iteration 15200: Loss = -11561.426791394875
3
Iteration 15300: Loss = -11561.38922673971
4
Iteration 15400: Loss = -11561.384026614785
5
Iteration 15500: Loss = -11561.385737780487
6
Iteration 15600: Loss = -11561.391455459874
7
Iteration 15700: Loss = -11561.391514611205
8
Iteration 15800: Loss = -11561.384051351131
9
Iteration 15900: Loss = -11550.283377750362
Iteration 16000: Loss = -11550.283640444653
1
Iteration 16100: Loss = -11550.301838729645
2
Iteration 16200: Loss = -11550.282423869292
Iteration 16300: Loss = -11550.289862424153
1
Iteration 16400: Loss = -11550.282074178256
Iteration 16500: Loss = -11550.306135734478
1
Iteration 16600: Loss = -11550.282084456947
2
Iteration 16700: Loss = -11550.28768791354
3
Iteration 16800: Loss = -11550.28206706618
Iteration 16900: Loss = -11550.285522975662
1
Iteration 17000: Loss = -11550.28222686587
2
Iteration 17100: Loss = -11550.28992839966
3
Iteration 17200: Loss = -11550.286305112488
4
Iteration 17300: Loss = -11550.288608197869
5
Iteration 17400: Loss = -11550.282091296263
6
Iteration 17500: Loss = -11550.28244300807
7
Iteration 17600: Loss = -11550.287415754738
8
Iteration 17700: Loss = -11550.295861674645
9
Iteration 17800: Loss = -11534.50568682246
Iteration 17900: Loss = -11534.45519669316
Iteration 18000: Loss = -11534.438972191088
Iteration 18100: Loss = -11534.437989158758
Iteration 18200: Loss = -11534.437999521653
1
Iteration 18300: Loss = -11534.43893329529
2
Iteration 18400: Loss = -11534.443935554762
3
Iteration 18500: Loss = -11534.438644876473
4
Iteration 18600: Loss = -11534.43948321242
5
Iteration 18700: Loss = -11534.437781455716
Iteration 18800: Loss = -11534.439311843858
1
Iteration 18900: Loss = -11534.446606909905
2
Iteration 19000: Loss = -11534.44481290269
3
Iteration 19100: Loss = -11534.439912652995
4
Iteration 19200: Loss = -11534.439237708544
5
Iteration 19300: Loss = -11534.460529717342
6
Iteration 19400: Loss = -11534.473803613102
7
Iteration 19500: Loss = -11534.439699585348
8
Iteration 19600: Loss = -11534.439368298034
9
Iteration 19700: Loss = -11534.440163301158
10
Stopping early at iteration 19700 due to no improvement.
tensor([[  4.7005,  -6.1332],
        [  3.8362,  -7.7382],
        [  7.1338,  -8.7583],
        [ -9.9848,   8.5546],
        [ -8.0638,   4.2216],
        [  7.5460,  -9.8155],
        [  8.6541, -10.0459],
        [ -9.4207,   7.3834],
        [  7.9553,  -9.4700],
        [ -8.6777,   6.8250],
        [ -9.3609,   6.3689],
        [  5.1461,  -6.6296],
        [ -7.8674,   6.3735],
        [ -9.8038,   8.3582],
        [  7.1264, -10.3512],
        [-10.2168,   8.5385],
        [  7.6635,  -9.2268],
        [  1.4673,  -2.9729],
        [ -8.7446,   4.9661],
        [-10.0718,   8.6620],
        [-11.2515,   7.2784],
        [  6.6425, -10.4476],
        [ -5.6350,   2.2124],
        [ -9.5933,   4.9781],
        [-10.9447,   9.5330],
        [  6.5188,  -7.9080],
        [  7.6717,  -9.2109],
        [-10.3740,   8.3252],
        [ -8.7871,   7.0299],
        [ -9.3199,   7.7855],
        [ -6.5221,   4.0146],
        [  7.6879,  -9.3936],
        [ -7.3329,   5.8924],
        [  6.4257,  -7.8659],
        [-10.3106,   8.8279],
        [  2.8910,  -4.3763],
        [  4.2394,  -5.7174],
        [-10.3634,   8.2001],
        [ -9.6130,   8.2169],
        [  4.2594,  -5.6792],
        [  5.0736,  -7.4297],
        [  7.1320,  -8.7622],
        [  6.1214,  -9.0827],
        [-10.9554,   7.5959],
        [ -5.8462,   2.9065],
        [ -6.3848,   3.2111],
        [ -8.9550,   7.5575],
        [  5.5456,  -7.0360],
        [-11.2216,   8.8913],
        [ -9.2003,   5.2298],
        [  5.4018,  -7.1814],
        [ -7.3709,   5.8617],
        [ -8.1587,   5.9879],
        [-10.6593,   8.3449],
        [ -9.1535,   4.7262],
        [  8.2441,  -9.6908],
        [  8.2894, -10.4180],
        [  7.7103,  -9.1207],
        [ -9.5620,   7.8486],
        [ -8.0750,   6.1186],
        [  5.6324,  -7.0855],
        [  5.9102,  -7.5795],
        [ -6.7255,   5.2667],
        [-10.2065,   8.2459],
        [  7.6442,  -9.0314],
        [  7.5689,  -9.5339],
        [ -4.4792,   2.5607],
        [  4.6317,  -7.9654],
        [  3.1981,  -5.6480],
        [ -9.1676,   7.6198],
        [  4.7604,  -7.4645],
        [  7.8930,  -9.9382],
        [  6.6632,  -8.4190],
        [ -8.2284,   6.7673],
        [  6.0321,  -7.4271],
        [  8.7522, -10.1677],
        [-10.1277,   8.6870],
        [  5.4635,  -6.9582],
        [-11.2120,   7.7089],
        [  1.3573,  -5.8467],
        [  8.4709, -10.4283],
        [  6.2836, -10.8988],
        [  7.5868,  -8.9952],
        [-10.2610,   8.7851],
        [ -9.5721,   8.1066],
        [  1.7578,  -3.3277],
        [-10.9485,   6.4418],
        [  7.8769, -10.4590],
        [ -9.9597,   8.5480],
        [  7.4975,  -8.9910],
        [  7.1307, -10.4213],
        [  8.4257,  -9.8346],
        [ -6.4389,   3.0912],
        [ -1.3657,  -0.0669],
        [ -5.0675,   3.6562],
        [ -9.8208,   8.4344],
        [  3.0405,  -5.8460],
        [  6.6934, -10.8985],
        [ -9.0561,   7.6565],
        [  7.9728,  -9.5428]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7276, 0.2724],
        [0.2903, 0.7097]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5010, 0.4990], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2032, 0.0960],
         [0.9349, 0.3950]],

        [[0.5146, 0.1071],
         [0.1207, 0.0337]],

        [[0.7501, 0.0976],
         [0.3612, 0.5017]],

        [[0.1912, 0.0917],
         [0.1828, 0.5264]],

        [[0.9907, 0.1008],
         [0.3467, 0.7791]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
Iteration 0: Loss = -21938.258838787526
Iteration 10: Loss = -11535.827146208918
Iteration 20: Loss = -11535.827543989648
1
Iteration 30: Loss = -11535.827546845343
2
Iteration 40: Loss = -11535.827546845343
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7079, 0.2921],
        [0.2734, 0.7266]], dtype=torch.float64)
alpha: tensor([0.4962, 0.5038])
beta: tensor([[[0.3876, 0.0967],
         [0.2847, 0.1990]],

        [[0.1831, 0.1073],
         [0.1385, 0.1305]],

        [[0.6010, 0.0977],
         [0.9629, 0.4647]],

        [[0.7263, 0.0917],
         [0.0974, 0.3418]],

        [[0.5958, 0.1008],
         [0.1279, 0.1221]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21937.65376413645
Iteration 100: Loss = -12356.740107391577
Iteration 200: Loss = -11972.887575690214
Iteration 300: Loss = -11898.23840989133
Iteration 400: Loss = -11866.136060320692
Iteration 500: Loss = -11812.36179611408
Iteration 600: Loss = -11773.889514483335
Iteration 700: Loss = -11739.456151414814
Iteration 800: Loss = -11734.929433243271
Iteration 900: Loss = -11707.006482290819
Iteration 1000: Loss = -11687.897389304411
Iteration 1100: Loss = -11598.933075714292
Iteration 1200: Loss = -11566.917663586239
Iteration 1300: Loss = -11546.126145162252
Iteration 1400: Loss = -11542.716673305811
Iteration 1500: Loss = -11542.616314938732
Iteration 1600: Loss = -11542.524422547953
Iteration 1700: Loss = -11542.247453688751
Iteration 1800: Loss = -11542.220114055892
Iteration 1900: Loss = -11542.199462019009
Iteration 2000: Loss = -11542.182472906285
Iteration 2100: Loss = -11542.168180311783
Iteration 2200: Loss = -11542.156038779014
Iteration 2300: Loss = -11542.145442794117
Iteration 2400: Loss = -11542.136170645088
Iteration 2500: Loss = -11542.12786828456
Iteration 2600: Loss = -11542.12052463745
Iteration 2700: Loss = -11542.114084245919
Iteration 2800: Loss = -11542.108577760979
Iteration 2900: Loss = -11542.103780405174
Iteration 3000: Loss = -11542.099509767944
Iteration 3100: Loss = -11542.095606410696
Iteration 3200: Loss = -11542.091631551157
Iteration 3300: Loss = -11542.085513506818
Iteration 3400: Loss = -11542.077472860465
Iteration 3500: Loss = -11542.074469732244
Iteration 3600: Loss = -11542.07203068105
Iteration 3700: Loss = -11542.069882262154
Iteration 3800: Loss = -11542.067926708674
Iteration 3900: Loss = -11542.066130374445
Iteration 4000: Loss = -11542.064527858865
Iteration 4100: Loss = -11542.063025869204
Iteration 4200: Loss = -11542.061616611762
Iteration 4300: Loss = -11542.060313084143
Iteration 4400: Loss = -11542.05913462715
Iteration 4500: Loss = -11542.058046069764
Iteration 4600: Loss = -11542.060745317607
1
Iteration 4700: Loss = -11542.056074247275
Iteration 4800: Loss = -11542.055179794785
Iteration 4900: Loss = -11542.054361934795
Iteration 5000: Loss = -11542.053987816702
Iteration 5100: Loss = -11542.052862670065
Iteration 5200: Loss = -11542.052185045173
Iteration 5300: Loss = -11542.051541507284
Iteration 5400: Loss = -11542.050969190188
Iteration 5500: Loss = -11542.050448885775
Iteration 5600: Loss = -11542.050366913536
Iteration 5700: Loss = -11542.049620586125
Iteration 5800: Loss = -11542.04897024985
Iteration 5900: Loss = -11542.048991236283
1
Iteration 6000: Loss = -11542.048115313997
Iteration 6100: Loss = -11542.047988208717
Iteration 6200: Loss = -11542.052793250274
1
Iteration 6300: Loss = -11542.047816411578
Iteration 6400: Loss = -11542.046837887601
Iteration 6500: Loss = -11542.049698175213
1
Iteration 6600: Loss = -11542.048610134398
2
Iteration 6700: Loss = -11542.045953872233
Iteration 6800: Loss = -11542.083022626863
1
Iteration 6900: Loss = -11542.04544660042
Iteration 7000: Loss = -11542.05228260737
1
Iteration 7100: Loss = -11542.07671998629
2
Iteration 7200: Loss = -11542.046409993833
3
Iteration 7300: Loss = -11542.05712732077
4
Iteration 7400: Loss = -11542.044887219065
Iteration 7500: Loss = -11542.059310245591
1
Iteration 7600: Loss = -11542.045685724272
2
Iteration 7700: Loss = -11542.037631862164
Iteration 7800: Loss = -11542.06082926616
1
Iteration 7900: Loss = -11542.03990011236
2
Iteration 8000: Loss = -11542.03694631288
Iteration 8100: Loss = -11542.037920041223
1
Iteration 8200: Loss = -11542.036727268056
Iteration 8300: Loss = -11542.036863382642
1
Iteration 8400: Loss = -11542.038080956505
2
Iteration 8500: Loss = -11542.036523104909
Iteration 8600: Loss = -11542.063393508746
1
Iteration 8700: Loss = -11542.037083681105
2
Iteration 8800: Loss = -11542.042986755083
3
Iteration 8900: Loss = -11542.040491028089
4
Iteration 9000: Loss = -11542.04447443368
5
Iteration 9100: Loss = -11542.036162899516
Iteration 9200: Loss = -11542.036582694445
1
Iteration 9300: Loss = -11542.036174873232
2
Iteration 9400: Loss = -11542.037277978492
3
Iteration 9500: Loss = -11542.041276052376
4
Iteration 9600: Loss = -11542.035718118106
Iteration 9700: Loss = -11542.03576607118
1
Iteration 9800: Loss = -11542.04970346953
2
Iteration 9900: Loss = -11542.076045227781
3
Iteration 10000: Loss = -11542.03723350952
4
Iteration 10100: Loss = -11542.039971651768
5
Iteration 10200: Loss = -11542.039278293974
6
Iteration 10300: Loss = -11542.037755796291
7
Iteration 10400: Loss = -11542.03598383445
8
Iteration 10500: Loss = -11542.037235947171
9
Iteration 10600: Loss = -11542.037803675126
10
Stopping early at iteration 10600 due to no improvement.
tensor([[ -6.1516,   4.7452],
        [ -6.6281,   5.0641],
        [ -8.8661,   6.6210],
        [  6.0226,  -9.4327],
        [  4.9252,  -7.1824],
        [-10.1996,   5.5844],
        [ -4.4114,   2.9018],
        [  6.8368,  -8.2438],
        [ -8.6122,   6.8231],
        [  6.6183,  -8.3260],
        [  5.7874,  -8.1179],
        [ -6.8292,   5.0286],
        [  6.1443,  -7.9404],
        [  6.8066,  -8.2000],
        [ -8.4265,   6.5574],
        [  6.3235,  -8.2658],
        [ -4.2329,   2.7980],
        [ -4.5743,  -0.0409],
        [  4.7031,  -6.1176],
        [  6.8564,  -8.3405],
        [  6.6278,  -8.1467],
        [ -8.0817,   6.6002],
        [  3.2281,  -4.6221],
        [  6.5344,  -8.3884],
        [  7.3963,  -8.9858],
        [ -7.3712,   5.9429],
        [ -8.2666,   6.7707],
        [  5.9156,  -7.3358],
        [  6.4113,  -7.8834],
        [  6.8250,  -8.3806],
        [  4.3837,  -6.0809],
        [ -8.3176,   6.6544],
        [  5.7591,  -7.1899],
        [ -7.6561,   6.0268],
        [  3.9600,  -8.2499],
        [ -4.4517,   2.8719],
        [ -5.7757,   4.2385],
        [  5.8485,  -8.6604],
        [  6.9888,  -9.6024],
        [ -5.8448,   4.1505],
        [ -6.9956,   5.5633],
        [ -8.5638,   6.4889],
        [ -8.5138,   6.1537],
        [  6.5355,  -8.0012],
        [  3.5511,  -5.1209],
        [  3.8903,  -5.6182],
        [  6.2890,  -9.6815],
        [ -6.9832,   5.3766],
        [  7.1344,  -8.5332],
        [  6.4753,  -7.8890],
        [ -8.4542,   4.1595],
        [  5.0496,  -8.1036],
        [  5.2672,  -8.0063],
        [  6.9902,  -8.4299],
        [  4.1443,  -6.2349],
        [ -8.5489,   6.8028],
        [ -9.3765,   7.1683],
        [ -8.2765,   6.4355],
        [  4.0573,  -5.4826],
        [  5.5169,  -8.0118],
        [ -7.0160,   5.6022],
        [ -7.1414,   5.7536],
        [  3.2396,  -4.7827],
        [  6.6787,  -8.2868],
        [ -8.1575,   6.6134],
        [ -7.9533,   6.5645],
        [  1.4355,  -5.5577],
        [ -7.1005,   5.5122],
        [ -5.4478,   3.4060],
        [  6.3201,  -7.7104],
        [ -6.9183,   5.4110],
        [ -9.6074,   7.0004],
        [ -7.9171,   6.0548],
        [  6.2417,  -7.8243],
        [ -7.4126,   6.0262],
        [ -7.5444,   6.1553],
        [  6.6135,  -8.8564],
        [ -6.9147,   5.5218],
        [  7.1687,  -9.0353],
        [ -5.3735,   1.8668],
        [ -8.1628,   6.2970],
        [ -8.0178,   6.6272],
        [ -7.9893,   6.5202],
        [  6.3864,  -8.3492],
        [  6.7303, -11.3455],
        [ -4.0847,   1.1012],
        [  6.6584,  -8.4251],
        [ -8.8641,   6.9634],
        [  6.0837,  -7.4916],
        [ -8.1691,   6.6877],
        [-10.5673,   6.6822],
        [ -9.0308,   7.0192],
        [  3.6437,  -5.8605],
        [ -0.1139,  -1.2809],
        [  3.6172,  -5.0285],
        [  7.1800,  -8.5812],
        [ -5.1663,   3.7278],
        [ -9.2854,   6.4947],
        [  6.5378,  -9.2236],
        [ -5.9712,   4.0280]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7027, 0.2973],
        [0.2732, 0.7268]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4975, 0.5025], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3965, 0.0962],
         [0.2847, 0.2022]],

        [[0.1831, 0.1072],
         [0.1385, 0.1305]],

        [[0.6010, 0.1014],
         [0.9629, 0.4647]],

        [[0.7263, 0.0917],
         [0.0974, 0.3418]],

        [[0.5958, 0.1008],
         [0.1279, 0.1221]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320089020589
Average Adjusted Rand Index: 0.9839984326594585
11540.416477843703
new:  [0.14275128738056142, 0.9919999711388391, 0.9919999711388391, 0.9840320089020589] [0.7064456265836553, 0.9919992163297293, 0.9919992163297293, 0.9839984326594585] [11787.566504520057, 11534.463637811892, 11534.440163301158, 11542.037803675126]
prior:  [0.9919999711388391, 0.5526466710778271, 0.9919999711388391, 0.9919999711388391] [0.9919992163297293, 0.8026518779443768, 0.9919992163297293, 0.9919992163297293] [11535.827546845343, 11728.090828269675, 11535.827543663789, 11535.827546845343]
-----------------------------------------------------------------------------------------
This iteration is 14
True Objective function: Loss = -11434.012324635527
Iteration 0: Loss = -22948.274680630235
Iteration 10: Loss = -12085.878441628096
Iteration 20: Loss = -11804.199619563604
Iteration 30: Loss = -11779.738930241276
Iteration 40: Loss = -11672.624733364386
Iteration 50: Loss = -11646.201109927046
Iteration 60: Loss = -11646.257774665339
1
Iteration 70: Loss = -11649.300347740756
2
Iteration 80: Loss = -11654.384024140021
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.2775, 0.7225],
        [0.3971, 0.6029]], dtype=torch.float64)
alpha: tensor([0.3812, 0.6188])
beta: tensor([[[0.3769, 0.0869],
         [0.0993, 0.2152]],

        [[0.8784, 0.1052],
         [0.9621, 0.2505]],

        [[0.5942, 0.1067],
         [0.2642, 0.7191]],

        [[0.6752, 0.0931],
         [0.9221, 0.5687]],

        [[0.3333, 0.1157],
         [0.1709, 0.8311]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 75
Adjusted Rand Index: 0.24427480916030533
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.011100693793362084
Global Adjusted Rand Index: 0.21357385223677328
Average Adjusted Rand Index: 0.6306336005666809
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23026.883955334226
Iteration 100: Loss = -12114.089856681385
Iteration 200: Loss = -12086.441322330747
Iteration 300: Loss = -12030.449840432437
Iteration 400: Loss = -11957.256856729146
Iteration 500: Loss = -11835.380193864621
Iteration 600: Loss = -11591.851903980507
Iteration 700: Loss = -11460.2027180613
Iteration 800: Loss = -11427.691043172688
Iteration 900: Loss = -11421.711978323412
Iteration 1000: Loss = -11421.528394702433
Iteration 1100: Loss = -11421.4164897544
Iteration 1200: Loss = -11421.162418236348
Iteration 1300: Loss = -11421.06381840279
Iteration 1400: Loss = -11421.019848984248
Iteration 1500: Loss = -11420.985979925343
Iteration 1600: Loss = -11420.9575775895
Iteration 1700: Loss = -11415.86042453
Iteration 1800: Loss = -11415.841498089263
Iteration 1900: Loss = -11415.825756463671
Iteration 2000: Loss = -11415.81242541076
Iteration 2100: Loss = -11415.801121342392
Iteration 2200: Loss = -11415.791477007984
Iteration 2300: Loss = -11415.783023320366
Iteration 2400: Loss = -11415.775618850854
Iteration 2500: Loss = -11415.769272572277
Iteration 2600: Loss = -11415.763416538713
Iteration 2700: Loss = -11415.7582856223
Iteration 2800: Loss = -11415.7538268936
Iteration 2900: Loss = -11415.749528700826
Iteration 3000: Loss = -11415.745920651334
Iteration 3100: Loss = -11415.74283174391
Iteration 3200: Loss = -11415.739463393025
Iteration 3300: Loss = -11415.736685704785
Iteration 3400: Loss = -11415.734176639007
Iteration 3500: Loss = -11415.73184297963
Iteration 3600: Loss = -11415.73316872579
1
Iteration 3700: Loss = -11415.730179470302
Iteration 3800: Loss = -11415.726041317403
Iteration 3900: Loss = -11415.729268624653
1
Iteration 4000: Loss = -11415.722856044766
Iteration 4100: Loss = -11415.721463157493
Iteration 4200: Loss = -11415.726334793431
1
Iteration 4300: Loss = -11415.718920302395
Iteration 4400: Loss = -11415.717780524052
Iteration 4500: Loss = -11415.72275854221
1
Iteration 4600: Loss = -11415.7157602573
Iteration 4700: Loss = -11415.717571472707
1
Iteration 4800: Loss = -11415.714013274328
Iteration 4900: Loss = -11415.71313444337
Iteration 5000: Loss = -11415.712341433742
Iteration 5100: Loss = -11415.71088700653
Iteration 5200: Loss = -11415.705885432037
Iteration 5300: Loss = -11415.70783239796
1
Iteration 5400: Loss = -11415.704451474357
Iteration 5500: Loss = -11415.703967955933
Iteration 5600: Loss = -11415.703541725985
Iteration 5700: Loss = -11415.703041724508
Iteration 5800: Loss = -11415.702719335186
Iteration 5900: Loss = -11415.7021911473
Iteration 6000: Loss = -11415.701782594504
Iteration 6100: Loss = -11415.706536525418
1
Iteration 6200: Loss = -11415.701065659525
Iteration 6300: Loss = -11415.700777602631
Iteration 6400: Loss = -11415.700709778848
Iteration 6500: Loss = -11415.700192441074
Iteration 6600: Loss = -11415.700002985566
Iteration 6700: Loss = -11415.699648636075
Iteration 6800: Loss = -11415.69982710123
1
Iteration 6900: Loss = -11415.699477456683
Iteration 7000: Loss = -11415.705150640826
1
Iteration 7100: Loss = -11415.699953895484
2
Iteration 7200: Loss = -11415.699925941082
3
Iteration 7300: Loss = -11415.71570538564
4
Iteration 7400: Loss = -11415.698442214078
Iteration 7500: Loss = -11415.699195780724
1
Iteration 7600: Loss = -11415.69796025373
Iteration 7700: Loss = -11415.699417034639
1
Iteration 7800: Loss = -11415.701416218726
2
Iteration 7900: Loss = -11415.707743899666
3
Iteration 8000: Loss = -11415.702359545112
4
Iteration 8100: Loss = -11415.70097347417
5
Iteration 8200: Loss = -11415.697957371813
Iteration 8300: Loss = -11415.719152633306
1
Iteration 8400: Loss = -11415.70134406938
2
Iteration 8500: Loss = -11415.697443815641
Iteration 8600: Loss = -11415.707826835218
1
Iteration 8700: Loss = -11415.703958030968
2
Iteration 8800: Loss = -11415.712157749858
3
Iteration 8900: Loss = -11415.698452740167
4
Iteration 9000: Loss = -11415.743030565949
5
Iteration 9100: Loss = -11415.696627915282
Iteration 9200: Loss = -11415.783185271752
1
Iteration 9300: Loss = -11415.696403923439
Iteration 9400: Loss = -11415.699917035485
1
Iteration 9500: Loss = -11415.707558159458
2
Iteration 9600: Loss = -11415.69632027708
Iteration 9700: Loss = -11415.701518325095
1
Iteration 9800: Loss = -11415.696387616556
2
Iteration 9900: Loss = -11415.69748062244
3
Iteration 10000: Loss = -11415.72518334297
4
Iteration 10100: Loss = -11415.782721320746
5
Iteration 10200: Loss = -11415.743595597372
6
Iteration 10300: Loss = -11415.705101003496
7
Iteration 10400: Loss = -11415.70312555492
8
Iteration 10500: Loss = -11415.6973601051
9
Iteration 10600: Loss = -11415.698084387157
10
Stopping early at iteration 10600 due to no improvement.
tensor([[ -8.9338,   4.3186],
        [  4.3924,  -9.0076],
        [  3.2432,  -7.8584],
        [  4.3904,  -9.0056],
        [-10.1963,   5.5811],
        [ -9.4893,   4.8741],
        [  4.5097,  -9.1249],
        [  4.0919,  -8.7072],
        [ -9.9577,   5.3424],
        [  3.8857,  -8.5009],
        [  2.3483,  -6.9635],
        [ -9.6178,   5.0026],
        [ -9.4698,   4.8545],
        [  3.8241,  -8.4393],
        [  3.9811,  -8.5963],
        [  2.8932,  -7.5084],
        [  4.2861,  -8.9013],
        [  5.0302,  -9.6454],
        [ -9.6852,   5.0699],
        [ -9.7529,   5.1377],
        [  4.0265,  -8.6417],
        [  4.4537,  -9.0690],
        [  4.2808,  -8.8961],
        [-10.4856,   5.8703],
        [ -9.9891,   5.3739],
        [  4.6704,  -9.2856],
        [ -9.8503,   5.2351],
        [ -9.1155,   4.5002],
        [ -9.4537,   4.8385],
        [  5.0911,  -9.7063],
        [  5.2256,  -9.8408],
        [  3.7626,  -8.3778],
        [  3.7396,  -8.3548],
        [ -7.1141,   2.4989],
        [ -9.7969,   5.1817],
        [-10.0821,   5.4668],
        [  4.0498,  -8.6651],
        [  3.5567,  -8.1719],
        [ -9.1612,   4.5460],
        [-10.1623,   5.5471],
        [  4.3836,  -8.9989],
        [  4.9204,  -9.5356],
        [-10.1099,   5.4946],
        [-10.0746,   5.4593],
        [ -9.1915,   4.5763],
        [-10.3047,   5.6895],
        [ -9.5575,   4.9423],
        [  3.2278,  -7.8431],
        [ -9.6386,   5.0234],
        [ -8.0099,   3.3947],
        [ -9.1304,   4.5151],
        [ -8.8914,   4.2762],
        [ -9.4258,   4.8106],
        [  4.1918,  -8.8070],
        [  4.3684,  -8.9836],
        [ -9.3302,   4.7149],
        [  3.5336,  -8.1489],
        [  4.6099,  -9.2252],
        [  4.7300,  -9.3452],
        [  5.0379,  -9.6531],
        [  5.1454,  -9.7606],
        [ -8.9664,   4.3512],
        [ -9.3013,   4.6861],
        [  4.4115,  -9.0268],
        [  5.6358, -10.2510],
        [  4.4945,  -9.1097],
        [-10.3036,   5.6884],
        [  4.3717,  -8.9869],
        [  4.9689,  -9.5841],
        [ -8.0201,   3.4049],
        [-10.2578,   5.6426],
        [  4.8535,  -9.4687],
        [ -6.6479,   2.0327],
        [ -7.1405,   2.5252],
        [ -8.8866,   4.2714],
        [ -9.8300,   5.2147],
        [ -9.2217,   4.6064],
        [ -9.3107,   4.6955],
        [ -9.7228,   5.1076],
        [ -9.8028,   5.1876],
        [ -9.5773,   4.9621],
        [ -9.6285,   5.0133],
        [  4.5358,  -9.1510],
        [-10.1474,   5.5322],
        [ -9.3935,   4.7783],
        [  5.3135,  -9.9288],
        [  3.2867,  -7.9019],
        [  4.1927,  -8.8079],
        [ -6.1480,   1.5328],
        [ -8.9762,   4.3609],
        [  4.8560,  -9.4713],
        [  4.3937,  -9.0089],
        [-10.1423,   5.5271],
        [ -8.0648,   3.4495],
        [  5.1167,  -9.7319],
        [ -8.5218,   3.9066],
        [ -9.2103,   4.5950],
        [  4.3110,  -8.9262],
        [  4.5760,  -9.1913],
        [  4.6235,  -9.2387]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7789, 0.2211],
        [0.3114, 0.6886]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4800, 0.5200], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.0869],
         [0.0993, 0.3971]],

        [[0.8784, 0.1051],
         [0.9621, 0.2505]],

        [[0.5942, 0.1046],
         [0.2642, 0.7191]],

        [[0.6752, 0.0931],
         [0.9221, 0.5687]],

        [[0.3333, 0.1089],
         [0.1709, 0.8311]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840318395231936
Average Adjusted Rand Index: 0.9839987774932922
Iteration 0: Loss = -25460.217289255736
Iteration 10: Loss = -11576.903889243711
Iteration 20: Loss = -11418.144529753381
Iteration 30: Loss = -11418.144515935788
Iteration 40: Loss = -11418.14452599517
1
Iteration 50: Loss = -11418.144525997861
2
Iteration 60: Loss = -11418.144525997861
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7789, 0.2211],
        [0.3122, 0.6878]], dtype=torch.float64)
alpha: tensor([0.5391, 0.4609])
beta: tensor([[[0.1961, 0.0869],
         [0.1112, 0.3889]],

        [[0.3341, 0.1058],
         [0.2379, 0.4010]],

        [[0.8647, 0.1047],
         [0.7226, 0.2816]],

        [[0.2120, 0.0932],
         [0.0886, 0.7415]],

        [[0.6322, 0.1089],
         [0.8144, 0.4410]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.97609580602389
Average Adjusted Rand Index: 0.9761598197715857
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25459.90514436399
Iteration 100: Loss = -12185.136541537802
Iteration 200: Loss = -11801.354268087041
Iteration 300: Loss = -11740.166638519051
Iteration 400: Loss = -11699.968768874269
Iteration 500: Loss = -11689.930909065522
Iteration 600: Loss = -11662.258133684718
Iteration 700: Loss = -11661.414271152213
Iteration 800: Loss = -11660.945849024187
Iteration 900: Loss = -11659.961532478157
Iteration 1000: Loss = -11659.244870750237
Iteration 1100: Loss = -11659.09413408215
Iteration 1200: Loss = -11658.983999126345
Iteration 1300: Loss = -11658.897525832988
Iteration 1400: Loss = -11658.827925451344
Iteration 1500: Loss = -11658.770764891171
Iteration 1600: Loss = -11658.72321556863
Iteration 1700: Loss = -11658.683135826677
Iteration 1800: Loss = -11658.648997896365
Iteration 1900: Loss = -11658.619629697607
Iteration 2000: Loss = -11658.594208325145
Iteration 2100: Loss = -11658.571976703324
Iteration 2200: Loss = -11658.552466896897
Iteration 2300: Loss = -11658.53528378327
Iteration 2400: Loss = -11658.519964933465
Iteration 2500: Loss = -11658.506335053036
Iteration 2600: Loss = -11658.494123257216
Iteration 2700: Loss = -11658.483129268157
Iteration 2800: Loss = -11658.473265879778
Iteration 2900: Loss = -11658.464284696605
Iteration 3000: Loss = -11658.45618233844
Iteration 3100: Loss = -11658.448794725393
Iteration 3200: Loss = -11658.44208438621
Iteration 3300: Loss = -11658.435888532378
Iteration 3400: Loss = -11658.43027112114
Iteration 3500: Loss = -11658.425087271742
Iteration 3600: Loss = -11658.420331792482
Iteration 3700: Loss = -11658.415908432307
Iteration 3800: Loss = -11658.41189144074
Iteration 3900: Loss = -11658.408148257151
Iteration 4000: Loss = -11658.404690017705
Iteration 4100: Loss = -11658.401458721579
Iteration 4200: Loss = -11658.398457707739
Iteration 4300: Loss = -11658.395691528856
Iteration 4400: Loss = -11658.393136695944
Iteration 4500: Loss = -11658.390733896289
Iteration 4600: Loss = -11658.388511088724
Iteration 4700: Loss = -11658.386411394833
Iteration 4800: Loss = -11658.384484162269
Iteration 4900: Loss = -11658.382599356173
Iteration 5000: Loss = -11658.380908805162
Iteration 5100: Loss = -11658.37931583064
Iteration 5200: Loss = -11658.377815876824
Iteration 5300: Loss = -11658.376418789863
Iteration 5400: Loss = -11658.375090978556
Iteration 5500: Loss = -11658.373857570892
Iteration 5600: Loss = -11658.376329876253
1
Iteration 5700: Loss = -11658.371603837606
Iteration 5800: Loss = -11658.370605299284
Iteration 5900: Loss = -11658.36963043705
Iteration 6000: Loss = -11658.368799974358
Iteration 6100: Loss = -11658.367859462614
Iteration 6200: Loss = -11658.36706477926
Iteration 6300: Loss = -11658.375765501953
1
Iteration 6400: Loss = -11658.366035660789
Iteration 6500: Loss = -11658.366941577391
1
Iteration 6600: Loss = -11658.373152555552
2
Iteration 6700: Loss = -11658.364775215256
Iteration 6800: Loss = -11658.364200010461
Iteration 6900: Loss = -11658.36255395078
Iteration 7000: Loss = -11658.363030677421
1
Iteration 7100: Loss = -11658.369704125089
2
Iteration 7200: Loss = -11658.361134743634
Iteration 7300: Loss = -11658.361518367079
1
Iteration 7400: Loss = -11658.36029656611
Iteration 7500: Loss = -11658.360299440941
1
Iteration 7600: Loss = -11658.360926008418
2
Iteration 7700: Loss = -11658.359317534521
Iteration 7800: Loss = -11658.359139359296
Iteration 7900: Loss = -11658.365914914188
1
Iteration 8000: Loss = -11658.358287583997
Iteration 8100: Loss = -11658.358137795809
Iteration 8200: Loss = -11658.551376972875
1
Iteration 8300: Loss = -11658.357518388384
Iteration 8400: Loss = -11658.387492543165
1
Iteration 8500: Loss = -11658.35707493426
Iteration 8600: Loss = -11658.362088006093
1
Iteration 8700: Loss = -11658.35659726563
Iteration 8800: Loss = -11658.377423607593
1
Iteration 8900: Loss = -11658.357319524068
2
Iteration 9000: Loss = -11658.35639319804
Iteration 9100: Loss = -11658.360325736687
1
Iteration 9200: Loss = -11658.355586675945
Iteration 9300: Loss = -11658.359019967993
1
Iteration 9400: Loss = -11658.355979725076
2
Iteration 9500: Loss = -11658.354712998122
Iteration 9600: Loss = -11658.346598974109
Iteration 9700: Loss = -11654.707712782832
Iteration 9800: Loss = -11654.740013449633
1
Iteration 9900: Loss = -11654.83912154342
2
Iteration 10000: Loss = -11652.207442551751
Iteration 10100: Loss = -11652.208141398407
1
Iteration 10200: Loss = -11652.216137062442
2
Iteration 10300: Loss = -11647.845553230096
Iteration 10400: Loss = -11644.411926527138
Iteration 10500: Loss = -11644.571076162814
1
Iteration 10600: Loss = -11643.997505377994
Iteration 10700: Loss = -11641.009278251526
Iteration 10800: Loss = -11634.506677315689
Iteration 10900: Loss = -11615.38505949899
Iteration 11000: Loss = -11585.155656348199
Iteration 11100: Loss = -11570.681833922064
Iteration 11200: Loss = -11549.609385290245
Iteration 11300: Loss = -11533.618617356637
Iteration 11400: Loss = -11502.668745070736
Iteration 11500: Loss = -11492.167733104146
Iteration 11600: Loss = -11475.269241637556
Iteration 11700: Loss = -11475.305667399714
1
Iteration 11800: Loss = -11475.267837182882
Iteration 11900: Loss = -11475.27139286578
1
Iteration 12000: Loss = -11431.220886022278
Iteration 12100: Loss = -11431.11331079206
Iteration 12200: Loss = -11431.11004801489
Iteration 12300: Loss = -11431.080602215154
Iteration 12400: Loss = -11431.007993073415
Iteration 12500: Loss = -11429.00324153065
Iteration 12600: Loss = -11415.98704736936
Iteration 12700: Loss = -11415.994561047231
1
Iteration 12800: Loss = -11415.971672759037
Iteration 12900: Loss = -11415.96928973284
Iteration 13000: Loss = -11415.963631289214
Iteration 13100: Loss = -11415.96430202129
1
Iteration 13200: Loss = -11415.984965081216
2
Iteration 13300: Loss = -11415.966914180628
3
Iteration 13400: Loss = -11415.972491373763
4
Iteration 13500: Loss = -11415.963230329311
Iteration 13600: Loss = -11415.963698231864
1
Iteration 13700: Loss = -11416.004777473538
2
Iteration 13800: Loss = -11415.963755360945
3
Iteration 13900: Loss = -11415.96563130583
4
Iteration 14000: Loss = -11416.049916427814
5
Iteration 14100: Loss = -11415.968529470072
6
Iteration 14200: Loss = -11415.967168062665
7
Iteration 14300: Loss = -11415.962919412785
Iteration 14400: Loss = -11415.964330656336
1
Iteration 14500: Loss = -11415.968039704274
2
Iteration 14600: Loss = -11415.963691191448
3
Iteration 14700: Loss = -11415.962800094889
Iteration 14800: Loss = -11415.97445847887
1
Iteration 14900: Loss = -11415.97634606671
2
Iteration 15000: Loss = -11415.962277068831
Iteration 15100: Loss = -11415.966296300794
1
Iteration 15200: Loss = -11415.96003056985
Iteration 15300: Loss = -11415.960442836405
1
Iteration 15400: Loss = -11415.96002625668
Iteration 15500: Loss = -11415.961967523894
1
Iteration 15600: Loss = -11415.97458194387
2
Iteration 15700: Loss = -11415.959942991514
Iteration 15800: Loss = -11415.96209478221
1
Iteration 15900: Loss = -11415.960068710143
2
Iteration 16000: Loss = -11415.965951387954
3
Iteration 16100: Loss = -11415.960068273927
4
Iteration 16200: Loss = -11415.95970004803
Iteration 16300: Loss = -11415.95951490189
Iteration 16400: Loss = -11415.960524513433
1
Iteration 16500: Loss = -11415.964864074773
2
Iteration 16600: Loss = -11415.967911627986
3
Iteration 16700: Loss = -11415.999642086685
4
Iteration 16800: Loss = -11415.965338984555
5
Iteration 16900: Loss = -11415.974123841002
6
Iteration 17000: Loss = -11415.965470757563
7
Iteration 17100: Loss = -11415.986901919423
8
Iteration 17200: Loss = -11416.004824903284
9
Iteration 17300: Loss = -11415.962344717575
10
Stopping early at iteration 17300 due to no improvement.
tensor([[ -9.3269,   6.9628],
        [  7.5336,  -9.9679],
        [  4.2713,  -6.9985],
        [  7.8460,  -9.2379],
        [ -9.5300,   8.1348],
        [ -9.7866,   8.3301],
        [  6.8817,  -8.3174],
        [  6.8959,  -8.3282],
        [-10.8742,   8.2819],
        [  5.8808,  -8.4005],
        [  3.8588,  -5.4710],
        [-10.7388,   9.3263],
        [-11.1927,   8.5060],
        [  5.8476,  -7.4724],
        [  6.4269,  -8.7849],
        [  3.3964,  -7.0156],
        [  7.7370,  -9.1256],
        [  8.1832,  -9.5848],
        [ -9.5961,   7.1521],
        [-11.1319,   8.2191],
        [  6.6781,  -8.5611],
        [  8.7149, -10.1033],
        [  7.5185,  -8.9081],
        [-12.1375,   7.5222],
        [-10.5425,   7.6933],
        [  2.4509,  -4.0885],
        [-10.4304,   8.0371],
        [ -9.8602,   8.0778],
        [-10.3154,   8.1805],
        [  8.0317,  -9.6295],
        [  4.4128,  -6.0004],
        [  5.4150,  -6.8211],
        [  8.3853, -10.3972],
        [ -5.7792,   3.8278],
        [ -9.4808,   8.0943],
        [ -9.9034,   8.4692],
        [  5.5453,  -9.6561],
        [  4.8906,  -7.4640],
        [ -7.8872,   6.4469],
        [-10.5271,   8.9854],
        [  7.8108, -10.2674],
        [  7.6878,  -9.2335],
        [ -9.9805,   8.5739],
        [ -9.8942,   8.4193],
        [ -8.0528,   6.4329],
        [-10.4547,   9.0384],
        [ -9.1656,   7.7687],
        [  4.9296,  -6.3269],
        [-10.3746,   8.2642],
        [ -6.4403,   4.9838],
        [ -9.5991,   8.1156],
        [ -7.4127,   5.9744],
        [-11.5084,   8.3860],
        [  7.4352,  -8.8803],
        [  8.0387,  -9.4252],
        [ -9.7721,   8.3102],
        [  5.3944,  -6.9440],
        [  6.9014,  -8.3317],
        [  7.1587,  -8.7923],
        [  6.9184,  -8.3237],
        [  7.2242,  -8.7867],
        [ -9.2083,   7.6404],
        [ -9.8779,   8.4824],
        [  7.7505,  -9.2086],
        [  7.6883, -10.5045],
        [  8.0772,  -9.6591],
        [-10.4973,   9.0705],
        [  7.7890,  -9.3970],
        [  7.7767,  -9.2444],
        [ -6.6453,   4.7776],
        [-10.7579,   8.4644],
        [  7.3005,  -9.6851],
        [ -5.0242,   3.6206],
        [ -7.0706,   2.5258],
        [ -8.8724,   7.1397],
        [-10.2658,   7.9446],
        [-10.7426,   7.8016],
        [-11.2102,   8.0050],
        [ -9.4814,   7.6496],
        [ -9.6982,   8.1418],
        [-11.0503,   8.1339],
        [ -9.7735,   7.0667],
        [  5.7668,  -8.4715],
        [-10.4628,   8.0248],
        [ -8.5949,   6.8067],
        [  8.6455, -10.0334],
        [  4.8781,  -6.4959],
        [  8.5155,  -9.9040],
        [ -4.5301,   3.1415],
        [ -8.9594,   7.5289],
        [  7.4160,  -9.8114],
        [  7.7927,  -9.5344],
        [-10.1293,   8.4857],
        [ -6.6118,   4.8119],
        [  7.0929,  -9.0293],
        [ -7.9064,   5.5649],
        [ -9.7740,   8.3645],
        [  6.3450,  -7.9469],
        [  7.7433,  -9.4007],
        [  7.8604,  -9.4738]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7785, 0.2215],
        [0.3125, 0.6875]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4807, 0.5193], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2000, 0.0869],
         [0.1112, 0.3975]],

        [[0.3341, 0.1054],
         [0.2379, 0.4010]],

        [[0.8647, 0.1044],
         [0.7226, 0.2816]],

        [[0.2120, 0.0935],
         [0.0886, 0.7415]],

        [[0.6322, 0.1090],
         [0.8144, 0.4410]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840318395231936
Average Adjusted Rand Index: 0.9839987774932922
Iteration 0: Loss = -18961.697760969088
Iteration 10: Loss = -12084.398272418564
Iteration 20: Loss = -11418.146711545201
Iteration 30: Loss = -11418.144498506026
Iteration 40: Loss = -11418.144525971598
1
Iteration 50: Loss = -11418.144529031268
2
Iteration 60: Loss = -11418.144529031268
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7789, 0.2211],
        [0.3122, 0.6878]], dtype=torch.float64)
alpha: tensor([0.5391, 0.4609])
beta: tensor([[[0.1961, 0.0869],
         [0.7656, 0.3889]],

        [[0.1826, 0.1058],
         [0.1625, 0.0441]],

        [[0.1086, 0.1047],
         [0.5707, 0.4965]],

        [[0.9434, 0.0932],
         [0.3490, 0.7905]],

        [[0.6519, 0.1089],
         [0.7594, 0.7167]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.97609580602389
Average Adjusted Rand Index: 0.9761598197715857
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18961.096023176036
Iteration 100: Loss = -12115.46659598447
Iteration 200: Loss = -12107.600006455305
Iteration 300: Loss = -12097.120665339842
Iteration 400: Loss = -11741.909871731712
Iteration 500: Loss = -11625.125401197842
Iteration 600: Loss = -11608.114377497353
Iteration 700: Loss = -11606.081872418305
Iteration 800: Loss = -11605.747063796534
Iteration 900: Loss = -11605.5526433823
Iteration 1000: Loss = -11605.43304357704
Iteration 1100: Loss = -11605.349371121161
Iteration 1200: Loss = -11605.287359552269
Iteration 1300: Loss = -11605.239593704688
Iteration 1400: Loss = -11605.201078892016
Iteration 1500: Loss = -11603.578200037242
Iteration 1600: Loss = -11601.808980811777
Iteration 1700: Loss = -11601.784409625358
Iteration 1800: Loss = -11601.763400861242
Iteration 1900: Loss = -11601.743129317552
Iteration 2000: Loss = -11601.724823193208
Iteration 2100: Loss = -11601.696945064281
Iteration 2200: Loss = -11601.677189509634
Iteration 2300: Loss = -11601.668120446284
Iteration 2400: Loss = -11601.659825887271
Iteration 2500: Loss = -11601.6502749952
Iteration 2600: Loss = -11601.324173512527
Iteration 2700: Loss = -11594.325356612413
Iteration 2800: Loss = -11594.318962004792
Iteration 2900: Loss = -11594.313967180478
Iteration 3000: Loss = -11594.309736578434
Iteration 3100: Loss = -11594.30604635178
Iteration 3200: Loss = -11594.302734701512
Iteration 3300: Loss = -11594.299743185575
Iteration 3400: Loss = -11594.29707107487
Iteration 3500: Loss = -11594.29464785049
Iteration 3600: Loss = -11594.292385725643
Iteration 3700: Loss = -11594.290390117492
Iteration 3800: Loss = -11594.288970312607
Iteration 3900: Loss = -11594.28682696162
Iteration 4000: Loss = -11594.285210437982
Iteration 4100: Loss = -11594.283786688862
Iteration 4200: Loss = -11594.282399724148
Iteration 4300: Loss = -11594.281082865638
Iteration 4400: Loss = -11594.279724458434
Iteration 4500: Loss = -11594.27750703771
Iteration 4600: Loss = -11588.418738535167
Iteration 4700: Loss = -11588.384734902218
Iteration 4800: Loss = -11588.382833231391
Iteration 4900: Loss = -11588.381448672326
Iteration 5000: Loss = -11588.386956579934
1
Iteration 5100: Loss = -11588.379408976354
Iteration 5200: Loss = -11588.378599760967
Iteration 5300: Loss = -11588.387683794528
1
Iteration 5400: Loss = -11588.377098466706
Iteration 5500: Loss = -11588.376329236375
Iteration 5600: Loss = -11588.375598837065
Iteration 5700: Loss = -11588.388778287263
1
Iteration 5800: Loss = -11588.43071139664
2
Iteration 5900: Loss = -11588.399733628105
3
Iteration 6000: Loss = -11588.36715302264
Iteration 6100: Loss = -11588.366571934908
Iteration 6200: Loss = -11588.379139175431
1
Iteration 6300: Loss = -11588.36575842553
Iteration 6400: Loss = -11588.37040113269
1
Iteration 6500: Loss = -11588.365067190858
Iteration 6600: Loss = -11588.462628907435
1
Iteration 6700: Loss = -11588.354669115162
Iteration 6800: Loss = -11588.355912174531
1
Iteration 6900: Loss = -11588.354125826792
Iteration 7000: Loss = -11588.353874763034
Iteration 7100: Loss = -11588.35405977824
1
Iteration 7200: Loss = -11588.353465941245
Iteration 7300: Loss = -11588.468351382697
1
Iteration 7400: Loss = -11588.353149049854
Iteration 7500: Loss = -11588.354216310714
1
Iteration 7600: Loss = -11588.353277037557
2
Iteration 7700: Loss = -11588.350792854355
Iteration 7800: Loss = -11582.385483624825
Iteration 7900: Loss = -11582.382968473084
Iteration 8000: Loss = -11582.38313716834
1
Iteration 8100: Loss = -11582.386760024372
2
Iteration 8200: Loss = -11582.38648889544
3
Iteration 8300: Loss = -11582.382267028186
Iteration 8400: Loss = -11582.386639681828
1
Iteration 8500: Loss = -11582.39179218299
2
Iteration 8600: Loss = -11582.384520379414
3
Iteration 8700: Loss = -11582.377823812845
Iteration 8800: Loss = -11582.377306913364
Iteration 8900: Loss = -11582.376892689386
Iteration 9000: Loss = -11582.377291208963
1
Iteration 9100: Loss = -11582.376805393833
Iteration 9200: Loss = -11582.380638122717
1
Iteration 9300: Loss = -11582.377138483373
2
Iteration 9400: Loss = -11582.36791069903
Iteration 9500: Loss = -11582.39007444144
1
Iteration 9600: Loss = -11582.366438933548
Iteration 9700: Loss = -11582.377138452852
1
Iteration 9800: Loss = -11582.368180849075
2
Iteration 9900: Loss = -11582.366201082588
Iteration 10000: Loss = -11582.367780098062
1
Iteration 10100: Loss = -11582.3660532256
Iteration 10200: Loss = -11582.366010236423
Iteration 10300: Loss = -11582.389937437987
1
Iteration 10400: Loss = -11582.366613801538
2
Iteration 10500: Loss = -11582.501319699433
3
Iteration 10600: Loss = -11582.365939892334
Iteration 10700: Loss = -11582.365464067423
Iteration 10800: Loss = -11582.62356136901
1
Iteration 10900: Loss = -11582.365287266366
Iteration 11000: Loss = -11582.418338663598
1
Iteration 11100: Loss = -11582.365700878456
2
Iteration 11200: Loss = -11582.385915875562
3
Iteration 11300: Loss = -11582.430885221653
4
Iteration 11400: Loss = -11582.36700269311
5
Iteration 11500: Loss = -11582.367897885888
6
Iteration 11600: Loss = -11582.371958920981
7
Iteration 11700: Loss = -11582.444221066886
8
Iteration 11800: Loss = -11582.365447091412
9
Iteration 11900: Loss = -11582.36574195334
10
Stopping early at iteration 11900 due to no improvement.
tensor([[ -8.6163,   6.1187],
        [  6.6310,  -8.0532],
        [  5.0496,  -6.4682],
        [  6.2255,  -7.8933],
        [ -8.2433,   6.8568],
        [ -8.9820,   6.6275],
        [  6.1100,  -7.5009],
        [  5.8555,  -7.3206],
        [ -8.6123,   6.9953],
        [  5.4043,  -6.7915],
        [  4.2470,  -5.6593],
        [ -9.0338,   6.5415],
        [ -9.1711,   7.1347],
        [  5.2081,  -7.0791],
        [  5.8466,  -7.2382],
        [  3.2041,  -5.6054],
        [  6.2910,  -8.3987],
        [  7.0118,  -8.7863],
        [ -9.3611,   6.1314],
        [ -8.8413,   7.4314],
        [  6.0052,  -8.2816],
        [  6.6001,  -8.0557],
        [  5.9621,  -7.5399],
        [ -9.5310,   7.5988],
        [ -8.8682,   7.3743],
        [  1.6261,  -3.3817],
        [ -8.7037,   7.0202],
        [ -9.1372,   6.4338],
        [ -8.9198,   7.2597],
        [  6.4713,  -7.9751],
        [  5.4306,  -8.4401],
        [  5.6675,  -7.4188],
        [  5.0753,  -6.4685],
        [ -5.3078,   3.5810],
        [ -7.9518,   6.5445],
        [ -8.7216,   7.0068],
        [  5.7529,  -7.4602],
        [  4.5208,  -5.9167],
        [ -8.6953,   6.2400],
        [ -8.7679,   7.3783],
        [  6.2348,  -7.9516],
        [  6.1060,  -7.7053],
        [ -8.7603,   6.8798],
        [ -8.4970,   7.0678],
        [ -7.2220,   5.7185],
        [ -9.5457,   8.1594],
        [ -8.5523,   6.3479],
        [  5.3484,  -6.8135],
        [ -8.7601,   6.9026],
        [ -7.4017,   5.1986],
        [ -8.8201,   6.9863],
        [ -8.1684,   6.0064],
        [ -8.0345,   6.6279],
        [  6.2196,  -7.6439],
        [  6.5314,  -9.2772],
        [ -8.5572,   6.6993],
        [  4.6905,  -6.2681],
        [  4.7965,  -8.8606],
        [  6.2444,  -8.2656],
        [  4.8704,  -8.8060],
        [  7.1502,  -8.5773],
        [ -7.8415,   6.4149],
        [ -8.5470,   6.9514],
        [  6.7059,  -8.1343],
        [  6.6142,  -8.0092],
        [  6.5881, -10.6800],
        [ -9.2825,   7.7167],
        [  6.4220,  -8.0602],
        [  6.4020,  -7.8203],
        [ -8.4774,   4.4475],
        [ -8.7108,   7.2163],
        [  6.5912,  -8.2303],
        [ -4.6359,   2.9328],
        [ -5.6394,   3.7097],
        [ -8.8909,   5.9036],
        [ -8.7543,   7.0019],
        [ -9.1622,   6.6282],
        [ -9.5087,   6.9215],
        [ -7.9104,   6.4882],
        [ -8.0800,   5.9399],
        [ -9.1177,   6.7972],
        [ -8.2838,   6.2428],
        [  7.2584,  -8.6476],
        [ -8.3409,   6.9542],
        [ -8.3927,   5.7451],
        [  7.0280, -11.0382],
        [  4.0038,  -6.0482],
        [  5.3826,  -8.6251],
        [ -5.4282,   1.2889],
        [ -8.5175,   6.8476],
        [  6.1363,  -7.5405],
        [  7.0658,  -8.5070],
        [ -8.7458,   7.0082],
        [ -7.1029,   5.5673],
        [  6.4923,  -7.8795],
        [ -7.2146,   5.7211],
        [ -8.7869,   6.7505],
        [  5.1152,  -7.4335],
        [  6.6294,  -8.2039],
        [  7.2170,  -8.8390]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6759, 0.3241],
        [0.5537, 0.4463]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4800, 0.5200], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2178, 0.0870],
         [0.7656, 0.3894]],

        [[0.1826, 0.0958],
         [0.1625, 0.0441]],

        [[0.1086, 0.1046],
         [0.5707, 0.4965]],

        [[0.9434, 0.0932],
         [0.3490, 0.7905]],

        [[0.6519, 0.1084],
         [0.7594, 0.7167]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 31
Adjusted Rand Index: 0.13904643419078636
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5173762418486811
Average Adjusted Rand Index: 0.8198088480017203
Iteration 0: Loss = -20094.33962720844
Iteration 10: Loss = -11418.145146586736
Iteration 20: Loss = -11418.144530953241
Iteration 30: Loss = -11418.14452801934
Iteration 40: Loss = -11418.14452935618
1
Iteration 50: Loss = -11418.14452935618
2
Iteration 60: Loss = -11418.14452935618
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6878, 0.3122],
        [0.2211, 0.7789]], dtype=torch.float64)
alpha: tensor([0.4609, 0.5391])
beta: tensor([[[0.3889, 0.0869],
         [0.1809, 0.1961]],

        [[0.8934, 0.1058],
         [0.7589, 0.7677]],

        [[0.6180, 0.1047],
         [0.8450, 0.5958]],

        [[0.7377, 0.0932],
         [0.5376, 0.2716]],

        [[0.3246, 0.1089],
         [0.3007, 0.8428]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.97609580602389
Average Adjusted Rand Index: 0.9761598197715857
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20094.58865068376
Iteration 100: Loss = -12033.78448042935
Iteration 200: Loss = -11565.661420934335
Iteration 300: Loss = -11464.724362370664
Iteration 400: Loss = -11453.276589691219
Iteration 500: Loss = -11445.464043374861
Iteration 600: Loss = -11445.132039622444
Iteration 700: Loss = -11444.938988480275
Iteration 800: Loss = -11444.810964080083
Iteration 900: Loss = -11444.720454292614
Iteration 1000: Loss = -11444.653441227716
Iteration 1100: Loss = -11444.600590159034
Iteration 1200: Loss = -11436.799934387513
Iteration 1300: Loss = -11435.985119957215
Iteration 1400: Loss = -11435.957825776295
Iteration 1500: Loss = -11435.935700029733
Iteration 1600: Loss = -11435.917415984004
Iteration 1700: Loss = -11435.902004617143
Iteration 1800: Loss = -11435.88890817767
Iteration 1900: Loss = -11435.87764681171
Iteration 2000: Loss = -11435.867842181293
Iteration 2100: Loss = -11435.859229717584
Iteration 2200: Loss = -11435.851629040078
Iteration 2300: Loss = -11435.844835796102
Iteration 2400: Loss = -11435.838722806518
Iteration 2500: Loss = -11435.833841510163
Iteration 2600: Loss = -11435.828452337062
Iteration 2700: Loss = -11435.824204020488
Iteration 2800: Loss = -11435.820497785493
Iteration 2900: Loss = -11435.81692514777
Iteration 3000: Loss = -11435.813784016327
Iteration 3100: Loss = -11435.81090929702
Iteration 3200: Loss = -11435.808296158712
Iteration 3300: Loss = -11435.805722395133
Iteration 3400: Loss = -11435.803371056203
Iteration 3500: Loss = -11435.800982420462
Iteration 3600: Loss = -11435.798161197574
Iteration 3700: Loss = -11435.772598487252
Iteration 3800: Loss = -11424.699035837326
Iteration 3900: Loss = -11424.697348438682
Iteration 4000: Loss = -11424.69577349551
Iteration 4100: Loss = -11424.694011834923
Iteration 4200: Loss = -11415.713037992382
Iteration 4300: Loss = -11415.713159079458
1
Iteration 4400: Loss = -11415.710777831524
Iteration 4500: Loss = -11415.709146681154
Iteration 4600: Loss = -11415.708277527872
Iteration 4700: Loss = -11415.707330260379
Iteration 4800: Loss = -11415.706801793003
Iteration 4900: Loss = -11415.70574870689
Iteration 5000: Loss = -11415.705104239769
Iteration 5100: Loss = -11415.708083728445
1
Iteration 5200: Loss = -11415.703878522707
Iteration 5300: Loss = -11415.70329558004
Iteration 5400: Loss = -11415.702780897414
Iteration 5500: Loss = -11415.70232491619
Iteration 5600: Loss = -11415.701866533009
Iteration 5700: Loss = -11415.70144892824
Iteration 5800: Loss = -11415.701066278309
Iteration 5900: Loss = -11415.70065018001
Iteration 6000: Loss = -11415.700351863832
Iteration 6100: Loss = -11415.700043320528
Iteration 6200: Loss = -11415.699720783792
Iteration 6300: Loss = -11415.699449692955
Iteration 6400: Loss = -11415.699172188939
Iteration 6500: Loss = -11415.6999994561
1
Iteration 6600: Loss = -11415.698719195701
Iteration 6700: Loss = -11415.698476874988
Iteration 6800: Loss = -11415.69824677191
Iteration 6900: Loss = -11415.712996457412
1
Iteration 7000: Loss = -11415.719724430372
2
Iteration 7100: Loss = -11415.697638425467
Iteration 7200: Loss = -11415.697621214857
Iteration 7300: Loss = -11415.701782408847
1
Iteration 7400: Loss = -11415.697224660056
Iteration 7500: Loss = -11415.698258410843
1
Iteration 7600: Loss = -11415.703110560897
2
Iteration 7700: Loss = -11415.696811297665
Iteration 7800: Loss = -11415.696968214768
1
Iteration 7900: Loss = -11415.73070451431
2
Iteration 8000: Loss = -11415.698897931441
3
Iteration 8100: Loss = -11415.700169746875
4
Iteration 8200: Loss = -11415.699893466637
5
Iteration 8300: Loss = -11415.70021903508
6
Iteration 8400: Loss = -11415.696051455563
Iteration 8500: Loss = -11415.695953206534
Iteration 8600: Loss = -11415.853716178397
1
Iteration 8700: Loss = -11415.69577026096
Iteration 8800: Loss = -11415.719104882955
1
Iteration 8900: Loss = -11415.69617381908
2
Iteration 9000: Loss = -11415.732011330672
3
Iteration 9100: Loss = -11415.695616148854
Iteration 9200: Loss = -11415.69948334013
1
Iteration 9300: Loss = -11415.747405589249
2
Iteration 9400: Loss = -11415.697309487336
3
Iteration 9500: Loss = -11415.695720290905
4
Iteration 9600: Loss = -11415.696977146696
5
Iteration 9700: Loss = -11415.700840806909
6
Iteration 9800: Loss = -11415.72336629297
7
Iteration 9900: Loss = -11415.695194578666
Iteration 10000: Loss = -11415.697343989397
1
Iteration 10100: Loss = -11415.697350809933
2
Iteration 10200: Loss = -11415.695095638346
Iteration 10300: Loss = -11415.69880420304
1
Iteration 10400: Loss = -11415.69572040726
2
Iteration 10500: Loss = -11415.730053942953
3
Iteration 10600: Loss = -11415.705280107944
4
Iteration 10700: Loss = -11415.730917746881
5
Iteration 10800: Loss = -11415.696398594266
6
Iteration 10900: Loss = -11415.695029183753
Iteration 11000: Loss = -11415.695832163665
1
Iteration 11100: Loss = -11415.69865832994
2
Iteration 11200: Loss = -11415.695111229026
3
Iteration 11300: Loss = -11415.694854852873
Iteration 11400: Loss = -11415.69488372001
1
Iteration 11500: Loss = -11415.72034186346
2
Iteration 11600: Loss = -11415.696046954059
3
Iteration 11700: Loss = -11415.69660626046
4
Iteration 11800: Loss = -11415.720533770596
5
Iteration 11900: Loss = -11415.71604579944
6
Iteration 12000: Loss = -11415.73672653842
7
Iteration 12100: Loss = -11415.805177793625
8
Iteration 12200: Loss = -11415.713264148197
9
Iteration 12300: Loss = -11415.697024092311
10
Stopping early at iteration 12300 due to no improvement.
tensor([[  6.9688,  -8.3740],
        [ -8.1327,   6.6236],
        [ -6.3786,   4.8986],
        [ -8.3500,   6.8564],
        [  7.3183,  -8.7385],
        [  7.2997,  -9.0932],
        [ -8.1387,   6.0967],
        [ -8.6457,   7.1950],
        [  6.5216,  -9.1755],
        [ -7.8234,   6.1451],
        [ -5.3963,   3.9179],
        [  6.2183,  -8.5709],
        [  7.0555,  -9.1246],
        [ -7.4313,   5.7365],
        [ -7.9368,   6.4695],
        [ -8.0467,   6.3200],
        [ -8.7800,   6.4418],
        [ -8.8358,   6.9078],
        [  6.0915,  -9.4644],
        [  7.1972,  -8.6756],
        [ -8.1724,   5.9083],
        [ -8.9387,   6.5917],
        [ -9.3372,   6.3696],
        [  7.4987,  -8.9970],
        [  6.6437,  -8.0304],
        [ -3.9615,   2.5708],
        [  6.9348,  -8.3597],
        [  7.4370,  -9.1363],
        [  7.2555,  -8.6996],
        [ -9.5475,   7.3766],
        [ -6.1280,   4.2772],
        [ -6.8064,   5.3523],
        [ -8.2662,   4.9567],
        [  4.0962,  -5.5280],
        [  6.8233, -10.5250],
        [  7.4260,  -8.8935],
        [ -7.3918,   5.9851],
        [ -7.1826,   5.1551],
        [  5.8554,  -7.5365],
        [  6.7773,  -8.9390],
        [ -8.1271,   6.7001],
        [ -8.6526,   6.9104],
        [  7.1307,  -8.5174],
        [  6.8008,  -8.1871],
        [  6.0700,  -7.4666],
        [  7.3257, -11.6676],
        [  6.9125,  -8.3015],
        [ -6.5094,   4.7383],
        [  6.8176,  -9.0021],
        [  5.0170,  -6.4098],
        [  7.2256,  -8.8061],
        [  5.7894,  -7.2173],
        [  7.4614,  -8.8501],
        [ -8.9709,   6.4455],
        [ -8.2638,   6.8479],
        [  6.8040,  -8.2008],
        [ -6.9993,   5.3530],
        [ -8.2393,   6.6133],
        [ -8.3408,   6.1540],
        [ -7.7016,   6.2895],
        [-10.3406,   5.7254],
        [  6.5323,  -9.3822],
        [  6.7628,  -8.1697],
        [ -8.5238,   6.7936],
        [ -9.2614,   7.7868],
        [ -8.4384,   7.0514],
        [  6.9856,  -8.3799],
        [-10.3402,   6.7353],
        [ -8.1211,   6.7183],
        [  4.8827,  -6.8415],
        [  7.1739,  -8.7447],
        [ -8.5503,   6.9091],
        [  3.6340,  -5.0210],
        [  4.1009,  -5.5079],
        [  6.9787,  -8.5011],
        [  6.2292,  -8.5374],
        [  6.9493,  -8.4312],
        [  7.0503,  -8.9687],
        [  5.1547,  -9.7699],
        [  6.0364,  -8.5219],
        [  6.5843,  -8.2269],
        [  6.5751,  -8.4676],
        [ -7.7727,   6.3858],
        [  7.0043,  -8.6233],
        [  6.3875,  -7.9365],
        [-10.4248,   7.4749],
        [ -8.7477,   4.5808],
        [ -9.2472,   7.8603],
        [  2.9699,  -4.7107],
        [  6.3812,  -8.0223],
        [ -8.5401,   6.6610],
        [ -9.0694,   6.5014],
        [  7.5604,  -8.9468],
        [  4.5179,  -6.9059],
        [ -8.0727,   6.6864],
        [  6.0941,  -7.6757],
        [  7.1155,  -8.7091],
        [ -7.5738,   6.1106],
        [ -9.2833,   7.2980],
        [ -9.9310,   7.2835]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6888, 0.3112],
        [0.2205, 0.7795]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5200, 0.4800], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3971, 0.0870],
         [0.1809, 0.2002]],

        [[0.8934, 0.1052],
         [0.7589, 0.7677]],

        [[0.6180, 0.1046],
         [0.8450, 0.5958]],

        [[0.7377, 0.0932],
         [0.5376, 0.2716]],

        [[0.3246, 0.1091],
         [0.3007, 0.8428]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840318395231936
Average Adjusted Rand Index: 0.9839987774932922
Iteration 0: Loss = -25597.16696963143
Iteration 10: Loss = -11843.373289352547
Iteration 20: Loss = -11418.14446630812
Iteration 30: Loss = -11418.144517003331
1
Iteration 40: Loss = -11418.144527060595
2
Iteration 50: Loss = -11418.144531517533
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.6878, 0.3122],
        [0.2211, 0.7789]], dtype=torch.float64)
alpha: tensor([0.4609, 0.5391])
beta: tensor([[[0.3889, 0.0869],
         [0.5441, 0.1961]],

        [[0.5409, 0.1058],
         [0.7865, 0.2936]],

        [[0.6925, 0.1047],
         [0.2697, 0.3220]],

        [[0.4118, 0.0932],
         [0.9616, 0.1714]],

        [[0.5968, 0.1089],
         [0.4744, 0.8596]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.97609580602389
Average Adjusted Rand Index: 0.9761598197715857
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25597.216270840003
Iteration 100: Loss = -12116.501323661712
Iteration 200: Loss = -12109.910522314718
Iteration 300: Loss = -12101.560638064853
Iteration 400: Loss = -12091.799130784786
Iteration 500: Loss = -11717.657152806036
Iteration 600: Loss = -11655.197391455325
Iteration 700: Loss = -11650.392582301463
Iteration 800: Loss = -11646.780879122945
Iteration 900: Loss = -11630.528877152503
Iteration 1000: Loss = -11620.885136455623
Iteration 1100: Loss = -11619.76496391816
Iteration 1200: Loss = -11619.589599498935
Iteration 1300: Loss = -11619.508290357982
Iteration 1400: Loss = -11619.449405266038
Iteration 1500: Loss = -11619.40425325772
Iteration 1600: Loss = -11619.358348452484
Iteration 1700: Loss = -11613.55126917017
Iteration 1800: Loss = -11613.525197876459
Iteration 1900: Loss = -11613.503233445721
Iteration 2000: Loss = -11613.483674098843
Iteration 2100: Loss = -11613.4653962074
Iteration 2200: Loss = -11613.448479845358
Iteration 2300: Loss = -11613.43397002374
Iteration 2400: Loss = -11613.413699553048
Iteration 2500: Loss = -11605.51666157796
Iteration 2600: Loss = -11605.506670634084
Iteration 2700: Loss = -11605.499119886977
Iteration 2800: Loss = -11605.492492309091
Iteration 2900: Loss = -11605.48655824661
Iteration 3000: Loss = -11605.481179275484
Iteration 3100: Loss = -11605.476440569139
Iteration 3200: Loss = -11605.472143072135
Iteration 3300: Loss = -11605.46828073527
Iteration 3400: Loss = -11605.464385300376
Iteration 3500: Loss = -11605.459121393596
Iteration 3600: Loss = -11589.969244253793
Iteration 3700: Loss = -11589.938765018504
Iteration 3800: Loss = -11589.93550164595
Iteration 3900: Loss = -11589.930585076698
Iteration 4000: Loss = -11589.92745908144
Iteration 4100: Loss = -11589.925487511808
Iteration 4200: Loss = -11589.92549163028
1
Iteration 4300: Loss = -11589.921610222204
Iteration 4400: Loss = -11589.92036259382
Iteration 4500: Loss = -11589.919086777798
Iteration 4600: Loss = -11589.918028206139
Iteration 4700: Loss = -11589.919151783228
1
Iteration 4800: Loss = -11589.917464179638
Iteration 4900: Loss = -11589.913562459295
Iteration 5000: Loss = -11589.91244835025
Iteration 5100: Loss = -11589.927711055334
1
Iteration 5200: Loss = -11589.90992497911
Iteration 5300: Loss = -11589.907463798188
Iteration 5400: Loss = -11589.729959790626
Iteration 5500: Loss = -11589.709359195684
Iteration 5600: Loss = -11589.708416386144
Iteration 5700: Loss = -11589.708102253257
Iteration 5800: Loss = -11589.707025792499
Iteration 5900: Loss = -11589.70651230809
Iteration 6000: Loss = -11589.707108608485
1
Iteration 6100: Loss = -11589.70546282364
Iteration 6200: Loss = -11589.70505424914
Iteration 6300: Loss = -11589.704622072208
Iteration 6400: Loss = -11589.70460845668
Iteration 6500: Loss = -11589.704207512706
Iteration 6600: Loss = -11589.707353954314
1
Iteration 6700: Loss = -11589.70499897897
2
Iteration 6800: Loss = -11589.703684340935
Iteration 6900: Loss = -11589.70408595956
1
Iteration 7000: Loss = -11589.702472045334
Iteration 7100: Loss = -11589.703928107
1
Iteration 7200: Loss = -11589.706895707599
2
Iteration 7300: Loss = -11589.701950541767
Iteration 7400: Loss = -11589.701611374914
Iteration 7500: Loss = -11589.702694495429
1
Iteration 7600: Loss = -11589.702160135128
2
Iteration 7700: Loss = -11589.700848085356
Iteration 7800: Loss = -11589.700699377081
Iteration 7900: Loss = -11589.700557624583
Iteration 8000: Loss = -11589.705388256805
1
Iteration 8100: Loss = -11589.700206835958
Iteration 8200: Loss = -11589.702891438756
1
Iteration 8300: Loss = -11589.702431523829
2
Iteration 8400: Loss = -11589.70296443929
3
Iteration 8500: Loss = -11589.7198439046
4
Iteration 8600: Loss = -11589.69954702523
Iteration 8700: Loss = -11589.699752677307
1
Iteration 8800: Loss = -11589.69932322294
Iteration 8900: Loss = -11589.69958512598
1
Iteration 9000: Loss = -11589.696383609336
Iteration 9100: Loss = -11589.69139700048
Iteration 9200: Loss = -11589.693219538514
1
Iteration 9300: Loss = -11589.691144409955
Iteration 9400: Loss = -11589.6917627341
1
Iteration 9500: Loss = -11589.696529125775
2
Iteration 9600: Loss = -11589.639259015348
Iteration 9700: Loss = -11589.578097367428
Iteration 9800: Loss = -11589.581986330828
1
Iteration 9900: Loss = -11589.577783800416
Iteration 10000: Loss = -11589.577821365463
1
Iteration 10100: Loss = -11589.577714339128
Iteration 10200: Loss = -11589.58040053816
1
Iteration 10300: Loss = -11589.577694304277
Iteration 10400: Loss = -11589.57765757722
Iteration 10500: Loss = -11589.590038674678
1
Iteration 10600: Loss = -11589.579908859245
2
Iteration 10700: Loss = -11589.57761251932
Iteration 10800: Loss = -11589.74155322008
1
Iteration 10900: Loss = -11589.577419571579
Iteration 11000: Loss = -11589.584061460446
1
Iteration 11100: Loss = -11589.577384632834
Iteration 11200: Loss = -11589.578412025376
1
Iteration 11300: Loss = -11589.643931317152
2
Iteration 11400: Loss = -11589.577388281341
3
Iteration 11500: Loss = -11589.577732146021
4
Iteration 11600: Loss = -11589.600288646654
5
Iteration 11700: Loss = -11589.583078677257
6
Iteration 11800: Loss = -11589.581511437276
7
Iteration 11900: Loss = -11589.598650399643
8
Iteration 12000: Loss = -11589.577232058327
Iteration 12100: Loss = -11589.583179652262
1
Iteration 12200: Loss = -11589.582267331392
2
Iteration 12300: Loss = -11589.581110491898
3
Iteration 12400: Loss = -11589.580746056641
4
Iteration 12500: Loss = -11589.578843816817
5
Iteration 12600: Loss = -11589.600481726464
6
Iteration 12700: Loss = -11589.576517236912
Iteration 12800: Loss = -11589.614576908578
1
Iteration 12900: Loss = -11589.577671630439
2
Iteration 13000: Loss = -11589.577070517675
3
Iteration 13100: Loss = -11589.581875271466
4
Iteration 13200: Loss = -11589.592610891934
5
Iteration 13300: Loss = -11589.626149669022
6
Iteration 13400: Loss = -11589.576484413867
Iteration 13500: Loss = -11589.576952133162
1
Iteration 13600: Loss = -11589.577985594706
2
Iteration 13700: Loss = -11589.56714769786
Iteration 13800: Loss = -11589.569999808857
1
Iteration 13900: Loss = -11589.571653796596
2
Iteration 14000: Loss = -11589.635980126444
3
Iteration 14100: Loss = -11589.567671448489
4
Iteration 14200: Loss = -11589.569232405045
5
Iteration 14300: Loss = -11582.564823296689
Iteration 14400: Loss = -11582.554105677493
Iteration 14500: Loss = -11582.550851688458
Iteration 14600: Loss = -11582.559053214105
1
Iteration 14700: Loss = -11582.548461910148
Iteration 14800: Loss = -11582.55257817978
1
Iteration 14900: Loss = -11582.549754392676
2
Iteration 15000: Loss = -11582.394024666008
Iteration 15100: Loss = -11582.38304904525
Iteration 15200: Loss = -11582.386966472206
1
Iteration 15300: Loss = -11582.383215838354
2
Iteration 15400: Loss = -11582.38476368522
3
Iteration 15500: Loss = -11582.384999622649
4
Iteration 15600: Loss = -11582.393850683708
5
Iteration 15700: Loss = -11582.38384912529
6
Iteration 15800: Loss = -11582.383885841014
7
Iteration 15900: Loss = -11582.383710495928
8
Iteration 16000: Loss = -11582.427405413495
9
Iteration 16100: Loss = -11582.383015616519
Iteration 16200: Loss = -11582.383682820137
1
Iteration 16300: Loss = -11582.448966342325
2
Iteration 16400: Loss = -11582.381752083951
Iteration 16500: Loss = -11582.38536145644
1
Iteration 16600: Loss = -11582.391668636867
2
Iteration 16700: Loss = -11582.381939809746
3
Iteration 16800: Loss = -11582.388727259558
4
Iteration 16900: Loss = -11582.38451633808
5
Iteration 17000: Loss = -11582.642175529563
6
Iteration 17100: Loss = -11582.382600075764
7
Iteration 17200: Loss = -11582.415201165733
8
Iteration 17300: Loss = -11582.337463398735
Iteration 17400: Loss = -11582.337913490606
1
Iteration 17500: Loss = -11582.335888026284
Iteration 17600: Loss = -11582.335699371288
Iteration 17700: Loss = -11582.476592363071
1
Iteration 17800: Loss = -11582.337140147296
2
Iteration 17900: Loss = -11582.471577867038
3
Iteration 18000: Loss = -11582.335522137078
Iteration 18100: Loss = -11582.348303593568
1
Iteration 18200: Loss = -11582.338834376804
2
Iteration 18300: Loss = -11582.362107578072
3
Iteration 18400: Loss = -11582.33534879984
Iteration 18500: Loss = -11582.363927045406
1
Iteration 18600: Loss = -11582.335361588339
2
Iteration 18700: Loss = -11582.335641913976
3
Iteration 18800: Loss = -11582.335392787629
4
Iteration 18900: Loss = -11582.335976966517
5
Iteration 19000: Loss = -11582.339487381128
6
Iteration 19100: Loss = -11582.335477294446
7
Iteration 19200: Loss = -11582.335461135062
8
Iteration 19300: Loss = -11582.336374583329
9
Iteration 19400: Loss = -11582.345709890655
10
Stopping early at iteration 19400 due to no improvement.
tensor([[  6.8389,  -9.0624],
        [ -9.7994,   6.3526],
        [ -6.5667,   4.9857],
        [ -9.1298,   7.2444],
        [  8.5877, -10.4702],
        [  8.9769, -10.9634],
        [ -9.0399,   6.4760],
        [ -8.3234,   5.6140],
        [  7.8951,  -9.9772],
        [ -7.8599,   4.4867],
        [ -6.1495,   3.7405],
        [  6.6078, -10.1175],
        [  7.5709, -11.5167],
        [ -6.6532,   4.3783],
        [ -7.5860,   6.1241],
        [ -5.2160,   3.5971],
        [ -8.9233,   7.5100],
        [-10.5445,   8.1468],
        [  7.5581,  -9.0274],
        [  8.3159,  -9.8895],
        [ -8.0409,   6.6214],
        [-11.4802,   7.5149],
        [ -8.6235,   7.1946],
        [  8.5341, -10.3472],
        [  8.3614,  -9.7563],
        [ -3.5835,   1.4259],
        [  8.0630,  -9.6783],
        [  8.0246,  -9.6272],
        [  8.2294, -10.3446],
        [-10.7799,   8.6510],
        [ -4.8828,   3.4152],
        [ -7.2598,   5.8614],
        [ -6.7900,   4.7614],
        [  3.5871,  -5.2933],
        [  7.2822,  -9.3390],
        [  8.4689, -10.8585],
        [ -8.4995,   6.8242],
        [ -6.1819,   4.2667],
        [  6.5453,  -8.8534],
        [  8.9580, -10.3785],
        [ -9.7254,   8.3322],
        [ -9.1588,   7.2291],
        [  8.2656, -10.0341],
        [  8.1049,  -9.6521],
        [  4.9813,  -8.5123],
        [  9.1882, -11.6193],
        [  7.2724,  -9.2117],
        [ -8.4518,   3.8365],
        [  8.4382,  -9.9992],
        [  5.8465,  -7.3465],
        [  8.0433,  -9.4302],
        [  6.5974,  -8.0161],
        [  8.4966,  -9.8955],
        [ -9.0060,   7.5241],
        [ -9.6123,   8.2249],
        [  8.9279, -11.2838],
        [ -6.3548,   4.5882],
        [ -7.7588,   5.9480],
        [ -8.5546,   7.0390],
        [ -7.7888,   5.8992],
        [ -8.2157,   5.8012],
        [  6.8718,  -9.7723],
        [  8.0966,  -9.8234],
        [ -9.6630,   7.6097],
        [-10.9056,   7.9628],
        [-10.8526,   8.2454],
        [  8.2977, -11.1362],
        [ -9.7977,   7.6151],
        [-10.0285,   7.4961],
        [  5.0176,  -7.8927],
        [  8.8170, -10.2199],
        [ -8.5206,   7.1137],
        [  2.6993,  -4.8620],
        [  3.4886,  -5.8562],
        [  7.1310,  -8.7491],
        [  8.1660,  -9.8640],
        [  7.9394,  -9.4789],
        [  8.5470, -11.2438],
        [  7.5505,  -9.8190],
        [  7.7139,  -9.1901],
        [  8.6649, -10.0549],
        [  7.3975,  -8.8260],
        [ -8.7312,   7.0500],
        [  8.8009, -10.6775],
        [  6.2324,  -8.3684],
        [-10.0847,   8.6917],
        [ -5.9894,   4.0642],
        [ -8.1957,   6.8088],
        [  2.5718,  -4.1431],
        [  7.9201, -10.8051],
        [ -9.1609,   7.5853],
        [-10.2269,   8.3688],
        [  8.5891,  -9.9883],
        [  5.7383,  -7.1708],
        [ -7.8634,   6.4195],
        [  5.3830,  -7.7833],
        [  7.7193, -11.1236],
        [ -6.9143,   5.5123],
        [ -9.4561,   8.0631],
        [-11.6115,   8.3241]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4463, 0.5537],
        [0.3237, 0.6763]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5141, 0.4859], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3896, 0.0866],
         [0.5441, 0.2176]],

        [[0.5409, 0.0959],
         [0.7865, 0.2936]],

        [[0.6925, 0.1046],
         [0.2697, 0.3220]],

        [[0.4118, 0.0931],
         [0.9616, 0.1714]],

        [[0.5968, 0.1083],
         [0.4744, 0.8596]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 69
Adjusted Rand Index: 0.13904643419078636
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5173762418486811
Average Adjusted Rand Index: 0.8198088480017203
11434.012324635527
new:  [0.9840318395231936, 0.5173762418486811, 0.9840318395231936, 0.5173762418486811] [0.9839987774932922, 0.8198088480017203, 0.9839987774932922, 0.8198088480017203] [11415.962344717575, 11582.36574195334, 11415.697024092311, 11582.345709890655]
prior:  [0.97609580602389, 0.97609580602389, 0.97609580602389, 0.97609580602389] [0.9761598197715857, 0.9761598197715857, 0.9761598197715857, 0.9761598197715857] [11418.144525997861, 11418.144529031268, 11418.14452935618, 11418.144531517533]
-----------------------------------------------------------------------------------------
This iteration is 15
True Objective function: Loss = -11569.961933338333
Iteration 0: Loss = -37212.90451298963
Iteration 10: Loss = -12270.415562568827
Iteration 20: Loss = -12033.179928492626
Iteration 30: Loss = -11736.417341072807
Iteration 40: Loss = -11734.374333583617
Iteration 50: Loss = -11730.904103652047
Iteration 60: Loss = -11730.904140242703
1
Iteration 70: Loss = -11730.904409541778
2
Iteration 80: Loss = -11730.904437721108
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.6351, 0.3649],
        [0.2759, 0.7241]], dtype=torch.float64)
alpha: tensor([0.4599, 0.5401])
beta: tensor([[[0.3786, 0.0956],
         [0.0414, 0.2176]],

        [[0.6544, 0.1003],
         [0.2347, 0.9120]],

        [[0.5575, 0.1026],
         [0.0569, 0.0635]],

        [[0.8290, 0.0990],
         [0.9383, 0.4904]],

        [[0.5312, 0.1101],
         [0.0531, 0.8408]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 78
Adjusted Rand Index: 0.3079630444346678
Global Adjusted Rand Index: 0.4667939510020829
Average Adjusted Rand Index: 0.8535918252166628
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37176.17226982658
Iteration 100: Loss = -12362.571596194264
Iteration 200: Loss = -12358.979099800705
Iteration 300: Loss = -12357.861288607963
Iteration 400: Loss = -12357.137963492563
Iteration 500: Loss = -12356.41945136545
Iteration 600: Loss = -12355.20225092583
Iteration 700: Loss = -12351.988184833692
Iteration 800: Loss = -12347.33559845237
Iteration 900: Loss = -12337.110017247063
Iteration 1000: Loss = -12332.622372588987
Iteration 1100: Loss = -12323.356640395636
Iteration 1200: Loss = -12308.614433660441
Iteration 1300: Loss = -12298.806813462492
Iteration 1400: Loss = -12278.961932275399
Iteration 1500: Loss = -12251.055006124863
Iteration 1600: Loss = -12214.195983260657
Iteration 1700: Loss = -12195.108930055903
Iteration 1800: Loss = -12169.815741436854
Iteration 1900: Loss = -12156.722178699543
Iteration 2000: Loss = -12151.542491847225
Iteration 2100: Loss = -12130.87879481258
Iteration 2200: Loss = -12126.946477273308
Iteration 2300: Loss = -12120.111071134657
Iteration 2400: Loss = -12113.539950623888
Iteration 2500: Loss = -12113.29289361967
Iteration 2600: Loss = -12110.046699818726
Iteration 2700: Loss = -12107.123802242631
Iteration 2800: Loss = -12101.887419206922
Iteration 2900: Loss = -12101.714181743924
Iteration 3000: Loss = -12101.65546539562
Iteration 3100: Loss = -12101.599909461765
Iteration 3200: Loss = -12101.504588314783
Iteration 3300: Loss = -12099.942920844975
Iteration 3400: Loss = -12091.578661445506
Iteration 3500: Loss = -12089.45487386358
Iteration 3600: Loss = -12089.352335292882
Iteration 3700: Loss = -12089.32349592183
Iteration 3800: Loss = -12089.302055046925
Iteration 3900: Loss = -12089.2841838757
Iteration 4000: Loss = -12089.261680985413
Iteration 4100: Loss = -12089.129680127311
Iteration 4200: Loss = -12086.405372058494
Iteration 4300: Loss = -12086.383481464225
Iteration 4400: Loss = -12086.361470015916
Iteration 4500: Loss = -12086.003472587316
Iteration 4600: Loss = -12082.216361330105
Iteration 4700: Loss = -12077.61802603767
Iteration 4800: Loss = -12077.599886561777
Iteration 4900: Loss = -12077.489846671497
Iteration 5000: Loss = -12067.897634727335
Iteration 5100: Loss = -12067.866686946787
Iteration 5200: Loss = -12067.855024873186
Iteration 5300: Loss = -12067.854519930748
Iteration 5400: Loss = -12067.833310262413
Iteration 5500: Loss = -12067.802217257748
Iteration 5600: Loss = -12067.150114349563
Iteration 5700: Loss = -12062.668557037834
Iteration 5800: Loss = -12062.661253263635
Iteration 5900: Loss = -12062.654441725155
Iteration 6000: Loss = -12062.65047765502
Iteration 6100: Loss = -12062.64731778323
Iteration 6200: Loss = -12062.642416677214
Iteration 6300: Loss = -12062.631910927612
Iteration 6400: Loss = -12055.263128361134
Iteration 6500: Loss = -12055.233735321948
Iteration 6600: Loss = -12055.22992961564
Iteration 6700: Loss = -12055.231246698668
1
Iteration 6800: Loss = -12055.231052971787
2
Iteration 6900: Loss = -12055.225671438851
Iteration 7000: Loss = -12055.224452409344
Iteration 7100: Loss = -12055.223389716497
Iteration 7200: Loss = -12055.224027762699
1
Iteration 7300: Loss = -12055.228589201175
2
Iteration 7400: Loss = -12055.219762853072
Iteration 7500: Loss = -12055.219233355245
Iteration 7600: Loss = -12055.223752868835
1
Iteration 7700: Loss = -12055.217706484571
Iteration 7800: Loss = -12055.223491970366
1
Iteration 7900: Loss = -12055.267004155292
2
Iteration 8000: Loss = -12055.217216517998
Iteration 8100: Loss = -12055.215768190099
Iteration 8200: Loss = -12055.368102164826
1
Iteration 8300: Loss = -12055.214742196596
Iteration 8400: Loss = -12055.21532884179
1
Iteration 8500: Loss = -12055.216844931116
2
Iteration 8600: Loss = -12055.213748781563
Iteration 8700: Loss = -12055.220333619121
1
Iteration 8800: Loss = -12055.214328474416
2
Iteration 8900: Loss = -12055.25468453255
3
Iteration 9000: Loss = -12055.22015116948
4
Iteration 9100: Loss = -12055.212147216129
Iteration 9200: Loss = -12055.212684558588
1
Iteration 9300: Loss = -12055.2115864326
Iteration 9400: Loss = -12055.214934777337
1
Iteration 9500: Loss = -12055.227608900419
2
Iteration 9600: Loss = -12055.209960928063
Iteration 9700: Loss = -12055.210951534431
1
Iteration 9800: Loss = -12055.221630972472
2
Iteration 9900: Loss = -12055.215339994578
3
Iteration 10000: Loss = -12055.209324859481
Iteration 10100: Loss = -12055.210473001573
1
Iteration 10200: Loss = -12055.208729287808
Iteration 10300: Loss = -12055.208894713018
1
Iteration 10400: Loss = -12055.208477292608
Iteration 10500: Loss = -12055.20883736242
1
Iteration 10600: Loss = -12055.213258947351
2
Iteration 10700: Loss = -12055.24956846131
3
Iteration 10800: Loss = -12055.377898867313
4
Iteration 10900: Loss = -12055.1882408835
Iteration 11000: Loss = -12055.187390977737
Iteration 11100: Loss = -12055.191981472126
1
Iteration 11200: Loss = -12055.199562432204
2
Iteration 11300: Loss = -12055.186991819974
Iteration 11400: Loss = -12055.18687808794
Iteration 11500: Loss = -12055.189724821605
1
Iteration 11600: Loss = -12055.234778411614
2
Iteration 11700: Loss = -12055.256102458457
3
Iteration 11800: Loss = -12055.188491605919
4
Iteration 11900: Loss = -12055.191036132186
5
Iteration 12000: Loss = -12055.187510695878
6
Iteration 12100: Loss = -12055.452384875307
7
Iteration 12200: Loss = -12055.195568304971
8
Iteration 12300: Loss = -12055.497260476624
9
Iteration 12400: Loss = -12055.18669252562
Iteration 12500: Loss = -12055.186357185707
Iteration 12600: Loss = -12055.186420103817
1
Iteration 12700: Loss = -12055.199751238279
2
Iteration 12800: Loss = -12055.195514583595
3
Iteration 12900: Loss = -12055.18681428497
4
Iteration 13000: Loss = -12055.18612972003
Iteration 13100: Loss = -12055.196267427598
1
Iteration 13200: Loss = -12055.18768636731
2
Iteration 13300: Loss = -12055.186073863693
Iteration 13400: Loss = -12055.186251159295
1
Iteration 13500: Loss = -12055.185841373219
Iteration 13600: Loss = -12055.18647953598
1
Iteration 13700: Loss = -12055.18596909234
2
Iteration 13800: Loss = -12055.216506392955
3
Iteration 13900: Loss = -12055.18703648962
4
Iteration 14000: Loss = -12055.188223710315
5
Iteration 14100: Loss = -12055.370963856178
6
Iteration 14200: Loss = -12055.181680878955
Iteration 14300: Loss = -12055.182746591992
1
Iteration 14400: Loss = -12055.181858252017
2
Iteration 14500: Loss = -12055.183479286576
3
Iteration 14600: Loss = -12055.185507033762
4
Iteration 14700: Loss = -12055.19608025137
5
Iteration 14800: Loss = -12055.202984627336
6
Iteration 14900: Loss = -12055.185242927515
7
Iteration 15000: Loss = -12055.181753112485
8
Iteration 15100: Loss = -12055.225201158251
9
Iteration 15200: Loss = -12055.183076165808
10
Stopping early at iteration 15200 due to no improvement.
tensor([[-4.9332e+00,  3.1797e-01],
        [ 4.1356e-01, -5.0288e+00],
        [-1.0531e+01,  5.9161e+00],
        [-1.0071e+01,  5.4556e+00],
        [ 2.0501e+00, -6.6653e+00],
        [-7.0966e+00,  2.4814e+00],
        [ 3.0913e+00, -7.7065e+00],
        [-1.0780e+01,  6.1648e+00],
        [-9.5772e-01, -3.6575e+00],
        [-1.0466e+01,  5.8510e+00],
        [ 1.7312e+00, -6.3464e+00],
        [ 3.7443e+00, -8.3596e+00],
        [-1.0489e+01,  5.8735e+00],
        [ 6.3370e-01, -5.2489e+00],
        [-8.2177e+00,  3.6025e+00],
        [ 1.4309e+00, -6.0462e+00],
        [-9.7582e+00,  5.1430e+00],
        [-1.0732e+01,  6.1169e+00],
        [-4.2915e-01, -4.1861e+00],
        [-9.1001e+00,  4.4849e+00],
        [ 9.5318e-01, -5.5684e+00],
        [-6.6171e+00,  2.0019e+00],
        [-9.6549e+00,  5.0396e+00],
        [-1.0059e+01,  5.4443e+00],
        [-1.0023e+01,  5.4075e+00],
        [-2.1070e-01, -4.4045e+00],
        [ 2.2457e-01, -4.8398e+00],
        [ 3.5647e+00, -8.1799e+00],
        [ 5.4084e+00, -1.0024e+01],
        [-1.0182e+01,  5.5669e+00],
        [ 1.2925e+00, -5.9077e+00],
        [ 1.5045e+00, -6.1197e+00],
        [-6.8665e-01, -3.9286e+00],
        [ 4.3967e+00, -9.0120e+00],
        [ 4.2692e+00, -8.8845e+00],
        [-9.5674e+00,  4.9522e+00],
        [-5.3663e-01, -4.0786e+00],
        [-9.2840e+00,  4.6688e+00],
        [-1.0206e+01,  5.5905e+00],
        [-9.7676e+00,  5.1524e+00],
        [ 4.3452e-01, -5.0497e+00],
        [ 3.2874e+00, -7.9027e+00],
        [ 2.8329e+00, -7.4481e+00],
        [ 1.4942e+00, -6.1094e+00],
        [-1.0623e+01,  6.0075e+00],
        [-1.0041e+01,  5.4256e+00],
        [ 1.0519e+00, -5.6672e+00],
        [ 4.0194e-01, -5.0172e+00],
        [-1.1742e+00, -3.4410e+00],
        [-9.4843e+00,  4.8690e+00],
        [-1.0431e+01,  5.8154e+00],
        [-8.4355e+00,  3.8203e+00],
        [-1.0936e-03, -4.6141e+00],
        [ 3.7630e+00, -8.3782e+00],
        [ 1.8396e+00, -6.4548e+00],
        [-7.8410e+00,  3.2257e+00],
        [ 2.4802e+00, -7.0954e+00],
        [-1.5373e-02, -4.5998e+00],
        [-7.2678e+00,  2.6526e+00],
        [-1.0164e+01,  5.5486e+00],
        [-1.0601e+01,  5.9862e+00],
        [-7.3711e+00,  2.7559e+00],
        [-7.4778e+00,  2.8626e+00],
        [ 1.3930e+00, -6.0082e+00],
        [ 1.0339e+00, -5.6491e+00],
        [-1.0253e+01,  5.6377e+00],
        [-9.5156e+00,  4.9003e+00],
        [-7.1136e+00,  2.4984e+00],
        [ 1.5982e+00, -6.2134e+00],
        [-1.0806e+01,  6.1911e+00],
        [-1.0164e+01,  5.5486e+00],
        [ 2.5860e+00, -7.2012e+00],
        [-9.8993e+00,  5.2841e+00],
        [-1.0269e+01,  5.6540e+00],
        [ 8.4422e-01, -5.4594e+00],
        [ 2.5234e+00, -7.1386e+00],
        [-5.6969e-01, -4.0455e+00],
        [ 3.7304e+00, -8.3456e+00],
        [ 3.5003e+00, -8.1155e+00],
        [-1.0385e+00, -3.5767e+00],
        [ 1.7456e+00, -6.3608e+00],
        [-8.2007e-01, -3.7952e+00],
        [-1.0513e+01,  5.8982e+00],
        [ 1.9120e+00, -6.5273e+00],
        [ 4.4675e+00, -9.0827e+00],
        [-1.0002e+01,  5.3864e+00],
        [-1.0656e+01,  6.0405e+00],
        [ 1.1594e+00, -5.7747e+00],
        [ 5.8883e-01, -5.2041e+00],
        [-1.0290e+01,  5.6748e+00],
        [-9.2414e+00,  4.6262e+00],
        [-1.1125e+01,  6.5101e+00],
        [ 1.9489e+00, -6.5641e+00],
        [-7.0845e+00,  2.4693e+00],
        [-1.0753e+01,  6.1373e+00],
        [ 1.6786e+00, -6.2938e+00],
        [-1.0889e+01,  6.2742e+00],
        [ 4.9058e+00, -9.5210e+00],
        [ 2.3556e+00, -6.9708e+00],
        [-1.1211e+00, -3.4942e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5040, 0.4960],
        [0.2662, 0.7338]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5246, 0.4754], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2074, 0.0959],
         [0.0414, 0.2923]],

        [[0.6544, 0.0992],
         [0.2347, 0.9120]],

        [[0.5575, 0.1016],
         [0.0569, 0.0635]],

        [[0.8290, 0.8420],
         [0.9383, 0.4904]],

        [[0.5312, 0.1043],
         [0.0531, 0.8408]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.6265493240736881
Average Adjusted Rand Index: 0.7761600705411591
Iteration 0: Loss = -39283.08998847924
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.4397,    nan]],

        [[0.3926,    nan],
         [0.3358, 0.0154]],

        [[0.4743,    nan],
         [0.2380, 0.2135]],

        [[0.6087,    nan],
         [0.4827, 0.4926]],

        [[0.9501,    nan],
         [0.2206, 0.9408]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39281.37713227731
Iteration 100: Loss = -12428.080107750455
Iteration 200: Loss = -12382.652829493263
Iteration 300: Loss = -12369.6545566529
Iteration 400: Loss = -12362.245758010424
Iteration 500: Loss = -12359.01923366497
Iteration 600: Loss = -12356.842142323616
Iteration 700: Loss = -12354.299143169284
Iteration 800: Loss = -12349.621198279314
Iteration 900: Loss = -12342.126612272705
Iteration 1000: Loss = -12330.654246876473
Iteration 1100: Loss = -12298.402266298142
Iteration 1200: Loss = -12239.30660952254
Iteration 1300: Loss = -12106.510253341963
Iteration 1400: Loss = -12034.622717115497
Iteration 1500: Loss = -11964.601079504617
Iteration 1600: Loss = -11946.407217740645
Iteration 1700: Loss = -11944.185608178763
Iteration 1800: Loss = -11943.570605841818
Iteration 1900: Loss = -11943.272059311814
Iteration 2000: Loss = -11943.062569521753
Iteration 2100: Loss = -11942.901369443605
Iteration 2200: Loss = -11942.770887654378
Iteration 2300: Loss = -11942.667121172708
Iteration 2400: Loss = -11942.58539162221
Iteration 2500: Loss = -11942.517427878945
Iteration 2600: Loss = -11942.459812884734
Iteration 2700: Loss = -11942.41037689275
Iteration 2800: Loss = -11942.36756509452
Iteration 2900: Loss = -11942.330059840937
Iteration 3000: Loss = -11942.297013835834
Iteration 3100: Loss = -11942.267706734194
Iteration 3200: Loss = -11942.241447709148
Iteration 3300: Loss = -11942.217848625793
Iteration 3400: Loss = -11942.196413540607
Iteration 3500: Loss = -11942.176908242152
Iteration 3600: Loss = -11942.158887551632
Iteration 3700: Loss = -11942.142107426895
Iteration 3800: Loss = -11942.126101086333
Iteration 3900: Loss = -11942.110431836703
Iteration 4000: Loss = -11942.093611651066
Iteration 4100: Loss = -11942.071257973324
Iteration 4200: Loss = -11941.990098667262
Iteration 4300: Loss = -11939.891002127735
Iteration 4400: Loss = -11939.853959740145
Iteration 4500: Loss = -11937.745722834716
Iteration 4600: Loss = -11937.468382703937
Iteration 4700: Loss = -11935.946726366747
Iteration 4800: Loss = -11935.938981243376
Iteration 4900: Loss = -11935.932775301739
Iteration 5000: Loss = -11935.92947130797
Iteration 5100: Loss = -11935.921385650616
Iteration 5200: Loss = -11935.915758641851
Iteration 5300: Loss = -11935.921442172827
1
Iteration 5400: Loss = -11935.891984316251
Iteration 5500: Loss = -11935.591327023616
Iteration 5600: Loss = -11935.543471995217
Iteration 5700: Loss = -11935.395636495274
Iteration 5800: Loss = -11935.37851347852
Iteration 5900: Loss = -11935.119218710342
Iteration 6000: Loss = -11935.015316146573
Iteration 6100: Loss = -11934.963576134156
Iteration 6200: Loss = -11934.417417677449
Iteration 6300: Loss = -11934.149423764997
Iteration 6400: Loss = -11933.786454116867
Iteration 6500: Loss = -11929.4765216718
Iteration 6600: Loss = -11922.854556154522
Iteration 6700: Loss = -11915.956961138349
Iteration 6800: Loss = -11915.655453773903
Iteration 6900: Loss = -11896.876583571238
Iteration 7000: Loss = -11893.862089264443
Iteration 7100: Loss = -11883.427438545164
Iteration 7200: Loss = -11865.682574870334
Iteration 7300: Loss = -11865.52515050007
Iteration 7400: Loss = -11862.373415091228
Iteration 7500: Loss = -11862.352298933096
Iteration 7600: Loss = -11862.340803101637
Iteration 7700: Loss = -11862.322308598692
Iteration 7800: Loss = -11859.288264227143
Iteration 7900: Loss = -11856.474709507254
Iteration 8000: Loss = -11856.26017139504
Iteration 8100: Loss = -11855.927756087749
Iteration 8200: Loss = -11854.993602623172
Iteration 8300: Loss = -11854.989426686876
Iteration 8400: Loss = -11854.825800164705
Iteration 8500: Loss = -11854.730124435217
Iteration 8600: Loss = -11854.341845215493
Iteration 8700: Loss = -11854.319878556216
Iteration 8800: Loss = -11854.316137576028
Iteration 8900: Loss = -11854.309996962544
Iteration 9000: Loss = -11853.663372683692
Iteration 9100: Loss = -11850.485439759193
Iteration 9200: Loss = -11850.316957768224
Iteration 9300: Loss = -11850.29822732025
Iteration 9400: Loss = -11850.291781906302
Iteration 9500: Loss = -11850.280047847427
Iteration 9600: Loss = -11850.279204084198
Iteration 9700: Loss = -11850.278548920069
Iteration 9800: Loss = -11850.277702705222
Iteration 9900: Loss = -11845.23809935302
Iteration 10000: Loss = -11844.097619456315
Iteration 10100: Loss = -11844.222651299337
1
Iteration 10200: Loss = -11844.093088946805
Iteration 10300: Loss = -11844.09787881268
1
Iteration 10400: Loss = -11835.321565382495
Iteration 10500: Loss = -11834.997224829342
Iteration 10600: Loss = -11834.958290925537
Iteration 10700: Loss = -11834.945959313842
Iteration 10800: Loss = -11834.942779494757
Iteration 10900: Loss = -11834.946038811126
1
Iteration 11000: Loss = -11834.919618879116
Iteration 11100: Loss = -11834.898996205979
Iteration 11200: Loss = -11827.068742060512
Iteration 11300: Loss = -11827.059113457412
Iteration 11400: Loss = -11827.059956181174
1
Iteration 11500: Loss = -11827.08516188081
2
Iteration 11600: Loss = -11827.056326667034
Iteration 11700: Loss = -11827.05656518096
1
Iteration 11800: Loss = -11827.055849230906
Iteration 11900: Loss = -11827.059608190179
1
Iteration 12000: Loss = -11827.055345619716
Iteration 12100: Loss = -11815.493902691582
Iteration 12200: Loss = -11815.439942198303
Iteration 12300: Loss = -11815.39110985005
Iteration 12400: Loss = -11815.388783602095
Iteration 12500: Loss = -11815.368100135207
Iteration 12600: Loss = -11815.393245237396
1
Iteration 12700: Loss = -11815.362957546407
Iteration 12800: Loss = -11815.362024676198
Iteration 12900: Loss = -11815.36267413704
1
Iteration 13000: Loss = -11815.366723529698
2
Iteration 13100: Loss = -11815.371686936596
3
Iteration 13200: Loss = -11812.032393746716
Iteration 13300: Loss = -11811.986112696737
Iteration 13400: Loss = -11811.986017454747
Iteration 13500: Loss = -11811.992385885973
1
Iteration 13600: Loss = -11811.987493147046
2
Iteration 13700: Loss = -11811.982520474385
Iteration 13800: Loss = -11812.007930138476
1
Iteration 13900: Loss = -11811.982271031926
Iteration 14000: Loss = -11811.980373841901
Iteration 14100: Loss = -11811.977522685605
Iteration 14200: Loss = -11810.840127464435
Iteration 14300: Loss = -11810.855206302991
1
Iteration 14400: Loss = -11810.879652379072
2
Iteration 14500: Loss = -11810.838842753064
Iteration 14600: Loss = -11807.22506767067
Iteration 14700: Loss = -11807.117468166873
Iteration 14800: Loss = -11807.101474669455
Iteration 14900: Loss = -11800.62588248376
Iteration 15000: Loss = -11800.04711973994
Iteration 15100: Loss = -11800.050679151633
1
Iteration 15200: Loss = -11800.04059076336
Iteration 15300: Loss = -11800.047286504574
1
Iteration 15400: Loss = -11800.039029215211
Iteration 15500: Loss = -11800.04218720545
1
Iteration 15600: Loss = -11800.038990297626
Iteration 15700: Loss = -11800.03801022708
Iteration 15800: Loss = -11800.055669577543
1
Iteration 15900: Loss = -11800.037031536993
Iteration 16000: Loss = -11800.042235230381
1
Iteration 16100: Loss = -11800.047033271434
2
Iteration 16200: Loss = -11797.447133684345
Iteration 16300: Loss = -11797.285787392135
Iteration 16400: Loss = -11787.859805827644
Iteration 16500: Loss = -11787.291466208902
Iteration 16600: Loss = -11787.27367040145
Iteration 16700: Loss = -11787.27217299045
Iteration 16800: Loss = -11787.340834059774
1
Iteration 16900: Loss = -11787.299285995272
2
Iteration 17000: Loss = -11787.27530254084
3
Iteration 17100: Loss = -11787.314908706643
4
Iteration 17200: Loss = -11787.271435013161
Iteration 17300: Loss = -11787.27234985896
1
Iteration 17400: Loss = -11787.31860816911
2
Iteration 17500: Loss = -11787.285417655621
3
Iteration 17600: Loss = -11787.271915516243
4
Iteration 17700: Loss = -11787.273017795456
5
Iteration 17800: Loss = -11787.291062891409
6
Iteration 17900: Loss = -11787.43496842562
7
Iteration 18000: Loss = -11787.273383872569
8
Iteration 18100: Loss = -11787.272977967112
9
Iteration 18200: Loss = -11787.289288136148
10
Stopping early at iteration 18200 due to no improvement.
tensor([[  2.4147,  -3.9491],
        [ -3.7352,   2.3346],
        [  7.2410,  -8.6499],
        [  7.8871,  -9.4308],
        [ -9.4996,   7.0150],
        [  5.0215,  -7.0661],
        [-10.1269,   7.4079],
        [  7.7005,  -9.5451],
        [ -1.8845,   0.4960],
        [  7.6203,  -9.0294],
        [ -5.7420,   4.3517],
        [ -9.1858,   7.2905],
        [  7.9453,  -9.3884],
        [ -4.8906,   2.3160],
        [  5.7947,  -8.2478],
        [ -8.0246,   4.2625],
        [  7.3762,  -9.8469],
        [  7.4482,  -8.8653],
        [ -3.1088,   1.7192],
        [  7.1516,  -8.5639],
        [ -9.2844,   6.7291],
        [  4.6024,  -6.0648],
        [  7.5301,  -9.0076],
        [  7.8968,  -9.2897],
        [  7.5128,  -9.0399],
        [ -3.4163,   1.9604],
        [ -4.1149,   2.6725],
        [ -8.8276,   7.3384],
        [ -9.2732,   7.6596],
        [  7.5685, -10.5509],
        [ -5.1430,   3.3517],
        [ -5.6405,   2.0706],
        [ -2.8439,   1.4026],
        [ -8.3541,   6.9607],
        [ -9.1442,   7.7018],
        [  7.6518,  -9.1754],
        [ -3.1870,   0.4543],
        [  6.9298,  -8.3246],
        [  7.4124, -10.3499],
        [  7.2656,  -8.6605],
        [ -8.2026,   6.7835],
        [ -7.2940,   5.6794],
        [ -8.8839,   6.7314],
        [ -5.8954,   4.1966],
        [  8.0213,  -9.6261],
        [  6.3710,  -9.9173],
        [ -4.7419,   3.3303],
        [ -9.1286,   7.2146],
        [ -2.3814,   0.6102],
        [  6.7875,  -8.6059],
        [  7.6465,  -9.0587],
        [  6.7714,  -8.1577],
        [ -3.6924,   1.3264],
        [ -7.6941,   6.2900],
        [ -8.4838,   6.0724],
        [  5.1484,  -8.3065],
        [ -6.8650,   5.4337],
        [ -7.8598,   6.4473],
        [  5.4067,  -6.8575],
        [  7.8994,  -9.4235],
        [  7.2403,  -8.8725],
        [  5.9568,  -7.7948],
        [  5.7934,  -7.2119],
        [ -4.9151,   3.1492],
        [ -5.3674,   3.9769],
        [  7.4924,  -9.6628],
        [  6.9644,  -8.3510],
        [  3.9103,  -8.0008],
        [ -7.1872,   5.7881],
        [  7.5257,  -9.1086],
        [  7.3247,  -8.7659],
        [ -6.5093,   4.9981],
        [  7.1133,  -9.6968],
        [  8.1406,  -9.8824],
        [ -4.6579,   3.0129],
        [ -7.1766,   5.6208],
        [ -7.5208,   5.8419],
        [ -9.5329,   7.8268],
        [ -9.0068,   7.5562],
        [ -2.6239,   1.2374],
        [ -8.9440,   6.9012],
        [ -2.3591,   0.7358],
        [  7.3689,  -8.8034],
        [ -8.9148,   5.5341],
        [ -9.5377,   7.2363],
        [  7.6326,  -9.3916],
        [  7.3295,  -8.8943],
        [ -8.1764,   6.2101],
        [ -5.0995,   1.7594],
        [  6.2354,  -9.2609],
        [  6.6325,  -8.8037],
        [  7.7830,  -9.8394],
        [ -5.9346,   4.4571],
        [  5.2052,  -6.7591],
        [  8.3647, -10.1956],
        [ -8.4901,   7.0809],
        [  7.8492,  -9.2374],
        [ -8.4674,   6.8124],
        [ -8.7757,   5.8794],
        [ -7.9346,   3.3194]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1586, 0.8414],
        [0.6789, 0.3211]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4725, 0.5275], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3475, 0.0956],
         [0.4397, 0.2619]],

        [[0.3926, 0.0986],
         [0.3358, 0.0154]],

        [[0.4743, 0.1006],
         [0.2380, 0.2135]],

        [[0.6087, 0.0931],
         [0.4827, 0.4926]],

        [[0.9501, 0.1050],
         [0.2206, 0.9408]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 92
Adjusted Rand Index: 0.7027008103753108
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 89
Adjusted Rand Index: 0.6046287815855185
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.07213943288776951
Average Adjusted Rand Index: 0.8534651347218951
Iteration 0: Loss = -21382.55389408647
Iteration 10: Loss = -11567.547559844572
Iteration 20: Loss = -11567.549191288015
1
Iteration 30: Loss = -11567.549186938504
2
Iteration 40: Loss = -11567.549186938504
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7871, 0.2129],
        [0.2321, 0.7679]], dtype=torch.float64)
alpha: tensor([0.5035, 0.4965])
beta: tensor([[[0.3867, 0.0957],
         [0.3308, 0.1969]],

        [[0.3971, 0.1004],
         [0.6686, 0.4497]],

        [[0.3917, 0.1028],
         [0.2248, 0.2315]],

        [[0.0040, 0.0995],
         [0.3231, 0.9525]],

        [[0.5266, 0.1062],
         [0.4298, 0.9736]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21382.321293321125
Iteration 100: Loss = -12331.279075063452
Iteration 200: Loss = -12248.670034186898
Iteration 300: Loss = -12200.8239682825
Iteration 400: Loss = -11840.03283355084
Iteration 500: Loss = -11672.254633072791
Iteration 600: Loss = -11626.497155132101
Iteration 700: Loss = -11625.849384239613
Iteration 800: Loss = -11625.52472819384
Iteration 900: Loss = -11625.15777238231
Iteration 1000: Loss = -11611.53763663895
Iteration 1100: Loss = -11599.479600335171
Iteration 1200: Loss = -11599.39823524309
Iteration 1300: Loss = -11599.337867646678
Iteration 1400: Loss = -11599.282910546479
Iteration 1500: Loss = -11598.475564748469
Iteration 1600: Loss = -11583.744925203817
Iteration 1700: Loss = -11583.715864787844
Iteration 1800: Loss = -11583.692275276182
Iteration 1900: Loss = -11583.672703841748
Iteration 2000: Loss = -11583.65613138412
Iteration 2100: Loss = -11583.641959766606
Iteration 2200: Loss = -11583.629676203347
Iteration 2300: Loss = -11583.618951797747
Iteration 2400: Loss = -11583.609520727598
Iteration 2500: Loss = -11583.601193796645
Iteration 2600: Loss = -11583.59379631095
Iteration 2700: Loss = -11583.587215002333
Iteration 2800: Loss = -11583.58125712143
Iteration 2900: Loss = -11583.575942944472
Iteration 3000: Loss = -11583.571117897365
Iteration 3100: Loss = -11583.56674488524
Iteration 3200: Loss = -11583.562801313608
Iteration 3300: Loss = -11583.559164023758
Iteration 3400: Loss = -11583.555830022216
Iteration 3500: Loss = -11583.552808951332
Iteration 3600: Loss = -11583.55000040199
Iteration 3700: Loss = -11583.547450046222
Iteration 3800: Loss = -11583.545117896194
Iteration 3900: Loss = -11583.542920841142
Iteration 4000: Loss = -11583.540933835118
Iteration 4100: Loss = -11583.539058603783
Iteration 4200: Loss = -11583.537342796035
Iteration 4300: Loss = -11583.535767608926
Iteration 4400: Loss = -11583.534257947213
Iteration 4500: Loss = -11583.53285866251
Iteration 4600: Loss = -11583.531560222546
Iteration 4700: Loss = -11583.538096111135
1
Iteration 4800: Loss = -11583.529258590992
Iteration 4900: Loss = -11583.5282465701
Iteration 5000: Loss = -11583.527216584338
Iteration 5100: Loss = -11583.52632320343
Iteration 5200: Loss = -11583.525526674799
Iteration 5300: Loss = -11583.524680306731
Iteration 5400: Loss = -11583.523921171083
Iteration 5500: Loss = -11583.52317669539
Iteration 5600: Loss = -11583.522567162578
Iteration 5700: Loss = -11583.521909925541
Iteration 5800: Loss = -11583.521365849147
Iteration 5900: Loss = -11583.568066733395
1
Iteration 6000: Loss = -11583.520280690684
Iteration 6100: Loss = -11583.519797895486
Iteration 6200: Loss = -11583.522624902233
1
Iteration 6300: Loss = -11583.518892667733
Iteration 6400: Loss = -11583.52041595821
1
Iteration 6500: Loss = -11583.518608461582
Iteration 6600: Loss = -11583.517724850322
Iteration 6700: Loss = -11583.517400808663
Iteration 6800: Loss = -11583.521833735273
1
Iteration 6900: Loss = -11583.536448217554
2
Iteration 7000: Loss = -11583.516424085625
Iteration 7100: Loss = -11583.51879988648
1
Iteration 7200: Loss = -11583.515894250928
Iteration 7300: Loss = -11583.622672160853
1
Iteration 7400: Loss = -11583.515409909773
Iteration 7500: Loss = -11583.516335702261
1
Iteration 7600: Loss = -11583.515383515089
Iteration 7700: Loss = -11583.514788570319
Iteration 7800: Loss = -11583.518170299578
1
Iteration 7900: Loss = -11583.523791389196
2
Iteration 8000: Loss = -11583.514146650297
Iteration 8100: Loss = -11583.514139558089
Iteration 8200: Loss = -11583.513494173081
Iteration 8300: Loss = -11573.465786803354
Iteration 8400: Loss = -11573.464760190638
Iteration 8500: Loss = -11573.464635995393
Iteration 8600: Loss = -11573.46470082732
1
Iteration 8700: Loss = -11573.467367351324
2
Iteration 8800: Loss = -11573.46495441929
3
Iteration 8900: Loss = -11573.464172939915
Iteration 9000: Loss = -11573.465021695807
1
Iteration 9100: Loss = -11573.464377796103
2
Iteration 9200: Loss = -11573.530128604147
3
Iteration 9300: Loss = -11573.463935489339
Iteration 9400: Loss = -11573.501042526466
1
Iteration 9500: Loss = -11573.485554409302
2
Iteration 9600: Loss = -11573.467045433314
3
Iteration 9700: Loss = -11573.464956867743
4
Iteration 9800: Loss = -11573.46371231785
Iteration 9900: Loss = -11573.463500746651
Iteration 10000: Loss = -11573.465448701887
1
Iteration 10100: Loss = -11573.465399914672
2
Iteration 10200: Loss = -11573.463221264312
Iteration 10300: Loss = -11573.463195910907
Iteration 10400: Loss = -11573.463959972378
1
Iteration 10500: Loss = -11573.465655345244
2
Iteration 10600: Loss = -11573.463135829737
Iteration 10700: Loss = -11573.490628122561
1
Iteration 10800: Loss = -11573.464991578508
2
Iteration 10900: Loss = -11573.470583017268
3
Iteration 11000: Loss = -11573.467898838684
4
Iteration 11100: Loss = -11573.470227551796
5
Iteration 11200: Loss = -11573.462846720608
Iteration 11300: Loss = -11573.463687634388
1
Iteration 11400: Loss = -11573.46312907829
2
Iteration 11500: Loss = -11573.465599063116
3
Iteration 11600: Loss = -11573.46697170907
4
Iteration 11700: Loss = -11573.480550021048
5
Iteration 11800: Loss = -11573.471636764449
6
Iteration 11900: Loss = -11573.463399641058
7
Iteration 12000: Loss = -11573.463824117556
8
Iteration 12100: Loss = -11573.462601703886
Iteration 12200: Loss = -11573.463681692798
1
Iteration 12300: Loss = -11573.462580583504
Iteration 12400: Loss = -11573.595947758799
1
Iteration 12500: Loss = -11573.462537366593
Iteration 12600: Loss = -11573.493347966602
1
Iteration 12700: Loss = -11573.545682856718
2
Iteration 12800: Loss = -11573.463925325566
3
Iteration 12900: Loss = -11573.462999949632
4
Iteration 13000: Loss = -11573.4634917756
5
Iteration 13100: Loss = -11573.463526318772
6
Iteration 13200: Loss = -11573.467822074133
7
Iteration 13300: Loss = -11573.465606332928
8
Iteration 13400: Loss = -11573.462531073517
Iteration 13500: Loss = -11573.511432356094
1
Iteration 13600: Loss = -11573.464991100927
2
Iteration 13700: Loss = -11573.462602380969
3
Iteration 13800: Loss = -11573.467557254013
4
Iteration 13900: Loss = -11573.465362061715
5
Iteration 14000: Loss = -11573.466176236387
6
Iteration 14100: Loss = -11573.479178647825
7
Iteration 14200: Loss = -11573.556383575022
8
Iteration 14300: Loss = -11573.462501024978
Iteration 14400: Loss = -11573.462411821441
Iteration 14500: Loss = -11573.476634235989
1
Iteration 14600: Loss = -11573.46235608044
Iteration 14700: Loss = -11573.462545376837
1
Iteration 14800: Loss = -11573.563686120866
2
Iteration 14900: Loss = -11573.462315042716
Iteration 15000: Loss = -11573.467520172806
1
Iteration 15100: Loss = -11573.463491892328
2
Iteration 15200: Loss = -11573.462367240327
3
Iteration 15300: Loss = -11573.467588659923
4
Iteration 15400: Loss = -11573.462290826144
Iteration 15500: Loss = -11573.464583302011
1
Iteration 15600: Loss = -11573.462404089007
2
Iteration 15700: Loss = -11573.462795893394
3
Iteration 15800: Loss = -11573.468853619961
4
Iteration 15900: Loss = -11573.463031779484
5
Iteration 16000: Loss = -11573.46240720655
6
Iteration 16100: Loss = -11573.46437715653
7
Iteration 16200: Loss = -11573.487964178797
8
Iteration 16300: Loss = -11573.463661489806
9
Iteration 16400: Loss = -11573.46390183893
10
Stopping early at iteration 16400 due to no improvement.
tensor([[  0.9399,  -3.2390],
        [ -6.4879,   3.4248],
        [  7.1743, -10.3081],
        [  7.7494,  -9.2222],
        [ -7.3160,   5.8205],
        [  3.9847,  -5.5174],
        [-12.9745,   8.3592],
        [  7.4903,  -9.8043],
        [ -4.4680,   2.8272],
        [  7.5790,  -9.8692],
        [ -6.4633,   5.0762],
        [-12.0419,   8.5774],
        [  7.7277,  -9.1725],
        [ -5.5562,   3.9567],
        [  5.7841,  -7.4415],
        [ -7.9112,   5.3006],
        [  7.2178,  -8.6858],
        [  7.6280,  -9.0241],
        [ -9.3136,   7.8018],
        [  7.1584,  -8.7673],
        [ -7.1455,   5.1818],
        [  3.0218,  -5.6110],
        [  4.2919,  -6.2501],
        [  7.4924,  -9.3270],
        [  7.8403,  -9.2373],
        [ -5.6630,   4.0591],
        [ -6.0182,   4.5781],
        [-10.1332,   7.7578],
        [-10.0152,   8.4230],
        [  6.1437, -10.7589],
        [ -6.6372,   5.0362],
        [ -8.8500,   5.3610],
        [ -3.8254,   2.3385],
        [-10.6327,   6.7836],
        [ -9.9255,   8.4833],
        [  7.5521,  -9.7238],
        [ -4.8587,   3.3081],
        [  6.2421,  -7.7941],
        [  7.1195,  -8.5952],
        [  6.8263,  -8.2150],
        [-10.4893,   8.0969],
        [ -8.7182,   7.0856],
        [-10.8898,   8.2529],
        [ -9.8808,   5.3805],
        [  7.8876,  -9.3197],
        [  7.5356,  -9.1233],
        [ -6.0802,   4.6482],
        [ -6.6050,   4.9231],
        [ -3.3406,   1.9543],
        [  6.8665,  -8.8275],
        [  7.2685,  -9.5922],
        [  5.3832,  -6.7975],
        [ -6.1085,   4.3853],
        [ -9.6431,   6.0464],
        [ -7.8206,   6.3888],
        [  4.4951,  -6.8535],
        [ -9.8559,   6.0489],
        [ -5.5398,   4.1211],
        [  4.4397,  -6.0087],
        [  8.0961,  -9.7191],
        [  7.8182,  -9.4366],
        [  2.1233,  -6.7385],
        [  4.4497,  -5.9612],
        [ -8.2418,   6.7054],
        [ -6.4516,   4.8914],
        [  7.5821,  -9.7294],
        [  7.1350,  -8.5237],
        [  4.0650,  -5.4756],
        [ -9.4993,   8.1129],
        [  8.0865,  -9.5601],
        [  7.2327,  -8.6697],
        [ -9.1100,   6.4079],
        [  5.9511,  -7.3375],
        [  7.5250, -10.0028],
        [ -6.4888,   3.3006],
        [ -9.5674,   7.5494],
        [ -4.7444,   3.0749],
        [ -9.4276,   7.5030],
        [ -9.9337,   8.3922],
        [ -4.6815,   3.2558],
        [ -8.6961,   5.4711],
        [ -4.4862,   2.7748],
        [  7.5261,  -9.5832],
        [ -9.6618,   7.8767],
        [-10.8152,   8.8458],
        [  5.3945,  -7.8465],
        [  7.8950, -10.0022],
        [ -6.8755,   5.4823],
        [ -6.3589,   3.4918],
        [  7.5301,  -8.9914],
        [  6.9431,  -8.3313],
        [  8.0697, -10.5908],
        [ -7.1009,   5.3972],
        [  3.5333,  -6.0073],
        [  7.7997,  -9.1975],
        [ -7.9182,   5.2808],
        [  7.8048,  -9.4892],
        [-10.6304,   9.0949],
        [ -8.1170,   5.8298],
        [ -4.1841,   2.7976]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7871, 0.2129],
        [0.2353, 0.7647]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4699, 0.5301], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3915, 0.0956],
         [0.3308, 0.2017]],

        [[0.3971, 0.1000],
         [0.6686, 0.4497]],

        [[0.3917, 0.1023],
         [0.2248, 0.2315]],

        [[0.0040, 0.0990],
         [0.3231, 0.9525]],

        [[0.5266, 0.1063],
         [0.4298, 0.9736]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.9919998119331364
Iteration 0: Loss = -20194.891742716125
Iteration 10: Loss = -12357.491916861276
Iteration 20: Loss = -12160.897921081592
Iteration 30: Loss = -11567.549195033249
Iteration 40: Loss = -11567.549196075957
1
Iteration 50: Loss = -11567.549196075957
2
Iteration 60: Loss = -11567.549196075957
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7679, 0.2321],
        [0.2129, 0.7871]], dtype=torch.float64)
alpha: tensor([0.4965, 0.5035])
beta: tensor([[[0.1969, 0.0957],
         [0.6295, 0.3867]],

        [[0.5709, 0.1004],
         [0.0753, 0.9104]],

        [[0.3427, 0.1028],
         [0.6681, 0.6820]],

        [[0.4152, 0.0995],
         [0.5984, 0.0244]],

        [[0.9567, 0.1062],
         [0.4052, 0.5962]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20195.13082904231
Iteration 100: Loss = -12354.774114554531
Iteration 200: Loss = -12274.782837888191
Iteration 300: Loss = -12016.878186471842
Iteration 400: Loss = -11936.39374455239
Iteration 500: Loss = -11877.975674200716
Iteration 600: Loss = -11856.85193917737
Iteration 700: Loss = -11855.97131157095
Iteration 800: Loss = -11849.17422328153
Iteration 900: Loss = -11829.686447359203
Iteration 1000: Loss = -11822.04491710054
Iteration 1100: Loss = -11820.95953441901
Iteration 1200: Loss = -11803.800135126257
Iteration 1300: Loss = -11801.622796283831
Iteration 1400: Loss = -11800.235511600653
Iteration 1500: Loss = -11799.60196071558
Iteration 1600: Loss = -11794.229201621796
Iteration 1700: Loss = -11794.084368005826
Iteration 1800: Loss = -11772.089402577414
Iteration 1900: Loss = -11763.3664553191
Iteration 2000: Loss = -11762.05143292409
Iteration 2100: Loss = -11752.798923081427
Iteration 2200: Loss = -11752.622176084069
Iteration 2300: Loss = -11752.569003962139
Iteration 2400: Loss = -11734.430183712453
Iteration 2500: Loss = -11734.408489958754
Iteration 2600: Loss = -11734.391636079847
Iteration 2700: Loss = -11734.377748439478
Iteration 2800: Loss = -11734.366038737591
Iteration 2900: Loss = -11734.35586870115
Iteration 3000: Loss = -11734.346912451922
Iteration 3100: Loss = -11734.338902310934
Iteration 3200: Loss = -11734.33167029873
Iteration 3300: Loss = -11734.325038912008
Iteration 3400: Loss = -11734.318697205405
Iteration 3500: Loss = -11734.312363633933
Iteration 3600: Loss = -11734.304839211736
Iteration 3700: Loss = -11734.290241789186
Iteration 3800: Loss = -11734.205417838333
Iteration 3900: Loss = -11734.191161197037
Iteration 4000: Loss = -11734.184892176578
Iteration 4100: Loss = -11734.156967076553
Iteration 4200: Loss = -11723.11056677728
Iteration 4300: Loss = -11723.104080226505
Iteration 4400: Loss = -11723.10063170424
Iteration 4500: Loss = -11723.097527235186
Iteration 4600: Loss = -11723.096205676715
Iteration 4700: Loss = -11723.091027837843
Iteration 4800: Loss = -11723.087878682416
Iteration 4900: Loss = -11723.082924947523
Iteration 5000: Loss = -11723.078813151484
Iteration 5100: Loss = -11723.075789250111
Iteration 5200: Loss = -11723.07422968969
Iteration 5300: Loss = -11723.075423380602
1
Iteration 5400: Loss = -11723.074892041537
2
Iteration 5500: Loss = -11723.072047659509
Iteration 5600: Loss = -11723.069605006358
Iteration 5700: Loss = -11723.068795882717
Iteration 5800: Loss = -11723.069294169582
1
Iteration 5900: Loss = -11723.069397316487
2
Iteration 6000: Loss = -11723.066273253211
Iteration 6100: Loss = -11723.090984396691
1
Iteration 6200: Loss = -11723.064796309676
Iteration 6300: Loss = -11723.065318146464
1
Iteration 6400: Loss = -11723.065379780997
2
Iteration 6500: Loss = -11723.06315701246
Iteration 6600: Loss = -11723.063578483614
1
Iteration 6700: Loss = -11723.066325887181
2
Iteration 6800: Loss = -11723.060422562685
Iteration 6900: Loss = -11723.05922657228
Iteration 7000: Loss = -11723.054013190012
Iteration 7100: Loss = -11723.074291196133
1
Iteration 7200: Loss = -11723.05305309077
Iteration 7300: Loss = -11723.054863033612
1
Iteration 7400: Loss = -11723.052134896856
Iteration 7500: Loss = -11723.052532118723
1
Iteration 7600: Loss = -11723.062115829418
2
Iteration 7700: Loss = -11723.05112685465
Iteration 7800: Loss = -11723.001083417885
Iteration 7900: Loss = -11723.092518169946
1
Iteration 8000: Loss = -11723.005849691914
2
Iteration 8100: Loss = -11723.006794772149
3
Iteration 8200: Loss = -11723.000409760283
Iteration 8300: Loss = -11722.999756042218
Iteration 8400: Loss = -11723.155610441228
1
Iteration 8500: Loss = -11723.000016456046
2
Iteration 8600: Loss = -11723.099434662246
3
Iteration 8700: Loss = -11723.002912046719
4
Iteration 8800: Loss = -11722.998036276467
Iteration 8900: Loss = -11722.998933510778
1
Iteration 9000: Loss = -11723.00668520268
2
Iteration 9100: Loss = -11723.04642672797
3
Iteration 9200: Loss = -11723.128341226293
4
Iteration 9300: Loss = -11722.99699786198
Iteration 9400: Loss = -11722.996842611929
Iteration 9500: Loss = -11723.002004324071
1
Iteration 9600: Loss = -11722.998466297866
2
Iteration 9700: Loss = -11723.00063068607
3
Iteration 9800: Loss = -11723.004061185105
4
Iteration 9900: Loss = -11722.998164627983
5
Iteration 10000: Loss = -11723.00072917272
6
Iteration 10100: Loss = -11723.041566641501
7
Iteration 10200: Loss = -11723.021111596438
8
Iteration 10300: Loss = -11723.005133003839
9
Iteration 10400: Loss = -11723.003919117395
10
Stopping early at iteration 10400 due to no improvement.
tensor([[  5.4576,  -6.8703],
        [ -0.1602,  -1.8867],
        [  6.0781,  -7.9880],
        [  6.5738,  -7.9614],
        [  5.2503,  -7.1395],
        [  1.8172,  -5.6748],
        [  1.4443,  -3.8514],
        [  5.2666,  -7.0099],
        [ -3.1946,   1.7661],
        [  6.2753,  -7.7272],
        [  3.5782,  -4.9912],
        [  2.6729,  -4.6321],
        [  6.2759,  -7.9439],
        [  5.5785,  -6.9689],
        [  5.6830,  -7.0734],
        [ -5.9336,   4.3284],
        [  6.0354,  -8.2928],
        [  6.9692,  -8.3729],
        [ -3.4289,   0.1524],
        [  5.7789,  -7.6584],
        [ -0.1301,  -1.9035],
        [  3.6547,  -5.3085],
        [  4.8315,  -6.5762],
        [  5.5804,  -8.0471],
        [  6.0186,  -7.5568],
        [  2.3987,  -3.8365],
        [  1.0552,  -2.4453],
        [ -3.2629,   0.0186],
        [  1.0349,  -2.7183],
        [  6.0958,  -7.4876],
        [ -3.0055,   1.6052],
        [  1.8508,  -3.4505],
        [  6.3659,  -8.9989],
        [  5.4999,  -7.2156],
        [ -6.3253,   4.9385],
        [  6.1708,  -7.8222],
        [  0.2872,  -1.6823],
        [  3.6748,  -5.0806],
        [  7.0954, -10.0965],
        [  5.7265,  -7.9046],
        [  1.4423,  -3.4176],
        [  2.0376,  -3.5043],
        [  0.1555,  -2.3826],
        [  4.1492,  -5.6399],
        [  6.1271,  -7.5206],
        [  4.5566,  -6.4179],
        [ -1.4534,  -0.4573],
        [ -1.6655,  -0.0241],
        [  2.5436,  -5.4895],
        [  6.2439,  -9.5675],
        [  6.7326,  -8.3062],
        [  4.6387,  -6.0263],
        [  0.5030,  -1.9811],
        [  4.9156,  -6.3316],
        [  1.7952,  -3.1826],
        [  4.1589,  -5.5551],
        [  2.8873,  -4.4628],
        [  3.2169,  -5.6481],
        [  5.9341,  -7.8410],
        [  4.9559,  -8.6832],
        [  6.6308,  -8.7251],
        [  5.5835,  -6.9886],
        [  4.9669,  -7.0781],
        [ -2.8535,   1.1252],
        [  3.7711,  -7.7303],
        [  6.8176,  -9.1747],
        [  5.4201,  -7.3014],
        [  3.0138,  -7.6290],
        [  1.4932,  -3.2904],
        [  6.1069,  -8.4107],
        [  5.9637,  -9.7588],
        [ -4.0562,   1.4231],
        [  4.8959,  -6.5800],
        [  5.4515,  -7.1669],
        [  1.6021,  -3.0750],
        [  2.4977,  -4.1041],
        [  1.8362,  -3.2816],
        [ -0.3749,  -1.6274],
        [  1.0265,  -2.6320],
        [ -0.0269,  -2.0481],
        [  2.9539,  -4.8498],
        [  1.6025,  -3.1289],
        [  5.9084,  -7.5947],
        [  4.7201,  -6.2016],
        [  0.3110,  -2.9578],
        [  5.4239,  -7.5947],
        [  6.9013,  -9.1634],
        [  2.1544,  -4.4810],
        [  6.2814,  -9.0434],
        [  5.8302, -10.4454],
        [  5.8788,  -7.6740],
        [  6.4785,  -7.9037],
        [  5.9307,  -8.4471],
        [  3.9106,  -6.9263],
        [  6.0547,  -9.3041],
        [  4.3151,  -5.7511],
        [  6.7372,  -8.8355],
        [ -2.4658,   1.0766],
        [  4.8764,  -6.4720],
        [  1.9130,  -3.9882]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6545, 0.3455],
        [0.2546, 0.7454]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8846, 0.1154], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2072, 0.0874],
         [0.6295, 0.3951]],

        [[0.5709, 0.1030],
         [0.0753, 0.9104]],

        [[0.3427, 0.1034],
         [0.6681, 0.6820]],

        [[0.4152, 0.0990],
         [0.5984, 0.0244]],

        [[0.9567, 0.1062],
         [0.4052, 0.5962]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.019588552743846268
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5767673876281849
Average Adjusted Rand Index: 0.7879166134396223
Iteration 0: Loss = -17706.41018930952
Iteration 10: Loss = -11572.872226273266
Iteration 20: Loss = -11567.549197140737
Iteration 30: Loss = -11567.549191516728
Iteration 40: Loss = -11567.549191516728
1
Iteration 50: Loss = -11567.549191516728
2
Iteration 60: Loss = -11567.549191516728
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7679, 0.2321],
        [0.2129, 0.7871]], dtype=torch.float64)
alpha: tensor([0.4965, 0.5035])
beta: tensor([[[0.1969, 0.0957],
         [0.5582, 0.3867]],

        [[0.6976, 0.1004],
         [0.0148, 0.8257]],

        [[0.0518, 0.1028],
         [0.4268, 0.7259]],

        [[0.2081, 0.0995],
         [0.9653, 0.5090]],

        [[0.2328, 0.1062],
         [0.2053, 0.8869]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17706.22368636168
Iteration 100: Loss = -12285.467307636207
Iteration 200: Loss = -11761.362204791425
Iteration 300: Loss = -11721.007380254148
Iteration 400: Loss = -11659.10882302921
Iteration 500: Loss = -11622.38464078209
Iteration 600: Loss = -11603.215765170591
Iteration 700: Loss = -11587.079859503838
Iteration 800: Loss = -11566.323471245334
Iteration 900: Loss = -11566.14775871911
Iteration 1000: Loss = -11565.967326489716
Iteration 1100: Loss = -11565.856384539515
Iteration 1200: Loss = -11565.800409777288
Iteration 1300: Loss = -11565.763432768437
Iteration 1400: Loss = -11565.734262349895
Iteration 1500: Loss = -11565.710872340083
Iteration 1600: Loss = -11565.69190227628
Iteration 1700: Loss = -11565.676166275438
Iteration 1800: Loss = -11565.6629382667
Iteration 1900: Loss = -11565.651598299097
Iteration 2000: Loss = -11565.641898486783
Iteration 2100: Loss = -11565.668822317355
1
Iteration 2200: Loss = -11565.625995654264
Iteration 2300: Loss = -11565.61956577969
Iteration 2400: Loss = -11565.613929890133
Iteration 2500: Loss = -11565.609013128427
Iteration 2600: Loss = -11565.604624824015
Iteration 2700: Loss = -11565.600748192404
Iteration 2800: Loss = -11565.598137890616
Iteration 2900: Loss = -11565.59406461975
Iteration 3000: Loss = -11565.591255116427
Iteration 3100: Loss = -11565.589771063223
Iteration 3200: Loss = -11565.586365139878
Iteration 3300: Loss = -11565.605721618158
1
Iteration 3400: Loss = -11565.5823116906
Iteration 3500: Loss = -11565.580467818882
Iteration 3600: Loss = -11565.579124047874
Iteration 3700: Loss = -11565.578991738463
Iteration 3800: Loss = -11565.606281876848
1
Iteration 3900: Loss = -11565.57947618713
2
Iteration 4000: Loss = -11565.574214324053
Iteration 4100: Loss = -11565.573275771612
Iteration 4200: Loss = -11565.572557902373
Iteration 4300: Loss = -11565.573722243382
1
Iteration 4400: Loss = -11565.569858980214
Iteration 4500: Loss = -11565.570017846449
1
Iteration 4600: Loss = -11565.569406973966
Iteration 4700: Loss = -11565.568451144498
Iteration 4800: Loss = -11565.568222627435
Iteration 4900: Loss = -11565.56668318199
Iteration 5000: Loss = -11565.566216478494
Iteration 5100: Loss = -11565.568032263443
1
Iteration 5200: Loss = -11565.6153167224
2
Iteration 5300: Loss = -11565.56454813746
Iteration 5400: Loss = -11565.563824845522
Iteration 5500: Loss = -11565.563550770912
Iteration 5600: Loss = -11565.57018116461
1
Iteration 5700: Loss = -11565.562877773747
Iteration 5800: Loss = -11565.5623864306
Iteration 5900: Loss = -11565.562744272389
1
Iteration 6000: Loss = -11565.586483344228
2
Iteration 6100: Loss = -11565.562317131653
Iteration 6200: Loss = -11565.614368216558
1
Iteration 6300: Loss = -11565.560932798759
Iteration 6400: Loss = -11565.560775050824
Iteration 6500: Loss = -11565.576877419506
1
Iteration 6600: Loss = -11565.575923954993
2
Iteration 6700: Loss = -11565.597516413669
3
Iteration 6800: Loss = -11565.566449263042
4
Iteration 6900: Loss = -11565.559725770237
Iteration 7000: Loss = -11565.561405424709
1
Iteration 7100: Loss = -11565.559398160889
Iteration 7200: Loss = -11565.559480837415
1
Iteration 7300: Loss = -11565.559280697229
Iteration 7400: Loss = -11565.559060520372
Iteration 7500: Loss = -11565.558971694652
Iteration 7600: Loss = -11565.558902375426
Iteration 7700: Loss = -11565.558713264145
Iteration 7800: Loss = -11565.558687909053
Iteration 7900: Loss = -11565.562018476892
1
Iteration 8000: Loss = -11565.661914860208
2
Iteration 8100: Loss = -11565.558348724124
Iteration 8200: Loss = -11565.558771309561
1
Iteration 8300: Loss = -11565.578325148781
2
Iteration 8400: Loss = -11565.558104600426
Iteration 8500: Loss = -11565.558746180623
1
Iteration 8600: Loss = -11565.558695264102
2
Iteration 8700: Loss = -11565.626630810279
3
Iteration 8800: Loss = -11565.563284515583
4
Iteration 8900: Loss = -11565.557834378167
Iteration 9000: Loss = -11565.558815259366
1
Iteration 9100: Loss = -11565.55775255426
Iteration 9200: Loss = -11565.557815099042
1
Iteration 9300: Loss = -11565.559853042783
2
Iteration 9400: Loss = -11565.557859711118
3
Iteration 9500: Loss = -11565.557550035168
Iteration 9600: Loss = -11565.560568042638
1
Iteration 9700: Loss = -11565.557491626578
Iteration 9800: Loss = -11565.63092169368
1
Iteration 9900: Loss = -11565.557455732524
Iteration 10000: Loss = -11565.557550717302
1
Iteration 10100: Loss = -11565.55745282849
Iteration 10200: Loss = -11565.568496288985
1
Iteration 10300: Loss = -11565.700569725075
2
Iteration 10400: Loss = -11565.557660538218
3
Iteration 10500: Loss = -11565.592539401345
4
Iteration 10600: Loss = -11565.557278583357
Iteration 10700: Loss = -11565.654447768897
1
Iteration 10800: Loss = -11565.557241171899
Iteration 10900: Loss = -11565.559669403397
1
Iteration 11000: Loss = -11565.558451513807
2
Iteration 11100: Loss = -11565.562300452937
3
Iteration 11200: Loss = -11565.557903155974
4
Iteration 11300: Loss = -11565.557756880282
5
Iteration 11400: Loss = -11565.56006499442
6
Iteration 11500: Loss = -11565.571876645468
7
Iteration 11600: Loss = -11565.557229509575
Iteration 11700: Loss = -11565.559651563193
1
Iteration 11800: Loss = -11565.55846448224
2
Iteration 11900: Loss = -11565.557215979912
Iteration 12000: Loss = -11565.685390171133
1
Iteration 12100: Loss = -11565.557170763124
Iteration 12200: Loss = -11565.601759370162
1
Iteration 12300: Loss = -11565.558543060575
2
Iteration 12400: Loss = -11565.558676193928
3
Iteration 12500: Loss = -11565.56268527658
4
Iteration 12600: Loss = -11565.557041582786
Iteration 12700: Loss = -11565.557294008435
1
Iteration 12800: Loss = -11565.557618620436
2
Iteration 12900: Loss = -11565.558495871639
3
Iteration 13000: Loss = -11565.565668692403
4
Iteration 13100: Loss = -11565.562575224174
5
Iteration 13200: Loss = -11565.557750758162
6
Iteration 13300: Loss = -11565.558433529472
7
Iteration 13400: Loss = -11565.586619466467
8
Iteration 13500: Loss = -11565.562164111301
9
Iteration 13600: Loss = -11565.562358949841
10
Stopping early at iteration 13600 due to no improvement.
tensor([[ -2.8490,   1.2982],
        [  4.2125,  -5.8195],
        [ -8.4101,   7.0042],
        [ -9.6537,   7.0166],
        [  5.3598,  -7.8908],
        [ -5.4508,   4.0238],
        [  6.3717,  -8.1605],
        [ -8.4816,   7.0569],
        [  1.4036,  -6.0188],
        [ -8.8862,   7.4836],
        [  4.8832,  -6.7273],
        [  6.0917,  -8.7088],
        [ -8.3738,   6.9671],
        [  3.7747,  -5.8420],
        [ -7.5582,   5.6819],
        [  5.7265,  -7.3467],
        [ -8.4804,   6.9391],
        [ -8.7166,   7.0018],
        [  3.2207,  -6.7508],
        [ -8.3311,   6.9241],
        [  5.5059,  -6.9985],
        [ -5.3327,   3.2772],
        [ -6.0475,   4.3969],
        [ -8.8330,   7.2835],
        [-10.7786,   6.3150],
        [  4.1483,  -5.7457],
        [  4.6657,  -6.0987],
        [  6.0440,  -8.6568],
        [  7.0199,  -8.4099],
        [ -9.2691,   5.4602],
        [  5.1900,  -6.5779],
        [  6.0971,  -7.8850],
        [  1.5054,  -4.7227],
        [  6.9058,  -8.3573],
        [  5.4602, -10.0754],
        [ -8.7748,   7.3644],
        [  3.4129,  -4.8807],
        [ -7.4589,   6.0473],
        [-10.6709,   6.6746],
        [ -7.8918,   6.3961],
        [  5.1644,  -6.5509],
        [  6.5435,  -8.0324],
        [  6.5774,  -7.9844],
        [  5.9306,  -7.3177],
        [-10.0015,   7.3349],
        [ -8.2836,   6.8973],
        [  3.4631,  -7.3562],
        [  5.1099,  -6.5872],
        [  1.9731,  -3.3886],
        [ -8.6266,   5.7570],
        [ -8.8735,   7.1040],
        [ -6.9626,   5.1364],
        [  4.5575,  -6.0699],
        [  7.0586,  -8.4756],
        [  5.8748,  -8.2701],
        [ -6.4826,   4.8541],
        [  6.2246,  -9.5609],
        [  4.2051,  -5.6129],
        [ -5.9473,   4.4974],
        [ -9.6653,   7.3575],
        [ -8.3492,   6.9425],
        [ -5.1385,   3.6502],
        [ -5.9680,   4.4161],
        [  4.9297,  -9.5449],
        [  5.0395,  -6.4336],
        [ -9.5599,   7.4130],
        [ -8.2080,   6.5397],
        [ -5.5419,   3.9737],
        [  5.8545,  -7.2737],
        [ -9.4196,   7.4099],
        [ -8.9331,   7.0677],
        [  6.1364,  -7.7802],
        [ -7.1996,   5.8132],
        [ -8.7364,   7.0741],
        [  4.2168,  -5.6586],
        [  5.3937,  -9.7412],
        [  2.4983,  -5.4493],
        [  6.7536,  -9.3025],
        [  6.5028,  -8.3757],
        [  3.3579,  -4.7442],
        [  5.5656,  -7.9643],
        [  1.8387,  -5.5346],
        [ -8.8294,   6.6758],
        [  4.9006,  -9.3323],
        [  7.1068, -10.7377],
        [ -7.4498,   5.7071],
        [ -8.6649,   7.1432],
        [  5.5136,  -6.9791],
        [  3.6508,  -6.3023],
        [ -8.6307,   6.6040],
        [ -8.9844,   6.2821],
        [-10.4712,   7.5421],
        [  5.5669,  -6.9532],
        [ -7.0658,   2.4506],
        [ -9.6532,   7.4665],
        [  5.8780,  -7.2735],
        [ -9.4936,   7.7575],
        [  6.7446,  -8.3125],
        [  6.2730,  -7.6870],
        [  2.7996,  -4.3309]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7705, 0.2295],
        [0.2094, 0.7906]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5335, 0.4665], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.0956],
         [0.5582, 0.3941]],

        [[0.6976, 0.1005],
         [0.0148, 0.8257]],

        [[0.0518, 0.1028],
         [0.4268, 0.7259]],

        [[0.2081, 0.0992],
         [0.9653, 0.5090]],

        [[0.2328, 0.1063],
         [0.2053, 0.8869]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11569.961933338333
new:  [0.07213943288776951, 0.9919999740011123, 0.5767673876281849, 1.0] [0.8534651347218951, 0.9919998119331364, 0.7879166134396223, 1.0] [11787.289288136148, 11573.46390183893, 11723.003919117395, 11565.562358949841]
prior:  [0.0, 0.9919999711388391, 0.9919999711388391, 0.9919999711388391] [0.0, 0.9919992163297293, 0.9919992163297293, 0.9919992163297293] [nan, 11567.549186938504, 11567.549196075957, 11567.549191516728]
-----------------------------------------------------------------------------------------
This iteration is 16
True Objective function: Loss = -11493.69508548985
Iteration 0: Loss = -18792.915450457844
Iteration 10: Loss = -12172.414938078158
Iteration 20: Loss = -11959.429345248447
Iteration 30: Loss = -11874.115288369547
Iteration 40: Loss = -11868.596205999856
Iteration 50: Loss = -11868.58655905707
Iteration 60: Loss = -11875.873468145328
1
Iteration 70: Loss = -11887.160408136293
2
Iteration 80: Loss = -11893.1069009156
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.1847, 0.8153],
        [0.3500, 0.6500]], dtype=torch.float64)
alpha: tensor([0.3380, 0.6620])
beta: tensor([[[0.3541, 0.1088],
         [0.4801, 0.2286]],

        [[0.1330, 0.1056],
         [0.7745, 0.0699]],

        [[0.3120, 0.1026],
         [0.5666, 0.8874]],

        [[0.7683, 0.1026],
         [0.2545, 0.9339]],

        [[0.5322, 0.1029],
         [0.1875, 0.8632]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.02673385152382511
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 78
Adjusted Rand Index: 0.30755685986793835
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 72
Adjusted Rand Index: 0.1867290930650465
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.02089387478645983
Average Adjusted Rand Index: 0.48836508836049236
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18823.11155805098
Iteration 100: Loss = -12203.742193433112
Iteration 200: Loss = -11849.122985706545
Iteration 300: Loss = -11492.623953117938
Iteration 400: Loss = -11489.449216371651
Iteration 500: Loss = -11489.05656908782
Iteration 600: Loss = -11488.867527625187
Iteration 700: Loss = -11488.757507755807
Iteration 800: Loss = -11488.685022523496
Iteration 900: Loss = -11488.637145244193
Iteration 1000: Loss = -11488.602343920227
Iteration 1100: Loss = -11488.579032477743
Iteration 1200: Loss = -11488.555632901613
Iteration 1300: Loss = -11488.539332971923
Iteration 1400: Loss = -11488.532704518306
Iteration 1500: Loss = -11488.515230867582
Iteration 1600: Loss = -11488.506160660545
Iteration 1700: Loss = -11488.49849157343
Iteration 1800: Loss = -11488.492482927772
Iteration 1900: Loss = -11488.486356826543
Iteration 2000: Loss = -11488.481507164819
Iteration 2100: Loss = -11488.493377865983
1
Iteration 2200: Loss = -11488.4734900032
Iteration 2300: Loss = -11488.470191609613
Iteration 2400: Loss = -11488.467304386968
Iteration 2500: Loss = -11488.46470718913
Iteration 2600: Loss = -11488.462654716213
Iteration 2700: Loss = -11488.460278867926
Iteration 2800: Loss = -11488.45849531348
Iteration 2900: Loss = -11488.456696200217
Iteration 3000: Loss = -11488.455174782854
Iteration 3100: Loss = -11488.453833104113
Iteration 3200: Loss = -11488.45248886485
Iteration 3300: Loss = -11488.452036132236
Iteration 3400: Loss = -11488.450299949607
Iteration 3500: Loss = -11488.450307351413
1
Iteration 3600: Loss = -11488.448401720618
Iteration 3700: Loss = -11488.44757514883
Iteration 3800: Loss = -11488.446903555268
Iteration 3900: Loss = -11488.446103233424
Iteration 4000: Loss = -11488.445473063819
Iteration 4100: Loss = -11488.444896644753
Iteration 4200: Loss = -11488.444275755784
Iteration 4300: Loss = -11488.44705724358
1
Iteration 4400: Loss = -11488.443313288552
Iteration 4500: Loss = -11488.44331938976
1
Iteration 4600: Loss = -11488.443780417767
2
Iteration 4700: Loss = -11488.455323244607
3
Iteration 4800: Loss = -11488.44241344539
Iteration 4900: Loss = -11488.442913540888
1
Iteration 5000: Loss = -11488.441144301558
Iteration 5100: Loss = -11488.492939611362
1
Iteration 5200: Loss = -11488.442831719314
2
Iteration 5300: Loss = -11488.440303689704
Iteration 5400: Loss = -11488.441021013829
1
Iteration 5500: Loss = -11488.439815719465
Iteration 5600: Loss = -11488.466404801384
1
Iteration 5700: Loss = -11488.439811828926
Iteration 5800: Loss = -11488.439256344001
Iteration 5900: Loss = -11488.456384807467
1
Iteration 6000: Loss = -11488.439680726395
2
Iteration 6100: Loss = -11488.439042233811
Iteration 6200: Loss = -11488.438890120482
Iteration 6300: Loss = -11488.440712977846
1
Iteration 6400: Loss = -11488.438964460154
2
Iteration 6500: Loss = -11488.438370738451
Iteration 6600: Loss = -11488.508043999314
1
Iteration 6700: Loss = -11488.43882907683
2
Iteration 6800: Loss = -11488.438206483072
Iteration 6900: Loss = -11488.439799420197
1
Iteration 7000: Loss = -11488.445096983733
2
Iteration 7100: Loss = -11488.470011119065
3
Iteration 7200: Loss = -11488.445679771949
4
Iteration 7300: Loss = -11488.443610450444
5
Iteration 7400: Loss = -11488.437971344187
Iteration 7500: Loss = -11488.445138810212
1
Iteration 7600: Loss = -11488.440102053795
2
Iteration 7700: Loss = -11488.44569914751
3
Iteration 7800: Loss = -11488.437449803278
Iteration 7900: Loss = -11488.437262669671
Iteration 8000: Loss = -11488.442928482002
1
Iteration 8100: Loss = -11488.443553968837
2
Iteration 8200: Loss = -11488.445844022608
3
Iteration 8300: Loss = -11488.440264598587
4
Iteration 8400: Loss = -11488.471341044338
5
Iteration 8500: Loss = -11488.437489136375
6
Iteration 8600: Loss = -11488.437229484809
Iteration 8700: Loss = -11488.43721239616
Iteration 8800: Loss = -11488.442218851234
1
Iteration 8900: Loss = -11488.440615168967
2
Iteration 9000: Loss = -11488.440641132305
3
Iteration 9100: Loss = -11488.454650131656
4
Iteration 9200: Loss = -11488.537412381744
5
Iteration 9300: Loss = -11488.437366653792
6
Iteration 9400: Loss = -11488.437733582816
7
Iteration 9500: Loss = -11488.437131319322
Iteration 9600: Loss = -11488.43692558739
Iteration 9700: Loss = -11488.437626364348
1
Iteration 9800: Loss = -11488.484575587263
2
Iteration 9900: Loss = -11488.438139455815
3
Iteration 10000: Loss = -11488.509853634036
4
Iteration 10100: Loss = -11488.437498080506
5
Iteration 10200: Loss = -11488.445329153295
6
Iteration 10300: Loss = -11488.437477039126
7
Iteration 10400: Loss = -11488.43969242213
8
Iteration 10500: Loss = -11488.4368727314
Iteration 10600: Loss = -11488.437195546518
1
Iteration 10700: Loss = -11488.449770976187
2
Iteration 10800: Loss = -11488.439408127453
3
Iteration 10900: Loss = -11488.437180225661
4
Iteration 11000: Loss = -11488.43711511234
5
Iteration 11100: Loss = -11488.440416745494
6
Iteration 11200: Loss = -11488.43873781862
7
Iteration 11300: Loss = -11488.436560440521
Iteration 11400: Loss = -11488.440142709824
1
Iteration 11500: Loss = -11488.43674953965
2
Iteration 11600: Loss = -11488.43696439495
3
Iteration 11700: Loss = -11488.439294116099
4
Iteration 11800: Loss = -11488.441141384714
5
Iteration 11900: Loss = -11488.441921515074
6
Iteration 12000: Loss = -11488.439279387365
7
Iteration 12100: Loss = -11488.44335842175
8
Iteration 12200: Loss = -11488.436604765528
9
Iteration 12300: Loss = -11488.438226750455
10
Stopping early at iteration 12300 due to no improvement.
tensor([[ -9.2649,   4.6497],
        [  4.7137,  -9.3289],
        [ -7.3172,   2.7020],
        [  4.9367,  -9.5519],
        [-10.5808,   5.9656],
        [  4.3216,  -8.9368],
        [ -6.2018,   1.5865],
        [  5.1297,  -9.7449],
        [  4.2774,  -8.8926],
        [  5.0625,  -9.6777],
        [  3.8763,  -8.4915],
        [-11.1296,   6.5144],
        [ -9.1584,   4.5432],
        [-10.3092,   5.6940],
        [ -7.1938,   2.5786],
        [ -7.8018,   3.1866],
        [  3.7736,  -8.3888],
        [ -9.9121,   5.2968],
        [  5.4924, -10.1076],
        [  3.1688,  -7.7840],
        [  0.8378,  -5.4530],
        [-10.2560,   5.6407],
        [  3.7798,  -8.3950],
        [ -9.5924,   4.9772],
        [ -8.8986,   4.2834],
        [-10.3255,   5.7103],
        [  2.0820,  -6.6972],
        [ -8.5410,   3.9258],
        [-11.1442,   6.5290],
        [ -9.3517,   4.7364],
        [ -6.4397,   1.8245],
        [  4.2845,  -8.8997],
        [-10.0268,   5.4116],
        [-10.8623,   6.2471],
        [  5.7124, -10.3276],
        [  4.0842,  -8.6994],
        [  1.8532,  -6.4684],
        [  3.4266,  -8.0419],
        [  4.1670,  -8.7822],
        [  4.6653,  -9.2805],
        [ -9.8684,   5.2532],
        [-10.9522,   6.3370],
        [  1.4929,  -6.1081],
        [ -9.2434,   4.6282],
        [ -9.8844,   5.2692],
        [ -8.9849,   4.3696],
        [-10.3947,   5.7795],
        [-11.0003,   6.3851],
        [-10.3150,   5.6998],
        [  3.6902,  -8.3054],
        [ -8.4050,   3.7898],
        [  3.2926,  -7.9078],
        [-10.7808,   6.1655],
        [ -7.7885,   3.1733],
        [  4.4735,  -9.0888],
        [  0.8375,  -5.4528],
        [  4.2930,  -8.9082],
        [  5.1119,  -9.7271],
        [  4.7483,  -9.3635],
        [  3.7892,  -8.4044],
        [  5.1244,  -9.7396],
        [-10.0228,   5.4076],
        [-10.8095,   6.1943],
        [  5.6573, -10.2726],
        [  4.7522,  -9.3674],
        [  4.7527,  -9.3680],
        [-11.1195,   6.5042],
        [  4.6524,  -9.2676],
        [ -7.2822,   2.6670],
        [  4.6861,  -9.3013],
        [ -8.4552,   3.8400],
        [  5.3978, -10.0130],
        [-11.2607,   6.6455],
        [  4.9945,  -9.6098],
        [  3.2529,  -7.8681],
        [ -9.5156,   4.9003],
        [ -9.6773,   5.0621],
        [  1.6287,  -6.2440],
        [-11.1209,   6.5057],
        [-10.3134,   5.6982],
        [  4.7915,  -9.4067],
        [  4.3274,  -8.9426],
        [  5.5951, -10.2104],
        [  5.3814,  -9.9967],
        [  5.3824,  -9.9976],
        [-10.9942,   6.3790],
        [  3.4297,  -8.0449],
        [  3.4259,  -8.0411],
        [ -9.7634,   5.1482],
        [  5.6459, -10.2611],
        [  3.9947,  -8.6099],
        [ -8.3251,   3.7099],
        [-10.0640,   5.4488],
        [-10.6603,   6.0451],
        [  0.7343,  -5.3496],
        [ -4.8540,   0.2388],
        [  5.1330,  -9.7482],
        [ -7.5512,   2.9359],
        [  6.1349, -10.7501],
        [-10.0592,   5.4439]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7739, 0.2261],
        [0.2712, 0.7288]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2006, 0.1089],
         [0.4801, 0.4026]],

        [[0.1330, 0.1035],
         [0.7745, 0.0699]],

        [[0.3120, 0.0909],
         [0.5666, 0.8874]],

        [[0.7683, 0.1039],
         [0.2545, 0.9339]],

        [[0.5322, 0.1038],
         [0.1875, 0.8632]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -25629.332186832275
Iteration 10: Loss = -12202.947276079127
Iteration 20: Loss = -12070.462979327118
Iteration 30: Loss = -11490.34157232809
Iteration 40: Loss = -11490.333480583182
Iteration 50: Loss = -11490.333451664603
Iteration 60: Loss = -11490.333447842533
Iteration 70: Loss = -11490.333447842533
1
Iteration 80: Loss = -11490.333447842533
2
Iteration 90: Loss = -11490.333447842533
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7721, 0.2279],
        [0.2739, 0.7261]], dtype=torch.float64)
alpha: tensor([0.5331, 0.4669])
beta: tensor([[[0.1969, 0.1090],
         [0.2032, 0.3942]],

        [[0.5194, 0.1035],
         [0.4038, 0.5682]],

        [[0.7976, 0.0910],
         [0.5300, 0.2517]],

        [[0.6393, 0.1044],
         [0.7025, 0.0457]],

        [[0.9296, 0.1037],
         [0.2547, 0.4977]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25629.480857995317
Iteration 100: Loss = -12229.029569758926
Iteration 200: Loss = -12203.879506054773
Iteration 300: Loss = -12193.233617863452
Iteration 400: Loss = -12183.632708236535
Iteration 500: Loss = -12115.903239616122
Iteration 600: Loss = -11952.111298485683
Iteration 700: Loss = -11797.774024667106
Iteration 800: Loss = -11793.581622046084
Iteration 900: Loss = -11790.35901113124
Iteration 1000: Loss = -11779.840291716766
Iteration 1100: Loss = -11767.333817657873
Iteration 1200: Loss = -11753.702117875582
Iteration 1300: Loss = -11748.250066444185
Iteration 1400: Loss = -11747.759780210961
Iteration 1500: Loss = -11740.644342755188
Iteration 1600: Loss = -11722.696922328161
Iteration 1700: Loss = -11722.579255999173
Iteration 1800: Loss = -11713.833998909964
Iteration 1900: Loss = -11713.729578233162
Iteration 2000: Loss = -11713.690875367609
Iteration 2100: Loss = -11713.653966843913
Iteration 2200: Loss = -11712.593371076622
Iteration 2300: Loss = -11712.526664539966
Iteration 2400: Loss = -11689.004026004222
Iteration 2500: Loss = -11688.533633963554
Iteration 2600: Loss = -11688.496682931795
Iteration 2700: Loss = -11688.464687200374
Iteration 2800: Loss = -11686.422620160596
Iteration 2900: Loss = -11677.750920355007
Iteration 3000: Loss = -11655.373279043539
Iteration 3100: Loss = -11655.168230097008
Iteration 3200: Loss = -11648.408538939417
Iteration 3300: Loss = -11648.386386950759
Iteration 3400: Loss = -11648.376606428865
Iteration 3500: Loss = -11648.368806818753
Iteration 3600: Loss = -11648.362153268825
Iteration 3700: Loss = -11648.356296448927
Iteration 3800: Loss = -11648.350601533204
Iteration 3900: Loss = -11641.829187692843
Iteration 4000: Loss = -11641.758746791242
Iteration 4100: Loss = -11641.754092629271
Iteration 4200: Loss = -11641.75146842611
Iteration 4300: Loss = -11641.670956976268
Iteration 4400: Loss = -11641.656257153127
Iteration 4500: Loss = -11641.653637173693
Iteration 4600: Loss = -11641.651267868816
Iteration 4700: Loss = -11641.649122455085
Iteration 4800: Loss = -11641.647180229738
Iteration 4900: Loss = -11641.64538562985
Iteration 5000: Loss = -11641.643733392668
Iteration 5100: Loss = -11641.642280371178
Iteration 5200: Loss = -11641.640768927133
Iteration 5300: Loss = -11641.63944547039
Iteration 5400: Loss = -11641.642706155888
1
Iteration 5500: Loss = -11641.637055356985
Iteration 5600: Loss = -11641.636268950342
Iteration 5700: Loss = -11641.621826857494
Iteration 5800: Loss = -11634.555265413157
Iteration 5900: Loss = -11634.552456797523
Iteration 6000: Loss = -11634.552201624409
Iteration 6100: Loss = -11634.550320300312
Iteration 6200: Loss = -11634.549583158476
Iteration 6300: Loss = -11634.54872625636
Iteration 6400: Loss = -11634.54839716868
Iteration 6500: Loss = -11634.549100533699
1
Iteration 6600: Loss = -11634.546853993634
Iteration 6700: Loss = -11634.548088630825
1
Iteration 6800: Loss = -11634.545741887838
Iteration 6900: Loss = -11634.54543852372
Iteration 7000: Loss = -11634.550313058695
1
Iteration 7100: Loss = -11634.5445350708
Iteration 7200: Loss = -11634.543604321374
Iteration 7300: Loss = -11634.541895677095
Iteration 7400: Loss = -11634.561789326597
1
Iteration 7500: Loss = -11634.533949431981
Iteration 7600: Loss = -11634.534141461027
1
Iteration 7700: Loss = -11634.533192296492
Iteration 7800: Loss = -11634.532868140166
Iteration 7900: Loss = -11634.553344603835
1
Iteration 8000: Loss = -11634.532332615288
Iteration 8100: Loss = -11634.53209854076
Iteration 8200: Loss = -11634.532033727293
Iteration 8300: Loss = -11634.534931808934
1
Iteration 8400: Loss = -11634.537183987564
2
Iteration 8500: Loss = -11634.53200679787
Iteration 8600: Loss = -11634.531133355607
Iteration 8700: Loss = -11634.530983492654
Iteration 8800: Loss = -11634.530813955706
Iteration 8900: Loss = -11634.530917844959
1
Iteration 9000: Loss = -11634.53053268616
Iteration 9100: Loss = -11634.534695477452
1
Iteration 9200: Loss = -11634.530163439307
Iteration 9300: Loss = -11634.82187100489
1
Iteration 9400: Loss = -11634.528893045317
Iteration 9500: Loss = -11634.53165509769
1
Iteration 9600: Loss = -11634.526987362986
Iteration 9700: Loss = -11634.527479312543
1
Iteration 9800: Loss = -11634.52670044548
Iteration 9900: Loss = -11634.526564934284
Iteration 10000: Loss = -11634.53161666239
1
Iteration 10100: Loss = -11634.526384326344
Iteration 10200: Loss = -11634.531135939915
1
Iteration 10300: Loss = -11634.526237132708
Iteration 10400: Loss = -11634.52672299195
1
Iteration 10500: Loss = -11634.526206512339
Iteration 10600: Loss = -11634.526099861445
Iteration 10700: Loss = -11634.525969830447
Iteration 10800: Loss = -11634.525996124705
1
Iteration 10900: Loss = -11634.525657569224
Iteration 11000: Loss = -11634.524984882695
Iteration 11100: Loss = -11634.59440844144
1
Iteration 11200: Loss = -11634.521547199578
Iteration 11300: Loss = -11634.52460427102
1
Iteration 11400: Loss = -11634.52111853695
Iteration 11500: Loss = -11634.529007213596
1
Iteration 11600: Loss = -11634.523959171205
2
Iteration 11700: Loss = -11634.207598882143
Iteration 11800: Loss = -11634.181230335786
Iteration 11900: Loss = -11634.177318734082
Iteration 12000: Loss = -11634.230763560114
1
Iteration 12100: Loss = -11632.615816787336
Iteration 12200: Loss = -11632.634543452537
1
Iteration 12300: Loss = -11632.60097078681
Iteration 12400: Loss = -11632.597367627399
Iteration 12500: Loss = -11632.63609024228
1
Iteration 12600: Loss = -11632.59804433231
2
Iteration 12700: Loss = -11632.597324412543
Iteration 12800: Loss = -11632.59601742436
Iteration 12900: Loss = -11632.596024202585
1
Iteration 13000: Loss = -11632.60968662158
2
Iteration 13100: Loss = -11632.596221706164
3
Iteration 13200: Loss = -11632.605021412046
4
Iteration 13300: Loss = -11632.59664515725
5
Iteration 13400: Loss = -11632.596066951723
6
Iteration 13500: Loss = -11632.626703227004
7
Iteration 13600: Loss = -11632.597268612782
8
Iteration 13700: Loss = -11632.596615578223
9
Iteration 13800: Loss = -11632.73411683642
10
Stopping early at iteration 13800 due to no improvement.
tensor([[ -8.0705,   6.0822],
        [  6.0390,  -8.1199],
        [ -5.0142,   3.5411],
        [  6.4113,  -8.2855],
        [ -8.7087,   6.5027],
        [  5.8950,  -7.8039],
        [ -4.5271,   1.8941],
        [  6.2691,  -8.3114],
        [  6.2732,  -7.8627],
        [  4.3547,  -5.7575],
        [  5.2062,  -6.8039],
        [-10.6649,   7.5421],
        [ -6.8314,   5.4381],
        [ -7.9754,   6.2513],
        [ -4.8811,   3.4932],
        [ -6.9938,   5.1998],
        [  5.1100,  -6.4992],
        [ -9.0099,   6.9837],
        [  6.4743,  -7.8606],
        [  4.1913,  -8.3815],
        [  3.2408,  -4.6909],
        [ -7.9566,   6.5701],
        [  4.3175,  -7.2004],
        [ -7.3413,   5.9538],
        [ -6.6649,   5.1872],
        [ -8.4399,   6.1420],
        [  3.0473,  -5.1017],
        [ -7.0637,   4.0434],
        [ -8.6367,   7.2486],
        [ -7.6865,   6.2873],
        [ -5.2548,   3.8665],
        [  5.4800,  -9.6583],
        [ -8.5616,   6.3729],
        [ -8.5597,   6.9156],
        [  7.1204,  -8.9608],
        [  4.0987,  -8.2320],
        [  3.1021,  -4.5444],
        [  4.5016,  -6.3558],
        [  5.3740,  -7.0540],
        [  6.1154,  -7.5032],
        [ -7.9171,   6.1200],
        [ -8.8257,   7.3866],
        [  2.4429,  -4.4561],
        [ -8.0204,   4.5210],
        [ -8.5662,   7.0321],
        [ -6.7432,   5.3563],
        [ -9.9246,   7.4687],
        [ -9.8865,   6.3718],
        [ -8.4852,   5.9286],
        [  3.1317,  -7.7249],
        [ -6.2300,   4.6701],
        [  3.7154,  -6.9031],
        [ -8.8676,   7.4545],
        [ -8.1897,   3.6390],
        [  5.8016,  -7.5959],
        [  3.0561,  -4.8690],
        [  5.3174,  -8.5138],
        [  6.5576,  -8.7132],
        [  4.7395,  -9.0965],
        [  5.0957,  -6.4958],
        [  2.3300,  -3.7171],
        [ -8.8235,   6.1301],
        [ -8.5481,   6.9802],
        [  6.7833,  -8.4561],
        [  6.2714,  -7.6591],
        [  5.8723,  -7.8293],
        [ -8.8912,   7.5036],
        [  6.0447,  -8.2019],
        [ -5.8643,   2.8027],
        [  6.5341,  -8.7201],
        [ -6.1521,   4.7246],
        [  6.0868,  -7.8259],
        [ -9.7798,   8.2299],
        [  6.6660,  -8.0806],
        [  4.6122,  -6.0792],
        [ -7.9308,   6.4441],
        [ -8.0704,   6.6452],
        [  2.4841,  -4.0270],
        [ -9.0507,   7.6176],
        [ -8.6652,   6.2083],
        [  6.1793,  -8.5629],
        [  5.5003,  -7.0944],
        [  7.1655,  -8.5542],
        [  6.4957,  -8.0388],
        [  6.6388,  -8.1099],
        [ -9.3715,   7.2273],
        [  4.3568,  -5.8913],
        [  4.6451,  -6.2166],
        [ -9.3629,   4.8047],
        [  5.9143,  -7.6769],
        [  4.6414,  -7.4835],
        [ -7.5663,   5.3459],
        [ -7.8973,   6.0684],
        [ -8.9345,   7.0253],
        [  1.7943,  -3.5559],
        [ -2.6856,   1.0140],
        [  6.5286,  -7.9448],
        [ -5.5282,   3.6057],
        [  2.1324,  -3.5562],
        [ -9.1762,   5.8003]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6640, 0.3360],
        [0.5165, 0.4835]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5095, 0.4905], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2064, 0.1088],
         [0.2032, 0.4041]],

        [[0.5194, 0.1025],
         [0.4038, 0.5682]],

        [[0.7976, 0.0909],
         [0.5300, 0.2517]],

        [[0.6393, 0.1038],
         [0.7025, 0.0457]],

        [[0.9296, 0.1037],
         [0.2547, 0.4977]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0052891859464676534
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5951434427288088
Average Adjusted Rand Index: 0.8010578371892935
Iteration 0: Loss = -26305.556442172885
Iteration 10: Loss = -12203.631590085688
Iteration 20: Loss = -12103.481583734503
Iteration 30: Loss = -11635.688545494519
Iteration 40: Loss = -11635.46664114496
Iteration 50: Loss = -11636.212700293157
1
Iteration 60: Loss = -11640.041826557217
2
Iteration 70: Loss = -11643.589028837965
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.4773, 0.5227],
        [0.3387, 0.6613]], dtype=torch.float64)
alpha: tensor([0.4135, 0.5865])
beta: tensor([[[0.3955, 0.1090],
         [0.5972, 0.2023]],

        [[0.6272, 0.1078],
         [0.3451, 0.1739]],

        [[0.0678, 0.0911],
         [0.4544, 0.9444]],

        [[0.4042, 0.1045],
         [0.8017, 0.9316]],

        [[0.7948, 0.1042],
         [0.2358, 0.1095]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.02624371546054492
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5767227471138195
Average Adjusted Rand Index: 0.805248743092109
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26304.854990054126
Iteration 100: Loss = -12236.57784039852
Iteration 200: Loss = -12202.829855541677
Iteration 300: Loss = -12089.0438128418
Iteration 400: Loss = -11868.076347396134
Iteration 500: Loss = -11837.573740903183
Iteration 600: Loss = -11791.201727913904
Iteration 700: Loss = -11774.124307815562
Iteration 800: Loss = -11754.885614388042
Iteration 900: Loss = -11741.31466796841
Iteration 1000: Loss = -11718.684998846971
Iteration 1100: Loss = -11713.011506001245
Iteration 1200: Loss = -11694.541161954638
Iteration 1300: Loss = -11679.087854940972
Iteration 1400: Loss = -11664.427555841006
Iteration 1500: Loss = -11657.533270757907
Iteration 1600: Loss = -11628.126684760613
Iteration 1700: Loss = -11626.978665116407
Iteration 1800: Loss = -11626.869183722763
Iteration 1900: Loss = -11610.927513610568
Iteration 2000: Loss = -11573.010608316654
Iteration 2100: Loss = -11572.895818850144
Iteration 2200: Loss = -11565.190145874409
Iteration 2300: Loss = -11565.130858717212
Iteration 2400: Loss = -11565.051521880205
Iteration 2500: Loss = -11510.727905552249
Iteration 2600: Loss = -11510.67847056377
Iteration 2700: Loss = -11510.647484918632
Iteration 2800: Loss = -11510.592196661939
Iteration 2900: Loss = -11491.212262145827
Iteration 3000: Loss = -11491.195047776997
Iteration 3100: Loss = -11491.181341936099
Iteration 3200: Loss = -11491.171787119738
Iteration 3300: Loss = -11491.160205171205
Iteration 3400: Loss = -11491.151700974671
Iteration 3500: Loss = -11491.144156105662
Iteration 3600: Loss = -11491.136414387205
Iteration 3700: Loss = -11491.036876452383
Iteration 3800: Loss = -11491.023340194244
Iteration 3900: Loss = -11491.018885600926
Iteration 4000: Loss = -11491.014353477529
Iteration 4100: Loss = -11491.010509805796
Iteration 4200: Loss = -11491.008434899444
Iteration 4300: Loss = -11491.003874735557
Iteration 4400: Loss = -11491.001169840605
Iteration 4500: Loss = -11491.014559638417
1
Iteration 4600: Loss = -11490.995893528692
Iteration 4700: Loss = -11490.993627800854
Iteration 4800: Loss = -11491.015953220533
1
Iteration 4900: Loss = -11490.989704817715
Iteration 5000: Loss = -11490.987812619533
Iteration 5100: Loss = -11490.98615126834
Iteration 5200: Loss = -11490.986013765445
Iteration 5300: Loss = -11490.983193090697
Iteration 5400: Loss = -11490.981832825812
Iteration 5500: Loss = -11490.980578781857
Iteration 5600: Loss = -11490.979515235747
Iteration 5700: Loss = -11490.978334479136
Iteration 5800: Loss = -11490.977252235261
Iteration 5900: Loss = -11490.976973226458
Iteration 6000: Loss = -11490.975742227205
Iteration 6100: Loss = -11490.988249927272
1
Iteration 6200: Loss = -11490.973652647986
Iteration 6300: Loss = -11490.972874218302
Iteration 6400: Loss = -11490.972082193624
Iteration 6500: Loss = -11490.97349299882
1
Iteration 6600: Loss = -11490.97296797297
2
Iteration 6700: Loss = -11490.976766099595
3
Iteration 6800: Loss = -11490.969719424433
Iteration 6900: Loss = -11490.982807456965
1
Iteration 7000: Loss = -11490.971227715527
2
Iteration 7100: Loss = -11490.969311880115
Iteration 7200: Loss = -11490.967607376882
Iteration 7300: Loss = -11490.968856548287
1
Iteration 7400: Loss = -11490.980671752182
2
Iteration 7500: Loss = -11490.98110259137
3
Iteration 7600: Loss = -11490.966499370237
Iteration 7700: Loss = -11490.969121309787
1
Iteration 7800: Loss = -11490.970481740707
2
Iteration 7900: Loss = -11490.965572089246
Iteration 8000: Loss = -11490.965233056566
Iteration 8100: Loss = -11490.966914901008
1
Iteration 8200: Loss = -11490.96589628709
2
Iteration 8300: Loss = -11490.966665789667
3
Iteration 8400: Loss = -11491.077900805507
4
Iteration 8500: Loss = -11490.964036503194
Iteration 8600: Loss = -11490.964155953456
1
Iteration 8700: Loss = -11490.964103144399
2
Iteration 8800: Loss = -11490.964899995002
3
Iteration 8900: Loss = -11490.9632571426
Iteration 9000: Loss = -11490.964478384378
1
Iteration 9100: Loss = -11490.963326559568
2
Iteration 9200: Loss = -11490.962877051434
Iteration 9300: Loss = -11490.981688757442
1
Iteration 9400: Loss = -11490.966879295811
2
Iteration 9500: Loss = -11490.970196815806
3
Iteration 9600: Loss = -11491.00719413969
4
Iteration 9700: Loss = -11490.96651660147
5
Iteration 9800: Loss = -11490.962256173674
Iteration 9900: Loss = -11490.969988389606
1
Iteration 10000: Loss = -11490.962634158352
2
Iteration 10100: Loss = -11490.962556877164
3
Iteration 10200: Loss = -11490.96426806664
4
Iteration 10300: Loss = -11490.970480280748
5
Iteration 10400: Loss = -11490.962458057043
6
Iteration 10500: Loss = -11488.518060974813
Iteration 10600: Loss = -11488.51248828574
Iteration 10700: Loss = -11488.502009954675
Iteration 10800: Loss = -11488.49908753075
Iteration 10900: Loss = -11488.498196186338
Iteration 11000: Loss = -11488.502266705334
1
Iteration 11100: Loss = -11488.518608973407
2
Iteration 11200: Loss = -11488.506783991814
3
Iteration 11300: Loss = -11488.501938878786
4
Iteration 11400: Loss = -11488.497490759208
Iteration 11500: Loss = -11488.497624353087
1
Iteration 11600: Loss = -11488.497898322166
2
Iteration 11700: Loss = -11488.501909477076
3
Iteration 11800: Loss = -11488.508480665269
4
Iteration 11900: Loss = -11488.497928743831
5
Iteration 12000: Loss = -11488.505367813284
6
Iteration 12100: Loss = -11488.536740216317
7
Iteration 12200: Loss = -11488.595289912846
8
Iteration 12300: Loss = -11488.497532558044
9
Iteration 12400: Loss = -11488.498574134837
10
Stopping early at iteration 12400 due to no improvement.
tensor([[  5.8321,  -7.6731],
        [ -8.1144,   6.0511],
        [  7.2182, -10.0204],
        [ -7.8397,   6.3455],
        [  7.0774,  -8.4640],
        [ -8.1118,   5.4603],
        [  3.1885,  -4.6057],
        [ -7.7085,   6.3136],
        [ -7.5682,   5.5882],
        [ -6.1128,   4.6561],
        [ -6.8096,   5.1796],
        [  6.9688,  -8.3551],
        [  7.3052,  -9.0174],
        [  6.0733,  -7.6720],
        [  4.1460,  -5.6286],
        [  3.1927,  -7.8079],
        [ -6.8940,   4.9932],
        [  6.7491,  -8.1523],
        [ -8.8620,   6.3923],
        [ -6.1922,   4.7854],
        [ -4.4011,   1.8840],
        [  6.4224,  -7.8532],
        [ -6.7522,   5.2531],
        [  5.9977,  -7.3840],
        [  5.7168,  -7.1863],
        [  6.5879,  -8.6866],
        [ -5.3131,   3.4664],
        [  5.3141,  -6.9277],
        [  7.5625,  -9.8060],
        [  5.7954,  -7.6444],
        [  3.3474,  -4.9176],
        [ -7.5845,   5.7948],
        [  6.2536,  -7.8282],
        [  7.0508,  -9.2415],
        [ -7.9968,   6.1864],
        [ -7.9868,   6.5848],
        [ -4.9734,   3.3436],
        [ -6.7239,   4.8009],
        [ -7.1905,   5.1803],
        [ -7.2653,   5.8086],
        [  5.6833, -10.2985],
        [  7.0147,  -8.4072],
        [ -5.3360,   2.2600],
        [  5.5594,  -7.0724],
        [  6.2927,  -7.8269],
        [  5.6326,  -7.0396],
        [  7.0918,  -8.8854],
        [  6.7336,  -8.1374],
        [  5.9418,  -8.7271],
        [ -8.2877,   3.7102],
        [  5.0849,  -6.8039],
        [ -6.3233,   4.8726],
        [  6.2247,  -9.4761],
        [  4.7731,  -6.1898],
        [ -7.5056,   5.1325],
        [ -4.0927,   2.1903],
        [ -8.4756,   6.8660],
        [ -9.2832,   4.6679],
        [ -7.3394,   5.8897],
        [ -6.7451,   5.3574],
        [ -4.4934,   2.8328],
        [  5.5746,  -8.2189],
        [  6.8493,  -8.2436],
        [ -7.5010,   6.0858],
        [ -7.5415,   6.0073],
        [ -7.3477,   5.9072],
        [  6.8094,  -8.5481],
        [ -8.4834,   5.4046],
        [  4.2621,  -5.7015],
        [ -7.4092,   5.9515],
        [  3.9136,  -8.0706],
        [ -8.8097,   4.3773],
        [  6.4737,  -9.0575],
        [ -7.7259,   6.2365],
        [ -6.2582,   4.8718],
        [  5.6406,  -8.0256],
        [  5.1508,  -8.5057],
        [ -4.7411,   3.1252],
        [  7.2951,  -8.9359],
        [  5.5094,  -8.5584],
        [ -7.4169,   6.0085],
        [ -7.1353,   5.5351],
        [ -7.8150,   5.9076],
        [ -7.8651,   6.1163],
        [ -7.9226,   6.4985],
        [  7.6859,  -9.2052],
        [ -6.8063,   4.6717],
        [ -6.8025,   4.5945],
        [  5.1963,  -8.1881],
        [ -7.4580,   5.7319],
        [ -8.4411,   4.1994],
        [  5.2643,  -6.6824],
        [  6.4329,  -8.2001],
        [  5.9628,  -8.7315],
        [ -4.1932,   1.8889],
        [  0.5453,  -4.5534],
        [ -7.6868,   6.1561],
        [  4.2930,  -6.2032],
        [ -4.1501,   2.1960],
        [  6.1176,  -8.9994]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7294, 0.2706],
        [0.2260, 0.7740]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4903, 0.5097], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4021, 0.1089],
         [0.5972, 0.2005]],

        [[0.6272, 0.1039],
         [0.3451, 0.1739]],

        [[0.0678, 0.0909],
         [0.4544, 0.9444]],

        [[0.4042, 0.1037],
         [0.8017, 0.9316]],

        [[0.7948, 0.1037],
         [0.2358, 0.1095]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999558239977
Average Adjusted Rand Index: 0.9919995611635631
Iteration 0: Loss = -27297.303411649842
Iteration 10: Loss = -12204.210059201334
Iteration 20: Loss = -12189.887998889979
Iteration 30: Loss = -11957.01020547543
Iteration 40: Loss = -11636.216571937222
Iteration 50: Loss = -11639.090337101572
1
Iteration 60: Loss = -11643.175043003821
2
Iteration 70: Loss = -11645.083018232617
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.4734, 0.5266],
        [0.3319, 0.6681]], dtype=torch.float64)
alpha: tensor([0.4083, 0.5917])
beta: tensor([[[0.3973, 0.1090],
         [0.9303, 0.2000]],

        [[0.0805, 0.1096],
         [0.6173, 0.6320]],

        [[0.8160, 0.0911],
         [0.4122, 0.4900]],

        [[0.7343, 0.1044],
         [0.3039, 0.6451]],

        [[0.3486, 0.1041],
         [0.4048, 0.3318]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.0055906778696917
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6075791865427782
Average Adjusted Rand Index: 0.7988818644260617
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27296.48930430342
Iteration 100: Loss = -12212.619577117719
Iteration 200: Loss = -12190.642762612486
Iteration 300: Loss = -12149.682606315062
Iteration 400: Loss = -12062.466878981797
Iteration 500: Loss = -12035.246227749203
Iteration 600: Loss = -11942.743255138028
Iteration 700: Loss = -11820.294910879385
Iteration 800: Loss = -11802.390607896796
Iteration 900: Loss = -11801.501957502327
Iteration 1000: Loss = -11788.746439131244
Iteration 1100: Loss = -11788.442927823688
Iteration 1200: Loss = -11788.26703628873
Iteration 1300: Loss = -11788.148249603375
Iteration 1400: Loss = -11788.06221067121
Iteration 1500: Loss = -11787.996224078437
Iteration 1600: Loss = -11787.943564756186
Iteration 1700: Loss = -11787.899293538338
Iteration 1800: Loss = -11787.85405897508
Iteration 1900: Loss = -11787.77752104552
Iteration 2000: Loss = -11787.750527558623
Iteration 2100: Loss = -11787.729827310803
Iteration 2200: Loss = -11787.7121324282
Iteration 2300: Loss = -11787.696794196909
Iteration 2400: Loss = -11787.683333268156
Iteration 2500: Loss = -11787.671470686106
Iteration 2600: Loss = -11787.660944892643
Iteration 2700: Loss = -11787.651435201573
Iteration 2800: Loss = -11787.642883673703
Iteration 2900: Loss = -11787.635097878258
Iteration 3000: Loss = -11787.62815171192
Iteration 3100: Loss = -11787.621955799515
Iteration 3200: Loss = -11787.616337695172
Iteration 3300: Loss = -11787.611305297723
Iteration 3400: Loss = -11787.606748675391
Iteration 3500: Loss = -11787.602560548761
Iteration 3600: Loss = -11787.598728704948
Iteration 3700: Loss = -11787.595162358804
Iteration 3800: Loss = -11787.591918441794
Iteration 3900: Loss = -11787.588940004449
Iteration 4000: Loss = -11787.586220481275
Iteration 4100: Loss = -11787.58360193107
Iteration 4200: Loss = -11787.581212157618
Iteration 4300: Loss = -11787.578956355015
Iteration 4400: Loss = -11787.59637587082
1
Iteration 4500: Loss = -11787.57497814978
Iteration 4600: Loss = -11787.57312341566
Iteration 4700: Loss = -11787.571376128095
Iteration 4800: Loss = -11787.569703601417
Iteration 4900: Loss = -11787.568506346466
Iteration 5000: Loss = -11787.565807308198
Iteration 5100: Loss = -11787.559705111888
Iteration 5200: Loss = -11787.526714667718
Iteration 5300: Loss = -11787.529195618708
1
Iteration 5400: Loss = -11787.522637488544
Iteration 5500: Loss = -11787.521329577665
Iteration 5600: Loss = -11787.519853665872
Iteration 5700: Loss = -11787.517914367529
Iteration 5800: Loss = -11787.514317814084
Iteration 5900: Loss = -11787.512370559527
Iteration 6000: Loss = -11787.511606224409
Iteration 6100: Loss = -11787.510825173735
Iteration 6200: Loss = -11787.510108289494
Iteration 6300: Loss = -11787.509393833185
Iteration 6400: Loss = -11787.509764008139
1
Iteration 6500: Loss = -11787.508272282015
Iteration 6600: Loss = -11787.507781585377
Iteration 6700: Loss = -11787.507521594902
Iteration 6800: Loss = -11787.506843959092
Iteration 6900: Loss = -11787.50685847711
1
Iteration 7000: Loss = -11787.506424913021
Iteration 7100: Loss = -11787.506051799961
Iteration 7200: Loss = -11787.50626428052
1
Iteration 7300: Loss = -11787.508079728761
2
Iteration 7400: Loss = -11787.505453293983
Iteration 7500: Loss = -11787.507536588297
1
Iteration 7600: Loss = -11787.504357128231
Iteration 7700: Loss = -11787.504580719124
1
Iteration 7800: Loss = -11787.514497350367
2
Iteration 7900: Loss = -11787.508562624542
3
Iteration 8000: Loss = -11787.50357836019
Iteration 8100: Loss = -11787.506822541482
1
Iteration 8200: Loss = -11787.50369220433
2
Iteration 8300: Loss = -11787.502511765988
Iteration 8400: Loss = -11786.768799304431
Iteration 8500: Loss = -11786.544571374185
Iteration 8600: Loss = -11786.54777914428
1
Iteration 8700: Loss = -11786.547703345801
2
Iteration 8800: Loss = -11786.547441496525
3
Iteration 8900: Loss = -11786.543727221786
Iteration 9000: Loss = -11786.542111280234
Iteration 9100: Loss = -11786.542511892461
1
Iteration 9200: Loss = -11786.556252445976
2
Iteration 9300: Loss = -11786.631921225726
3
Iteration 9400: Loss = -11786.6398813815
4
Iteration 9500: Loss = -11786.542435279676
5
Iteration 9600: Loss = -11786.540643654902
Iteration 9700: Loss = -11786.542153021886
1
Iteration 9800: Loss = -11786.55373504038
2
Iteration 9900: Loss = -11786.540437450405
Iteration 10000: Loss = -11786.623177300007
1
Iteration 10100: Loss = -11786.540200479805
Iteration 10200: Loss = -11786.747010732377
1
Iteration 10300: Loss = -11786.539973845516
Iteration 10400: Loss = -11785.06486443214
Iteration 10500: Loss = -11783.984086963974
Iteration 10600: Loss = -11783.984470324596
1
Iteration 10700: Loss = -11783.992548577913
2
Iteration 10800: Loss = -11783.903984621646
Iteration 10900: Loss = -11783.89478784426
Iteration 11000: Loss = -11783.895341813883
1
Iteration 11100: Loss = -11783.894426978108
Iteration 11200: Loss = -11783.895002758783
1
Iteration 11300: Loss = -11783.85747780297
Iteration 11400: Loss = -11783.850570585924
Iteration 11500: Loss = -11783.866260841554
1
Iteration 11600: Loss = -11783.84994564265
Iteration 11700: Loss = -11767.098676174242
Iteration 11800: Loss = -11765.143289575319
Iteration 11900: Loss = -11765.121946200492
Iteration 12000: Loss = -11765.11616327566
Iteration 12100: Loss = -11765.070864734475
Iteration 12200: Loss = -11764.964011595506
Iteration 12300: Loss = -11764.95351771938
Iteration 12400: Loss = -11764.949054975097
Iteration 12500: Loss = -11764.949914308263
1
Iteration 12600: Loss = -11764.966262120808
2
Iteration 12700: Loss = -11764.945982466032
Iteration 12800: Loss = -11764.070823676706
Iteration 12900: Loss = -11764.072367178582
1
Iteration 13000: Loss = -11764.175791643662
2
Iteration 13100: Loss = -11764.068804936482
Iteration 13200: Loss = -11764.066444487687
Iteration 13300: Loss = -11764.07076354119
1
Iteration 13400: Loss = -11764.066248498795
Iteration 13500: Loss = -11764.066204558401
Iteration 13600: Loss = -11764.097280879449
1
Iteration 13700: Loss = -11764.068868239427
2
Iteration 13800: Loss = -11764.070637074228
3
Iteration 13900: Loss = -11764.067217938926
4
Iteration 14000: Loss = -11764.067424409717
5
Iteration 14100: Loss = -11764.06720745242
6
Iteration 14200: Loss = -11764.214395519257
7
Iteration 14300: Loss = -11764.062518190554
Iteration 14400: Loss = -11764.062026446169
Iteration 14500: Loss = -11764.121153843118
1
Iteration 14600: Loss = -11764.067436779093
2
Iteration 14700: Loss = -11764.061052067556
Iteration 14800: Loss = -11764.045772606942
Iteration 14900: Loss = -11764.187021044741
1
Iteration 15000: Loss = -11764.041589013968
Iteration 15100: Loss = -11764.036711156563
Iteration 15200: Loss = -11764.036316229804
Iteration 15300: Loss = -11764.041802604788
1
Iteration 15400: Loss = -11764.036263986503
Iteration 15500: Loss = -11764.087380078689
1
Iteration 15600: Loss = -11764.023551636885
Iteration 15700: Loss = -11764.024656185384
1
Iteration 15800: Loss = -11764.02660010271
2
Iteration 15900: Loss = -11764.023994021523
3
Iteration 16000: Loss = -11764.026805538
4
Iteration 16100: Loss = -11764.024524319988
5
Iteration 16200: Loss = -11764.024214181434
6
Iteration 16300: Loss = -11764.13990125223
7
Iteration 16400: Loss = -11764.023298680844
Iteration 16500: Loss = -11764.030058842964
1
Iteration 16600: Loss = -11764.021958613865
Iteration 16700: Loss = -11764.025200366528
1
Iteration 16800: Loss = -11764.021939953958
Iteration 16900: Loss = -11764.029850753006
1
Iteration 17000: Loss = -11764.032362492506
2
Iteration 17100: Loss = -11764.075232352981
3
Iteration 17200: Loss = -11764.023004203787
4
Iteration 17300: Loss = -11764.02237996968
5
Iteration 17400: Loss = -11764.02524339343
6
Iteration 17500: Loss = -11764.028609287976
7
Iteration 17600: Loss = -11764.022196140872
8
Iteration 17700: Loss = -11764.025994978902
9
Iteration 17800: Loss = -11764.02401418052
10
Stopping early at iteration 17800 due to no improvement.
tensor([[  7.2801,  -8.7603],
        [ -6.9479,   3.8137],
        [  7.5388,  -8.9800],
        [ -8.7600,   5.2003],
        [  9.0434, -10.4337],
        [ -6.7798,   4.7053],
        [  7.7626,  -9.2382],
        [ -8.3155,   6.6159],
        [ -7.0494,   4.9249],
        [ -4.3527,   2.6722],
        [ -6.1578,   4.4787],
        [  8.5559, -10.3685],
        [  8.4841,  -9.9117],
        [  7.2239,  -9.3226],
        [  4.1036,  -5.8712],
        [  4.8773,  -8.0605],
        [ -5.1467,   3.7602],
        [  7.3301, -10.2142],
        [ -7.8684,   6.4815],
        [ -5.4057,   4.0120],
        [ -3.9318,   2.1799],
        [  8.5275, -10.0882],
        [ -5.1119,   3.7179],
        [  8.1834, -11.2711],
        [  7.9754, -10.4229],
        [  8.1412,  -9.8524],
        [ -4.4068,   1.3870],
        [  8.1323,  -9.7126],
        [  7.7380, -11.4520],
        [  7.0587,  -9.0147],
        [  4.6875,  -6.1128],
        [ -6.4683,   4.2582],
        [  8.1874,  -9.8015],
        [  8.1552,  -9.5762],
        [ -9.2594,   7.3686],
        [ -5.6616,   4.1997],
        [ -2.9770,   1.5889],
        [ -4.6968,   3.2990],
        [ -5.6027,   3.9653],
        [ -6.4192,   4.7020],
        [  8.1386,  -9.5249],
        [  8.7977, -10.5371],
        [ -2.5158,   1.0981],
        [  8.3757, -10.1772],
        [  7.7283,  -9.9652],
        [  6.3976,  -8.2273],
        [  8.4599, -10.3350],
        [  8.5732, -11.6086],
        [  7.8800,  -9.3248],
        [ -4.9978,   2.2684],
        [  9.0213, -10.6676],
        [ -4.8627,   3.4601],
        [  7.8014,  -9.5042],
        [  5.7773,  -7.2390],
        [ -6.1793,   4.3488],
        [-10.6489,   6.0337],
        [ -6.9315,   5.0442],
        [ -8.5284,   6.4823],
        [ -8.6163,   4.0010],
        [ -5.5079,   3.4187],
        [ -2.8212,   1.0584],
        [  7.7782,  -9.8367],
        [  8.8217, -10.2984],
        [ -7.3511,   5.6680],
        [ -7.7925,   5.4643],
        [ -6.7245,   5.3094],
        [  8.3417, -11.1439],
        [ -6.8584,   4.2026],
        [  7.6152,  -9.7195],
        [ -7.6731,   5.7825],
        [  8.8502, -13.2351],
        [ -6.4679,   4.6636],
        [  7.9915,  -9.5364],
        [ -8.3642,   6.7780],
        [ -6.7173,   3.6633],
        [  7.3259,  -8.8938],
        [  6.9272,  -8.9284],
        [ -2.9821,   0.2710],
        [  8.0288, -10.6695],
        [  8.1754,  -9.7133],
        [ -7.0445,   5.2889],
        [ -6.0715,   4.3332],
        [ -8.3255,   6.8101],
        [ -8.8118,   6.6600],
        [ -8.1616,   6.2390],
        [  8.5867, -10.1772],
        [ -4.7415,   3.2852],
        [ -5.0446,   2.8646],
        [  9.4657, -10.8710],
        [ -6.4990,   5.0907],
        [ -5.8351,   4.4475],
        [  5.9614,  -7.7083],
        [  7.8238,  -9.2108],
        [  7.9979,  -9.4651],
        [ -2.6872,   1.0398],
        [  1.4655,  -5.1368],
        [ -8.3759,   6.4751],
        [  8.1732,  -9.5634],
        [ -2.3559,   0.9543],
        [  7.9197, -10.0466]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3961, 0.6039],
        [0.3727, 0.6273]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4931, 0.5069], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3647, 0.1087],
         [0.9303, 0.2400]],

        [[0.0805, 0.1027],
         [0.6173, 0.6320]],

        [[0.8160, 0.0898],
         [0.4122, 0.4900]],

        [[0.7343, 0.0979],
         [0.3039, 0.6451]],

        [[0.3486, 0.1030],
         [0.4048, 0.3318]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 80
Adjusted Rand Index: 0.354197325985534
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 77
Adjusted Rand Index: 0.28514463947159197
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.1336082620466455
Average Adjusted Rand Index: 0.7198680482954206
Iteration 0: Loss = -22351.438613759165
Iteration 10: Loss = -12204.140701788465
Iteration 20: Loss = -12201.692396243689
Iteration 30: Loss = -12073.134025401645
Iteration 40: Loss = -11636.371375194143
Iteration 50: Loss = -11635.643537635788
Iteration 60: Loss = -11637.26509375901
1
Iteration 70: Loss = -11641.75239346293
2
Iteration 80: Loss = -11644.480023684824
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.6605, 0.3395],
        [0.5227, 0.4773]], dtype=torch.float64)
alpha: tensor([0.5861, 0.4139])
beta: tensor([[[0.2024, 0.1090],
         [0.9296, 0.3953]],

        [[0.5245, 0.1085],
         [0.2504, 0.0590]],

        [[0.4774, 0.0911],
         [0.5539, 0.0933]],

        [[0.3001, 0.1045],
         [0.1941, 0.9384]],

        [[0.0412, 0.1042],
         [0.0102, 0.1235]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.02624371546054492
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5767227471138195
Average Adjusted Rand Index: 0.805248743092109
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22351.040783981174
Iteration 100: Loss = -12203.771869551469
Iteration 200: Loss = -12175.896940697696
Iteration 300: Loss = -12061.88549310887
Iteration 400: Loss = -11921.238123221025
Iteration 500: Loss = -11669.905030302454
Iteration 600: Loss = -11624.718280540856
Iteration 700: Loss = -11620.800635436186
Iteration 800: Loss = -11600.8777002815
Iteration 900: Loss = -11592.01949905787
Iteration 1000: Loss = -11589.904229123087
Iteration 1100: Loss = -11587.951885834133
Iteration 1200: Loss = -11587.799404729329
Iteration 1300: Loss = -11587.601624125717
Iteration 1400: Loss = -11575.92858762149
Iteration 1500: Loss = -11575.861129546385
Iteration 1600: Loss = -11575.809232099216
Iteration 1700: Loss = -11575.747896804356
Iteration 1800: Loss = -11562.570098081322
Iteration 1900: Loss = -11562.529135789775
Iteration 2000: Loss = -11546.388410342002
Iteration 2100: Loss = -11546.31963285797
Iteration 2200: Loss = -11533.733005305578
Iteration 2300: Loss = -11533.7083740306
Iteration 2400: Loss = -11533.678600682262
Iteration 2500: Loss = -11516.860928676417
Iteration 2600: Loss = -11516.843280282992
Iteration 2700: Loss = -11516.83073239134
Iteration 2800: Loss = -11516.820200276174
Iteration 2900: Loss = -11516.81111605108
Iteration 3000: Loss = -11516.803175709345
Iteration 3100: Loss = -11516.796153947262
Iteration 3200: Loss = -11516.789813578422
Iteration 3300: Loss = -11516.784158023911
Iteration 3400: Loss = -11516.77910648369
Iteration 3500: Loss = -11516.7744175506
Iteration 3600: Loss = -11516.770292989653
Iteration 3700: Loss = -11516.770670320193
1
Iteration 3800: Loss = -11516.762938983804
Iteration 3900: Loss = -11516.759814353794
Iteration 4000: Loss = -11516.756839349138
Iteration 4100: Loss = -11516.754154427348
Iteration 4200: Loss = -11516.751643590627
Iteration 4300: Loss = -11516.752524094107
1
Iteration 4400: Loss = -11516.747270023194
Iteration 4500: Loss = -11516.745273889683
Iteration 4600: Loss = -11516.743457139944
Iteration 4700: Loss = -11516.741769870541
Iteration 4800: Loss = -11516.740597153193
Iteration 4900: Loss = -11516.740329763208
Iteration 5000: Loss = -11516.737368414255
Iteration 5100: Loss = -11516.736081271938
Iteration 5200: Loss = -11516.735334605839
Iteration 5300: Loss = -11516.733893415267
Iteration 5400: Loss = -11516.73268647818
Iteration 5500: Loss = -11516.731835068507
Iteration 5600: Loss = -11516.730964136303
Iteration 5700: Loss = -11516.729941733223
Iteration 5800: Loss = -11516.73582136581
1
Iteration 5900: Loss = -11516.728433155336
Iteration 6000: Loss = -11516.728299463852
Iteration 6100: Loss = -11516.727971767301
Iteration 6200: Loss = -11497.527270605045
Iteration 6300: Loss = -11497.51847493464
Iteration 6400: Loss = -11497.521992071019
1
Iteration 6500: Loss = -11497.523936504122
2
Iteration 6600: Loss = -11497.519180142119
3
Iteration 6700: Loss = -11497.541388102107
4
Iteration 6800: Loss = -11497.514842747652
Iteration 6900: Loss = -11497.514373872424
Iteration 7000: Loss = -11497.513981940849
Iteration 7100: Loss = -11497.513972889954
Iteration 7200: Loss = -11497.516560949225
1
Iteration 7300: Loss = -11497.533107550094
2
Iteration 7400: Loss = -11497.512901758853
Iteration 7500: Loss = -11497.512667334708
Iteration 7600: Loss = -11497.514349583866
1
Iteration 7700: Loss = -11497.515355242283
2
Iteration 7800: Loss = -11497.512836754742
3
Iteration 7900: Loss = -11497.519581017577
4
Iteration 8000: Loss = -11497.509573093183
Iteration 8100: Loss = -11497.510750762776
1
Iteration 8200: Loss = -11497.509118141934
Iteration 8300: Loss = -11497.514178468884
1
Iteration 8400: Loss = -11497.50871775881
Iteration 8500: Loss = -11497.509790500559
1
Iteration 8600: Loss = -11491.209907757255
Iteration 8700: Loss = -11491.209543177287
Iteration 8800: Loss = -11491.21807813525
1
Iteration 8900: Loss = -11491.21125106842
2
Iteration 9000: Loss = -11491.211148419427
3
Iteration 9100: Loss = -11491.268407670055
4
Iteration 9200: Loss = -11491.209186360637
Iteration 9300: Loss = -11491.209272933691
1
Iteration 9400: Loss = -11491.239906187557
2
Iteration 9500: Loss = -11491.208605491822
Iteration 9600: Loss = -11491.20780783723
Iteration 9700: Loss = -11491.210067238508
1
Iteration 9800: Loss = -11491.228794166993
2
Iteration 9900: Loss = -11491.063383423632
Iteration 10000: Loss = -11491.057090657569
Iteration 10100: Loss = -11491.069457540587
1
Iteration 10200: Loss = -11491.068890843315
2
Iteration 10300: Loss = -11491.173844038813
3
Iteration 10400: Loss = -11491.077130849384
4
Iteration 10500: Loss = -11491.058794711178
5
Iteration 10600: Loss = -11491.110550373345
6
Iteration 10700: Loss = -11491.069481093013
7
Iteration 10800: Loss = -11491.05574205348
Iteration 10900: Loss = -11491.06494029163
1
Iteration 11000: Loss = -11491.055209082466
Iteration 11100: Loss = -11491.057646226802
1
Iteration 11200: Loss = -11491.062456838054
2
Iteration 11300: Loss = -11491.057225780025
3
Iteration 11400: Loss = -11491.059693684532
4
Iteration 11500: Loss = -11491.069302523902
5
Iteration 11600: Loss = -11491.108294253534
6
Iteration 11700: Loss = -11491.078787759237
7
Iteration 11800: Loss = -11491.056789579194
8
Iteration 11900: Loss = -11491.122042875997
9
Iteration 12000: Loss = -11491.103472940908
10
Stopping early at iteration 12000 due to no improvement.
tensor([[ -8.5997,   5.0809],
        [  6.0446,  -8.2354],
        [-10.2396,   8.0940],
        [  6.1513,  -8.5982],
        [ -9.7592,   7.3701],
        [  6.0837,  -9.5842],
        [ -8.5047,   6.5306],
        [  6.5349,  -8.6043],
        [  7.3116,  -8.8862],
        [  4.7656,  -6.1554],
        [  5.1518,  -7.3783],
        [ -8.8217,   7.4352],
        [ -7.8413,   5.5153],
        [ -8.5420,   6.1220],
        [ -6.1959,   3.5079],
        [ -6.2510,   4.7591],
        [  5.4042,  -6.7947],
        [ -8.7927,   5.7573],
        [  6.4357,  -8.6078],
        [  4.1423,  -6.9941],
        [  2.5080,  -3.9043],
        [ -8.5327,   6.7170],
        [  4.9232,  -7.4209],
        [-10.5047,   8.0396],
        [ -7.8010,   6.3876],
        [ -8.3345,   6.7152],
        [  3.6165,  -5.2462],
        [ -8.5717,   6.5621],
        [ -9.2026,   7.7615],
        [ -7.1310,   5.7187],
        [ -4.8626,   3.3904],
        [  5.4124,  -7.7970],
        [ -8.6382,   7.0145],
        [ -8.6204,   7.0977],
        [  6.8771,  -8.2691],
        [  5.2734,  -7.2059],
        [  3.5139,  -4.9111],
        [  4.9538,  -6.6269],
        [  5.1441,  -7.2542],
        [  5.8237,  -8.4623],
        [-10.2091,   7.1678],
        [ -9.1357,   7.6990],
        [  3.1087,  -4.6022],
        [ -7.2061,   5.4271],
        [ -8.9216,   7.2406],
        [ -7.8500,   6.1913],
        [ -8.1861,   6.6596],
        [ -9.0053,   7.5720],
        [ -9.4106,   6.3800],
        [  5.3534,  -6.7947],
        [ -9.4882,   7.8158],
        [  4.3426,  -7.0286],
        [ -9.3429,   7.7895],
        [ -6.3818,   4.6065],
        [  5.4967,  -6.9050],
        [  6.6323,  -8.0221],
        [  6.8295,  -8.7117],
        [  6.5796,  -8.2845],
        [  5.9089,  -7.9141],
        [  4.8419,  -7.3887],
        [  2.3135,  -5.1110],
        [ -9.3817,   6.4316],
        [ -9.2764,   7.7792],
        [  6.6404,  -8.3893],
        [  6.2356,  -7.6448],
        [  6.0346,  -7.4223],
        [ -9.4362,   7.4129],
        [  4.9508,  -8.7838],
        [ -5.6677,   4.2017],
        [  6.0407,  -7.7600],
        [ -6.9435,   5.0921],
        [  6.2276,  -7.9649],
        [ -8.4549,   7.0422],
        [  6.4725,  -8.9522],
        [  4.6035,  -6.5650],
        [ -9.2748,   4.6723],
        [-10.0058,   5.3905],
        [  3.0310,  -4.9625],
        [ -9.3487,   7.5921],
        [ -9.3036,   7.6262],
        [  5.8445,  -7.7884],
        [  6.2473,  -7.7437],
        [  6.4041,  -8.0612],
        [  6.4490,  -7.8442],
        [  6.4327,  -8.0468],
        [ -9.6510,   8.0146],
        [  4.8300,  -6.7754],
        [  4.8573,  -6.6895],
        [ -9.3842,   7.3644],
        [  6.2355,  -7.6587],
        [  5.4709,  -6.8573],
        [ -6.5950,   5.2083],
        [ -9.5549,   5.8000],
        [ -9.9184,   7.2177],
        [  1.9913,  -4.1544],
        [ -3.1828,   1.7962],
        [  5.9549,  -8.0889],
        [ -7.7542,   6.2617],
        [  2.2080,  -4.2188],
        [ -7.5724,   6.1771]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7713, 0.2287],
        [0.2780, 0.7220]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5099, 0.4901], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2015, 0.1089],
         [0.9296, 0.4046]],

        [[0.5245, 0.1034],
         [0.2504, 0.0590]],

        [[0.4774, 0.0908],
         [0.5539, 0.0933]],

        [[0.3001, 0.1050],
         [0.1941, 0.9384]],

        [[0.0412, 0.1040],
         [0.0102, 0.1235]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999328927508
Average Adjusted Rand Index: 0.9919998119331364
11493.69508548985
new:  [0.5951434427288088, 0.9919999558239977, 0.1336082620466455, 0.9919999328927508] [0.8010578371892935, 0.9919995611635631, 0.7198680482954206, 0.9919998119331364] [11632.73411683642, 11488.498574134837, 11764.02401418052, 11491.103472940908]
prior:  [1.0, 0.5767227471138195, 0.6075791865427782, 0.5767227471138195] [1.0, 0.805248743092109, 0.7988818644260617, 0.805248743092109] [11490.333447842533, 11643.589028837965, 11645.083018232617, 11644.480023684824]
-----------------------------------------------------------------------------------------
This iteration is 17
True Objective function: Loss = -11664.421020332491
Iteration 0: Loss = -15274.958474351599
Iteration 10: Loss = -11659.968876822002
Iteration 20: Loss = -11659.968878116895
1
Iteration 30: Loss = -11659.968878116895
2
Iteration 40: Loss = -11659.968878116895
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.8091, 0.1909],
        [0.2195, 0.7805]], dtype=torch.float64)
alpha: tensor([0.4859, 0.5141])
beta: tensor([[[0.1950, 0.1042],
         [0.4087, 0.3977]],

        [[0.1800, 0.0998],
         [0.2652, 0.0572]],

        [[0.1896, 0.0990],
         [0.9117, 0.3084]],

        [[0.6237, 0.0972],
         [0.5147, 0.0245]],

        [[0.7268, 0.1080],
         [0.3641, 0.5516]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999860917265
Average Adjusted Rand Index: 0.9919995611635631
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15228.44172006714
Iteration 100: Loss = -11836.029585561655
Iteration 200: Loss = -11657.851740375549
Iteration 300: Loss = -11657.41423936974
Iteration 400: Loss = -11657.234998186723
Iteration 500: Loss = -11657.136476551841
Iteration 600: Loss = -11657.076364397059
Iteration 700: Loss = -11657.036165470645
Iteration 800: Loss = -11657.007728615805
Iteration 900: Loss = -11656.986739516307
Iteration 1000: Loss = -11656.970797527685
Iteration 1100: Loss = -11656.958354038463
Iteration 1200: Loss = -11656.948444484458
Iteration 1300: Loss = -11656.94041117729
Iteration 1400: Loss = -11656.933772870787
Iteration 1500: Loss = -11656.928256810712
Iteration 1600: Loss = -11656.92360946767
Iteration 1700: Loss = -11656.919722435647
Iteration 1800: Loss = -11656.916299883882
Iteration 1900: Loss = -11656.913324738578
Iteration 2000: Loss = -11656.910813499011
Iteration 2100: Loss = -11656.908592954213
Iteration 2200: Loss = -11656.90660173491
Iteration 2300: Loss = -11656.904839142046
Iteration 2400: Loss = -11656.903322735636
Iteration 2500: Loss = -11656.901885313984
Iteration 2600: Loss = -11656.900731623298
Iteration 2700: Loss = -11656.899591727522
Iteration 2800: Loss = -11656.8985841244
Iteration 2900: Loss = -11656.897693387427
Iteration 3000: Loss = -11656.896854633478
Iteration 3100: Loss = -11656.896099348627
Iteration 3200: Loss = -11656.896747143488
1
Iteration 3300: Loss = -11656.894811822774
Iteration 3400: Loss = -11656.894226918246
Iteration 3500: Loss = -11656.893688623679
Iteration 3600: Loss = -11656.8931883599
Iteration 3700: Loss = -11656.89274870011
Iteration 3800: Loss = -11656.892382026757
Iteration 3900: Loss = -11656.906197165126
1
Iteration 4000: Loss = -11656.89163919364
Iteration 4100: Loss = -11656.891311832029
Iteration 4200: Loss = -11656.89103898764
Iteration 4300: Loss = -11656.891568618754
1
Iteration 4400: Loss = -11656.890595159277
Iteration 4500: Loss = -11656.890225548885
Iteration 4600: Loss = -11656.889999413339
Iteration 4700: Loss = -11656.889900977052
Iteration 4800: Loss = -11656.89022692977
1
Iteration 4900: Loss = -11656.892224211859
2
Iteration 5000: Loss = -11656.89179438031
3
Iteration 5100: Loss = -11656.889081516572
Iteration 5200: Loss = -11656.889089137345
1
Iteration 5300: Loss = -11656.888852433694
Iteration 5400: Loss = -11656.890757883004
1
Iteration 5500: Loss = -11656.956947163799
2
Iteration 5600: Loss = -11656.88842997246
Iteration 5700: Loss = -11656.891424592453
1
Iteration 5800: Loss = -11656.888256238413
Iteration 5900: Loss = -11656.888394350895
1
Iteration 6000: Loss = -11656.907316670155
2
Iteration 6100: Loss = -11656.89385967774
3
Iteration 6200: Loss = -11656.887860346746
Iteration 6300: Loss = -11656.904284916725
1
Iteration 6400: Loss = -11656.901207135696
2
Iteration 6500: Loss = -11656.929581821547
3
Iteration 6600: Loss = -11656.889883782984
4
Iteration 6700: Loss = -11656.890126469838
5
Iteration 6800: Loss = -11656.892789667008
6
Iteration 6900: Loss = -11656.890684221607
7
Iteration 7000: Loss = -11656.887471630615
Iteration 7100: Loss = -11656.887466333816
Iteration 7200: Loss = -11656.888211687445
1
Iteration 7300: Loss = -11656.904613732237
2
Iteration 7400: Loss = -11656.913654005766
3
Iteration 7500: Loss = -11656.891188170932
4
Iteration 7600: Loss = -11656.912457819226
5
Iteration 7700: Loss = -11656.993955768792
6
Iteration 7800: Loss = -11656.894063271528
7
Iteration 7900: Loss = -11656.889387878664
8
Iteration 8000: Loss = -11656.88939051345
9
Iteration 8100: Loss = -11656.89482412142
10
Stopping early at iteration 8100 due to no improvement.
tensor([[-10.7796,   6.1644],
        [ -9.4000,   4.7848],
        [-10.2373,   5.6221],
        [-10.9779,   6.3626],
        [-10.6320,   6.0168],
        [  4.0554,  -8.6706],
        [  0.3405,  -4.9557],
        [ -9.1481,   4.5329],
        [-10.3970,   5.7818],
        [  5.6168, -10.2320],
        [ -9.6106,   4.9954],
        [  4.4924,  -9.1076],
        [  3.5313,  -8.1465],
        [ -9.4230,   4.8078],
        [  5.2953,  -9.9106],
        [  4.9674,  -9.5826],
        [-10.5533,   5.9381],
        [ -9.2249,   4.6097],
        [ -9.6340,   5.0188],
        [-10.8732,   6.2580],
        [-10.8955,   6.2803],
        [  4.3198,  -8.9350],
        [  5.8481, -10.4634],
        [  1.0966,  -5.7119],
        [ -9.2629,   4.6476],
        [-11.1712,   6.5560],
        [ -9.9464,   5.3312],
        [  2.4660,  -7.0813],
        [  4.5768,  -9.1921],
        [-10.3068,   5.6916],
        [ -8.1288,   3.5136],
        [  5.3770,  -9.9922],
        [ -9.1080,   4.4928],
        [  4.3272,  -8.9424],
        [-10.7622,   6.1470],
        [  4.4727,  -9.0880],
        [ -7.5458,   2.9306],
        [-10.2257,   5.6105],
        [  4.8478,  -9.4630],
        [ -9.4012,   4.7860],
        [-10.1528,   5.5376],
        [-10.5334,   5.9182],
        [-10.2520,   5.6368],
        [  5.6855, -10.3007],
        [ -8.1713,   3.5561],
        [ -9.5388,   4.9236],
        [  5.3893, -10.0045],
        [ -6.2746,   1.6594],
        [  4.3353,  -8.9505],
        [  4.0686,  -8.6838],
        [-10.5515,   5.9363],
        [ -9.1376,   4.5224],
        [  2.3335,  -6.9487],
        [-10.2348,   5.6196],
        [ -6.9057,   2.2905],
        [  2.7135,  -7.3287],
        [ -8.8804,   4.2652],
        [  2.9703,  -7.5856],
        [ -9.9344,   5.3192],
        [  6.0143, -10.6295],
        [-10.8408,   6.2256],
        [ -9.3716,   4.7564],
        [  4.1417,  -8.7569],
        [-10.4028,   5.7876],
        [ -9.7293,   5.1141],
        [ -7.3806,   2.7654],
        [  5.1240,  -9.7393],
        [  4.1294,  -8.7446],
        [ -9.9385,   5.3233],
        [  4.1573,  -8.7725],
        [  3.6781,  -8.2933],
        [  5.0565,  -9.6718],
        [ -9.6807,   5.0655],
        [ -9.8109,   5.1956],
        [  5.5429, -10.1581],
        [ -9.9892,   5.3740],
        [  5.2676,  -9.8828],
        [ -9.4212,   4.8060],
        [  5.5546, -10.1698],
        [  5.7774, -10.3926],
        [-10.5550,   5.9398],
        [-10.6655,   6.0503],
        [-10.8181,   6.2029],
        [-10.2296,   5.6144],
        [-10.4791,   5.8638],
        [  5.1155,  -9.7307],
        [ -6.7797,   2.1645],
        [  3.5992,  -8.2145],
        [-10.5024,   5.8872],
        [ -9.5144,   4.8991],
        [  5.2053,  -9.8205],
        [-10.4862,   5.8710],
        [ -9.4646,   4.8494],
        [-10.4910,   5.8758],
        [  4.5554,  -9.1706],
        [-10.3581,   5.7429],
        [-10.8158,   6.2006],
        [  4.3375,  -8.9527],
        [-10.1808,   5.5656],
        [  3.9850,  -8.6002]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8101, 0.1899],
        [0.2209, 0.7791]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3994, 0.6006], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.1042],
         [0.4087, 0.4061]],

        [[0.1800, 0.0998],
         [0.2652, 0.0572]],

        [[0.1896, 0.0989],
         [0.9117, 0.3084]],

        [[0.6237, 0.0972],
         [0.5147, 0.0245]],

        [[0.7268, 0.1081],
         [0.3641, 0.5516]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -24432.34050042261
Iteration 10: Loss = -11659.968870817771
Iteration 20: Loss = -11659.96887716503
1
Iteration 30: Loss = -11659.96887716503
2
Iteration 40: Loss = -11659.96887716503
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7805, 0.2195],
        [0.1909, 0.8091]], dtype=torch.float64)
alpha: tensor([0.5141, 0.4859])
beta: tensor([[[0.3977, 0.1042],
         [0.3204, 0.1950]],

        [[0.6617, 0.0998],
         [0.8568, 0.1420]],

        [[0.9523, 0.0990],
         [0.0841, 0.6870]],

        [[0.3319, 0.0972],
         [0.0439, 0.1465]],

        [[0.3102, 0.1080],
         [0.6711, 0.8263]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999860917265
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24431.85926538045
Iteration 100: Loss = -12543.25025641528
Iteration 200: Loss = -12504.374177358593
Iteration 300: Loss = -12397.97310715728
Iteration 400: Loss = -12223.931568807153
Iteration 500: Loss = -12014.21267069363
Iteration 600: Loss = -11829.790578611295
Iteration 700: Loss = -11719.069321766276
Iteration 800: Loss = -11712.768403124277
Iteration 900: Loss = -11700.391217261236
Iteration 1000: Loss = -11689.472069568172
Iteration 1100: Loss = -11689.312901298395
Iteration 1200: Loss = -11689.202476702067
Iteration 1300: Loss = -11689.12001549211
Iteration 1400: Loss = -11689.056244039684
Iteration 1500: Loss = -11689.005503965174
Iteration 1600: Loss = -11688.964314872117
Iteration 1700: Loss = -11688.930271576419
Iteration 1800: Loss = -11688.901728348312
Iteration 1900: Loss = -11688.877449511463
Iteration 2000: Loss = -11688.856517758117
Iteration 2100: Loss = -11688.838185048038
Iteration 2200: Loss = -11688.821721682836
Iteration 2300: Loss = -11688.806308873047
Iteration 2400: Loss = -11688.79088451542
Iteration 2500: Loss = -11676.312924303596
Iteration 2600: Loss = -11676.291526538134
Iteration 2700: Loss = -11676.044978775439
Iteration 2800: Loss = -11665.063551550285
Iteration 2900: Loss = -11664.982208881056
Iteration 3000: Loss = -11656.98166292339
Iteration 3100: Loss = -11656.973884939664
Iteration 3200: Loss = -11656.967178860106
Iteration 3300: Loss = -11656.961323917747
Iteration 3400: Loss = -11656.956204157159
Iteration 3500: Loss = -11656.951683151163
Iteration 3600: Loss = -11656.947651691838
Iteration 3700: Loss = -11656.943963581063
Iteration 3800: Loss = -11656.940717503436
Iteration 3900: Loss = -11656.937670787329
Iteration 4000: Loss = -11656.934942687692
Iteration 4100: Loss = -11656.932408122706
Iteration 4200: Loss = -11656.93004642985
Iteration 4300: Loss = -11656.927840714376
Iteration 4400: Loss = -11656.925902686437
Iteration 4500: Loss = -11656.924007623138
Iteration 4600: Loss = -11656.922241726796
Iteration 4700: Loss = -11656.92061641535
Iteration 4800: Loss = -11656.919141895305
Iteration 4900: Loss = -11656.917721294849
Iteration 5000: Loss = -11656.91638510554
Iteration 5100: Loss = -11656.915165865023
Iteration 5200: Loss = -11656.91400133435
Iteration 5300: Loss = -11656.912940664308
Iteration 5400: Loss = -11656.91221570148
Iteration 5500: Loss = -11656.910963373664
Iteration 5600: Loss = -11656.910090695788
Iteration 5700: Loss = -11656.909254248518
Iteration 5800: Loss = -11656.908595730463
Iteration 5900: Loss = -11656.907727077221
Iteration 6000: Loss = -11656.907012103055
Iteration 6100: Loss = -11656.906398601068
Iteration 6200: Loss = -11656.907070320189
1
Iteration 6300: Loss = -11656.90522852065
Iteration 6400: Loss = -11656.915762193677
1
Iteration 6500: Loss = -11656.904244409223
Iteration 6600: Loss = -11656.908648418043
1
Iteration 6700: Loss = -11656.906464453341
2
Iteration 6800: Loss = -11656.905904863495
3
Iteration 6900: Loss = -11656.906936005784
4
Iteration 7000: Loss = -11656.909517134874
5
Iteration 7100: Loss = -11656.907095432134
6
Iteration 7200: Loss = -11656.906184726855
7
Iteration 7300: Loss = -11656.909264129734
8
Iteration 7400: Loss = -11656.900608432956
Iteration 7500: Loss = -11656.91917971268
1
Iteration 7600: Loss = -11656.923400848815
2
Iteration 7700: Loss = -11656.90358320986
3
Iteration 7800: Loss = -11656.899571665079
Iteration 7900: Loss = -11656.899493334797
Iteration 8000: Loss = -11656.901130206248
1
Iteration 8100: Loss = -11656.9074098426
2
Iteration 8200: Loss = -11656.901129754413
3
Iteration 8300: Loss = -11656.908440265013
4
Iteration 8400: Loss = -11656.907479502068
5
Iteration 8500: Loss = -11657.027987481772
6
Iteration 8600: Loss = -11656.897998762459
Iteration 8700: Loss = -11656.899033488957
1
Iteration 8800: Loss = -11656.956166050026
2
Iteration 8900: Loss = -11656.89823533272
3
Iteration 9000: Loss = -11656.898206644146
4
Iteration 9100: Loss = -11656.910775163004
5
Iteration 9200: Loss = -11656.897272506842
Iteration 9300: Loss = -11656.946635134891
1
Iteration 9400: Loss = -11656.897154096809
Iteration 9500: Loss = -11656.896890170905
Iteration 9600: Loss = -11656.89736022435
1
Iteration 9700: Loss = -11656.896755367765
Iteration 9800: Loss = -11656.915141003428
1
Iteration 9900: Loss = -11656.896744725193
Iteration 10000: Loss = -11656.896620637262
Iteration 10100: Loss = -11656.903626117151
1
Iteration 10200: Loss = -11656.912549572495
2
Iteration 10300: Loss = -11656.900835744202
3
Iteration 10400: Loss = -11656.897364640821
4
Iteration 10500: Loss = -11656.903082075643
5
Iteration 10600: Loss = -11656.898809911934
6
Iteration 10700: Loss = -11656.896563024628
Iteration 10800: Loss = -11656.920626788162
1
Iteration 10900: Loss = -11656.897720236866
2
Iteration 11000: Loss = -11656.897794478447
3
Iteration 11100: Loss = -11656.90989431612
4
Iteration 11200: Loss = -11656.907217169854
5
Iteration 11300: Loss = -11656.909365806752
6
Iteration 11400: Loss = -11656.89696919478
7
Iteration 11500: Loss = -11656.959263494475
8
Iteration 11600: Loss = -11656.904188931105
9
Iteration 11700: Loss = -11656.89936934801
10
Stopping early at iteration 11700 due to no improvement.
tensor([[  6.1673,  -9.7031],
        [  6.5685,  -8.0622],
        [  6.2336,  -8.8118],
        [  6.6885,  -8.9650],
        [  6.5068,  -9.1113],
        [ -8.8690,   6.2993],
        [ -3.3437,   1.9526],
        [  6.1738,  -8.4459],
        [  6.3270,  -9.7780],
        [ -9.1962,   7.5847],
        [  6.2540,  -7.6413],
        [ -8.1742,   6.7064],
        [ -6.8228,   5.1949],
        [  6.7413,  -8.1283],
        [ -9.0405,   7.5657],
        [ -8.2402,   6.2328],
        [  7.1814,  -8.8699],
        [  7.3775, -11.9928],
        [  6.4713,  -8.0498],
        [  7.3199,  -8.8114],
        [  7.0498,  -8.5974],
        [ -8.1618,   6.6490],
        [ -8.3626,   6.6368],
        [ -4.9580,   1.8514],
        [  6.4899,  -7.8799],
        [  6.7573,  -8.1966],
        [  6.6458,  -8.2909],
        [-10.8769,   8.7593],
        [ -8.0220,   6.0574],
        [  6.3000,  -7.6887],
        [  5.7695,  -7.4800],
        [ -9.3727,   7.7414],
        [  6.1335,  -7.7290],
        [-10.1301,   7.6536],
        [  6.6925,  -8.4925],
        [ -8.0364,   6.2820],
        [  4.5047,  -5.9770],
        [  6.8635,  -8.3948],
        [ -7.4052,   5.6650],
        [  6.8536,  -8.2416],
        [  6.6303,  -8.2770],
        [  6.8088,  -8.3838],
        [  6.3010,  -8.0416],
        [ -8.3874,   6.9910],
        [  5.6048,  -7.3191],
        [  6.1439,  -7.9942],
        [ -8.9977,   6.8542],
        [  7.5299,  -9.3475],
        [ -7.7601,   5.7608],
        [ -7.2983,   5.6803],
        [  6.7767,  -8.2752],
        [  5.0198,  -8.6208],
        [ -5.9164,   3.3736],
        [  6.5878,  -8.0112],
        [  8.1070, -10.1983],
        [ -5.7154,   4.3290],
        [  6.4823,  -8.2872],
        [ -9.4344,   7.9366],
        [  6.0907,  -7.5556],
        [ -9.6325,   7.2376],
        [  6.6492,  -8.1551],
        [  6.3781,  -7.7826],
        [ -8.9168,   5.7895],
        [  6.7442,  -8.1678],
        [  6.2240,  -7.6363],
        [  4.3808,  -5.7959],
        [ -8.1161,   6.1501],
        [ -8.3946,   6.3035],
        [  5.9719,  -7.6325],
        [ -6.2433,   4.5966],
        [ -7.9278,   6.5415],
        [ -8.2487,   6.5466],
        [  6.5200,  -9.6035],
        [  5.8766,  -8.2758],
        [ -8.9184,   7.4776],
        [  6.6196,  -8.4325],
        [ -7.9321,   6.5297],
        [  6.0365,  -7.4485],
        [-10.7080,   6.5449],
        [-11.1348,   7.8009],
        [  6.6218,  -9.1360],
        [  6.5208, -10.7798],
        [  6.8020,  -8.2331],
        [  6.4388,  -7.8991],
        [  6.1944,  -8.7503],
        [ -8.1769,   6.3612],
        [  3.7691,  -5.1847],
        [ -6.7104,   5.1106],
        [  7.0082,  -9.1438],
        [  6.8303,  -8.2398],
        [ -7.6831,   6.2328],
        [  5.9841, -10.5993],
        [  6.6637,  -8.0727],
        [  6.8879,  -8.3212],
        [ -8.3753,   6.4914],
        [  6.5208,  -8.2009],
        [  6.5462,  -8.3127],
        [ -8.6496,   6.4349],
        [  6.2719,  -7.9543],
        [ -7.2099,   5.2732]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7802, 0.2198],
        [0.1898, 0.8102]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5965, 0.4035], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4058, 0.1042],
         [0.3204, 0.1988]],

        [[0.6617, 0.0998],
         [0.8568, 0.1420]],

        [[0.9523, 0.0989],
         [0.0841, 0.6870]],

        [[0.3319, 0.0973],
         [0.0439, 0.1465]],

        [[0.3102, 0.1080],
         [0.6711, 0.8263]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -28015.655235106784
Iteration 10: Loss = -12279.69502132085
Iteration 20: Loss = -12112.503409425653
Iteration 30: Loss = -12028.43521915953
Iteration 40: Loss = -11996.048512942232
Iteration 50: Loss = -11994.64714158263
Iteration 60: Loss = -11994.234165267206
Iteration 70: Loss = -11994.211674378677
Iteration 80: Loss = -11994.218827199224
1
Iteration 90: Loss = -11994.221404284643
2
Iteration 100: Loss = -11994.22205641411
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.4066, 0.5934],
        [0.6397, 0.3603]], dtype=torch.float64)
alpha: tensor([0.4965, 0.5035])
beta: tensor([[[0.3067, 0.1025],
         [0.6024, 0.2983]],

        [[0.1575, 0.0986],
         [0.4601, 0.2748]],

        [[0.2058, 0.0958],
         [0.2955, 0.2893]],

        [[0.1070, 0.0950],
         [0.5412, 0.0980]],

        [[0.3444, 0.1053],
         [0.8046, 0.3707]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080740404436667
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.02903138056831896
Average Adjusted Rand Index: 0.8907327477903181
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28015.65101224145
Iteration 100: Loss = -12481.819711764423
Iteration 200: Loss = -12166.400531868125
Iteration 300: Loss = -11827.373928505298
Iteration 400: Loss = -11726.173268730912
Iteration 500: Loss = -11725.099136726805
Iteration 600: Loss = -11712.846165456198
Iteration 700: Loss = -11702.807708609947
Iteration 800: Loss = -11702.62201024759
Iteration 900: Loss = -11702.49090592149
Iteration 1000: Loss = -11696.605451669986
Iteration 1100: Loss = -11696.243878511084
Iteration 1200: Loss = -11686.542018228685
Iteration 1300: Loss = -11686.49441378413
Iteration 1400: Loss = -11675.769075500231
Iteration 1500: Loss = -11674.503623875347
Iteration 1600: Loss = -11666.88544481908
Iteration 1700: Loss = -11657.061286350914
Iteration 1800: Loss = -11657.0427038685
Iteration 1900: Loss = -11657.027131853307
Iteration 2000: Loss = -11657.013739613829
Iteration 2100: Loss = -11657.00217291955
Iteration 2200: Loss = -11656.992125291476
Iteration 2300: Loss = -11656.983275836114
Iteration 2400: Loss = -11656.97547954175
Iteration 2500: Loss = -11656.96862387791
Iteration 2600: Loss = -11656.962391382945
Iteration 2700: Loss = -11656.95682578929
Iteration 2800: Loss = -11656.951855157555
Iteration 2900: Loss = -11656.94735343998
Iteration 3000: Loss = -11656.943221542764
Iteration 3100: Loss = -11656.939541960515
Iteration 3200: Loss = -11656.936091792577
Iteration 3300: Loss = -11656.933022760124
Iteration 3400: Loss = -11656.930171751717
Iteration 3500: Loss = -11656.927578957548
Iteration 3600: Loss = -11656.925239830003
Iteration 3700: Loss = -11656.923094987193
Iteration 3800: Loss = -11656.921074629869
Iteration 3900: Loss = -11656.919226706408
Iteration 4000: Loss = -11656.917570691587
Iteration 4100: Loss = -11656.921826637164
1
Iteration 4200: Loss = -11656.91455378748
Iteration 4300: Loss = -11656.91323359105
Iteration 4400: Loss = -11656.911971690291
Iteration 4500: Loss = -11656.910772590889
Iteration 4600: Loss = -11656.909706358438
Iteration 4700: Loss = -11656.90873782217
Iteration 4800: Loss = -11656.908055875769
Iteration 4900: Loss = -11656.907634685133
Iteration 5000: Loss = -11656.906045678905
Iteration 5100: Loss = -11656.914271959704
1
Iteration 5200: Loss = -11656.90454863026
Iteration 5300: Loss = -11656.9047005436
1
Iteration 5400: Loss = -11656.903231147702
Iteration 5500: Loss = -11656.903195258192
Iteration 5600: Loss = -11656.902095625137
Iteration 5700: Loss = -11656.901681523786
Iteration 5800: Loss = -11656.91317574423
1
Iteration 5900: Loss = -11656.900633804373
Iteration 6000: Loss = -11656.900194229684
Iteration 6100: Loss = -11656.90041271734
1
Iteration 6200: Loss = -11656.89949283376
Iteration 6300: Loss = -11656.89918428538
Iteration 6400: Loss = -11656.89868152929
Iteration 6500: Loss = -11656.898480285554
Iteration 6600: Loss = -11656.9047786655
1
Iteration 6700: Loss = -11656.897887788346
Iteration 6800: Loss = -11656.897539509277
Iteration 6900: Loss = -11656.89722454908
Iteration 7000: Loss = -11656.90626866624
1
Iteration 7100: Loss = -11656.896815207312
Iteration 7200: Loss = -11656.89742025314
1
Iteration 7300: Loss = -11656.896376316543
Iteration 7400: Loss = -11656.90119855239
1
Iteration 7500: Loss = -11656.896247965342
Iteration 7600: Loss = -11656.904328461344
1
Iteration 7700: Loss = -11656.895642992074
Iteration 7800: Loss = -11656.89587468141
1
Iteration 7900: Loss = -11656.895375209111
Iteration 8000: Loss = -11656.897191798533
1
Iteration 8100: Loss = -11656.910932496983
2
Iteration 8200: Loss = -11656.89500871085
Iteration 8300: Loss = -11656.898324457603
1
Iteration 8400: Loss = -11656.895208652448
2
Iteration 8500: Loss = -11656.895740374592
3
Iteration 8600: Loss = -11656.89637441889
4
Iteration 8700: Loss = -11656.903063514752
5
Iteration 8800: Loss = -11656.903030118674
6
Iteration 8900: Loss = -11656.904684503243
7
Iteration 9000: Loss = -11656.90158711278
8
Iteration 9100: Loss = -11656.895113815823
9
Iteration 9200: Loss = -11656.89505408958
10
Stopping early at iteration 9200 due to no improvement.
tensor([[  6.9591,  -8.7096],
        [  6.4352,  -8.1337],
        [  6.5646,  -7.9517],
        [  6.7798,  -8.2838],
        [  6.3495,  -7.8460],
        [ -7.2159,   5.7395],
        [ -4.0457,   1.2510],
        [  5.6400,  -7.2628],
        [  6.3134,  -7.7005],
        [ -8.4490,   6.0130],
        [  4.8491,  -8.6646],
        [ -7.6372,   5.3875],
        [ -6.6747,   4.8552],
        [  6.5989,  -8.7673],
        [ -8.6053,   5.1829],
        [ -7.1066,   5.4169],
        [  6.1444,  -8.2058],
        [  7.0545,  -9.5771],
        [  6.2239,  -7.6676],
        [  6.5029,  -8.3407],
        [  6.5794, -11.1946],
        [ -7.5288,   5.9181],
        [ -7.5570,   6.0097],
        [ -4.1316,   2.6812],
        [  6.4010,  -7.7928],
        [  6.8464,  -8.2393],
        [  6.9018,  -8.2967],
        [ -5.4690,   4.0826],
        [ -6.8991,   5.4971],
        [  6.1649,  -7.5701],
        [  5.1248,  -6.5584],
        [ -7.3201,   5.8491],
        [  6.1941,  -7.6942],
        [ -6.7094,   5.1751],
        [  6.0529,  -7.8977],
        [ -9.1573,   5.6253],
        [  4.4535,  -6.0149],
        [  6.8864,  -8.4262],
        [ -7.2778,   5.6443],
        [  6.8462,  -8.2410],
        [  6.0373,  -7.6187],
        [  6.3380,  -8.1938],
        [  6.2302,  -7.6223],
        [ -7.5085,   6.0826],
        [  6.3998,  -8.6971],
        [  5.7697,  -7.4625],
        [ -7.4486,   6.0570],
        [  3.1304,  -4.8090],
        [ -6.8616,   5.4013],
        [ -7.4724,   5.1759],
        [  5.8246,  -9.2041],
        [  5.4105,  -6.8793],
        [ -5.3573,   3.9355],
        [  6.2493,  -8.2602],
        [  3.8692,  -5.3549],
        [ -5.7504,   4.3043],
        [  6.1572,  -8.5098],
        [ -6.0019,   4.5176],
        [  5.9567,  -7.3811],
        [ -9.4861,   6.4385],
        [  6.5060, -11.1212],
        [  6.1672,  -7.5579],
        [ -5.6564,   4.1470],
        [  6.6705,  -9.1079],
        [  5.7345,  -7.3845],
        [  4.3956,  -5.7854],
        [ -7.6122,   5.6630],
        [ -7.2285,   5.7241],
        [  5.5758,  -8.4428],
        [ -6.0571,   4.6704],
        [ -7.4454,   5.5335],
        [ -7.1415,   5.6172],
        [  5.3422,  -9.9574],
        [  5.7837,  -7.1717],
        [ -7.3219,   5.8301],
        [  5.9248,  -7.3640],
        [ -7.1738,   5.7872],
        [  5.4719,  -8.2255],
        [ -8.0081,   5.7887],
        [ -7.5276,   6.0165],
        [  7.1833,  -9.3132],
        [  6.2541,  -7.8825],
        [  6.4229,  -8.8782],
        [  6.3149,  -7.7869],
        [  6.2238,  -9.6851],
        [ -7.2370,   5.8481],
        [  2.8228,  -6.2793],
        [ -7.4910,   5.0182],
        [  6.3217,  -8.6037],
        [  6.5246,  -8.8021],
        [ -7.2547,   5.7593],
        [  6.1113,  -7.7877],
        [  6.5177,  -7.9371],
        [  6.3249,  -9.1572],
        [ -7.9700,   5.7697],
        [  6.3962,  -7.9471],
        [  6.3554,  -7.9041],
        [ -6.6715,   5.2813],
        [  6.0563,  -8.2952],
        [ -6.8102,   5.4227]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7807, 0.2193],
        [0.1895, 0.8105]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6008, 0.3992], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4058, 0.1042],
         [0.6024, 0.1991]],

        [[0.1575, 0.1000],
         [0.4601, 0.2748]],

        [[0.2058, 0.0990],
         [0.2955, 0.2893]],

        [[0.1070, 0.0973],
         [0.5412, 0.0980]],

        [[0.3444, 0.1080],
         [0.8046, 0.3707]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -18637.273225449422
Iteration 10: Loss = -11659.969053785502
Iteration 20: Loss = -11659.96887716503
Iteration 30: Loss = -11659.96887716503
1
Iteration 40: Loss = -11659.96887716503
2
Iteration 50: Loss = -11659.96887716503
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7805, 0.2195],
        [0.1909, 0.8091]], dtype=torch.float64)
alpha: tensor([0.5141, 0.4859])
beta: tensor([[[0.3977, 0.1042],
         [0.3646, 0.1950]],

        [[0.4713, 0.0998],
         [0.9095, 0.4235]],

        [[0.8678, 0.0990],
         [0.8704, 0.4615]],

        [[0.9818, 0.0972],
         [0.5656, 0.0263]],

        [[0.0021, 0.1080],
         [0.9678, 0.9151]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999860917265
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18636.73257700839
Iteration 100: Loss = -12541.734033184195
Iteration 200: Loss = -12357.210727790383
Iteration 300: Loss = -11928.101507298397
Iteration 400: Loss = -11848.481363019087
Iteration 500: Loss = -11842.344165075323
Iteration 600: Loss = -11833.840141017026
Iteration 700: Loss = -11833.022229039374
Iteration 800: Loss = -11832.25027738354
Iteration 900: Loss = -11828.032715043208
Iteration 1000: Loss = -11827.872163742515
Iteration 1100: Loss = -11827.794970843366
Iteration 1200: Loss = -11827.74215955409
Iteration 1300: Loss = -11827.699177166782
Iteration 1400: Loss = -11827.66284290206
Iteration 1500: Loss = -11827.639723097313
Iteration 1600: Loss = -11827.621734319757
Iteration 1700: Loss = -11827.60677182566
Iteration 1800: Loss = -11827.594104440317
Iteration 1900: Loss = -11827.582986269415
Iteration 2000: Loss = -11827.575857650232
Iteration 2100: Loss = -11827.61742782914
1
Iteration 2200: Loss = -11827.44693610533
Iteration 2300: Loss = -11827.440079538028
Iteration 2400: Loss = -11827.434374162076
Iteration 2500: Loss = -11827.432316000844
Iteration 2600: Loss = -11827.42498757778
Iteration 2700: Loss = -11827.4210515872
Iteration 2800: Loss = -11827.417693204616
Iteration 2900: Loss = -11827.414494153085
Iteration 3000: Loss = -11827.41138599271
Iteration 3100: Loss = -11827.408671912914
Iteration 3200: Loss = -11827.406339488643
Iteration 3300: Loss = -11827.403997314934
Iteration 3400: Loss = -11827.401917772739
Iteration 3500: Loss = -11827.403884905581
1
Iteration 3600: Loss = -11827.398249964928
Iteration 3700: Loss = -11827.396658503369
Iteration 3800: Loss = -11827.395255447902
Iteration 3900: Loss = -11827.394155452483
Iteration 4000: Loss = -11827.392662514487
Iteration 4100: Loss = -11827.391536012228
Iteration 4200: Loss = -11827.391493230005
Iteration 4300: Loss = -11827.389501492042
Iteration 4400: Loss = -11827.388613793175
Iteration 4500: Loss = -11827.412312852224
1
Iteration 4600: Loss = -11827.386911444006
Iteration 4700: Loss = -11827.386178647259
Iteration 4800: Loss = -11827.38553505448
Iteration 4900: Loss = -11827.384944033634
Iteration 5000: Loss = -11827.38512749638
1
Iteration 5100: Loss = -11827.383698575833
Iteration 5200: Loss = -11827.383836610734
1
Iteration 5300: Loss = -11827.383342537076
Iteration 5400: Loss = -11827.382299235693
Iteration 5500: Loss = -11827.381973737009
Iteration 5600: Loss = -11827.38138929251
Iteration 5700: Loss = -11827.381142902936
Iteration 5800: Loss = -11827.380656987762
Iteration 5900: Loss = -11827.381854879568
1
Iteration 6000: Loss = -11827.41809474985
2
Iteration 6100: Loss = -11827.379879163125
Iteration 6200: Loss = -11827.379497104086
Iteration 6300: Loss = -11827.382359902043
1
Iteration 6400: Loss = -11827.378888413146
Iteration 6500: Loss = -11827.379103925887
1
Iteration 6600: Loss = -11827.378479223362
Iteration 6700: Loss = -11827.37990187247
1
Iteration 6800: Loss = -11827.37804199388
Iteration 6900: Loss = -11827.384442369425
1
Iteration 7000: Loss = -11827.377728838186
Iteration 7100: Loss = -11827.377494094546
Iteration 7200: Loss = -11827.377521951086
1
Iteration 7300: Loss = -11827.377407111742
Iteration 7400: Loss = -11827.39639847404
1
Iteration 7500: Loss = -11827.376907122425
Iteration 7600: Loss = -11827.37735612907
1
Iteration 7700: Loss = -11827.376640828621
Iteration 7800: Loss = -11827.387724808144
1
Iteration 7900: Loss = -11827.376401188609
Iteration 8000: Loss = -11827.376377107194
Iteration 8100: Loss = -11827.412250584364
1
Iteration 8200: Loss = -11827.37612462818
Iteration 8300: Loss = -11827.3769167826
1
Iteration 8400: Loss = -11827.37600838253
Iteration 8500: Loss = -11827.375891961603
Iteration 8600: Loss = -11827.382512892027
1
Iteration 8700: Loss = -11827.37865966658
2
Iteration 8800: Loss = -11827.461099300233
3
Iteration 8900: Loss = -11827.383790822078
4
Iteration 9000: Loss = -11827.378835562162
5
Iteration 9100: Loss = -11827.376020089521
6
Iteration 9200: Loss = -11827.375987252111
7
Iteration 9300: Loss = -11827.379671435727
8
Iteration 9400: Loss = -11827.380833561005
9
Iteration 9500: Loss = -11827.376374747706
10
Stopping early at iteration 9500 due to no improvement.
tensor([[  6.9788,  -8.3748],
        [  6.6020,  -8.8977],
        [  6.3455,  -8.4882],
        [  5.1327,  -9.4470],
        [  6.5601,  -8.1365],
        [ -7.8379,   6.1114],
        [ -4.2046,   2.6522],
        [  5.8633,  -7.2741],
        [  6.0550,  -7.5769],
        [ -7.9667,   6.5802],
        [  5.7114,  -7.1780],
        [ -8.3792,   6.9337],
        [ -6.0321,   4.4931],
        [  6.2569,  -7.6573],
        [ -7.4093,   6.0176],
        [ -7.2026,   5.8163],
        [  6.7905,  -8.4754],
        [  6.6674,  -8.0659],
        [  6.6582,  -8.4646],
        [  5.6086,  -9.4002],
        [  6.6234,  -8.0398],
        [ -8.1551,   6.4236],
        [ -8.8450,   5.8881],
        [ -5.0318,   3.4774],
        [  6.2210,  -7.8835],
        [  6.7313,  -9.1769],
        [  6.3253,  -8.0480],
        [ -6.5814,   5.1817],
        [ -7.2388,   5.2030],
        [  6.2348,  -7.7581],
        [  5.3449,  -6.7372],
        [ -8.4738,   6.9718],
        [  6.2856,  -8.0583],
        [ -8.7658,   5.7921],
        [  6.4995,  -8.0097],
        [ -7.1888,   5.7659],
        [  4.6887,  -7.0042],
        [  5.6203,  -9.4527],
        [ -8.2563,   5.6780],
        [  6.2744,  -7.6622],
        [  5.4721,  -6.9623],
        [  6.1872,  -7.5755],
        [  6.0643,  -7.5828],
        [ -7.7706,   6.1950],
        [  4.5110,  -8.6827],
        [  4.9671,  -9.0466],
        [ -8.2412,   5.8810],
        [  5.7246,  -7.5050],
        [ -7.6458,   5.9094],
        [ -7.1842,   5.6782],
        [  6.3199,  -7.8258],
        [  4.1178,  -8.0874],
        [ -5.6909,   2.2366],
        [  5.7831,  -7.6402],
        [  6.2420,  -7.6415],
        [ -6.4145,   1.9840],
        [  6.5433,  -8.1690],
        [ -5.6131,   3.8645],
        [  5.8161,  -7.2492],
        [-10.4437,   5.9449],
        [  6.5150,  -9.4662],
        [  6.9873,  -8.5218],
        [ -8.2226,   5.2866],
        [  6.1585,  -8.7886],
        [  5.3110,  -7.1428],
        [  3.0198,  -5.7466],
        [ -7.5546,   5.5532],
        [ -7.6298,   5.2782],
        [  6.3037,  -7.8822],
        [ -5.7085,   3.8335],
        [ -7.9171,   6.4408],
        [ -8.3787,   5.0119],
        [  6.2227,  -8.0956],
        [  6.0177,  -7.4045],
        [ -8.0865,   6.6273],
        [  6.3315,  -7.7233],
        [ -7.1290,   5.7215],
        [  5.8662,  -7.2927],
        [ -8.0508,   6.6217],
        [ -9.2994,   6.5121],
        [  7.1465,  -8.6832],
        [  6.5709,  -8.2135],
        [  6.6772,  -8.1941],
        [  6.0042,  -7.4127],
        [  6.8448,  -8.4001],
        [ -7.2587,   5.6893],
        [  4.4128,  -6.0573],
        [ -8.3426,   4.9783],
        [  6.4611,  -7.9041],
        [  7.1537,  -8.7946],
        [ -9.1032,   4.4879],
        [  6.0824,  -7.4713],
        [  6.0759,  -7.5934],
        [  5.7494,  -7.2610],
        [ -7.4382,   5.7537],
        [  6.1186,  -9.1041],
        [  7.1421,  -9.0953],
        [ -6.9412,   4.5454],
        [  6.0927,  -7.5472],
        [ -7.5703,   5.3568]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4858, 0.5142],
        [0.3976, 0.6024]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6001, 0.3999], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4077, 0.1040],
         [0.3646, 0.2101]],

        [[0.4713, 0.1093],
         [0.9095, 0.4235]],

        [[0.8678, 0.0991],
         [0.8704, 0.4615]],

        [[0.9818, 0.0972],
         [0.5656, 0.0263]],

        [[0.0021, 0.1077],
         [0.9678, 0.9151]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 74
Adjusted Rand Index: 0.22409191428789305
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4946067249221185
Average Adjusted Rand Index: 0.8448183828575786
Iteration 0: Loss = -23221.014435958237
Iteration 10: Loss = -11824.453861352273
Iteration 20: Loss = -11659.968878588827
Iteration 30: Loss = -11659.96887716503
Iteration 40: Loss = -11659.96887716503
1
Iteration 50: Loss = -11659.96887716503
2
Iteration 60: Loss = -11659.96887716503
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7805, 0.2195],
        [0.1909, 0.8091]], dtype=torch.float64)
alpha: tensor([0.5141, 0.4859])
beta: tensor([[[0.3977, 0.1042],
         [0.5092, 0.1950]],

        [[0.3783, 0.0998],
         [0.4728, 0.4536]],

        [[0.7730, 0.0990],
         [0.4526, 0.4443]],

        [[0.3884, 0.0972],
         [0.2099, 0.6083]],

        [[0.4745, 0.1080],
         [0.0259, 0.3855]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999860917265
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23220.8929561593
Iteration 100: Loss = -12369.275707903864
Iteration 200: Loss = -12028.065903361505
Iteration 300: Loss = -11985.130107945031
Iteration 400: Loss = -11949.921877592247
Iteration 500: Loss = -11930.5094663105
Iteration 600: Loss = -11910.522034125173
Iteration 700: Loss = -11910.098885395066
Iteration 800: Loss = -11899.54205925923
Iteration 900: Loss = -11891.226455708216
Iteration 1000: Loss = -11876.340837390277
Iteration 1100: Loss = -11866.333960718528
Iteration 1200: Loss = -11866.277643719004
Iteration 1300: Loss = -11856.932954027647
Iteration 1400: Loss = -11856.851540193044
Iteration 1500: Loss = -11848.398141187568
Iteration 1600: Loss = -11848.368570698743
Iteration 1700: Loss = -11848.343883376612
Iteration 1800: Loss = -11847.387560403691
Iteration 1900: Loss = -11847.361296863943
Iteration 2000: Loss = -11847.350261216856
Iteration 2100: Loss = -11847.340652381901
Iteration 2200: Loss = -11847.331249887571
Iteration 2300: Loss = -11847.323626080039
Iteration 2400: Loss = -11847.318283137918
Iteration 2500: Loss = -11847.313442223873
Iteration 2600: Loss = -11847.309260306882
Iteration 2700: Loss = -11847.30544989643
Iteration 2800: Loss = -11847.302130539383
Iteration 2900: Loss = -11847.299122080789
Iteration 3000: Loss = -11847.300044946434
1
Iteration 3100: Loss = -11847.293874922401
Iteration 3200: Loss = -11847.291644531528
Iteration 3300: Loss = -11847.289576709634
Iteration 3400: Loss = -11847.28766509452
Iteration 3500: Loss = -11847.28596890899
Iteration 3600: Loss = -11847.302645077216
1
Iteration 3700: Loss = -11847.282911885724
Iteration 3800: Loss = -11847.281531458648
Iteration 3900: Loss = -11847.280284810373
Iteration 4000: Loss = -11847.279120092231
Iteration 4100: Loss = -11847.278011826349
Iteration 4200: Loss = -11847.277040055918
Iteration 4300: Loss = -11847.27603379775
Iteration 4400: Loss = -11847.275121296783
Iteration 4500: Loss = -11847.274217052744
Iteration 4600: Loss = -11847.273061995866
Iteration 4700: Loss = -11847.265109993745
Iteration 4800: Loss = -11847.270751463107
1
Iteration 4900: Loss = -11847.26443542463
Iteration 5000: Loss = -11843.43243141254
Iteration 5100: Loss = -11843.407008276376
Iteration 5200: Loss = -11843.406145662386
Iteration 5300: Loss = -11843.406522903184
1
Iteration 5400: Loss = -11843.406335171625
2
Iteration 5500: Loss = -11843.405870490162
Iteration 5600: Loss = -11843.404849224406
Iteration 5700: Loss = -11843.40448967092
Iteration 5800: Loss = -11843.403585203538
Iteration 5900: Loss = -11843.412148813422
1
Iteration 6000: Loss = -11843.40530733367
2
Iteration 6100: Loss = -11843.402424437563
Iteration 6200: Loss = -11843.402179198463
Iteration 6300: Loss = -11843.402357587633
1
Iteration 6400: Loss = -11843.401855847025
Iteration 6500: Loss = -11838.895975626707
Iteration 6600: Loss = -11838.894645231996
Iteration 6700: Loss = -11838.897078004857
1
Iteration 6800: Loss = -11838.895343623979
2
Iteration 6900: Loss = -11838.895544598496
3
Iteration 7000: Loss = -11838.893625481112
Iteration 7100: Loss = -11838.895716815201
1
Iteration 7200: Loss = -11838.893640824916
2
Iteration 7300: Loss = -11838.90731594166
3
Iteration 7400: Loss = -11838.895271078909
4
Iteration 7500: Loss = -11838.893175701987
Iteration 7600: Loss = -11838.895331368132
1
Iteration 7700: Loss = -11838.89269253861
Iteration 7800: Loss = -11838.892728523499
1
Iteration 7900: Loss = -11838.892453158984
Iteration 8000: Loss = -11838.893469346694
1
Iteration 8100: Loss = -11838.930484790026
2
Iteration 8200: Loss = -11838.914447042436
3
Iteration 8300: Loss = -11838.891940970394
Iteration 8400: Loss = -11838.891873483248
Iteration 8500: Loss = -11839.071923908605
1
Iteration 8600: Loss = -11838.891655458434
Iteration 8700: Loss = -11838.891663148714
1
Iteration 8800: Loss = -11838.891563295534
Iteration 8900: Loss = -11838.89137211116
Iteration 9000: Loss = -11838.891226313232
Iteration 9100: Loss = -11827.530339035813
Iteration 9200: Loss = -11827.492593050918
Iteration 9300: Loss = -11827.493233822988
1
Iteration 9400: Loss = -11827.51029632487
2
Iteration 9500: Loss = -11827.492189291934
Iteration 9600: Loss = -11827.643055586326
1
Iteration 9700: Loss = -11827.492125791929
Iteration 9800: Loss = -11827.492854775193
1
Iteration 9900: Loss = -11827.498432386816
2
Iteration 10000: Loss = -11827.641944442688
3
Iteration 10100: Loss = -11827.50082097525
4
Iteration 10200: Loss = -11827.496911820515
5
Iteration 10300: Loss = -11827.507953541763
6
Iteration 10400: Loss = -11827.491851446639
Iteration 10500: Loss = -11827.492102815622
1
Iteration 10600: Loss = -11827.500235641864
2
Iteration 10700: Loss = -11827.49181215594
Iteration 10800: Loss = -11827.538255244039
1
Iteration 10900: Loss = -11827.491675208934
Iteration 11000: Loss = -11827.494072029853
1
Iteration 11100: Loss = -11827.493645776021
2
Iteration 11200: Loss = -11827.494817218683
3
Iteration 11300: Loss = -11827.494557071808
4
Iteration 11400: Loss = -11827.492132032745
5
Iteration 11500: Loss = -11827.493336604317
6
Iteration 11600: Loss = -11827.50667255893
7
Iteration 11700: Loss = -11827.491685043176
8
Iteration 11800: Loss = -11827.491714357659
9
Iteration 11900: Loss = -11827.49154446408
Iteration 12000: Loss = -11827.495206745949
1
Iteration 12100: Loss = -11827.491655342103
2
Iteration 12200: Loss = -11827.500767018282
3
Iteration 12300: Loss = -11827.49162251323
4
Iteration 12400: Loss = -11827.495008162849
5
Iteration 12500: Loss = -11827.509551280022
6
Iteration 12600: Loss = -11827.490751502333
Iteration 12700: Loss = -11827.4973195769
1
Iteration 12800: Loss = -11827.497466884392
2
Iteration 12900: Loss = -11827.506749443983
3
Iteration 13000: Loss = -11827.49369967208
4
Iteration 13100: Loss = -11827.49074856472
Iteration 13200: Loss = -11827.492760856116
1
Iteration 13300: Loss = -11827.544501003415
2
Iteration 13400: Loss = -11827.495298388776
3
Iteration 13500: Loss = -11827.491461208621
4
Iteration 13600: Loss = -11827.492695447021
5
Iteration 13700: Loss = -11827.490933938792
6
Iteration 13800: Loss = -11827.492760652032
7
Iteration 13900: Loss = -11827.519374179266
8
Iteration 14000: Loss = -11827.490482747171
Iteration 14100: Loss = -11827.49097417402
1
Iteration 14200: Loss = -11827.493084989004
2
Iteration 14300: Loss = -11827.490623032514
3
Iteration 14400: Loss = -11827.524459069688
4
Iteration 14500: Loss = -11827.491642529672
5
Iteration 14600: Loss = -11827.496642643107
6
Iteration 14700: Loss = -11827.498024760444
7
Iteration 14800: Loss = -11827.490421525146
Iteration 14900: Loss = -11827.503171148623
1
Iteration 15000: Loss = -11827.490975180346
2
Iteration 15100: Loss = -11827.526635640885
3
Iteration 15200: Loss = -11827.500868637737
4
Iteration 15300: Loss = -11827.547664616826
5
Iteration 15400: Loss = -11827.490424083791
6
Iteration 15500: Loss = -11827.515750273304
7
Iteration 15600: Loss = -11827.4969998247
8
Iteration 15700: Loss = -11827.490521107798
9
Iteration 15800: Loss = -11827.504697648947
10
Stopping early at iteration 15800 due to no improvement.
tensor([[  8.7239, -10.9035],
        [  8.3650, -10.6095],
        [  8.3595, -10.0394],
        [  7.2482, -10.7044],
        [  7.9388,  -9.3738],
        [ -9.0945,   7.2860],
        [ -4.2457,   2.6111],
        [  6.4342,  -8.1280],
        [  8.2452,  -9.7494],
        [ -8.5730,   6.9731],
        [  6.6783,  -8.1198],
        [ -7.6649,   6.2362],
        [ -6.2026,   4.3337],
        [  8.6937, -10.1030],
        [ -8.6184,   6.8631],
        [ -8.3885,   6.1433],
        [  8.0234,  -9.8096],
        [  7.8716,  -9.4677],
        [  7.8315,  -9.2793],
        [  8.3911, -11.0891],
        [  7.7650,  -9.6661],
        [-10.2488,   8.0231],
        [-10.3108,   8.7509],
        [ -5.0870,   3.4203],
        [  7.2769,  -8.8896],
        [  8.0079,  -9.8748],
        [  6.9245,  -8.3910],
        [ -5.0806,   3.0686],
        [ -7.8970,   5.1185],
        [  6.6755,  -9.4877],
        [  4.4520,  -8.7185],
        [ -8.4655,   7.0542],
        [  6.9307,  -8.8731],
        [ -7.2557,   4.5726],
        [  7.4711,  -8.9048],
        [ -8.2553,   6.4467],
        [  5.0357,  -7.3612],
        [  7.8076,  -9.2401],
        [ -7.7342,   6.3288],
        [  8.2229,  -9.8274],
        [  6.7832,  -8.2366],
        [  8.4428, -10.0365],
        [  7.2915, -10.6960],
        [ -8.6136,   7.2271],
        [  5.8366,  -7.2738],
        [  6.3535,  -7.7590],
        [ -8.7009,   7.2068],
        [  4.0849,  -5.7187],
        [ -9.4616,   7.9762],
        [ -8.1571,   6.5215],
        [  7.9602,  -9.3885],
        [  4.5061,  -7.7934],
        [ -4.9773,   2.9485],
        [  7.5240, -10.7642],
        [  4.7467,  -6.1340],
        [ -5.0073,   3.3933],
        [  8.0223,  -9.4789],
        [ -5.6296,   3.8315],
        [  6.4714,  -9.1088],
        [-10.2260,   8.1762],
        [  7.7985,  -9.5835],
        [  8.1200,  -9.5679],
        [ -5.3401,   2.9343],
        [  7.7054, -10.1928],
        [  6.3341,  -7.7421],
        [  3.2798,  -5.4601],
        [ -8.6285,   6.3428],
        [ -8.7551,   7.0715],
        [  7.1814,  -9.0465],
        [ -5.7165,   3.8692],
        [ -9.9502,   7.4722],
        [ -9.0579,   7.5981],
        [  7.3902,  -9.9712],
        [  7.4011,  -8.8049],
        [-10.0938,   7.7910],
        [  6.5414,  -9.8964],
        [ -8.1984,   6.4235],
        [  5.7425,  -7.5209],
        [ -9.0558,   7.4746],
        [ -9.0551,   7.5018],
        [  8.4090, -10.2196],
        [  7.8869,  -9.9039],
        [  7.9207, -10.5700],
        [  7.4482,  -9.6658],
        [  7.5475,  -9.2257],
        [ -8.3695,   6.2784],
        [  4.4984,  -5.8960],
        [ -7.9481,   6.5581],
        [  8.1374,  -9.5361],
        [  8.0384, -10.3177],
        [ -8.9443,   5.8554],
        [  7.1114, -10.0710],
        [  7.6401,  -9.9366],
        [  7.2940, -10.5548],
        [ -9.1210,   7.3534],
        [  8.1581, -10.1971],
        [  8.0407,  -9.5327],
        [ -6.8031,   4.6828],
        [  7.5611,  -9.2640],
        [ -6.7685,   4.7400]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4885, 0.5115],
        [0.3981, 0.6019]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6000, 0.4000], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4077, 0.1041],
         [0.5092, 0.2102]],

        [[0.3783, 0.1087],
         [0.4728, 0.4536]],

        [[0.7730, 0.0992],
         [0.4526, 0.4443]],

        [[0.3884, 0.0973],
         [0.2099, 0.6083]],

        [[0.4745, 0.1075],
         [0.0259, 0.3855]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 75
Adjusted Rand Index: 0.2437309420197389
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4889791248930344
Average Adjusted Rand Index: 0.8487461884039478
11664.421020332491
new:  [1.0, 1.0, 0.4946067249221185, 0.4889791248930344] [1.0, 1.0, 0.8448183828575786, 0.8487461884039478] [11656.89936934801, 11656.89505408958, 11827.376374747706, 11827.504697648947]
prior:  [0.9919999860917265, 0.02903138056831896, 0.9919999860917265, 0.9919999860917265] [0.9919995611635631, 0.8907327477903181, 0.9919995611635631, 0.9919995611635631] [11659.96887716503, 11994.22205641411, 11659.96887716503, 11659.96887716503]
-----------------------------------------------------------------------------------------
This iteration is 18
True Objective function: Loss = -11316.677548412514
Iteration 0: Loss = -23075.265440970223
Iteration 10: Loss = -11496.69646944425
Iteration 20: Loss = -11496.534244064725
Iteration 30: Loss = -11496.534221810742
Iteration 40: Loss = -11496.534233065602
1
Iteration 50: Loss = -11496.534237089172
2
Iteration 60: Loss = -11496.534237089172
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6335, 0.3665],
        [0.4755, 0.5245]], dtype=torch.float64)
alpha: tensor([0.5755, 0.4245])
beta: tensor([[[0.2198, 0.1061],
         [0.6579, 0.3791]],

        [[0.3652, 0.0893],
         [0.7829, 0.9389]],

        [[0.0451, 0.1017],
         [0.1114, 0.0523]],

        [[0.5643, 0.0948],
         [0.2055, 0.8572]],

        [[0.9170, 0.0874],
         [0.4276, 0.2696]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 17
Adjusted Rand Index: 0.4305125790394188
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4450427429033501
Average Adjusted Rand Index: 0.8861025158078839
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23061.556582697103
Iteration 100: Loss = -11967.656796157713
Iteration 200: Loss = -11329.316290282211
Iteration 300: Loss = -11308.768407013265
Iteration 400: Loss = -11308.187714360069
Iteration 500: Loss = -11307.931938303745
Iteration 600: Loss = -11307.807436008949
Iteration 700: Loss = -11307.731452374137
Iteration 800: Loss = -11307.680361933566
Iteration 900: Loss = -11307.643877492528
Iteration 1000: Loss = -11307.616832654257
Iteration 1100: Loss = -11307.596094542738
Iteration 1200: Loss = -11307.579812360602
Iteration 1300: Loss = -11307.566595897295
Iteration 1400: Loss = -11307.556127389702
Iteration 1500: Loss = -11307.547480163612
Iteration 1600: Loss = -11307.540285687708
Iteration 1700: Loss = -11307.534126873812
Iteration 1800: Loss = -11307.528945642753
Iteration 1900: Loss = -11307.524453053358
Iteration 2000: Loss = -11307.520541537453
Iteration 2100: Loss = -11307.517154216257
Iteration 2200: Loss = -11307.514146231106
Iteration 2300: Loss = -11307.511537563603
Iteration 2400: Loss = -11307.511417480117
Iteration 2500: Loss = -11307.507058526164
Iteration 2600: Loss = -11307.505164850663
Iteration 2700: Loss = -11307.503562072005
Iteration 2800: Loss = -11307.501868830974
Iteration 2900: Loss = -11307.500426006709
Iteration 3000: Loss = -11307.499142164088
Iteration 3100: Loss = -11307.498029691287
Iteration 3200: Loss = -11307.49697830376
Iteration 3300: Loss = -11307.496059153187
Iteration 3400: Loss = -11307.495171605422
Iteration 3500: Loss = -11307.494417061482
Iteration 3600: Loss = -11307.493674401761
Iteration 3700: Loss = -11307.49300840221
Iteration 3800: Loss = -11307.492385641486
Iteration 3900: Loss = -11307.49181808102
Iteration 4000: Loss = -11307.491301712944
Iteration 4100: Loss = -11307.490751437379
Iteration 4200: Loss = -11307.490364530522
Iteration 4300: Loss = -11307.489922394341
Iteration 4400: Loss = -11307.48946760014
Iteration 4500: Loss = -11307.50077929307
1
Iteration 4600: Loss = -11307.488766271863
Iteration 4700: Loss = -11307.4883878087
Iteration 4800: Loss = -11307.488410415948
1
Iteration 4900: Loss = -11307.487724553881
Iteration 5000: Loss = -11307.487579724499
Iteration 5100: Loss = -11307.486878863481
Iteration 5200: Loss = -11307.486120290418
Iteration 5300: Loss = -11307.487307339077
1
Iteration 5400: Loss = -11307.485486550202
Iteration 5500: Loss = -11307.489913073086
1
Iteration 5600: Loss = -11307.485025196305
Iteration 5700: Loss = -11307.505184862162
1
Iteration 5800: Loss = -11307.484681122161
Iteration 5900: Loss = -11307.484553066543
Iteration 6000: Loss = -11307.48638902755
1
Iteration 6100: Loss = -11307.48431128831
Iteration 6200: Loss = -11307.484182278764
Iteration 6300: Loss = -11307.486965945916
1
Iteration 6400: Loss = -11307.483974263807
Iteration 6500: Loss = -11307.483899310128
Iteration 6600: Loss = -11307.484152394294
1
Iteration 6700: Loss = -11307.485527588724
2
Iteration 6800: Loss = -11307.48371958788
Iteration 6900: Loss = -11307.483549577151
Iteration 7000: Loss = -11307.601837655013
1
Iteration 7100: Loss = -11307.483384306752
Iteration 7200: Loss = -11307.48620096621
1
Iteration 7300: Loss = -11307.483287685132
Iteration 7400: Loss = -11307.483289323833
1
Iteration 7500: Loss = -11307.504498943026
2
Iteration 7600: Loss = -11307.483101054131
Iteration 7700: Loss = -11307.620640987254
1
Iteration 7800: Loss = -11307.48300734822
Iteration 7900: Loss = -11307.522010695006
1
Iteration 8000: Loss = -11307.482921596245
Iteration 8100: Loss = -11307.483044965107
1
Iteration 8200: Loss = -11307.524975515138
2
Iteration 8300: Loss = -11307.51990756552
3
Iteration 8400: Loss = -11307.482842040095
Iteration 8500: Loss = -11307.483683178543
1
Iteration 8600: Loss = -11307.482977830114
2
Iteration 8700: Loss = -11307.485371883731
3
Iteration 8800: Loss = -11307.4834675487
4
Iteration 8900: Loss = -11307.483260524452
5
Iteration 9000: Loss = -11307.48272771178
Iteration 9100: Loss = -11307.482640662578
Iteration 9200: Loss = -11307.483941822564
1
Iteration 9300: Loss = -11307.482565956028
Iteration 9400: Loss = -11307.484322194117
1
Iteration 9500: Loss = -11307.49022377271
2
Iteration 9600: Loss = -11307.485087632273
3
Iteration 9700: Loss = -11307.489357176912
4
Iteration 9800: Loss = -11307.482495273616
Iteration 9900: Loss = -11307.483807837689
1
Iteration 10000: Loss = -11307.49045659297
2
Iteration 10100: Loss = -11307.483022604642
3
Iteration 10200: Loss = -11307.483854014083
4
Iteration 10300: Loss = -11307.528719822529
5
Iteration 10400: Loss = -11307.51352749552
6
Iteration 10500: Loss = -11307.511150187049
7
Iteration 10600: Loss = -11307.484956821783
8
Iteration 10700: Loss = -11307.543582980594
9
Iteration 10800: Loss = -11307.482144956406
Iteration 10900: Loss = -11307.482683165106
1
Iteration 11000: Loss = -11307.48236457399
2
Iteration 11100: Loss = -11307.503780736442
3
Iteration 11200: Loss = -11307.53456046706
4
Iteration 11300: Loss = -11307.495578183212
5
Iteration 11400: Loss = -11307.490802669661
6
Iteration 11500: Loss = -11307.496027525936
7
Iteration 11600: Loss = -11307.485217953863
8
Iteration 11700: Loss = -11307.485685279706
9
Iteration 11800: Loss = -11307.481963615905
Iteration 11900: Loss = -11307.540097126512
1
Iteration 12000: Loss = -11307.547780168683
2
Iteration 12100: Loss = -11307.483969200197
3
Iteration 12200: Loss = -11307.483953815341
4
Iteration 12300: Loss = -11307.481821776131
Iteration 12400: Loss = -11307.482567192907
1
Iteration 12500: Loss = -11307.482380996335
2
Iteration 12600: Loss = -11307.483538982047
3
Iteration 12700: Loss = -11307.48220090456
4
Iteration 12800: Loss = -11307.561653987133
5
Iteration 12900: Loss = -11307.484154680275
6
Iteration 13000: Loss = -11307.606548140071
7
Iteration 13100: Loss = -11307.489945886999
8
Iteration 13200: Loss = -11307.482090847374
9
Iteration 13300: Loss = -11307.483914568587
10
Stopping early at iteration 13300 due to no improvement.
tensor([[-11.8164,   7.2012],
        [ -6.3040,   1.6888],
        [  4.2014,  -8.8166],
        [ -9.1080,   4.4928],
        [  0.5697,  -5.1850],
        [ -5.9496,   1.3344],
        [ -9.4631,   4.8479],
        [ -9.6118,   4.9966],
        [  3.6442,  -8.2594],
        [ -6.7968,   2.1816],
        [ -8.7479,   4.1327],
        [ -8.0694,   3.4542],
        [  4.9215,  -9.5367],
        [ -5.8389,   1.2237],
        [  5.1497,  -9.7649],
        [ -7.1133,   2.4981],
        [ -8.1079,   3.4927],
        [-10.3709,   5.7556],
        [ -8.1579,   3.5427],
        [ -1.7065,  -2.9087],
        [ -9.1963,   4.5811],
        [  5.8222, -10.4374],
        [ -2.2008,  -2.4144],
        [ -6.4014,   1.7862],
        [-10.2111,   5.5959],
        [ -7.4479,   2.8326],
        [  3.5875,  -8.2028],
        [ -9.1182,   4.5029],
        [  2.7185,  -7.3337],
        [ -9.2279,   4.6126],
        [  3.3055,  -7.9208],
        [ -7.2783,   2.6630],
        [  3.9191,  -8.5343],
        [  5.6732, -10.2884],
        [  4.4714,  -9.0866],
        [ -8.4282,   3.8130],
        [  4.9430,  -9.5582],
        [ -5.4888,   0.8735],
        [ -6.6978,   2.0826],
        [  5.6001, -10.2153],
        [ -7.3497,   2.7345],
        [  5.6129, -10.2281],
        [  5.9192, -10.5344],
        [ -9.1877,   4.5724],
        [ -9.6243,   5.0091],
        [  5.4625, -10.0777],
        [  3.0953,  -7.7106],
        [ -5.8794,   1.2642],
        [  2.5005,  -7.1157],
        [ -4.8029,   0.1877],
        [  4.6543,  -9.2695],
        [ -6.2123,   1.5971],
        [ -9.9683,   5.3531],
        [  4.4266,  -9.0418],
        [ -6.5349,   1.9197],
        [-10.1383,   5.5231],
        [  2.9230,  -7.5383],
        [ -9.9172,   5.3020],
        [  4.0687,  -8.6839],
        [  5.5278, -10.1430],
        [ -7.3868,   2.7716],
        [  5.5737, -10.1890],
        [ -7.6038,   2.9886],
        [-10.8184,   6.2031],
        [ -8.6980,   4.0828],
        [  5.4210, -10.0362],
        [-11.3478,   6.7326],
        [ -6.7509,   2.1356],
        [ -5.2222,   0.6070],
        [ -7.5633,   2.9480],
        [ -8.0256,   3.4104],
        [ -7.5146,   2.8994],
        [  6.2747, -10.8899],
        [  5.6526, -10.2678],
        [ -4.0281,  -0.5871],
        [ -6.2017,   1.5865],
        [  5.0818,  -9.6970],
        [ -9.4164,   4.8011],
        [  2.4382,  -7.0534],
        [ -0.3820,  -4.2332],
        [ -6.6681,   2.0528],
        [  5.1219,  -9.7371],
        [  6.0070, -10.6222],
        [  5.5749, -10.1901],
        [ -9.2631,   4.6479],
        [ -5.5274,   0.9122],
        [-10.5890,   5.9738],
        [ -9.8152,   5.2000],
        [ -7.2822,   2.6670],
        [ -8.5864,   3.9712],
        [  0.2040,  -4.8192],
        [ -8.5827,   3.9675],
        [  5.3234,  -9.9386],
        [ -7.2444,   2.6292],
        [ -5.3710,   0.7558],
        [  6.2284, -10.8436],
        [-10.8097,   6.1944],
        [-10.0148,   5.3995],
        [ -7.1505,   2.5353],
        [ -8.0227,   3.4075]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7527, 0.2473],
        [0.2359, 0.7641]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3834, 0.6166], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4084, 0.1067],
         [0.6579, 0.1968]],

        [[0.3652, 0.0956],
         [0.7829, 0.9389]],

        [[0.0451, 0.1018],
         [0.1114, 0.0523]],

        [[0.5643, 0.0948],
         [0.2055, 0.8572]],

        [[0.9170, 0.0875],
         [0.4276, 0.2696]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599252625159036
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919997667277575
Average Adjusted Rand Index: 0.9919850525031807
Iteration 0: Loss = -35846.232158579034
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.0757,    nan]],

        [[0.1540,    nan],
         [0.4380, 0.2737]],

        [[0.6857,    nan],
         [0.2767, 0.4464]],

        [[0.9917,    nan],
         [0.3814, 0.2420]],

        [[0.2449,    nan],
         [0.3550, 0.5945]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35842.924314562595
Iteration 100: Loss = -12092.454583146897
Iteration 200: Loss = -12088.873779835392
Iteration 300: Loss = -12086.971760277456
Iteration 400: Loss = -12085.80424168721
Iteration 500: Loss = -12085.017040254132
Iteration 600: Loss = -12084.715244032946
Iteration 700: Loss = -12084.063217092353
Iteration 800: Loss = -12083.75456002376
Iteration 900: Loss = -12083.495381707075
Iteration 1000: Loss = -12083.21908475841
Iteration 1100: Loss = -12081.209705263336
Iteration 1200: Loss = -12074.646190099511
Iteration 1300: Loss = -12061.480583515602
Iteration 1400: Loss = -12049.417484467956
Iteration 1500: Loss = -12035.041243280813
Iteration 1600: Loss = -12015.608916949934
Iteration 1700: Loss = -11986.461234996928
Iteration 1800: Loss = -11913.085666065199
Iteration 1900: Loss = -11857.25005167203
Iteration 2000: Loss = -11808.437241455582
Iteration 2100: Loss = -11773.591922142085
Iteration 2200: Loss = -11760.247948717473
Iteration 2300: Loss = -11750.8101460656
Iteration 2400: Loss = -11736.93204016515
Iteration 2500: Loss = -11733.524633774989
Iteration 2600: Loss = -11725.965452763818
Iteration 2700: Loss = -11725.740344128186
Iteration 2800: Loss = -11725.372002636006
Iteration 2900: Loss = -11724.238220045712
Iteration 3000: Loss = -11715.640205098101
Iteration 3100: Loss = -11702.194046678822
Iteration 3200: Loss = -11699.94488964658
Iteration 3300: Loss = -11699.399081775644
Iteration 3400: Loss = -11693.310078981218
Iteration 3500: Loss = -11693.043129018926
Iteration 3600: Loss = -11686.313539902168
Iteration 3700: Loss = -11685.52007367144
Iteration 3800: Loss = -11685.849425056576
1
Iteration 3900: Loss = -11685.409049841288
Iteration 4000: Loss = -11685.385660165595
Iteration 4100: Loss = -11685.365961248812
Iteration 4200: Loss = -11685.513531806311
1
Iteration 4300: Loss = -11685.333771519889
Iteration 4400: Loss = -11685.320294194451
Iteration 4500: Loss = -11685.308054805992
Iteration 4600: Loss = -11685.29699365107
Iteration 4700: Loss = -11685.285955259938
Iteration 4800: Loss = -11685.350983868191
1
Iteration 4900: Loss = -11685.260717718342
Iteration 5000: Loss = -11678.210390580905
Iteration 5100: Loss = -11675.66481842445
Iteration 5200: Loss = -11675.64118360616
Iteration 5300: Loss = -11675.837607924117
1
Iteration 5400: Loss = -11675.610467052335
Iteration 5500: Loss = -11675.605324733462
Iteration 5600: Loss = -11675.600992441721
Iteration 5700: Loss = -11675.597355350274
Iteration 5800: Loss = -11675.622156515867
1
Iteration 5900: Loss = -11675.589652667431
Iteration 6000: Loss = -11675.586578351027
Iteration 6100: Loss = -11675.589503641324
1
Iteration 6200: Loss = -11675.580681136798
Iteration 6300: Loss = -11675.57814935735
Iteration 6400: Loss = -11675.577946636955
Iteration 6500: Loss = -11675.573410677742
Iteration 6600: Loss = -11675.574898112662
1
Iteration 6700: Loss = -11675.568785204416
Iteration 6800: Loss = -11675.566707471602
Iteration 6900: Loss = -11675.57551779096
1
Iteration 7000: Loss = -11675.56623541419
Iteration 7100: Loss = -11675.559601927087
Iteration 7200: Loss = -11675.52518350499
Iteration 7300: Loss = -11675.376598799356
Iteration 7400: Loss = -11675.377554376331
1
Iteration 7500: Loss = -11675.370127630598
Iteration 7600: Loss = -11675.376123894544
1
Iteration 7700: Loss = -11675.414105192804
2
Iteration 7800: Loss = -11675.349502207904
Iteration 7900: Loss = -11666.679552460619
Iteration 8000: Loss = -11666.677203119472
Iteration 8100: Loss = -11666.677196337328
Iteration 8200: Loss = -11666.678681913398
1
Iteration 8300: Loss = -11666.673858243394
Iteration 8400: Loss = -11666.727024775346
1
Iteration 8500: Loss = -11666.668226979105
Iteration 8600: Loss = -11662.937311007337
Iteration 8700: Loss = -11662.959098265448
1
Iteration 8800: Loss = -11662.933659679811
Iteration 8900: Loss = -11662.933350446987
Iteration 9000: Loss = -11662.9392336067
1
Iteration 9100: Loss = -11662.910996370389
Iteration 9200: Loss = -11662.918385211105
1
Iteration 9300: Loss = -11662.909774122345
Iteration 9400: Loss = -11662.909251457862
Iteration 9500: Loss = -11662.908694906164
Iteration 9600: Loss = -11662.911065364804
1
Iteration 9700: Loss = -11662.910192532094
2
Iteration 9800: Loss = -11662.907595513403
Iteration 9900: Loss = -11662.908265763492
1
Iteration 10000: Loss = -11662.907731810716
2
Iteration 10100: Loss = -11662.91359121614
3
Iteration 10200: Loss = -11662.936473513175
4
Iteration 10300: Loss = -11662.905937410656
Iteration 10400: Loss = -11662.905966542605
1
Iteration 10500: Loss = -11662.926978498852
2
Iteration 10600: Loss = -11662.905298229307
Iteration 10700: Loss = -11662.90518749251
Iteration 10800: Loss = -11663.202576548467
1
Iteration 10900: Loss = -11662.906246892748
2
Iteration 11000: Loss = -11659.346064860287
Iteration 11100: Loss = -11659.375347393425
1
Iteration 11200: Loss = -11659.350651233566
2
Iteration 11300: Loss = -11659.341508039066
Iteration 11400: Loss = -11659.381062984023
1
Iteration 11500: Loss = -11659.347981148312
2
Iteration 11600: Loss = -11659.345379187522
3
Iteration 11700: Loss = -11659.34159196138
4
Iteration 11800: Loss = -11659.396233400978
5
Iteration 11900: Loss = -11659.340550080366
Iteration 12000: Loss = -11659.357857382947
1
Iteration 12100: Loss = -11659.340423410482
Iteration 12200: Loss = -11659.340288224203
Iteration 12300: Loss = -11659.362157353682
1
Iteration 12400: Loss = -11659.41473410685
2
Iteration 12500: Loss = -11659.446432489722
3
Iteration 12600: Loss = -11659.34212152906
4
Iteration 12700: Loss = -11659.333333802939
Iteration 12800: Loss = -11659.330362848072
Iteration 12900: Loss = -11645.11790676212
Iteration 13000: Loss = -11633.338510520567
Iteration 13100: Loss = -11633.337609233926
Iteration 13200: Loss = -11633.34508471256
1
Iteration 13300: Loss = -11633.339630703756
2
Iteration 13400: Loss = -11633.338055452312
3
Iteration 13500: Loss = -11618.323847498154
Iteration 13600: Loss = -11618.31129316478
Iteration 13700: Loss = -11618.30882766226
Iteration 13800: Loss = -11618.309099168695
1
Iteration 13900: Loss = -11618.39025741345
2
Iteration 14000: Loss = -11618.308639801602
Iteration 14100: Loss = -11618.30913982876
1
Iteration 14200: Loss = -11618.31603364792
2
Iteration 14300: Loss = -11618.307896549984
Iteration 14400: Loss = -11618.332590431772
1
Iteration 14500: Loss = -11618.322986183262
2
Iteration 14600: Loss = -11618.337228963203
3
Iteration 14700: Loss = -11618.325610008456
4
Iteration 14800: Loss = -11618.301410818625
Iteration 14900: Loss = -11618.310320685727
1
Iteration 15000: Loss = -11618.322851366664
2
Iteration 15100: Loss = -11618.31633773361
3
Iteration 15200: Loss = -11618.303780886368
4
Iteration 15300: Loss = -11618.303964235252
5
Iteration 15400: Loss = -11618.30168935972
6
Iteration 15500: Loss = -11618.303137254854
7
Iteration 15600: Loss = -11609.990805013724
Iteration 15700: Loss = -11609.961189047164
Iteration 15800: Loss = -11609.952639079549
Iteration 15900: Loss = -11609.950937384352
Iteration 16000: Loss = -11610.150685313745
1
Iteration 16100: Loss = -11609.951128560586
2
Iteration 16200: Loss = -11609.965179982586
3
Iteration 16300: Loss = -11609.950826153743
Iteration 16400: Loss = -11609.967694069173
1
Iteration 16500: Loss = -11609.950775375757
Iteration 16600: Loss = -11609.962265350981
1
Iteration 16700: Loss = -11609.953362600765
2
Iteration 16800: Loss = -11609.951626463926
3
Iteration 16900: Loss = -11610.071391099109
4
Iteration 17000: Loss = -11609.958280710774
5
Iteration 17100: Loss = -11609.956628387285
6
Iteration 17200: Loss = -11609.952257877416
7
Iteration 17300: Loss = -11609.951591079498
8
Iteration 17400: Loss = -11609.965778934795
9
Iteration 17500: Loss = -11609.952504252133
10
Stopping early at iteration 17500 due to no improvement.
tensor([[ -3.7221,   0.9657],
        [ -1.6042,   0.0285],
        [ -9.5583,   7.7552],
        [ -3.5813,   2.1900],
        [ -8.9519,   7.3975],
        [ -3.6740,   1.8979],
        [ -4.1320,   2.6748],
        [ -6.5545,   4.5543],
        [ -5.5415,   3.6815],
        [ -2.2123,   0.7273],
        [ -3.1808,   1.7693],
        [ -5.7144,   3.8942],
        [ -7.1360,   5.7232],
        [ -0.3064,  -1.1922],
        [-10.0754,   7.9390],
        [ -3.3484,   1.4484],
        [ -6.8039,   5.1777],
        [ -4.9067,   0.8674],
        [ -5.8970,   3.2978],
        [ -4.3974,   2.7764],
        [ -3.3978,   1.9666],
        [ -8.4436,   7.0359],
        [ -5.8163,   4.3040],
        [ -2.5362,   0.7302],
        [ -5.6415,   3.3387],
        [ -3.9210,   0.6034],
        [ -5.3867,   3.2594],
        [ -2.9482,   1.2575],
        [ -8.9554,   7.5130],
        [ -2.5429,   0.4058],
        [ -9.1874,   7.0797],
        [ -3.4506,   2.0586],
        [ -5.5930,   4.1289],
        [ -7.5657,   5.9607],
        [ -9.6690,   8.2701],
        [ -6.1680,   4.6867],
        [ -4.5043,   3.0799],
        [ -3.4547,   1.7930],
        [ -7.2134,   5.3180],
        [ -9.8700,   7.1981],
        [ -4.9975,   3.1912],
        [ -9.0286,   7.2612],
        [-10.2181,   7.1370],
        [ -4.0308,   1.1114],
        [ -3.0632,   0.9233],
        [ -9.3224,   7.9181],
        [ -8.2502,   6.7996],
        [ -5.8496,   4.0548],
        [ -9.6486,   7.5650],
        [ -1.4443,  -0.4628],
        [ -7.1802,   5.7696],
        [ -4.3735,   2.9582],
        [ -4.8094,   3.4220],
        [ -8.5452,   7.1266],
        [ -4.7324,   1.4264],
        [ -3.5812,   1.1788],
        [ -7.3903,   5.8348],
        [ -2.8064,   1.1611],
        [ -8.6540,   7.2360],
        [ -8.4805,   6.9987],
        [ -5.8341,   4.2106],
        [ -9.9527,   7.8087],
        [ -2.8908,   1.4763],
        [ -7.5658,   5.4386],
        [  2.8023,  -4.2519],
        [ -9.7679,   8.0095],
        [ -6.2692,   4.7025],
        [ -6.7509,   5.2831],
        [ -4.6078,   2.8752],
        [ -7.6969,   5.8992],
        [ -2.1830,   0.7746],
        [ -3.9737,   2.4035],
        [-11.3205,   6.7052],
        [ -4.6515,   3.0883],
        [ -5.3849,   3.6540],
        [ -9.0934,   7.1948],
        [ -7.5047,   4.2305],
        [  2.3736,  -4.3646],
        [ -5.7090,   3.4672],
        [ -5.5643,   3.0973],
        [ -4.4839,   1.5451],
        [ -8.6275,   7.0160],
        [ -9.7788,   7.9095],
        [ -6.3390,   4.9528],
        [ -1.8534,   0.4417],
        [ -4.4895,   3.0064],
        [ -6.3847,   4.9237],
        [ -5.5097,   2.4186],
        [ -4.9286,   2.8208],
        [ -4.9587,   3.0223],
        [ -7.3058,   5.8726],
        [ -4.6723,   3.2696],
        [ -7.7695,   6.3112],
        [ -2.4697,   0.9517],
        [ -5.5300,   4.1385],
        [ -9.6850,   7.2360],
        [ -5.9615,   4.5630],
        [ -6.3938,   4.9113],
        [ -2.7144,   1.3201],
        [ -6.2607,   3.0868]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5104, 0.4896],
        [0.3785, 0.6215]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0362, 0.9638], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3971, 0.0832],
         [0.0757, 0.2031]],

        [[0.1540, 0.0984],
         [0.4380, 0.2737]],

        [[0.6857, 0.1073],
         [0.2767, 0.4464]],

        [[0.9917, 0.0956],
         [0.3814, 0.2420]],

        [[0.2449, 0.0965],
         [0.3550, 0.5945]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: -0.020678759622205528
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 68
Adjusted Rand Index: 0.11873642354847024
Global Adjusted Rand Index: 0.3029273336416777
Average Adjusted Rand Index: 0.5956096520058353
Iteration 0: Loss = -18967.766624874886
Iteration 10: Loss = -11861.910992999
Iteration 20: Loss = -11310.296873736586
Iteration 30: Loss = -11310.296914543625
1
Iteration 40: Loss = -11310.296914549148
2
Iteration 50: Loss = -11310.296914549148
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7490, 0.2510],
        [0.2384, 0.7616]], dtype=torch.float64)
alpha: tensor([0.4565, 0.5435])
beta: tensor([[[0.4000, 0.1065],
         [0.9546, 0.1931]],

        [[0.1251, 0.0961],
         [0.2853, 0.7884]],

        [[0.6953, 0.1019],
         [0.0938, 0.1690]],

        [[0.3881, 0.0953],
         [0.2908, 0.1621]],

        [[0.5904, 0.0875],
         [0.4878, 0.7638]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18968.0079377254
Iteration 100: Loss = -12074.351851507148
Iteration 200: Loss = -11775.784865161335
Iteration 300: Loss = -11628.008683178485
Iteration 400: Loss = -11604.239287886623
Iteration 500: Loss = -11601.875327885273
Iteration 600: Loss = -11601.30173502804
Iteration 700: Loss = -11601.044164785995
Iteration 800: Loss = -11600.869906304923
Iteration 900: Loss = -11600.737569791292
Iteration 1000: Loss = -11600.626213387532
Iteration 1100: Loss = -11596.59935857843
Iteration 1200: Loss = -11594.676847974988
Iteration 1300: Loss = -11587.109919972254
Iteration 1400: Loss = -11586.928011513555
Iteration 1500: Loss = -11586.88021275807
Iteration 1600: Loss = -11586.85061324313
Iteration 1700: Loss = -11586.829315454688
Iteration 1800: Loss = -11586.812712033112
Iteration 1900: Loss = -11586.799351253816
Iteration 2000: Loss = -11586.788296298388
Iteration 2100: Loss = -11586.778904113327
Iteration 2200: Loss = -11586.770931522133
Iteration 2300: Loss = -11586.763977769418
Iteration 2400: Loss = -11586.757891627998
Iteration 2500: Loss = -11586.752525631406
Iteration 2600: Loss = -11586.747704681187
Iteration 2700: Loss = -11586.743445400343
Iteration 2800: Loss = -11586.739476382165
Iteration 2900: Loss = -11586.735790271921
Iteration 3000: Loss = -11586.731802817965
Iteration 3100: Loss = -11586.72467409328
Iteration 3200: Loss = -11586.717818466183
Iteration 3300: Loss = -11586.714680901125
Iteration 3400: Loss = -11586.712261610803
Iteration 3500: Loss = -11586.710109698859
Iteration 3600: Loss = -11586.70824047147
Iteration 3700: Loss = -11586.7065076496
Iteration 3800: Loss = -11586.704908903079
Iteration 3900: Loss = -11586.703468662963
Iteration 4000: Loss = -11586.702300632533
Iteration 4100: Loss = -11586.700982383438
Iteration 4200: Loss = -11586.699929290506
Iteration 4300: Loss = -11586.704257655938
1
Iteration 4400: Loss = -11586.697631007959
Iteration 4500: Loss = -11586.696699029164
Iteration 4600: Loss = -11586.695300041492
Iteration 4700: Loss = -11586.692202867811
Iteration 4800: Loss = -11586.685439829227
Iteration 4900: Loss = -11586.688130685783
1
Iteration 5000: Loss = -11586.68365015764
Iteration 5100: Loss = -11586.682895744556
Iteration 5200: Loss = -11586.68289856812
1
Iteration 5300: Loss = -11586.681500086388
Iteration 5400: Loss = -11586.701163858887
1
Iteration 5500: Loss = -11586.680423740796
Iteration 5600: Loss = -11586.679880897542
Iteration 5700: Loss = -11586.679402262307
Iteration 5800: Loss = -11586.678812484344
Iteration 5900: Loss = -11586.677878084975
Iteration 6000: Loss = -11586.67617876401
Iteration 6100: Loss = -11586.67489445204
Iteration 6200: Loss = -11586.673390904845
Iteration 6300: Loss = -11586.674254466776
1
Iteration 6400: Loss = -11586.672672391138
Iteration 6500: Loss = -11586.683576068148
1
Iteration 6600: Loss = -11586.680517211276
2
Iteration 6700: Loss = -11586.6714801402
Iteration 6800: Loss = -11586.671168480732
Iteration 6900: Loss = -11586.671365020096
1
Iteration 7000: Loss = -11586.670458254985
Iteration 7100: Loss = -11586.66955944499
Iteration 7200: Loss = -11586.713319856675
1
Iteration 7300: Loss = -11586.668194501857
Iteration 7400: Loss = -11586.66513133214
Iteration 7500: Loss = -11586.665205674148
1
Iteration 7600: Loss = -11586.66484750981
Iteration 7700: Loss = -11586.677356747285
1
Iteration 7800: Loss = -11586.664559867962
Iteration 7900: Loss = -11586.668838883777
1
Iteration 8000: Loss = -11586.669977140671
2
Iteration 8100: Loss = -11586.664496264411
Iteration 8200: Loss = -11586.672376119823
1
Iteration 8300: Loss = -11586.692612632161
2
Iteration 8400: Loss = -11586.663680526368
Iteration 8500: Loss = -11586.696628165446
1
Iteration 8600: Loss = -11586.663436468953
Iteration 8700: Loss = -11586.66635939493
1
Iteration 8800: Loss = -11586.663276678535
Iteration 8900: Loss = -11586.67955759243
1
Iteration 9000: Loss = -11586.663435426764
2
Iteration 9100: Loss = -11586.66679203709
3
Iteration 9200: Loss = -11586.661516880857
Iteration 9300: Loss = -11586.661532561759
1
Iteration 9400: Loss = -11586.661423169115
Iteration 9500: Loss = -11586.661874538175
1
Iteration 9600: Loss = -11586.678792558496
2
Iteration 9700: Loss = -11586.66125273005
Iteration 9800: Loss = -11586.673247899298
1
Iteration 9900: Loss = -11586.661153625173
Iteration 10000: Loss = -11586.661279740572
1
Iteration 10100: Loss = -11586.661715489705
2
Iteration 10200: Loss = -11586.661026099284
Iteration 10300: Loss = -11586.66850970008
1
Iteration 10400: Loss = -11586.740699974021
2
Iteration 10500: Loss = -11586.660948025989
Iteration 10600: Loss = -11586.66170641932
1
Iteration 10700: Loss = -11586.66325236283
2
Iteration 10800: Loss = -11586.660884286563
Iteration 10900: Loss = -11586.718379629194
1
Iteration 11000: Loss = -11586.665022885729
2
Iteration 11100: Loss = -11586.661689292077
3
Iteration 11200: Loss = -11586.665693826022
4
Iteration 11300: Loss = -11586.667505261401
5
Iteration 11400: Loss = -11586.66114589176
6
Iteration 11500: Loss = -11586.661739128449
7
Iteration 11600: Loss = -11586.688175648682
8
Iteration 11700: Loss = -11586.747665345798
9
Iteration 11800: Loss = -11586.66489303416
10
Stopping early at iteration 11800 due to no improvement.
tensor([[ -2.7141,   1.1528],
        [  0.0145,  -1.4980],
        [ -7.9073,   6.1501],
        [ -3.1263,   1.7234],
        [ -9.6990,   7.9279],
        [ -2.4978,   0.5441],
        [ -3.1564,   1.2173],
        [ -6.8305,   2.8855],
        [ -4.8330,   3.4303],
        [ -2.0317,   0.4994],
        [ -2.7195,   1.2168],
        [ -5.0392,   1.7407],
        [ -5.6676,   4.1997],
        [  0.7343,  -2.2483],
        [ -8.0921,   6.2746],
        [ -2.9227,   1.2548],
        [ -5.8544,   4.3283],
        [ -3.2018,   1.8026],
        [ -4.4028,   2.6353],
        [ -3.5135,   1.7320],
        [ -4.4111,   0.9472],
        [ -8.2025,   6.4014],
        [ -4.4597,   3.0733],
        [ -1.6227,   0.2084],
        [ -4.9891,   2.8183],
        [ -3.8547,   0.7406],
        [ -4.9232,   3.5024],
        [ -2.3729,   0.4197],
        [ -3.9874,   2.5893],
        [ -2.2793,   0.2639],
        [ -4.9136,   3.4892],
        [ -3.4672,   2.0733],
        [ -5.5436,   3.9086],
        [ -8.5991,   3.9839],
        [-10.0349,   6.8343],
        [ -4.6325,   3.1490],
        [ -4.4048,   2.3940],
        [ -3.9014,  -0.2077],
        [ -5.7577,   3.4275],
        [ -7.7128,   5.8145],
        [ -4.3513,   2.9646],
        [-10.8770,   6.2617],
        [ -8.9880,   7.5241],
        [ -2.7102,   0.9414],
        [ -2.5318,   0.9442],
        [ -7.6266,   5.4724],
        [ -8.7729,   6.2486],
        [ -4.4930,   2.6588],
        [ -4.8613,   3.3651],
        [ -0.9985,  -0.3937],
        [ -6.8321,   5.3022],
        [ -4.3451,   2.7404],
        [ -4.1299,   2.6901],
        [ -8.3402,   6.0632],
        [ -4.9198,   0.3046],
        [ -2.7392,   1.0633],
        [ -6.8410,   4.8243],
        [ -2.8425,   0.6638],
        [ -6.7574,   5.3272],
        [ -8.2346,   6.8205],
        [ -6.2830,   2.4831],
        [ -7.5254,   6.0985],
        [ -3.0420,   1.3736],
        [ -6.3591,   4.8765],
        [  3.5850,  -5.0200],
        [ -8.7848,   6.9169],
        [ -5.6652,   3.7488],
        [ -6.0265,   4.5835],
        [ -5.0243,   2.3128],
        [ -7.3148,   5.9150],
        [ -1.9945,   0.5627],
        [ -3.4175,   1.8355],
        [ -8.1851,   6.6968],
        [ -4.4892,   3.0996],
        [ -4.6880,   3.2967],
        [ -9.5508,   7.2309],
        [ -6.5472,   4.6976],
        [  3.0759,  -4.4680],
        [ -4.9648,   3.2138],
        [ -5.0948,   3.3799],
        [ -4.5277,   0.8733],
        [ -7.7320,   5.4674],
        [ -8.2370,   5.9216],
        [ -5.7447,   4.2895],
        [ -0.3669,  -1.0490],
        [ -4.4600,   2.8101],
        [ -5.5385,   4.1522],
        [ -4.4093,   2.2893],
        [ -3.2344,   1.7476],
        [ -4.9183,   1.8420],
        [ -7.0874,   4.7584],
        [ -4.8494,   2.0470],
        [ -8.7455,   7.3011],
        [ -2.5412,   1.1151],
        [ -3.7731,   2.2209],
        [ -8.3633,   6.9079],
        [ -5.5268,   1.9114],
        [ -5.5867,   4.0836],
        [ -2.8174,   0.7639],
        [ -5.0785,   2.9702]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3681, 0.6319],
        [0.4369, 0.5631]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0559, 0.9441], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4021, 0.0940],
         [0.9546, 0.2078]],

        [[0.1251, 0.0959],
         [0.2853, 0.7884]],

        [[0.6953, 0.1062],
         [0.0938, 0.1690]],

        [[0.3881, 0.0966],
         [0.2908, 0.1621]],

        [[0.5904, 0.0877],
         [0.4878, 0.7638]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.030334812424364664
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 74
Adjusted Rand Index: 0.22409191428789305
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.2769411011421268
Average Adjusted Rand Index: 0.6307507620999957
Iteration 0: Loss = -21146.863106296954
Iteration 10: Loss = -11310.277145865315
Iteration 20: Loss = -11310.296914593853
1
Iteration 30: Loss = -11310.296913729871
2
Iteration 40: Loss = -11310.296913729871
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7616, 0.2384],
        [0.2510, 0.7490]], dtype=torch.float64)
alpha: tensor([0.5435, 0.4565])
beta: tensor([[[0.1931, 0.1065],
         [0.4106, 0.4000]],

        [[0.2643, 0.0961],
         [0.7581, 0.0315]],

        [[0.5654, 0.1019],
         [0.3353, 0.1212]],

        [[0.9138, 0.0953],
         [0.1937, 0.1721]],

        [[0.2087, 0.0875],
         [0.8369, 0.5010]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21146.545290611008
Iteration 100: Loss = -11708.858243486535
Iteration 200: Loss = -11620.822943556142
Iteration 300: Loss = -11611.625596520287
Iteration 400: Loss = -11605.125657158256
Iteration 500: Loss = -11599.223532146301
Iteration 600: Loss = -11598.83594841139
Iteration 700: Loss = -11598.73805654742
Iteration 800: Loss = -11598.691752039342
Iteration 900: Loss = -11598.659521792306
Iteration 1000: Loss = -11598.63564943847
Iteration 1100: Loss = -11598.617220022425
Iteration 1200: Loss = -11598.603135781721
Iteration 1300: Loss = -11598.591553759283
Iteration 1400: Loss = -11598.582084410744
Iteration 1500: Loss = -11598.57386018526
Iteration 1600: Loss = -11598.567234385573
Iteration 1700: Loss = -11598.561898558213
Iteration 1800: Loss = -11598.55739657426
Iteration 1900: Loss = -11598.553571601276
Iteration 2000: Loss = -11598.550283404747
Iteration 2100: Loss = -11598.547406836777
Iteration 2200: Loss = -11598.546680206367
Iteration 2300: Loss = -11598.5425917361
Iteration 2400: Loss = -11598.54063642151
Iteration 2500: Loss = -11598.53911874108
Iteration 2600: Loss = -11598.53734775728
Iteration 2700: Loss = -11598.535969156406
Iteration 2800: Loss = -11598.534708720772
Iteration 2900: Loss = -11598.533596530939
Iteration 3000: Loss = -11598.532533157084
Iteration 3100: Loss = -11598.53150635923
Iteration 3200: Loss = -11598.530327509841
Iteration 3300: Loss = -11598.527831592666
Iteration 3400: Loss = -11598.361527131194
Iteration 3500: Loss = -11598.359811140914
Iteration 3600: Loss = -11598.35912177711
Iteration 3700: Loss = -11598.358520906357
Iteration 3800: Loss = -11598.360063842605
1
Iteration 3900: Loss = -11598.357323597607
Iteration 4000: Loss = -11598.356950592442
Iteration 4100: Loss = -11598.356320379226
Iteration 4200: Loss = -11598.355818051677
Iteration 4300: Loss = -11598.355186167142
Iteration 4400: Loss = -11598.361757046445
1
Iteration 4500: Loss = -11598.351852496557
Iteration 4600: Loss = -11596.545322493894
Iteration 4700: Loss = -11596.549374548325
1
Iteration 4800: Loss = -11596.540530042705
Iteration 4900: Loss = -11596.522986707952
Iteration 5000: Loss = -11596.507602275864
Iteration 5100: Loss = -11596.507247112375
Iteration 5200: Loss = -11596.506889035882
Iteration 5300: Loss = -11596.506750736515
Iteration 5400: Loss = -11596.50655740208
Iteration 5500: Loss = -11596.506547732488
Iteration 5600: Loss = -11596.506388107711
Iteration 5700: Loss = -11596.506386732231
Iteration 5800: Loss = -11596.513562288394
1
Iteration 5900: Loss = -11596.505795494391
Iteration 6000: Loss = -11596.50647258091
1
Iteration 6100: Loss = -11596.51253087486
2
Iteration 6200: Loss = -11596.505568484092
Iteration 6300: Loss = -11596.50542570849
Iteration 6400: Loss = -11596.505465564122
1
Iteration 6500: Loss = -11596.505469593718
2
Iteration 6600: Loss = -11596.507423873703
3
Iteration 6700: Loss = -11596.505127080161
Iteration 6800: Loss = -11596.505004360439
Iteration 6900: Loss = -11596.505025706674
1
Iteration 7000: Loss = -11596.508911903073
2
Iteration 7100: Loss = -11596.560456067375
3
Iteration 7200: Loss = -11596.50477080615
Iteration 7300: Loss = -11596.508217140654
1
Iteration 7400: Loss = -11596.504658337315
Iteration 7500: Loss = -11596.504695462037
1
Iteration 7600: Loss = -11596.504632557026
Iteration 7700: Loss = -11596.504502938647
Iteration 7800: Loss = -11596.50450634099
1
Iteration 7900: Loss = -11596.504494279909
Iteration 8000: Loss = -11596.504426547639
Iteration 8100: Loss = -11596.506712536337
1
Iteration 8200: Loss = -11596.504364848193
Iteration 8300: Loss = -11596.505086215127
1
Iteration 8400: Loss = -11596.505601948851
2
Iteration 8500: Loss = -11596.507518087254
3
Iteration 8600: Loss = -11596.504307868214
Iteration 8700: Loss = -11596.504284045672
Iteration 8800: Loss = -11596.50438243
1
Iteration 8900: Loss = -11596.504201353544
Iteration 9000: Loss = -11596.504147989572
Iteration 9100: Loss = -11596.505010742394
1
Iteration 9200: Loss = -11596.504123913042
Iteration 9300: Loss = -11596.677695730326
1
Iteration 9400: Loss = -11596.504064569182
Iteration 9500: Loss = -11596.50399612762
Iteration 9600: Loss = -11596.508495622238
1
Iteration 9700: Loss = -11596.503862216296
Iteration 9800: Loss = -11596.48026247711
Iteration 9900: Loss = -11596.47927150178
Iteration 10000: Loss = -11596.520755115645
1
Iteration 10100: Loss = -11596.478438747528
Iteration 10200: Loss = -11596.493198431128
1
Iteration 10300: Loss = -11596.478419442992
Iteration 10400: Loss = -11596.496580142051
1
Iteration 10500: Loss = -11596.478439722016
2
Iteration 10600: Loss = -11596.64658910082
3
Iteration 10700: Loss = -11596.478407714838
Iteration 10800: Loss = -11596.478390895145
Iteration 10900: Loss = -11596.47899294201
1
Iteration 11000: Loss = -11596.478387141038
Iteration 11100: Loss = -11596.68923845459
1
Iteration 11200: Loss = -11596.478372211594
Iteration 11300: Loss = -11596.478355546567
Iteration 11400: Loss = -11596.484306346854
1
Iteration 11500: Loss = -11596.478348633835
Iteration 11600: Loss = -11596.494082173422
1
Iteration 11700: Loss = -11596.478367359754
2
Iteration 11800: Loss = -11596.479659407361
3
Iteration 11900: Loss = -11596.478428654042
4
Iteration 12000: Loss = -11596.484831957845
5
Iteration 12100: Loss = -11596.478358156124
6
Iteration 12200: Loss = -11596.513039089192
7
Iteration 12300: Loss = -11596.474248587234
Iteration 12400: Loss = -11596.474190304087
Iteration 12500: Loss = -11596.491744671746
1
Iteration 12600: Loss = -11596.474177951433
Iteration 12700: Loss = -11596.474192247784
1
Iteration 12800: Loss = -11596.47518574365
2
Iteration 12900: Loss = -11596.474178452623
3
Iteration 13000: Loss = -11596.504516591505
4
Iteration 13100: Loss = -11596.47420760922
5
Iteration 13200: Loss = -11596.509309160285
6
Iteration 13300: Loss = -11596.490480584893
7
Iteration 13400: Loss = -11596.478899587817
8
Iteration 13500: Loss = -11596.474007878196
Iteration 13600: Loss = -11596.48282077957
1
Iteration 13700: Loss = -11596.47485492874
2
Iteration 13800: Loss = -11596.474139120974
3
Iteration 13900: Loss = -11596.475039789259
4
Iteration 14000: Loss = -11596.473945614052
Iteration 14100: Loss = -11596.488243883035
1
Iteration 14200: Loss = -11596.475248778916
2
Iteration 14300: Loss = -11596.474102196442
3
Iteration 14400: Loss = -11596.474366719787
4
Iteration 14500: Loss = -11596.477239640311
5
Iteration 14600: Loss = -11596.476119727604
6
Iteration 14700: Loss = -11596.499670173687
7
Iteration 14800: Loss = -11596.483668225272
8
Iteration 14900: Loss = -11596.474334540175
9
Iteration 15000: Loss = -11596.436253105574
Iteration 15100: Loss = -11596.437991170882
1
Iteration 15200: Loss = -11596.455275374155
2
Iteration 15300: Loss = -11596.437936267232
3
Iteration 15400: Loss = -11596.436436399192
4
Iteration 15500: Loss = -11596.436235749909
Iteration 15600: Loss = -11596.435999702118
Iteration 15700: Loss = -11596.471014235398
1
Iteration 15800: Loss = -11596.436837129007
2
Iteration 15900: Loss = -11596.4361198004
3
Iteration 16000: Loss = -11596.436152697406
4
Iteration 16100: Loss = -11596.437141082584
5
Iteration 16200: Loss = -11596.477535462953
6
Iteration 16300: Loss = -11596.435955133846
Iteration 16400: Loss = -11596.436570681779
1
Iteration 16500: Loss = -11596.435971125198
2
Iteration 16600: Loss = -11596.436125692098
3
Iteration 16700: Loss = -11596.435954687931
Iteration 16800: Loss = -11596.44547202528
1
Iteration 16900: Loss = -11596.435951859961
Iteration 17000: Loss = -11596.484184144592
1
Iteration 17100: Loss = -11596.435941564974
Iteration 17200: Loss = -11596.435949905557
1
Iteration 17300: Loss = -11596.436268765352
2
Iteration 17400: Loss = -11596.435933678504
Iteration 17500: Loss = -11596.456091531843
1
Iteration 17600: Loss = -11596.43597452351
2
Iteration 17700: Loss = -11596.436923682017
3
Iteration 17800: Loss = -11596.455639726932
4
Iteration 17900: Loss = -11596.435960928278
5
Iteration 18000: Loss = -11596.455738806424
6
Iteration 18100: Loss = -11596.436182609732
7
Iteration 18200: Loss = -11596.439294481375
8
Iteration 18300: Loss = -11596.435955442728
9
Iteration 18400: Loss = -11596.436098282025
10
Stopping early at iteration 18400 due to no improvement.
tensor([[  5.8284,  -7.8323],
        [ -2.2972,   0.8919],
        [ -6.4566,   5.0684],
        [  4.0706,  -5.7880],
        [ -4.1558,   2.1706],
        [ -0.8817,  -0.5886],
        [  4.3378,  -6.5271],
        [  5.4941,  -7.0837],
        [ -9.5169,   5.6441],
        [  1.2218,  -2.8187],
        [  1.8362,  -3.2947],
        [  3.5328,  -5.5024],
        [ -9.1893,   7.7914],
        [ -2.7641,   0.7399],
        [ -9.2720,   7.7097],
        [  0.5876,  -3.1802],
        [  3.4563,  -5.1950],
        [  5.4360,  -6.8315],
        [  3.0334,  -4.9845],
        [ -6.5926,   3.1613],
        [  5.8261,  -7.6334],
        [-12.9166,   9.1946],
        [ -2.3297,   0.8741],
        [  2.7405,  -4.3185],
        [  4.8769,  -8.1224],
        [  9.3897, -10.8659],
        [ -6.5693,   5.1650],
        [  5.0967,  -7.3049],
        [ -6.3437,   4.6409],
        [  3.2166,  -4.7917],
        [ -7.5540,   3.9453],
        [  3.4273,  -5.0188],
        [ -8.9858,   7.1195],
        [ -9.7176,   7.7944],
        [ -8.8411,   7.3302],
        [  3.4748,  -6.4499],
        [-11.0828,   6.8872],
        [  2.1883,  -3.7406],
        [  1.6251,  -3.0291],
        [-11.7303,   7.6591],
        [  1.6452,  -3.1121],
        [ -9.5740,   8.1869],
        [ -9.1641,   7.3874],
        [  4.9091,  -8.3936],
        [  4.1439,  -5.9364],
        [ -9.2849,   7.5556],
        [ -7.4183,   5.7259],
        [  0.3504,  -2.2526],
        [ -6.1706,   4.6023],
        [ -0.7296,  -1.3143],
        [ -8.1701,   6.4248],
        [  3.3086,  -4.7328],
        [  4.7434,  -6.7793],
        [-12.2613,   9.1710],
        [  1.0863,  -2.9503],
        [  6.7907,  -8.6527],
        [ -7.6317,   5.7493],
        [  4.2196,  -6.0053],
        [ -7.0627,   5.1358],
        [ -7.9542,   6.2652],
        [  2.6023,  -3.9955],
        [-10.2430,   7.4224],
        [  2.6510,  -7.1929],
        [  4.1139,  -7.4907],
        [  0.6593,  -2.4319],
        [-10.1130,   8.1298],
        [  6.5763,  -8.4421],
        [  9.0013, -10.3879],
        [  1.1474,  -3.2248],
        [  8.6683, -10.4386],
        [  2.4760,  -4.1312],
        [  3.5924,  -6.7252],
        [ -9.9770,   6.5215],
        [-10.3852,   8.9969],
        [ -0.6370,  -0.8258],
        [  2.7682,  -4.1596],
        [ -9.1430,   7.7532],
        [  1.8443,  -3.2306],
        [ -8.2858,   6.5070],
        [ -3.2581,   1.8495],
        [  1.3470,  -3.4618],
        [ -8.8231,   7.3992],
        [ -9.0629,   7.4136],
        [ -9.8167,   8.2711],
        [  2.3528,  -4.2427],
        [  1.8516,  -3.7709],
        [  4.9159,  -6.4523],
        [  5.0126,  -6.6432],
        [  0.8688,  -3.2182],
        [  2.8058,  -4.3057],
        [ -6.5180,   5.1174],
        [  2.7957,  -4.4458],
        [-10.2278,   8.0912],
        [  3.6048,  -5.1245],
        [ -1.8478,   0.0302],
        [ -4.6216,   1.6616],
        [  3.9297,  -8.5449],
        [  5.6158,  -7.1175],
        [  1.8722,  -3.3147],
        [  2.4741,  -4.4790]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4075, 0.5925],
        [0.6764, 0.3236]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5662, 0.4338], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2512, 0.1041],
         [0.4106, 0.3328]],

        [[0.2643, 0.0934],
         [0.7581, 0.0315]],

        [[0.5654, 0.0994],
         [0.3353, 0.1212]],

        [[0.9138, 0.0936],
         [0.1937, 0.1721]],

        [[0.2087, 0.0905],
         [0.8369, 0.5010]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8079912862954653
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 12
Adjusted Rand Index: 0.5734430082256169
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 14
Adjusted Rand Index: 0.5134896063688633
Global Adjusted Rand Index: 0.06993680890569262
Average Adjusted Rand Index: 0.7474673093656135
Iteration 0: Loss = -19215.413731178836
Iteration 10: Loss = -11310.280113723156
Iteration 20: Loss = -11310.29691375382
1
Iteration 30: Loss = -11310.296913729871
2
Iteration 40: Loss = -11310.296913729871
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7616, 0.2384],
        [0.2510, 0.7490]], dtype=torch.float64)
alpha: tensor([0.5435, 0.4565])
beta: tensor([[[0.1931, 0.1065],
         [0.4859, 0.4000]],

        [[0.5310, 0.0961],
         [0.9956, 0.4006]],

        [[0.4067, 0.1019],
         [0.3065, 0.9441]],

        [[0.1635, 0.0953],
         [0.0722, 0.9642]],

        [[0.3121, 0.0875],
         [0.6936, 0.9675]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19215.15182310672
Iteration 100: Loss = -12090.197572593617
Iteration 200: Loss = -12072.835305597016
Iteration 300: Loss = -12059.558666797708
Iteration 400: Loss = -12056.260978157743
Iteration 500: Loss = -12051.10574078084
Iteration 600: Loss = -12045.840883446326
Iteration 700: Loss = -12015.550216002162
Iteration 800: Loss = -11792.7960951137
Iteration 900: Loss = -11518.914954556116
Iteration 1000: Loss = -11380.077921645345
Iteration 1100: Loss = -11376.382974201782
Iteration 1200: Loss = -11376.077532199897
Iteration 1300: Loss = -11375.977822315108
Iteration 1400: Loss = -11375.918642529115
Iteration 1500: Loss = -11375.877510957122
Iteration 1600: Loss = -11375.848018081993
Iteration 1700: Loss = -11375.822818928218
Iteration 1800: Loss = -11375.801681432325
Iteration 1900: Loss = -11375.806999980217
1
Iteration 2000: Loss = -11375.772878937834
Iteration 2100: Loss = -11375.76225261578
Iteration 2200: Loss = -11375.753238901592
Iteration 2300: Loss = -11375.746087626963
Iteration 2400: Loss = -11375.738547264133
Iteration 2500: Loss = -11375.732403587233
Iteration 2600: Loss = -11375.729866429294
Iteration 2700: Loss = -11375.721470841325
Iteration 2800: Loss = -11375.716389329005
Iteration 2900: Loss = -11375.711955186285
Iteration 3000: Loss = -11375.708469929981
Iteration 3100: Loss = -11375.705169507686
Iteration 3200: Loss = -11375.702235935565
Iteration 3300: Loss = -11375.714550619583
1
Iteration 3400: Loss = -11375.693999115905
Iteration 3500: Loss = -11372.023249512167
Iteration 3600: Loss = -11371.977843941133
Iteration 3700: Loss = -11372.012796866367
1
Iteration 3800: Loss = -11371.97354630019
Iteration 3900: Loss = -11371.971836535253
Iteration 4000: Loss = -11372.00993058941
1
Iteration 4100: Loss = -11371.968766137383
Iteration 4200: Loss = -11371.974855597684
1
Iteration 4300: Loss = -11371.997104839287
2
Iteration 4400: Loss = -11371.971167544789
3
Iteration 4500: Loss = -11371.992039741626
4
Iteration 4600: Loss = -11371.982386723612
5
Iteration 4700: Loss = -11372.046823606268
6
Iteration 4800: Loss = -11371.962283871642
Iteration 4900: Loss = -11371.988931315482
1
Iteration 5000: Loss = -11371.960746190809
Iteration 5100: Loss = -11371.960096287296
Iteration 5200: Loss = -11372.046413523685
1
Iteration 5300: Loss = -11371.953276202028
Iteration 5400: Loss = -11354.66951712009
Iteration 5500: Loss = -11354.669950533827
1
Iteration 5600: Loss = -11354.671036162605
2
Iteration 5700: Loss = -11354.678417872645
3
Iteration 5800: Loss = -11354.774077596558
4
Iteration 5900: Loss = -11354.665330102544
Iteration 6000: Loss = -11354.663964925605
Iteration 6100: Loss = -11354.667476907656
1
Iteration 6200: Loss = -11354.668402434902
2
Iteration 6300: Loss = -11354.662385698542
Iteration 6400: Loss = -11340.974757514725
Iteration 6500: Loss = -11340.968152462805
Iteration 6600: Loss = -11340.965848541551
Iteration 6700: Loss = -11340.966494496319
1
Iteration 6800: Loss = -11341.023236072358
2
Iteration 6900: Loss = -11340.965060126759
Iteration 7000: Loss = -11340.96490187048
Iteration 7100: Loss = -11341.001801720538
1
Iteration 7200: Loss = -11340.984800320817
2
Iteration 7300: Loss = -11340.967974114714
3
Iteration 7400: Loss = -11340.972339929313
4
Iteration 7500: Loss = -11340.96613925675
5
Iteration 7600: Loss = -11340.971972887253
6
Iteration 7700: Loss = -11340.969490382056
7
Iteration 7800: Loss = -11340.964496014743
Iteration 7900: Loss = -11340.963894374418
Iteration 8000: Loss = -11340.964275038368
1
Iteration 8100: Loss = -11341.111553593206
2
Iteration 8200: Loss = -11340.964709613332
3
Iteration 8300: Loss = -11341.013154385522
4
Iteration 8400: Loss = -11341.015326624665
5
Iteration 8500: Loss = -11340.977184343896
6
Iteration 8600: Loss = -11340.97115379658
7
Iteration 8700: Loss = -11340.963930405782
8
Iteration 8800: Loss = -11340.962940775777
Iteration 8900: Loss = -11340.96312789545
1
Iteration 9000: Loss = -11340.967201630865
2
Iteration 9100: Loss = -11340.96712124322
3
Iteration 9200: Loss = -11340.962688158417
Iteration 9300: Loss = -11341.112488304165
1
Iteration 9400: Loss = -11340.96118357413
Iteration 9500: Loss = -11340.960410950023
Iteration 9600: Loss = -11340.96485045358
1
Iteration 9700: Loss = -11340.967050309486
2
Iteration 9800: Loss = -11341.133437984174
3
Iteration 9900: Loss = -11340.951009365926
Iteration 10000: Loss = -11340.951236325895
1
Iteration 10100: Loss = -11340.952196572161
2
Iteration 10200: Loss = -11340.956817084609
3
Iteration 10300: Loss = -11340.97405213154
4
Iteration 10400: Loss = -11340.966565941562
5
Iteration 10500: Loss = -11340.950871234723
Iteration 10600: Loss = -11340.951695675545
1
Iteration 10700: Loss = -11340.953269692835
2
Iteration 10800: Loss = -11340.951643990957
3
Iteration 10900: Loss = -11340.951084539136
4
Iteration 11000: Loss = -11340.952652342054
5
Iteration 11100: Loss = -11340.961897305611
6
Iteration 11200: Loss = -11341.00441016002
7
Iteration 11300: Loss = -11340.953092298187
8
Iteration 11400: Loss = -11340.951397933477
9
Iteration 11500: Loss = -11340.951018336682
10
Stopping early at iteration 11500 due to no improvement.
tensor([[  7.9345,  -9.3208],
        [  5.3474,  -7.0765],
        [ -3.6524,   2.1122],
        [  6.5518,  -9.4259],
        [ -2.9555,   1.5434],
        [  2.2678,  -3.8240],
        [  8.4597, -10.3532],
        [  7.9335,  -9.4317],
        [  6.4842,  -8.0562],
        [  7.0411, -10.0254],
        [  8.1147,  -9.9830],
        [  7.5484, -10.7599],
        [ -7.5897,   6.0654],
        [  7.6623,  -9.0532],
        [ -6.3875,   4.9171],
        [  8.5507, -10.0709],
        [  7.0308,  -9.8833],
        [  7.7985, -10.0300],
        [  3.2444,  -4.6870],
        [  6.6243,  -8.1214],
        [  4.0503,  -6.0471],
        [ -8.1075,   6.7160],
        [  0.3718,  -1.8542],
        [  2.4310,  -3.8177],
        [  7.3374,  -8.7903],
        [  2.7782,  -4.1701],
        [ -6.0375,   3.5215],
        [  5.0835,  -6.5713],
        [ -3.6987,   2.2546],
        [  7.8055, -10.6874],
        [ -4.9645,   2.4982],
        [  2.2968,  -5.5260],
        [ -3.0103,   1.6154],
        [ -6.5511,   4.1429],
        [ -8.2135,   5.4610],
        [  8.3795,  -9.7939],
        [ -6.9928,   5.6048],
        [  2.9172,  -5.1052],
        [  7.3786,  -8.8422],
        [ -6.8773,   4.9093],
        [  8.3295,  -9.8036],
        [ -7.7710,   6.2537],
        [ -7.5459,   6.1195],
        [  4.3541,  -5.7531],
        [  6.7214,  -9.6890],
        [ -5.8381,   4.3959],
        [ -4.9255,   3.2677],
        [  6.5070,  -7.9644],
        [ -5.2549,   2.8529],
        [  1.5855,  -3.1049],
        [ -7.3686,   5.9531],
        [  2.7155,  -4.2127],
        [  6.2288,  -8.7413],
        [ -6.6180,   5.2315],
        [  3.8824,  -5.6374],
        [  6.9469,  -8.4612],
        [  7.9005,  -9.4152],
        [  8.3233,  -9.9908],
        [ -5.9581,   3.9124],
        [ -7.3487,   5.5754],
        [  7.7266,  -9.8257],
        [ -9.7004,   5.1521],
        [  2.5071,  -5.0225],
        [  8.0981, -10.2598],
        [  7.4289,  -8.9772],
        [ -7.2543,   5.3100],
        [  5.9743,  -7.3611],
        [  1.7738,  -3.5395],
        [  0.2509,  -4.6647],
        [  3.0565,  -4.4875],
        [  7.8741,  -9.4422],
        [  4.4891,  -6.4666],
        [ -8.4995,   6.9109],
        [ -6.1789,   4.4321],
        [  6.9407,  -8.5790],
        [  1.4039,  -2.8191],
        [ -8.2578,   5.9684],
        [  8.1384,  -9.5264],
        [ -4.8366,   2.2944],
        [ -1.4046,  -0.2792],
        [  7.9615,  -9.3574],
        [ -8.5841,   5.8592],
        [ -7.2981,   5.0012],
        [  7.2291,  -8.6211],
        [  7.7396,  -9.1322],
        [  2.6885,  -4.0831],
        [  8.1331,  -9.6754],
        [  8.3802,  -9.8358],
        [  2.4379,  -3.9427],
        [  7.0849, -10.2012],
        [  7.3973,  -8.8247],
        [  7.1018,  -8.8390],
        [ -7.6069,   6.2205],
        [  2.3681,  -4.6629],
        [  7.1221,  -8.6639],
        [ -3.6852,   1.6881],
        [  8.3536,  -9.7660],
        [  8.4375, -10.0191],
        [  7.3769,  -9.1285],
        [  8.0262,  -9.4130]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7669, 0.2331],
        [0.2280, 0.7720]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6712, 0.3288], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1928, 0.1267],
         [0.4859, 0.4109]],

        [[0.5310, 0.0960],
         [0.9956, 0.4006]],

        [[0.4067, 0.1017],
         [0.3065, 0.9441]],

        [[0.1635, 0.0949],
         [0.0722, 0.9642]],

        [[0.3121, 0.0875],
         [0.6936, 0.9675]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.807091453542207
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603184297205622
Average Adjusted Rand Index: 0.9614182907084414
11316.677548412514
new:  [0.3029273336416777, 0.2769411011421268, 0.06993680890569262, 0.9603184297205622] [0.5956096520058353, 0.6307507620999957, 0.7474673093656135, 0.9614182907084414] [11609.952504252133, 11586.66489303416, 11596.436098282025, 11340.951018336682]
prior:  [0.0, 1.0, 1.0, 1.0] [0.0, 1.0, 1.0, 1.0] [nan, 11310.296914549148, 11310.296913729871, 11310.296913729871]
-----------------------------------------------------------------------------------------
This iteration is 19
True Objective function: Loss = -11388.322789415199
Iteration 0: Loss = -33795.04367501509
Iteration 10: Loss = -11377.126581768905
Iteration 20: Loss = -11377.124627919467
Iteration 30: Loss = -11377.124719755646
1
Iteration 40: Loss = -11377.124722831739
2
Iteration 50: Loss = -11377.124722827044
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7440, 0.2560],
        [0.3104, 0.6896]], dtype=torch.float64)
alpha: tensor([0.5155, 0.4845])
beta: tensor([[[0.1880, 0.0995],
         [0.1685, 0.3994]],

        [[0.2257, 0.0929],
         [0.0756, 0.8065]],

        [[0.9869, 0.1051],
         [0.4831, 0.4766]],

        [[0.9353, 0.1041],
         [0.5754, 0.5129]],

        [[0.3254, 0.0850],
         [0.9356, 0.5420]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320404224087
Average Adjusted Rand Index: 0.9839993730966995
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33456.65713755162
Iteration 100: Loss = -12173.17084940369
Iteration 200: Loss = -12115.836502359027
Iteration 300: Loss = -11810.27038267749
Iteration 400: Loss = -11807.52830583017
Iteration 500: Loss = -11806.408114410362
Iteration 600: Loss = -11805.578096367799
Iteration 700: Loss = -11805.218686273798
Iteration 800: Loss = -11804.974309088531
Iteration 900: Loss = -11804.79925136858
Iteration 1000: Loss = -11804.668581346808
Iteration 1100: Loss = -11804.567488331653
Iteration 1200: Loss = -11804.487615914895
Iteration 1300: Loss = -11804.424278121878
Iteration 1400: Loss = -11804.372547832394
Iteration 1500: Loss = -11804.329350817343
Iteration 1600: Loss = -11804.29225617721
Iteration 1700: Loss = -11804.25666787135
Iteration 1800: Loss = -11804.223111075802
Iteration 1900: Loss = -11804.198917623438
Iteration 2000: Loss = -11804.178641541619
Iteration 2100: Loss = -11804.16072548218
Iteration 2200: Loss = -11804.143156305978
Iteration 2300: Loss = -11798.728483256169
Iteration 2400: Loss = -11797.965332118063
Iteration 2500: Loss = -11797.952050041229
Iteration 2600: Loss = -11797.941340370284
Iteration 2700: Loss = -11797.932101655742
Iteration 2800: Loss = -11797.923848866627
Iteration 2900: Loss = -11797.916424812542
Iteration 3000: Loss = -11797.909686936413
Iteration 3100: Loss = -11797.903708547428
Iteration 3200: Loss = -11797.898072992179
Iteration 3300: Loss = -11797.892981648689
Iteration 3400: Loss = -11797.88834797508
Iteration 3500: Loss = -11797.88409465607
Iteration 3600: Loss = -11797.880156597457
Iteration 3700: Loss = -11797.876624350545
Iteration 3800: Loss = -11797.87328895382
Iteration 3900: Loss = -11797.870227244532
Iteration 4000: Loss = -11797.867379189991
Iteration 4100: Loss = -11797.864715864704
Iteration 4200: Loss = -11797.86267437179
Iteration 4300: Loss = -11797.886181051726
1
Iteration 4400: Loss = -11797.862276270434
Iteration 4500: Loss = -11797.856743882372
Iteration 4600: Loss = -11797.855504134197
Iteration 4700: Loss = -11797.855690010869
1
Iteration 4800: Loss = -11797.850456106304
Iteration 4900: Loss = -11797.850642035428
1
Iteration 5000: Loss = -11797.89511624009
2
Iteration 5100: Loss = -11797.846049729613
Iteration 5200: Loss = -11797.848898888862
1
Iteration 5300: Loss = -11797.84794866848
2
Iteration 5400: Loss = -11797.841739439753
Iteration 5500: Loss = -11797.841990669336
1
Iteration 5600: Loss = -11797.840081510949
Iteration 5700: Loss = -11797.837558956731
Iteration 5800: Loss = -11797.836857985287
Iteration 5900: Loss = -11797.837610238825
1
Iteration 6000: Loss = -11797.835311568027
Iteration 6100: Loss = -11797.837570674577
1
Iteration 6200: Loss = -11797.837482331528
2
Iteration 6300: Loss = -11797.836312554457
3
Iteration 6400: Loss = -11797.832406609217
Iteration 6500: Loss = -11797.836058674864
1
Iteration 6600: Loss = -11797.831416605533
Iteration 6700: Loss = -11797.842481451189
1
Iteration 6800: Loss = -11797.831019876548
Iteration 6900: Loss = -11797.830755931851
Iteration 7000: Loss = -11797.829889464014
Iteration 7100: Loss = -11797.840456630054
1
Iteration 7200: Loss = -11797.828663393422
Iteration 7300: Loss = -11797.828256736695
Iteration 7400: Loss = -11797.838804892024
1
Iteration 7500: Loss = -11797.827213031283
Iteration 7600: Loss = -11797.825587786381
Iteration 7700: Loss = -11797.840602016626
1
Iteration 7800: Loss = -11797.823800357812
Iteration 7900: Loss = -11797.826616773435
1
Iteration 8000: Loss = -11797.825456329974
2
Iteration 8100: Loss = -11797.823000245508
Iteration 8200: Loss = -11797.823009344616
1
Iteration 8300: Loss = -11797.829981606068
2
Iteration 8400: Loss = -11797.823062510604
3
Iteration 8500: Loss = -11797.821096233549
Iteration 8600: Loss = -11797.821268124611
1
Iteration 8700: Loss = -11797.853123972593
2
Iteration 8800: Loss = -11797.820526235977
Iteration 8900: Loss = -11797.820391302663
Iteration 9000: Loss = -11797.82056206917
1
Iteration 9100: Loss = -11797.825861306115
2
Iteration 9200: Loss = -11797.828455470693
3
Iteration 9300: Loss = -11797.822217728582
4
Iteration 9400: Loss = -11797.828716511003
5
Iteration 9500: Loss = -11797.824234188254
6
Iteration 9600: Loss = -11797.819599765033
Iteration 9700: Loss = -11797.820396432322
1
Iteration 9800: Loss = -11797.8243147249
2
Iteration 9900: Loss = -11797.819333943642
Iteration 10000: Loss = -11797.817383056095
Iteration 10100: Loss = -11797.6940301193
Iteration 10200: Loss = -11797.691941086094
Iteration 10300: Loss = -11797.750201035733
1
Iteration 10400: Loss = -11797.69131686132
Iteration 10500: Loss = -11797.69005768353
Iteration 10600: Loss = -11797.692909169968
1
Iteration 10700: Loss = -11797.689915106985
Iteration 10800: Loss = -11797.689841676747
Iteration 10900: Loss = -11797.689894593997
1
Iteration 11000: Loss = -11797.689765594812
Iteration 11100: Loss = -11797.709888497113
1
Iteration 11200: Loss = -11797.689633248085
Iteration 11300: Loss = -11797.690509349628
1
Iteration 11400: Loss = -11797.689594773083
Iteration 11500: Loss = -11797.691251230735
1
Iteration 11600: Loss = -11797.689759358442
2
Iteration 11700: Loss = -11797.697930460172
3
Iteration 11800: Loss = -11797.690256329819
4
Iteration 11900: Loss = -11797.69121723349
5
Iteration 12000: Loss = -11797.703095151537
6
Iteration 12100: Loss = -11797.699534942321
7
Iteration 12200: Loss = -11797.69020472924
8
Iteration 12300: Loss = -11797.68933703185
Iteration 12400: Loss = -11797.724323504994
1
Iteration 12500: Loss = -11797.689226028066
Iteration 12600: Loss = -11797.68975594271
1
Iteration 12700: Loss = -11797.69369535517
2
Iteration 12800: Loss = -11797.740853049834
3
Iteration 12900: Loss = -11797.689286662338
4
Iteration 13000: Loss = -11797.689888460436
5
Iteration 13100: Loss = -11797.7097324611
6
Iteration 13200: Loss = -11797.700763494722
7
Iteration 13300: Loss = -11797.718529857792
8
Iteration 13400: Loss = -11797.784079109655
9
Iteration 13500: Loss = -11797.690462967757
10
Stopping early at iteration 13500 due to no improvement.
tensor([[-11.9259,   7.3107],
        [-10.1205,   5.5053],
        [-12.2729,   7.6577],
        [-12.2678,   7.6526],
        [-12.1023,   7.4871],
        [-11.8309,   7.2157],
        [-12.2090,   7.5938],
        [-12.2833,   7.6681],
        [-12.0038,   7.3886],
        [-12.3702,   7.7550],
        [-11.4130,   6.7978],
        [-11.5868,   6.9716],
        [ -9.6658,   5.0506],
        [-12.2383,   7.6231],
        [-12.2266,   7.6113],
        [-11.7347,   7.1194],
        [-11.6440,   7.0287],
        [ -9.2991,   4.6838],
        [ -9.6451,   5.0299],
        [-12.3158,   7.7006],
        [-10.1589,   5.5437],
        [-10.0474,   5.4322],
        [-12.3712,   7.7560],
        [-11.4543,   6.8391],
        [-11.7129,   7.0977],
        [-12.2911,   7.6759],
        [-11.4141,   6.7988],
        [-11.9294,   7.3142],
        [-12.6860,   8.0708],
        [-12.2269,   7.6117],
        [ -9.8302,   5.2150],
        [-11.9696,   7.3544],
        [-12.0343,   7.4191],
        [-11.8267,   7.2115],
        [-12.6459,   8.0307],
        [-11.7256,   7.1104],
        [-11.5905,   6.9752],
        [-12.2405,   7.6252],
        [-12.1049,   7.4897],
        [-12.2427,   7.6275],
        [-10.1270,   5.5118],
        [-11.9572,   7.3420],
        [-11.9190,   7.3038],
        [-11.7780,   7.1627],
        [-11.9870,   7.3717],
        [-10.2326,   5.6174],
        [-11.4538,   6.8386],
        [-10.1848,   5.5695],
        [-12.2162,   7.6010],
        [-11.9409,   7.3257],
        [-12.1577,   7.5425],
        [-10.1330,   5.5178],
        [-12.1170,   7.5017],
        [-11.3818,   6.7665],
        [-10.1199,   5.5047],
        [-12.1402,   7.5250],
        [-11.9779,   7.3627],
        [-11.8185,   7.2032],
        [-12.3268,   7.7115],
        [-12.1510,   7.5357],
        [-12.1469,   7.5317],
        [-10.1495,   5.5343],
        [-11.7128,   7.0976],
        [-12.3567,   7.7415],
        [-10.1704,   5.5552],
        [-12.0083,   7.3931],
        [-10.0418,   5.4266],
        [-10.0937,   5.4785],
        [ -9.8658,   5.2506],
        [-11.8453,   7.2301],
        [-12.0369,   7.4217],
        [-10.1517,   5.5364],
        [-12.4622,   7.8470],
        [-11.8554,   7.2401],
        [-11.8359,   7.2207],
        [ -9.2364,   4.6212],
        [-10.0882,   5.4730],
        [-11.9671,   7.3519],
        [-10.2942,   5.6790],
        [-12.1549,   7.5397],
        [ -9.8296,   5.2144],
        [-12.2245,   7.6093],
        [-10.1647,   5.5495],
        [-11.7848,   7.1696],
        [-12.2958,   7.6806],
        [-10.1196,   5.5044],
        [-12.5148,   7.8996],
        [-11.9485,   7.3333],
        [-11.7019,   7.0867],
        [-10.2546,   5.6393],
        [-11.9975,   7.3822],
        [-12.2809,   7.6657],
        [-10.2349,   5.6197],
        [-10.2432,   5.6280],
        [-12.1202,   7.5050],
        [-11.6410,   7.0258],
        [-11.9186,   7.3034],
        [-11.6911,   7.0759],
        [-11.9949,   7.3796],
        [-11.7792,   7.1640]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7185, 0.2815],
        [0.3928, 0.6072]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0182e-07, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1970, 0.4709],
         [0.1685, 0.2967]],

        [[0.2257, 0.0925],
         [0.0756, 0.8065]],

        [[0.9869, 0.1027],
         [0.4831, 0.4766]],

        [[0.9353, 0.1018],
         [0.5754, 0.5129]],

        [[0.3254, 0.0842],
         [0.9356, 0.5420]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.6392907046187845
Average Adjusted Rand Index: 0.7681589618018609
Iteration 0: Loss = -33981.03250104189
Iteration 10: Loss = -12177.65490057335
Iteration 20: Loss = -11565.005551312057
Iteration 30: Loss = -11566.664606328439
1
Iteration 40: Loss = -11565.68896806603
2
Iteration 50: Loss = -11564.661746266187
Iteration 60: Loss = -11564.413824332547
Iteration 70: Loss = -11564.352584629925
Iteration 80: Loss = -11564.337435009082
Iteration 90: Loss = -11564.333669314776
Iteration 100: Loss = -11564.332741553542
Iteration 110: Loss = -11564.332513203031
Iteration 120: Loss = -11564.332447111348
Iteration 130: Loss = -11564.332435900367
Iteration 140: Loss = -11564.33242710192
Iteration 150: Loss = -11564.332432555571
1
Iteration 160: Loss = -11564.332432555566
2
Iteration 170: Loss = -11564.332432555566
3
Stopping early at iteration 169 due to no improvement.
pi: tensor([[0.6017, 0.3983],
        [0.2644, 0.7356]], dtype=torch.float64)
alpha: tensor([0.4534, 0.5466])
beta: tensor([[[0.3845, 0.0994],
         [0.9276, 0.2029]],

        [[0.9500, 0.0927],
         [0.5442, 0.4441]],

        [[0.5873, 0.1045],
         [0.0339, 0.2553]],

        [[0.3683, 0.1037],
         [0.0671, 0.4680]],

        [[0.6105, 0.0970],
         [0.8667, 0.9748]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 67
Adjusted Rand Index: 0.10324103093543749
Global Adjusted Rand Index: 0.5232266451963831
Average Adjusted Rand Index: 0.8046475792837869
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33979.879362348795
Iteration 100: Loss = -12246.528137319536
Iteration 200: Loss = -12214.034837507163
Iteration 300: Loss = -12200.790621481208
Iteration 400: Loss = -12189.929474795224
Iteration 500: Loss = -12178.946324433115
Iteration 600: Loss = -12087.086989820531
Iteration 700: Loss = -12053.476993111619
Iteration 800: Loss = -12037.675232819418
Iteration 900: Loss = -12002.76269297077
Iteration 1000: Loss = -11957.08756037981
Iteration 1100: Loss = -11888.565937043255
Iteration 1200: Loss = -11867.897089976683
Iteration 1300: Loss = -11840.908363141592
Iteration 1400: Loss = -11840.56599759763
Iteration 1500: Loss = -11839.8027445389
Iteration 1600: Loss = -11834.76952231928
Iteration 1700: Loss = -11834.622810149649
Iteration 1800: Loss = -11834.552786459742
Iteration 1900: Loss = -11834.508217121378
Iteration 2000: Loss = -11834.453055447559
Iteration 2100: Loss = -11834.377619418245
Iteration 2200: Loss = -11834.188957532513
Iteration 2300: Loss = -11834.150157832022
Iteration 2400: Loss = -11834.127460635547
Iteration 2500: Loss = -11834.107678311791
Iteration 2600: Loss = -11834.090346929112
Iteration 2700: Loss = -11834.07535396512
Iteration 2800: Loss = -11834.062074236039
Iteration 2900: Loss = -11834.050180176131
Iteration 3000: Loss = -11834.039674896223
Iteration 3100: Loss = -11834.030062261569
Iteration 3200: Loss = -11834.022646998752
Iteration 3300: Loss = -11834.014724289449
Iteration 3400: Loss = -11834.008109020711
Iteration 3500: Loss = -11834.007666257374
Iteration 3600: Loss = -11833.996496595484
Iteration 3700: Loss = -11833.991412411686
Iteration 3800: Loss = -11833.986690513855
Iteration 3900: Loss = -11833.982298912837
Iteration 4000: Loss = -11833.978147916863
Iteration 4100: Loss = -11833.974263476948
Iteration 4200: Loss = -11833.97609597992
1
Iteration 4300: Loss = -11833.967232714102
Iteration 4400: Loss = -11833.963906721947
Iteration 4500: Loss = -11833.960776401116
Iteration 4600: Loss = -11833.959706742608
Iteration 4700: Loss = -11833.954523922934
Iteration 4800: Loss = -11833.95122196547
Iteration 4900: Loss = -11833.94682386214
Iteration 5000: Loss = -11833.94192543211
Iteration 5100: Loss = -11833.938188903821
Iteration 5200: Loss = -11833.934120569662
Iteration 5300: Loss = -11833.928908948163
Iteration 5400: Loss = -11833.920556145085
Iteration 5500: Loss = -11833.899775414122
Iteration 5600: Loss = -11827.161627680147
Iteration 5700: Loss = -11817.860836903057
Iteration 5800: Loss = -11805.287873674919
Iteration 5900: Loss = -11753.894471889162
Iteration 6000: Loss = -11735.430653545185
Iteration 6100: Loss = -11731.423838580306
Iteration 6200: Loss = -11717.905991228638
Iteration 6300: Loss = -11709.388041397722
Iteration 6400: Loss = -11709.281942115753
Iteration 6500: Loss = -11700.178278105133
Iteration 6600: Loss = -11700.110027231718
Iteration 6700: Loss = -11700.096237278807
Iteration 6800: Loss = -11699.994299171512
Iteration 6900: Loss = -11698.753824132455
Iteration 7000: Loss = -11698.739987190555
Iteration 7100: Loss = -11698.391825582761
Iteration 7200: Loss = -11698.367839476698
Iteration 7300: Loss = -11698.353259731659
Iteration 7400: Loss = -11698.348498418824
Iteration 7500: Loss = -11698.28905472547
Iteration 7600: Loss = -11698.289608991476
1
Iteration 7700: Loss = -11698.285559841763
Iteration 7800: Loss = -11698.26931793601
Iteration 7900: Loss = -11693.32971870794
Iteration 8000: Loss = -11693.330202416313
1
Iteration 8100: Loss = -11693.32192092767
Iteration 8200: Loss = -11693.318580029974
Iteration 8300: Loss = -11688.071164093442
Iteration 8400: Loss = -11680.017087192127
Iteration 8500: Loss = -11679.992822245393
Iteration 8600: Loss = -11679.987414842366
Iteration 8700: Loss = -11679.988897561545
1
Iteration 8800: Loss = -11680.026482217429
2
Iteration 8900: Loss = -11679.986961812509
Iteration 9000: Loss = -11679.983793177564
Iteration 9100: Loss = -11674.560671422025
Iteration 9200: Loss = -11674.533623525575
Iteration 9300: Loss = -11674.591716589086
1
Iteration 9400: Loss = -11674.532782514338
Iteration 9500: Loss = -11674.533024616101
1
Iteration 9600: Loss = -11674.5325852464
Iteration 9700: Loss = -11674.491660470927
Iteration 9800: Loss = -11674.485871775843
Iteration 9900: Loss = -11674.463647732493
Iteration 10000: Loss = -11674.463290228008
Iteration 10100: Loss = -11674.46356549169
1
Iteration 10200: Loss = -11674.466424667089
2
Iteration 10300: Loss = -11674.748756278872
3
Iteration 10400: Loss = -11674.462175985516
Iteration 10500: Loss = -11674.563453118917
1
Iteration 10600: Loss = -11674.460561953365
Iteration 10700: Loss = -11674.459123605808
Iteration 10800: Loss = -11674.4170217926
Iteration 10900: Loss = -11674.416726567446
Iteration 11000: Loss = -11674.407705907857
Iteration 11100: Loss = -11674.407118510751
Iteration 11200: Loss = -11674.407327977102
1
Iteration 11300: Loss = -11674.409736863681
2
Iteration 11400: Loss = -11674.40826288356
3
Iteration 11500: Loss = -11674.400290658436
Iteration 11600: Loss = -11674.40942249514
1
Iteration 11700: Loss = -11674.400801144442
2
Iteration 11800: Loss = -11674.433279461662
3
Iteration 11900: Loss = -11674.403580645494
4
Iteration 12000: Loss = -11674.417249391516
5
Iteration 12100: Loss = -11674.396968501871
Iteration 12200: Loss = -11674.397862768643
1
Iteration 12300: Loss = -11674.401159875799
2
Iteration 12400: Loss = -11674.449341553473
3
Iteration 12500: Loss = -11674.366981725327
Iteration 12600: Loss = -11674.366897704147
Iteration 12700: Loss = -11674.36757382868
1
Iteration 12800: Loss = -11674.366651485037
Iteration 12900: Loss = -11674.371427696953
1
Iteration 13000: Loss = -11674.368489370961
2
Iteration 13100: Loss = -11674.366261905321
Iteration 13200: Loss = -11674.494964016569
1
Iteration 13300: Loss = -11674.36616978125
Iteration 13400: Loss = -11674.372514243198
1
Iteration 13500: Loss = -11674.365480526285
Iteration 13600: Loss = -11674.370924167795
1
Iteration 13700: Loss = -11674.381595164085
2
Iteration 13800: Loss = -11674.368313670715
3
Iteration 13900: Loss = -11674.363697588862
Iteration 14000: Loss = -11674.368387870934
1
Iteration 14100: Loss = -11674.363182793813
Iteration 14200: Loss = -11674.363514662122
1
Iteration 14300: Loss = -11673.682638975713
Iteration 14400: Loss = -11673.683169681586
1
Iteration 14500: Loss = -11673.68283377879
2
Iteration 14600: Loss = -11673.68243172602
Iteration 14700: Loss = -11673.6844770007
1
Iteration 14800: Loss = -11673.682205035715
Iteration 14900: Loss = -11673.681402957569
Iteration 15000: Loss = -11673.681455821568
1
Iteration 15100: Loss = -11673.68130056703
Iteration 15200: Loss = -11673.851764126455
1
Iteration 15300: Loss = -11673.681101009986
Iteration 15400: Loss = -11673.681015751306
Iteration 15500: Loss = -11673.698918340418
1
Iteration 15600: Loss = -11673.680265800092
Iteration 15700: Loss = -11673.679482369653
Iteration 15800: Loss = -11673.6782405458
Iteration 15900: Loss = -11673.678662116372
1
Iteration 16000: Loss = -11673.681099194733
2
Iteration 16100: Loss = -11673.676235680012
Iteration 16200: Loss = -11673.676823470774
1
Iteration 16300: Loss = -11673.676000090416
Iteration 16400: Loss = -11673.676155629868
1
Iteration 16500: Loss = -11673.67592156752
Iteration 16600: Loss = -11673.680113358005
1
Iteration 16700: Loss = -11673.675922772769
2
Iteration 16800: Loss = -11673.70114780977
3
Iteration 16900: Loss = -11673.67588220718
Iteration 17000: Loss = -11673.675866804599
Iteration 17100: Loss = -11673.722060905266
1
Iteration 17200: Loss = -11673.675855998425
Iteration 17300: Loss = -11673.675843394518
Iteration 17400: Loss = -11673.681607767958
1
Iteration 17500: Loss = -11673.701495474883
2
Iteration 17600: Loss = -11673.68063709303
3
Iteration 17700: Loss = -11673.675583250106
Iteration 17800: Loss = -11673.687370936987
1
Iteration 17900: Loss = -11673.675487108558
Iteration 18000: Loss = -11673.675571273861
1
Iteration 18100: Loss = -11673.67556136115
2
Iteration 18200: Loss = -11673.676253357931
3
Iteration 18300: Loss = -11673.675730985953
4
Iteration 18400: Loss = -11673.927819834118
5
Iteration 18500: Loss = -11673.675335606298
Iteration 18600: Loss = -11673.773799712457
1
Iteration 18700: Loss = -11673.675335998047
2
Iteration 18800: Loss = -11673.675425682995
3
Iteration 18900: Loss = -11673.746024740574
4
Iteration 19000: Loss = -11673.676682431926
5
Iteration 19100: Loss = -11673.675606902736
6
Iteration 19200: Loss = -11673.675708996747
7
Iteration 19300: Loss = -11673.675279634535
Iteration 19400: Loss = -11673.676332225137
1
Iteration 19500: Loss = -11673.675288980769
2
Iteration 19600: Loss = -11673.6761750615
3
Iteration 19700: Loss = -11673.675265915435
Iteration 19800: Loss = -11673.675961229754
1
Iteration 19900: Loss = -11665.336361355285
tensor([[-10.3116,   8.2327],
        [ -8.1895,   6.7037],
        [  1.8457,  -4.1345],
        [ -9.8695,   7.9629],
        [  2.7741,  -4.9414],
        [ -9.1646,   7.6202],
        [  3.4055,  -5.3099],
        [  2.9625,  -7.0350],
        [  7.0311,  -8.6688],
        [  3.4643,  -5.1787],
        [  2.7658,  -4.9863],
        [-10.7206,   8.1998],
        [  5.5620,  -6.9503],
        [ -8.7897,   7.1405],
        [  4.0119,  -7.9282],
        [ -3.4376,  -1.1776],
        [ -9.7871,   7.2373],
        [ -9.9867,   8.5885],
        [ -9.1456,   7.7148],
        [  7.5872,  -8.9797],
        [ -9.9500,   8.3201],
        [ -9.7945,   8.3475],
        [  6.1236,  -8.2384],
        [ -9.0804,   7.6609],
        [  3.7452,  -5.1758],
        [  3.2131,  -4.5994],
        [-10.4661,   8.8139],
        [ -8.2447,   5.9637],
        [  3.5176,  -4.9104],
        [  2.3435,  -6.6753],
        [-10.0853,   8.0234],
        [ -8.4611,   7.0738],
        [-10.0696,   7.8306],
        [ -7.6705,   6.2066],
        [  6.6275,  -8.0375],
        [ -9.5439,   7.7398],
        [ -9.3406,   7.9410],
        [  3.5421,  -5.0035],
        [  4.6176,  -7.7847],
        [  4.7632,  -6.2525],
        [ -9.5567,   7.8800],
        [ -9.2276,   7.6577],
        [  5.4013,  -7.8072],
        [  5.8207,  -8.2316],
        [ -9.2641,   7.3041],
        [  3.4609,  -4.9101],
        [  7.0692,  -8.9001],
        [ -2.7613,  -0.2576],
        [  4.9018,  -7.3859],
        [ -9.9847,   8.5615],
        [-10.6804,   8.2580],
        [ -9.5842,   8.1898],
        [  5.2161,  -9.6972],
        [  4.4073,  -5.8146],
        [ -9.4238,   7.6004],
        [  4.4044,  -8.7117],
        [  4.0919,  -8.3995],
        [ -7.9219,   6.1597],
        [ -3.1083,   0.2576],
        [  2.7534,  -5.7935],
        [  0.5488,  -2.2772],
        [ -8.9062,   6.3907],
        [  6.8957,  -9.3483],
        [ -0.9876,  -2.5123],
        [ -8.1929,   6.7131],
        [  5.3641,  -6.8721],
        [-10.0895,   8.6630],
        [-10.1741,   8.7877],
        [-10.0694,   8.4763],
        [-11.1928,   8.5923],
        [  2.2443,  -3.6918],
        [ -9.6444,   7.8372],
        [ -6.3349,   4.7376],
        [-11.2512,   8.2133],
        [  0.4388,  -2.3344],
        [-10.5455,   8.5325],
        [ -9.8110,   8.1676],
        [ -9.8318,   7.8945],
        [ -9.9485,   8.4523],
        [  3.2693,  -4.8981],
        [ -9.7745,   8.0835],
        [  2.8211,  -7.4364],
        [ -9.0554,   7.5386],
        [ -1.5652,   0.1735],
        [ -9.3758,   7.7748],
        [ -8.5842,   7.1908],
        [  4.4319,  -6.0972],
        [  3.7520,  -5.2008],
        [  3.5584,  -4.9527],
        [-10.6902,   7.3344],
        [-10.0782,   8.6619],
        [  1.7676,  -6.1435],
        [ -6.1141,   4.5949],
        [-10.5100,   8.2647],
        [-11.4083,   8.4644],
        [  3.8677,  -5.8213],
        [ -8.8301,   6.4282],
        [ -9.1174,   6.8473],
        [  2.1766,  -4.3849],
        [ -9.0815,   7.6761]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3405, 0.6595],
        [0.7570, 0.2430]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4313, 0.5687], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2645, 0.0948],
         [0.9276, 0.3267]],

        [[0.9500, 0.0941],
         [0.5442, 0.4441]],

        [[0.5873, 0.1047],
         [0.0339, 0.2553]],

        [[0.3683, 0.0999],
         [0.0671, 0.4680]],

        [[0.6105, 0.0837],
         [0.8667, 0.9748]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824283882000855
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721141809334062
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369635135591801
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.772151675588645
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.03969424650279201
Average Adjusted Rand Index: 0.8247303290635735
Iteration 0: Loss = -23151.949524981075
Iteration 10: Loss = -11377.475402956255
Iteration 20: Loss = -11377.12636656115
Iteration 30: Loss = -11377.12474068584
Iteration 40: Loss = -11377.12472806804
Iteration 50: Loss = -11377.12472644722
Iteration 60: Loss = -11377.12472644722
1
Iteration 70: Loss = -11377.12472644722
2
Iteration 80: Loss = -11377.12472644722
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7440, 0.2560],
        [0.3104, 0.6896]], dtype=torch.float64)
alpha: tensor([0.5155, 0.4845])
beta: tensor([[[0.1880, 0.0995],
         [0.0827, 0.3994]],

        [[0.7443, 0.0929],
         [0.1151, 0.9673]],

        [[0.9440, 0.1051],
         [0.5410, 0.1334]],

        [[0.5167, 0.1041],
         [0.6709, 0.4993]],

        [[0.8388, 0.0850],
         [0.3153, 0.9188]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320404224087
Average Adjusted Rand Index: 0.9839993730966995
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23151.877506254754
Iteration 100: Loss = -12226.452887857236
Iteration 200: Loss = -11879.062210006074
Iteration 300: Loss = -11672.210631313239
Iteration 400: Loss = -11632.744496438818
Iteration 500: Loss = -11610.572452257285
Iteration 600: Loss = -11592.457308010213
Iteration 700: Loss = -11558.675500605017
Iteration 800: Loss = -11505.061321534326
Iteration 900: Loss = -11409.4026441193
Iteration 1000: Loss = -11389.52975565587
Iteration 1100: Loss = -11389.294224328041
Iteration 1200: Loss = -11389.155738322283
Iteration 1300: Loss = -11389.060059247675
Iteration 1400: Loss = -11388.988676430203
Iteration 1500: Loss = -11388.93268506615
Iteration 1600: Loss = -11388.88685500241
Iteration 1700: Loss = -11388.847084593275
Iteration 1800: Loss = -11388.808135869773
Iteration 1900: Loss = -11388.765745973375
Iteration 2000: Loss = -11388.741467848395
Iteration 2100: Loss = -11388.723109841028
Iteration 2200: Loss = -11388.707162032948
Iteration 2300: Loss = -11388.693185088272
Iteration 2400: Loss = -11388.680892552778
Iteration 2500: Loss = -11388.669999132768
Iteration 2600: Loss = -11388.660356491282
Iteration 2700: Loss = -11388.651641819262
Iteration 2800: Loss = -11388.6438519429
Iteration 2900: Loss = -11388.636797891713
Iteration 3000: Loss = -11388.630374735174
Iteration 3100: Loss = -11388.62438486596
Iteration 3200: Loss = -11388.618659039435
Iteration 3300: Loss = -11388.612431802603
Iteration 3400: Loss = -11388.603566113068
Iteration 3500: Loss = -11388.597918940068
Iteration 3600: Loss = -11388.593852446456
Iteration 3700: Loss = -11388.590356878523
Iteration 3800: Loss = -11388.587169744564
Iteration 3900: Loss = -11388.584320052836
Iteration 4000: Loss = -11388.581617392292
Iteration 4100: Loss = -11388.579191216246
Iteration 4200: Loss = -11388.577072363632
Iteration 4300: Loss = -11388.574804988024
Iteration 4400: Loss = -11388.57286849784
Iteration 4500: Loss = -11388.571003171237
Iteration 4600: Loss = -11388.573988034856
1
Iteration 4700: Loss = -11388.567749081636
Iteration 4800: Loss = -11388.566276105295
Iteration 4900: Loss = -11388.567624123978
1
Iteration 5000: Loss = -11388.56393774095
Iteration 5100: Loss = -11388.562542551987
Iteration 5200: Loss = -11388.561571809149
Iteration 5300: Loss = -11388.566615616586
1
Iteration 5400: Loss = -11388.562699809754
2
Iteration 5500: Loss = -11388.558691832064
Iteration 5600: Loss = -11388.558306952229
Iteration 5700: Loss = -11388.561720883234
1
Iteration 5800: Loss = -11388.557723439677
Iteration 5900: Loss = -11388.558254812704
1
Iteration 6000: Loss = -11388.555526763821
Iteration 6100: Loss = -11388.553731150565
Iteration 6200: Loss = -11388.559822657597
1
Iteration 6300: Loss = -11388.555528804995
2
Iteration 6400: Loss = -11388.569261738938
3
Iteration 6500: Loss = -11388.549885774435
Iteration 6600: Loss = -11388.550841962046
1
Iteration 6700: Loss = -11388.553517305681
2
Iteration 6800: Loss = -11388.557928460805
3
Iteration 6900: Loss = -11388.548217682579
Iteration 7000: Loss = -11388.550697671244
1
Iteration 7100: Loss = -11388.565327881763
2
Iteration 7200: Loss = -11388.547084423055
Iteration 7300: Loss = -11388.548553699942
1
Iteration 7400: Loss = -11388.546444449199
Iteration 7500: Loss = -11388.550048238896
1
Iteration 7600: Loss = -11388.5517022649
2
Iteration 7700: Loss = -11388.549064142318
3
Iteration 7800: Loss = -11388.54994362076
4
Iteration 7900: Loss = -11388.5515555851
5
Iteration 8000: Loss = -11388.563879518131
6
Iteration 8100: Loss = -11388.54802588602
7
Iteration 8200: Loss = -11388.545730408385
Iteration 8300: Loss = -11388.554231697713
1
Iteration 8400: Loss = -11388.558059754852
2
Iteration 8500: Loss = -11388.550807846503
3
Iteration 8600: Loss = -11388.549533055842
4
Iteration 8700: Loss = -11388.546610234262
5
Iteration 8800: Loss = -11388.550510327788
6
Iteration 8900: Loss = -11388.553462631611
7
Iteration 9000: Loss = -11388.55457141441
8
Iteration 9100: Loss = -11388.604860688802
9
Iteration 9200: Loss = -11388.571718933812
10
Stopping early at iteration 9200 due to no improvement.
tensor([[ -9.5936,   5.7910],
        [ -6.6346,   4.9569],
        [  4.2851,  -5.7560],
        [ -9.0295,   6.8945],
        [  5.2466,  -6.6360],
        [ -7.9720,   5.9289],
        [  5.3760,  -6.7640],
        [  5.8629,  -7.3374],
        [  5.8035,  -7.3173],
        [  5.7832,  -7.4980],
        [  5.4286,  -6.8974],
        [ -8.6890,   6.4953],
        [  5.7148,  -7.6112],
        [ -6.9529,   4.8427],
        [  5.7456,  -8.1139],
        [  1.2904,  -3.1238],
        [ -2.6261,   1.1579],
        [ -8.9191,   6.9736],
        [ -7.3956,   5.5615],
        [  5.8961,  -8.2577],
        [ -8.3500,   6.8523],
        [ -9.8495,   7.6355],
        [  5.8655,  -7.3423],
        [ -3.4141,   1.4913],
        [  5.5323,  -6.9303],
        [  5.3291,  -6.8050],
        [ -8.9779,   6.4647],
        [ -4.8753,   3.4832],
        [  6.0284, -10.0332],
        [  5.4361,  -7.8221],
        [ -7.5473,   5.6248],
        [ -6.7000,   4.6255],
        [ -9.4747,   6.7752],
        [ -5.9787,   4.1230],
        [  6.1203,  -7.7685],
        [ -7.3869,   5.9236],
        [ -8.2649,   6.2017],
        [  5.4630,  -6.8720],
        [  6.1129,  -8.4607],
        [  5.4930,  -6.8896],
        [ -7.7881,   6.1468],
        [ -8.4098,   5.8828],
        [  5.9876,  -7.6447],
        [  5.7539,  -7.1577],
        [ -8.2542,   6.7363],
        [  5.7252,  -7.1127],
        [  6.0261,  -8.4648],
        [  0.4566,  -2.8428],
        [  5.5902,  -7.0198],
        [ -9.0662,   7.3469],
        [ -8.0035,   6.3760],
        [ -8.1502,   6.7622],
        [  6.0291,  -7.5204],
        [  5.9109,  -7.6517],
        [ -7.6922,   6.0246],
        [  5.8637,  -8.5738],
        [  6.1492,  -7.6103],
        [ -7.7150,   3.5539],
        [  1.1274,  -5.0756],
        [  5.0160,  -8.6709],
        [  1.1322,  -4.7916],
        [ -8.1711,   6.7720],
        [  6.0209, -10.3481],
        [  3.1102,  -4.6047],
        [ -7.6407,   6.0405],
        [  6.0650,  -8.0574],
        [ -8.1652,   6.6298],
        [ -8.7683,   7.0828],
        [ -7.6178,   6.2007],
        [-11.2491,   6.6339],
        [  5.4227,  -7.5503],
        [ -7.8250,   6.3924],
        [ -8.0670,   6.6687],
        [ -8.5165,   7.1243],
        [  4.1853,  -5.7643],
        [ -8.4082,   6.8782],
        [ -7.7348,   5.7919],
        [ -8.2671,   6.8125],
        [ -8.3807,   6.8543],
        [  5.3122,  -7.0151],
        [ -8.6893,   6.3291],
        [  5.9164,  -8.1279],
        [ -8.9780,   7.3814],
        [  3.2599,  -4.7977],
        [ -9.2406,   6.4805],
        [ -7.1345,   5.6748],
        [  6.1908,  -7.5771],
        [  5.2165,  -7.5661],
        [  3.9483,  -6.1544],
        [ -8.4666,   7.0208],
        [ -7.6022,   6.2107],
        [  5.1122,  -6.8889],
        [ -3.6959,   2.2846],
        [ -7.7659,   6.0998],
        [ -9.7653,   7.2278],
        [  5.3262,  -7.0223],
        [ -5.4611,   4.0364],
        [ -8.0621,   5.9962],
        [  3.6997,  -6.9990],
        [ -7.9455,   6.5532]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7463, 0.2537],
        [0.3136, 0.6864]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4707, 0.5293], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1912, 0.0990],
         [0.0827, 0.4084]],

        [[0.7443, 0.0928],
         [0.1151, 0.9673]],

        [[0.9440, 0.1111],
         [0.5410, 0.1334]],

        [[0.5167, 0.1042],
         [0.6709, 0.4993]],

        [[0.8388, 0.0850],
         [0.3153, 0.9188]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961807113047
Average Adjusted Rand Index: 0.9761614280947526
Iteration 0: Loss = -17452.06780904922
Iteration 10: Loss = -11377.136510309887
Iteration 20: Loss = -11377.12485058298
Iteration 30: Loss = -11377.12473625822
Iteration 40: Loss = -11377.124725703836
Iteration 50: Loss = -11377.124725758422
1
Iteration 60: Loss = -11377.12472569907
Iteration 70: Loss = -11377.124725758422
1
Iteration 80: Loss = -11377.12472569907
2
Iteration 90: Loss = -11377.124725758422
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.6896, 0.3104],
        [0.2560, 0.7440]], dtype=torch.float64)
alpha: tensor([0.4845, 0.5155])
beta: tensor([[[0.3994, 0.0995],
         [0.7829, 0.1880]],

        [[0.6262, 0.0929],
         [0.8374, 0.5518]],

        [[0.5251, 0.1051],
         [0.2803, 0.6178]],

        [[0.2108, 0.1041],
         [0.1086, 0.0926]],

        [[0.9338, 0.0850],
         [0.0199, 0.1800]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320404224087
Average Adjusted Rand Index: 0.9839993730966995
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17451.996558133233
Iteration 100: Loss = -11637.572218265683
Iteration 200: Loss = -11471.26402561926
Iteration 300: Loss = -11388.85283394794
Iteration 400: Loss = -11376.525462821712
Iteration 500: Loss = -11376.065257771503
Iteration 600: Loss = -11375.83265836457
Iteration 700: Loss = -11375.690138888367
Iteration 800: Loss = -11375.594949592063
Iteration 900: Loss = -11375.527469351618
Iteration 1000: Loss = -11375.477631848335
Iteration 1100: Loss = -11375.439601002949
Iteration 1200: Loss = -11375.40979197406
Iteration 1300: Loss = -11375.38601380179
Iteration 1400: Loss = -11375.366627112055
Iteration 1500: Loss = -11375.350604409628
Iteration 1600: Loss = -11375.337209322235
Iteration 1700: Loss = -11375.325941865449
Iteration 1800: Loss = -11375.31632874536
Iteration 1900: Loss = -11375.307976761404
Iteration 2000: Loss = -11375.30079422228
Iteration 2100: Loss = -11375.29482752204
Iteration 2200: Loss = -11375.289011238556
Iteration 2300: Loss = -11375.284128568495
Iteration 2400: Loss = -11375.279808579722
Iteration 2500: Loss = -11375.275994522637
Iteration 2600: Loss = -11375.272534232996
Iteration 2700: Loss = -11375.269454180896
Iteration 2800: Loss = -11375.266674685468
Iteration 2900: Loss = -11375.269994789973
1
Iteration 3000: Loss = -11375.261876269507
Iteration 3100: Loss = -11375.259792055387
Iteration 3200: Loss = -11375.257909959308
Iteration 3300: Loss = -11375.256181553874
Iteration 3400: Loss = -11375.254641041032
Iteration 3500: Loss = -11375.254377359799
Iteration 3600: Loss = -11375.251842578664
Iteration 3700: Loss = -11375.257284355903
1
Iteration 3800: Loss = -11375.24948364677
Iteration 3900: Loss = -11375.248445195626
Iteration 4000: Loss = -11375.248624100877
1
Iteration 4100: Loss = -11375.246605192382
Iteration 4200: Loss = -11375.24582216313
Iteration 4300: Loss = -11375.248263371452
1
Iteration 4400: Loss = -11375.244290011924
Iteration 4500: Loss = -11375.243773800235
Iteration 4600: Loss = -11375.247104994123
1
Iteration 4700: Loss = -11375.24242021286
Iteration 4800: Loss = -11375.24190154854
Iteration 4900: Loss = -11375.241813769824
Iteration 5000: Loss = -11375.24097724476
Iteration 5100: Loss = -11375.24046401488
Iteration 5200: Loss = -11375.240082747714
Iteration 5300: Loss = -11375.239704254096
Iteration 5400: Loss = -11375.239514360697
Iteration 5500: Loss = -11375.239008812086
Iteration 5600: Loss = -11375.238733625301
Iteration 5700: Loss = -11375.239202621295
1
Iteration 5800: Loss = -11375.238107725276
Iteration 5900: Loss = -11375.24093844701
1
Iteration 6000: Loss = -11375.238136186468
2
Iteration 6100: Loss = -11375.245657389438
3
Iteration 6200: Loss = -11375.23893015358
4
Iteration 6300: Loss = -11375.24838746335
5
Iteration 6400: Loss = -11375.237978532246
Iteration 6500: Loss = -11375.240081847847
1
Iteration 6600: Loss = -11375.23775672407
Iteration 6700: Loss = -11375.258116267138
1
Iteration 6800: Loss = -11375.256802735079
2
Iteration 6900: Loss = -11375.260123090979
3
Iteration 7000: Loss = -11375.237060889627
Iteration 7100: Loss = -11375.243643090356
1
Iteration 7200: Loss = -11375.248620299877
2
Iteration 7300: Loss = -11375.240066601633
3
Iteration 7400: Loss = -11375.239305106328
4
Iteration 7500: Loss = -11375.243466597485
5
Iteration 7600: Loss = -11375.235212776224
Iteration 7700: Loss = -11375.23657479746
1
Iteration 7800: Loss = -11375.239426489754
2
Iteration 7900: Loss = -11375.237116220269
3
Iteration 8000: Loss = -11375.290464085741
4
Iteration 8100: Loss = -11375.238149989555
5
Iteration 8200: Loss = -11375.23590843463
6
Iteration 8300: Loss = -11375.235011373765
Iteration 8400: Loss = -11375.2353463009
1
Iteration 8500: Loss = -11375.234661515073
Iteration 8600: Loss = -11375.270869025571
1
Iteration 8700: Loss = -11375.243021370483
2
Iteration 8800: Loss = -11375.242218187122
3
Iteration 8900: Loss = -11375.257916964094
4
Iteration 9000: Loss = -11375.23882846284
5
Iteration 9100: Loss = -11375.238180175678
6
Iteration 9200: Loss = -11375.255200647593
7
Iteration 9300: Loss = -11375.234262460961
Iteration 9400: Loss = -11375.234780754132
1
Iteration 9500: Loss = -11375.234494952729
2
Iteration 9600: Loss = -11375.234332303986
3
Iteration 9700: Loss = -11375.238128557126
4
Iteration 9800: Loss = -11375.234403282991
5
Iteration 9900: Loss = -11375.238550486629
6
Iteration 10000: Loss = -11375.295290656059
7
Iteration 10100: Loss = -11375.283192069983
8
Iteration 10200: Loss = -11375.293403686375
9
Iteration 10300: Loss = -11375.239235372865
10
Stopping early at iteration 10300 due to no improvement.
tensor([[  7.4138,  -8.8184],
        [  4.0847,  -7.2877],
        [ -5.6330,   4.2466],
        [  7.0753,  -8.9638],
        [ -7.1188,   5.5519],
        [  6.3743,  -7.7696],
        [ -7.0805,   5.3624],
        [ -7.8150,   5.8827],
        [ -8.1192,   6.6509],
        [ -8.1842,   6.2767],
        [ -8.4787,   4.5494],
        [  6.1319,  -8.6729],
        [ -9.0092,   5.6060],
        [  4.8367,  -6.9383],
        [ -8.0760,   5.4457],
        [ -3.1720,   1.1216],
        [  0.9726,  -2.8700],
        [  7.2802,  -8.9727],
        [  6.1079,  -7.7144],
        [ -8.2576,   6.7018],
        [  7.0503,  -8.8279],
        [  6.8423,  -8.4931],
        [ -8.0314,   6.6245],
        [  1.5470,  -3.3956],
        [ -8.4021,   5.8136],
        [ -7.1475,   5.6389],
        [  6.9228,  -9.7569],
        [  2.8165,  -5.5555],
        [ -7.7781,   5.8444],
        [ -9.1153,   7.3275],
        [  6.0820,  -7.8365],
        [  5.0932,  -6.5087],
        [  6.2640,  -8.6276],
        [  4.0525,  -6.1169],
        [ -8.1768,   6.6684],
        [  6.2199,  -7.6266],
        [  5.9033,  -8.6168],
        [ -8.3785,   5.0606],
        [ -8.0892,   6.6529],
        [ -8.3054,   5.9438],
        [  6.3617,  -7.8205],
        [  5.5445,  -8.1554],
        [ -8.3497,   6.4093],
        [ -7.9869,   6.2781],
        [  6.5222,  -8.1054],
        [ -8.8792,   7.0317],
        [ -8.7583,   7.3718],
        [ -2.2648,   0.8776],
        [ -8.8155,   6.3644],
        [  6.3241,  -9.6484],
        [  6.9677,  -8.7112],
        [  6.7212,  -8.4982],
        [ -8.2851,   6.6028],
        [ -7.5236,   5.8662],
        [  6.5987,  -8.1135],
        [ -8.2255,   6.6445],
        [-11.0056,   6.3903],
        [  4.9656,  -6.3552],
        [ -3.8318,   2.1976],
        [ -7.9898,   5.5547],
        [ -8.5274,   6.6885],
        [  5.6613,  -8.1506],
        [ -9.3382,   6.4971],
        [ -4.4711,   3.0848],
        [  3.6883,  -5.0747],
        [ -7.5105,   6.1060],
        [  7.1106,  -8.9160],
        [  6.6489,  -8.9152],
        [  6.6241,  -9.4337],
        [  7.1470,  -8.6160],
        [ -7.9894,   5.3946],
        [  6.7726,  -8.2833],
        [  4.3555,  -5.7990],
        [  6.9395,  -8.4904],
        [ -9.1327,   7.7153],
        [  6.5214,  -8.2673],
        [  5.6912,  -9.4377],
        [  6.1440,  -7.9862],
        [  7.1681,  -8.6383],
        [ -7.6136,   5.7895],
        [  6.1199,  -7.8543],
        [ -9.4043,   6.8378],
        [  5.3106,  -7.2675],
        [ -6.2532,   1.6380],
        [  6.0659,  -8.9271],
        [  5.6412,  -7.0617],
        [ -7.9234,   6.4757],
        [ -8.2470,   5.8404],
        [ -5.7827,   4.2226],
        [  6.8020,  -8.8705],
        [  6.7753,  -8.1617],
        [ -7.2532,   4.9513],
        [  2.3139,  -3.7248],
        [  6.6889,  -8.2117],
        [  6.7000,  -8.4899],
        [ -7.4199,   6.0196],
        [  3.6309,  -5.8508],
        [  6.0839,  -7.9090],
        [ -6.0405,   4.5229],
        [  6.3888,  -7.7787]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6909, 0.3091],
        [0.2546, 0.7454]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5297, 0.4703], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4078, 0.1000],
         [0.7829, 0.1915]],

        [[0.6262, 0.0930],
         [0.8374, 0.5518]],

        [[0.5251, 0.1050],
         [0.2803, 0.6178]],

        [[0.2108, 0.1040],
         [0.1086, 0.0926]],

        [[0.9338, 0.0850],
         [0.0199, 0.1800]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320404224087
Average Adjusted Rand Index: 0.9839993730966995
Iteration 0: Loss = -19632.943310740142
Iteration 10: Loss = -11377.118991217656
Iteration 20: Loss = -11377.124679179758
1
Iteration 30: Loss = -11377.124727301258
2
Iteration 40: Loss = -11377.124722827044
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7440, 0.2560],
        [0.3104, 0.6896]], dtype=torch.float64)
alpha: tensor([0.5155, 0.4845])
beta: tensor([[[0.1880, 0.0995],
         [0.7887, 0.3994]],

        [[0.1897, 0.0929],
         [0.9915, 0.7637]],

        [[0.8791, 0.1051],
         [0.7691, 0.6247]],

        [[0.3013, 0.1041],
         [0.2093, 0.2765]],

        [[0.5358, 0.0850],
         [0.9144, 0.5307]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320404224087
Average Adjusted Rand Index: 0.9839993730966995
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19632.772521858344
Iteration 100: Loss = -12159.336728892598
Iteration 200: Loss = -12110.515652342621
Iteration 300: Loss = -11860.751229158694
Iteration 400: Loss = -11850.859265803443
Iteration 500: Loss = -11850.106179314445
Iteration 600: Loss = -11849.875982342679
Iteration 700: Loss = -11849.7480280495
Iteration 800: Loss = -11849.66285005763
Iteration 900: Loss = -11849.602441132693
Iteration 1000: Loss = -11849.557647839949
Iteration 1100: Loss = -11849.52285182753
Iteration 1200: Loss = -11849.494476238455
Iteration 1300: Loss = -11849.470640795214
Iteration 1400: Loss = -11849.449735282253
Iteration 1500: Loss = -11849.430629784369
Iteration 1600: Loss = -11849.412387847628
Iteration 1700: Loss = -11849.39679475319
Iteration 1800: Loss = -11849.370877020512
Iteration 1900: Loss = -11849.337456524221
Iteration 2000: Loss = -11849.257588293025
Iteration 2100: Loss = -11838.81562667343
Iteration 2200: Loss = -11768.04305259396
Iteration 2300: Loss = -11697.119013566919
Iteration 2400: Loss = -11695.545803290168
Iteration 2500: Loss = -11683.010671955388
Iteration 2600: Loss = -11682.673355787296
Iteration 2700: Loss = -11682.650509657913
Iteration 2800: Loss = -11682.592189151535
Iteration 2900: Loss = -11682.554011816415
Iteration 3000: Loss = -11682.542334470972
Iteration 3100: Loss = -11682.53038388975
Iteration 3200: Loss = -11682.52774166079
Iteration 3300: Loss = -11682.511442778448
Iteration 3400: Loss = -11680.896542043414
Iteration 3500: Loss = -11680.764598852316
Iteration 3600: Loss = -11676.386957206189
Iteration 3700: Loss = -11676.338372124297
Iteration 3800: Loss = -11676.23358094938
Iteration 3900: Loss = -11676.22533931372
Iteration 4000: Loss = -11676.21798862042
Iteration 4100: Loss = -11676.214931888966
Iteration 4200: Loss = -11676.211453112377
Iteration 4300: Loss = -11676.05336124796
Iteration 4400: Loss = -11675.866479768243
Iteration 4500: Loss = -11675.866301751805
Iteration 4600: Loss = -11675.868446684855
1
Iteration 4700: Loss = -11675.862862608723
Iteration 4800: Loss = -11675.861564891915
Iteration 4900: Loss = -11665.856317416374
Iteration 5000: Loss = -11665.755560493808
Iteration 5100: Loss = -11665.74127815589
Iteration 5200: Loss = -11665.750595313571
1
Iteration 5300: Loss = -11665.739078469474
Iteration 5400: Loss = -11665.810338854706
1
Iteration 5500: Loss = -11665.737608572897
Iteration 5600: Loss = -11665.74507838476
1
Iteration 5700: Loss = -11665.735388987501
Iteration 5800: Loss = -11665.732905610128
Iteration 5900: Loss = -11665.73204140685
Iteration 6000: Loss = -11665.731360588892
Iteration 6100: Loss = -11665.753118398497
1
Iteration 6200: Loss = -11665.730799011833
Iteration 6300: Loss = -11665.730596235535
Iteration 6400: Loss = -11665.73945698404
1
Iteration 6500: Loss = -11665.730156080954
Iteration 6600: Loss = -11665.729938163078
Iteration 6700: Loss = -11665.729820587012
Iteration 6800: Loss = -11665.731680767229
1
Iteration 6900: Loss = -11665.72987842853
2
Iteration 7000: Loss = -11665.729317909185
Iteration 7100: Loss = -11665.729444969045
1
Iteration 7200: Loss = -11665.730648098523
2
Iteration 7300: Loss = -11665.732915786997
3
Iteration 7400: Loss = -11665.728732057936
Iteration 7500: Loss = -11665.743347821563
1
Iteration 7600: Loss = -11665.728443225076
Iteration 7700: Loss = -11665.861649337776
1
Iteration 7800: Loss = -11665.72780865085
Iteration 7900: Loss = -11665.728179198934
1
Iteration 8000: Loss = -11665.889304411718
2
Iteration 8100: Loss = -11665.728097339277
3
Iteration 8200: Loss = -11665.75246196653
4
Iteration 8300: Loss = -11665.730724441568
5
Iteration 8400: Loss = -11665.739364203453
6
Iteration 8500: Loss = -11665.727720775956
Iteration 8600: Loss = -11665.727309882259
Iteration 8700: Loss = -11665.726852776952
Iteration 8800: Loss = -11665.738434630151
1
Iteration 8900: Loss = -11665.726186911745
Iteration 9000: Loss = -11665.73174927293
1
Iteration 9100: Loss = -11665.741885721794
2
Iteration 9200: Loss = -11665.71926931039
Iteration 9300: Loss = -11665.724430946222
1
Iteration 9400: Loss = -11665.718326974962
Iteration 9500: Loss = -11665.754127720309
1
Iteration 9600: Loss = -11665.718122965322
Iteration 9700: Loss = -11665.71845992456
1
Iteration 9800: Loss = -11665.718327492654
2
Iteration 9900: Loss = -11665.718178158384
3
Iteration 10000: Loss = -11665.737347194554
4
Iteration 10100: Loss = -11665.717908336352
Iteration 10200: Loss = -11665.746650569748
1
Iteration 10300: Loss = -11665.719525668168
2
Iteration 10400: Loss = -11665.72841569343
3
Iteration 10500: Loss = -11665.716283496842
Iteration 10600: Loss = -11665.722021460455
1
Iteration 10700: Loss = -11665.718379797372
2
Iteration 10800: Loss = -11665.722186309413
3
Iteration 10900: Loss = -11665.726493627411
4
Iteration 11000: Loss = -11665.980124305746
5
Iteration 11100: Loss = -11665.717863513815
6
Iteration 11200: Loss = -11665.717176022268
7
Iteration 11300: Loss = -11665.715818690014
Iteration 11400: Loss = -11665.739856829607
1
Iteration 11500: Loss = -11665.732024062778
2
Iteration 11600: Loss = -11665.715712435427
Iteration 11700: Loss = -11665.716692583968
1
Iteration 11800: Loss = -11665.715713890884
2
Iteration 11900: Loss = -11665.71917533672
3
Iteration 12000: Loss = -11665.71579458669
4
Iteration 12100: Loss = -11665.717560094039
5
Iteration 12200: Loss = -11665.71601864831
6
Iteration 12300: Loss = -11665.715298144507
Iteration 12400: Loss = -11665.715947907158
1
Iteration 12500: Loss = -11665.716158671645
2
Iteration 12600: Loss = -11665.748856962784
3
Iteration 12700: Loss = -11664.309010834484
Iteration 12800: Loss = -11664.296258394823
Iteration 12900: Loss = -11664.613473251527
1
Iteration 13000: Loss = -11664.262975442969
Iteration 13100: Loss = -11664.26295524388
Iteration 13200: Loss = -11664.263248757481
1
Iteration 13300: Loss = -11664.262975563252
2
Iteration 13400: Loss = -11664.267592261634
3
Iteration 13500: Loss = -11664.265317792282
4
Iteration 13600: Loss = -11664.27013583997
5
Iteration 13700: Loss = -11664.263816514682
6
Iteration 13800: Loss = -11664.26558509299
7
Iteration 13900: Loss = -11664.263588749433
8
Iteration 14000: Loss = -11664.263102358045
9
Iteration 14100: Loss = -11664.320637181203
10
Stopping early at iteration 14100 due to no improvement.
tensor([[ -9.8238,   8.3969],
        [ -7.5231,   6.1263],
        [  2.1182,  -6.7335],
        [ -9.1759,   7.5582],
        [  3.7093,  -5.0965],
        [-10.0869,   7.9133],
        [  6.4308,  -8.3004],
        [  5.0023,  -6.6892],
        [  6.0760,  -7.8428],
        [  2.7022,  -7.0509],
        [  3.0830,  -6.2761],
        [ -8.7884,   7.3554],
        [  5.4594,  -6.9795],
        [ -7.5556,   5.9717],
        [  4.8135,  -6.6478],
        [ -1.0421,  -0.7454],
        [ -3.9962,   2.5989],
        [ -8.9843,   7.4951],
        [ -9.2642,   6.6280],
        [  6.8618,  -8.4783],
        [ -9.6849,   8.1159],
        [ -8.7173,   6.9462],
        [  6.4217,  -7.8434],
        [ -4.9518,   3.5317],
        [  5.8445,  -7.2733],
        [  4.7281,  -6.2580],
        [ -9.5545,   8.0685],
        [ -6.5961,   4.9411],
        [  4.4048,  -5.8148],
        [  3.4485,  -5.6447],
        [ -8.6225,   6.8682],
        [ -9.0491,   7.5759],
        [ -9.2049,   7.8175],
        [ -6.6033,   4.6003],
        [  5.0059,  -9.4049],
        [ -8.6885,   6.5460],
        [ -8.8413,   7.3630],
        [  3.5848,  -5.0073],
        [  5.9106,  -7.5685],
        [  4.6512,  -6.1778],
        [ -8.6198,   7.0064],
        [-10.5771,   5.9619],
        [  5.7195,  -7.8241],
        [  4.7358,  -9.3510],
        [ -8.4094,   6.2666],
        [  5.4020,  -6.8442],
        [  7.1116,  -8.9506],
        [ -0.2523,  -1.2313],
        [  6.1545,  -7.5539],
        [ -9.8446,   7.9545],
        [-11.9945,   7.7412],
        [ -8.9303,   7.1111],
        [  6.3431,  -7.7779],
        [  4.7975,  -6.7809],
        [ -8.6191,   6.9415],
        [  4.9370,  -7.4127],
        [  7.1101,  -8.5285],
        [ -8.0972,   6.4831],
        [  0.0493,  -1.5399],
        [  3.5885,  -5.0129],
        [ -0.5019,  -2.6877],
        [ -7.8087,   6.4073],
        [  6.3960,  -9.6364],
        [ -0.1494,  -1.7781],
        [ -6.3023,   4.3429],
        [  4.8702,  -7.0859],
        [ -9.7058,   7.9790],
        [-11.1664,   6.6524],
        [ -8.9653,   6.9893],
        [-10.3509,   7.9536],
        [  2.8162,  -6.0806],
        [ -8.8924,   6.9899],
        [ -6.9416,   3.8828],
        [ -9.3483,   6.6929],
        [  1.2231,  -3.6546],
        [ -9.3078,   7.3392],
        [ -8.8655,   7.3528],
        [ -8.8145,   7.3124],
        [ -9.1798,   7.7866],
        [  3.8368,  -5.7518],
        [ -8.7256,   6.9715],
        [  5.3161,  -7.3830],
        [ -9.2298,   5.4078],
        [ -0.1469,  -2.6240],
        [ -8.5833,   7.1627],
        [ -7.9076,   6.2398],
        [  4.5841,  -8.3636],
        [  3.5959,  -6.1188],
        [  3.1452,  -4.5354],
        [ -9.0729,   7.6553],
        [ -9.0007,   7.4631],
        [  2.3020,  -5.5222],
        [ -4.1348,   2.1240],
        [ -9.9148,   7.4654],
        [ -9.5890,   7.5203],
        [  4.4168,  -5.8824],
        [ -8.4915,   5.7102],
        [ -8.2390,   6.8205],
        [  2.0191,  -4.1665],
        [ -8.3468,   6.6088]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6139, 0.3861],
        [0.5557, 0.4443]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4602, 0.5398], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2480, 0.0987],
         [0.7887, 0.3576]],

        [[0.1897, 0.0922],
         [0.9915, 0.7637]],

        [[0.8791, 0.1038],
         [0.7691, 0.6247]],

        [[0.3013, 0.0992],
         [0.2093, 0.2765]],

        [[0.5358, 0.0837],
         [0.9144, 0.5307]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 14
Adjusted Rand Index: 0.513657303929177
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 12
Adjusted Rand Index: 0.5735833098393008
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.08344473735735221
Average Adjusted Rand Index: 0.7936083068866775
11388.322789415199
new:  [0.03969424650279201, 0.9760961807113047, 0.9840320404224087, 0.08344473735735221] [0.8247303290635735, 0.9761614280947526, 0.9839993730966995, 0.7936083068866775] [11665.335355386955, 11388.571718933812, 11375.239235372865, 11664.320637181203]
prior:  [0.5232266451963831, 0.9840320404224087, 0.9840320404224087, 0.9840320404224087] [0.8046475792837869, 0.9839993730966995, 0.9839993730966995, 0.9839993730966995] [11564.332432555566, 11377.12472644722, 11377.124725758422, 11377.124722827044]
-----------------------------------------------------------------------------------------
This iteration is 20
True Objective function: Loss = -11447.126705591018
Iteration 0: Loss = -22569.719672617302
Iteration 10: Loss = -11991.929234285955
Iteration 20: Loss = -11443.304653365685
Iteration 30: Loss = -11443.300957909192
Iteration 40: Loss = -11443.300962279649
1
Iteration 50: Loss = -11443.30096228551
2
Iteration 60: Loss = -11443.300962285513
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7388, 0.2612],
        [0.2644, 0.7356]], dtype=torch.float64)
alpha: tensor([0.4738, 0.5262])
beta: tensor([[[0.3912, 0.1101],
         [0.9601, 0.1898]],

        [[0.1064, 0.1024],
         [0.0041, 0.7436]],

        [[0.6462, 0.1044],
         [0.2765, 0.1154]],

        [[0.4147, 0.0998],
         [0.3989, 0.7122]],

        [[0.5387, 0.0934],
         [0.2615, 0.1083]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961941296443
Average Adjusted Rand Index: 0.9761612713656115
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22667.638465185195
Iteration 100: Loss = -11959.100246119548
Iteration 200: Loss = -11781.346648506173
Iteration 300: Loss = -11696.560444080476
Iteration 400: Loss = -11624.36581450303
Iteration 500: Loss = -11551.932085647433
Iteration 600: Loss = -11482.248656635433
Iteration 700: Loss = -11447.225788680242
Iteration 800: Loss = -11441.72088077578
Iteration 900: Loss = -11441.438768715168
Iteration 1000: Loss = -11441.282911181721
Iteration 1100: Loss = -11441.184632440167
Iteration 1200: Loss = -11441.116780566208
Iteration 1300: Loss = -11441.06647136769
Iteration 1400: Loss = -11441.028062355657
Iteration 1500: Loss = -11440.997987420687
Iteration 1600: Loss = -11440.973776741821
Iteration 1700: Loss = -11440.953792202778
Iteration 1800: Loss = -11440.937251483829
Iteration 1900: Loss = -11440.923243067871
Iteration 2000: Loss = -11440.911269876318
Iteration 2100: Loss = -11440.9009859597
Iteration 2200: Loss = -11440.892032276764
Iteration 2300: Loss = -11440.884155630649
Iteration 2400: Loss = -11440.877205386025
Iteration 2500: Loss = -11440.87105637028
Iteration 2600: Loss = -11440.865693971899
Iteration 2700: Loss = -11440.860903235744
Iteration 2800: Loss = -11440.856641781564
Iteration 2900: Loss = -11440.852778920273
Iteration 3000: Loss = -11440.84934720194
Iteration 3100: Loss = -11440.846449535236
Iteration 3200: Loss = -11440.843422756549
Iteration 3300: Loss = -11440.840923574799
Iteration 3400: Loss = -11440.83859230124
Iteration 3500: Loss = -11440.836448152364
Iteration 3600: Loss = -11440.834518618502
Iteration 3700: Loss = -11440.832754606876
Iteration 3800: Loss = -11440.832670725847
Iteration 3900: Loss = -11440.829615166756
Iteration 4000: Loss = -11440.82823921651
Iteration 4100: Loss = -11440.827022253812
Iteration 4200: Loss = -11440.825813696742
Iteration 4300: Loss = -11440.824740271377
Iteration 4400: Loss = -11440.82371207232
Iteration 4500: Loss = -11440.826112742183
1
Iteration 4600: Loss = -11440.82190717516
Iteration 4700: Loss = -11440.821136984354
Iteration 4800: Loss = -11440.832766301139
1
Iteration 4900: Loss = -11440.81980935182
Iteration 5000: Loss = -11440.818997641558
Iteration 5100: Loss = -11440.818378902237
Iteration 5200: Loss = -11440.817866742485
Iteration 5300: Loss = -11440.8173694442
Iteration 5400: Loss = -11440.81683705079
Iteration 5500: Loss = -11440.823464306182
1
Iteration 5600: Loss = -11440.823129580509
2
Iteration 5700: Loss = -11440.815536665821
Iteration 5800: Loss = -11440.815153546602
Iteration 5900: Loss = -11440.814842584463
Iteration 6000: Loss = -11440.814594390155
Iteration 6100: Loss = -11440.814113113804
Iteration 6200: Loss = -11440.816642294189
1
Iteration 6300: Loss = -11440.813553672973
Iteration 6400: Loss = -11440.815644392978
1
Iteration 6500: Loss = -11440.815443120582
2
Iteration 6600: Loss = -11440.812873162624
Iteration 6700: Loss = -11440.815155935044
1
Iteration 6800: Loss = -11440.814528957813
2
Iteration 6900: Loss = -11440.816254471294
3
Iteration 7000: Loss = -11440.812658808198
Iteration 7100: Loss = -11440.813239945595
1
Iteration 7200: Loss = -11440.813998037604
2
Iteration 7300: Loss = -11440.811905042017
Iteration 7400: Loss = -11440.816228044494
1
Iteration 7500: Loss = -11440.813767816788
2
Iteration 7600: Loss = -11440.811347955527
Iteration 7700: Loss = -11440.816546682589
1
Iteration 7800: Loss = -11440.815869343862
2
Iteration 7900: Loss = -11440.810812193837
Iteration 8000: Loss = -11440.811080146304
1
Iteration 8100: Loss = -11440.810770949294
Iteration 8200: Loss = -11440.810950456813
1
Iteration 8300: Loss = -11440.818671451962
2
Iteration 8400: Loss = -11440.821852291607
3
Iteration 8500: Loss = -11440.819526947256
4
Iteration 8600: Loss = -11440.817001746966
5
Iteration 8700: Loss = -11440.81389616982
6
Iteration 8800: Loss = -11440.83916299771
7
Iteration 8900: Loss = -11440.813322447853
8
Iteration 9000: Loss = -11440.815412462322
9
Iteration 9100: Loss = -11440.825164695796
10
Stopping early at iteration 9100 due to no improvement.
tensor([[ -8.1109,   3.4957],
        [ -8.8004,   4.1852],
        [  5.0371,  -9.6524],
        [  4.1971,  -8.8123],
        [ -8.5575,   3.9423],
        [ -7.6084,   2.9932],
        [ -0.9733,  -3.6419],
        [ -8.8045,   4.1893],
        [  5.0419,  -9.6571],
        [ -6.3887,   1.7735],
        [ -7.1834,   2.5682],
        [  3.7481,  -8.3633],
        [  5.1055,  -9.7207],
        [ -9.5541,   4.9388],
        [ -5.8119,   1.1967],
        [ -8.2274,   3.6122],
        [ -3.7211,  -0.8941],
        [ -7.8004,   3.1852],
        [  2.6909,  -7.3062],
        [  2.7725,  -7.3877],
        [ -8.3187,   3.7034],
        [  2.7660,  -7.3812],
        [ -8.7122,   4.0970],
        [ -7.7288,   3.1136],
        [  1.9563,  -6.5715],
        [ -6.8163,   2.2011],
        [ -7.9968,   3.3815],
        [ -6.6640,   2.0488],
        [ -7.2727,   2.6575],
        [  4.0106,  -8.6258],
        [ -8.4060,   3.7908],
        [ -8.2811,   3.6658],
        [ -4.8293,   0.2140],
        [ -8.2393,   3.6241],
        [ -6.2049,   1.5897],
        [  0.7867,  -5.4019],
        [ -7.8494,   3.2342],
        [  1.8174,  -6.4326],
        [ -8.9658,   4.3506],
        [  4.0241,  -8.6393],
        [ -7.9617,   3.3465],
        [ -7.5915,   2.9763],
        [  3.5249,  -8.1401],
        [  0.7719,  -5.3871],
        [ -7.1957,   2.5804],
        [ -0.0535,  -4.5617],
        [ -8.1675,   3.5522],
        [ -8.2423,   3.6271],
        [ -8.1176,   3.5024],
        [ -7.7155,   3.1003],
        [ -2.3697,  -2.2455],
        [  4.7385,  -9.3538],
        [  3.9693,  -8.5845],
        [ -8.0243,   3.4091],
        [  3.5200,  -8.1352],
        [ -7.1161,   2.5009],
        [ -7.6405,   3.0252],
        [  3.6520,  -8.2672],
        [  2.7090,  -7.3242],
        [ -8.3487,   3.7335],
        [ -2.0465,  -2.5687],
        [  4.1839,  -8.7991],
        [  5.0295,  -9.6447],
        [ -7.8126,   3.1973],
        [  4.4271,  -9.0424],
        [ -8.3646,   3.7494],
        [ -8.3077,   3.6925],
        [ -7.6837,   3.0685],
        [ -9.0736,   4.4584],
        [ -5.0536,   0.4384],
        [ -4.0276,  -0.5876],
        [ -9.3541,   4.7389],
        [ -8.1809,   3.5657],
        [  4.0400,  -8.6552],
        [  3.5864,  -8.2016],
        [  1.6369,  -6.2521],
        [  4.5962,  -9.2114],
        [  2.6989,  -7.3141],
        [  5.4912, -10.1064],
        [  4.3278,  -8.9430],
        [  4.4434,  -9.0586],
        [  1.9102,  -6.5255],
        [  2.0102,  -6.6254],
        [ -8.8056,   4.1904],
        [ -7.1621,   2.5469],
        [  3.2845,  -7.8998],
        [ -6.8327,   2.2175],
        [ -7.5671,   2.9519],
        [  4.5048,  -9.1200],
        [ -8.0518,   3.4366],
        [ -8.4707,   3.8555],
        [ -8.7124,   4.0971],
        [  4.2584,  -8.8737],
        [  3.5518,  -8.1670],
        [ -8.1315,   3.5163],
        [ -8.3239,   3.7086],
        [  3.6005,  -8.2158],
        [ -7.9643,   3.3491],
        [  4.4190,  -9.0342],
        [  4.3001,  -8.9154]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7423, 0.2577],
        [0.2603, 0.7397]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4312, 0.5688], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3995, 0.1097],
         [0.9601, 0.1935]],

        [[0.1064, 0.1028],
         [0.0041, 0.7436]],

        [[0.6462, 0.1043],
         [0.2765, 0.1154]],

        [[0.4147, 0.0997],
         [0.3989, 0.7122]],

        [[0.5387, 0.0934],
         [0.2615, 0.1083]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320648505942
Average Adjusted Rand Index: 0.9839994671371317
Iteration 0: Loss = -17465.641573759865
Iteration 10: Loss = -11443.30879855991
Iteration 20: Loss = -11443.300958991485
Iteration 30: Loss = -11443.300962007575
1
Iteration 40: Loss = -11443.300961225032
2
Iteration 50: Loss = -11443.300961224955
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7356, 0.2644],
        [0.2612, 0.7388]], dtype=torch.float64)
alpha: tensor([0.5262, 0.4738])
beta: tensor([[[0.1898, 0.1101],
         [0.5939, 0.3912]],

        [[0.2670, 0.1024],
         [0.0633, 0.3257]],

        [[0.4524, 0.1044],
         [0.0648, 0.2763]],

        [[0.3236, 0.0998],
         [0.5202, 0.3044]],

        [[0.1478, 0.0934],
         [0.7372, 0.8254]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961941296443
Average Adjusted Rand Index: 0.9761612713656115
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17465.303276908904
Iteration 100: Loss = -11562.841345791654
Iteration 200: Loss = -11444.333260926316
Iteration 300: Loss = -11442.571520828573
Iteration 400: Loss = -11441.915587455955
Iteration 500: Loss = -11441.582155462798
Iteration 600: Loss = -11441.385131459065
Iteration 700: Loss = -11441.257501968743
Iteration 800: Loss = -11441.169370322274
Iteration 900: Loss = -11441.10557344767
Iteration 1000: Loss = -11441.057729923008
Iteration 1100: Loss = -11441.02080870476
Iteration 1200: Loss = -11440.991716485549
Iteration 1300: Loss = -11440.968270846257
Iteration 1400: Loss = -11440.949101150687
Iteration 1500: Loss = -11440.933233322312
Iteration 1600: Loss = -11440.919933360827
Iteration 1700: Loss = -11440.908666162117
Iteration 1800: Loss = -11440.899066696311
Iteration 1900: Loss = -11440.890797613254
Iteration 2000: Loss = -11440.883604798528
Iteration 2100: Loss = -11440.877354884742
Iteration 2200: Loss = -11440.871844512787
Iteration 2300: Loss = -11440.867026605081
Iteration 2400: Loss = -11440.862693666684
Iteration 2500: Loss = -11440.8588381726
Iteration 2600: Loss = -11440.855458861677
Iteration 2700: Loss = -11440.852354493945
Iteration 2800: Loss = -11440.849575605218
Iteration 2900: Loss = -11440.847062335752
Iteration 3000: Loss = -11440.84478040583
Iteration 3100: Loss = -11440.842705696445
Iteration 3200: Loss = -11440.840824735811
Iteration 3300: Loss = -11440.839107034395
Iteration 3400: Loss = -11440.83751752253
Iteration 3500: Loss = -11440.836010849383
Iteration 3600: Loss = -11440.83472485269
Iteration 3700: Loss = -11440.833470814388
Iteration 3800: Loss = -11440.832284899392
Iteration 3900: Loss = -11440.83127463619
Iteration 4000: Loss = -11440.830293653524
Iteration 4100: Loss = -11440.829370965554
Iteration 4200: Loss = -11440.828479527381
Iteration 4300: Loss = -11440.82765963003
Iteration 4400: Loss = -11440.826941901596
Iteration 4500: Loss = -11440.826238037007
Iteration 4600: Loss = -11440.825586338975
Iteration 4700: Loss = -11440.824942232199
Iteration 4800: Loss = -11440.824392973891
Iteration 4900: Loss = -11440.823811498005
Iteration 5000: Loss = -11440.823323464467
Iteration 5100: Loss = -11440.822880137186
Iteration 5200: Loss = -11440.822452029555
Iteration 5300: Loss = -11440.822106652546
Iteration 5400: Loss = -11440.821684653485
Iteration 5500: Loss = -11440.821351269451
Iteration 5600: Loss = -11440.821414195992
1
Iteration 5700: Loss = -11440.82073135339
Iteration 5800: Loss = -11440.820420618651
Iteration 5900: Loss = -11440.820845295953
1
Iteration 6000: Loss = -11440.819935045702
Iteration 6100: Loss = -11440.819805953304
Iteration 6200: Loss = -11440.819958488217
1
Iteration 6300: Loss = -11440.819430933576
Iteration 6400: Loss = -11440.827926598775
1
Iteration 6500: Loss = -11440.819113687201
Iteration 6600: Loss = -11440.818784317657
Iteration 6700: Loss = -11440.818545833645
Iteration 6800: Loss = -11440.818708978175
1
Iteration 6900: Loss = -11440.827919701584
2
Iteration 7000: Loss = -11440.818812995156
3
Iteration 7100: Loss = -11440.817971056478
Iteration 7200: Loss = -11440.820609762217
1
Iteration 7300: Loss = -11440.915050962381
2
Iteration 7400: Loss = -11440.817537099285
Iteration 7500: Loss = -11440.820623101748
1
Iteration 7600: Loss = -11440.817230842316
Iteration 7700: Loss = -11440.817878278114
1
Iteration 7800: Loss = -11440.817011309122
Iteration 7900: Loss = -11440.817680408345
1
Iteration 8000: Loss = -11440.81676763775
Iteration 8100: Loss = -11440.817204708717
1
Iteration 8200: Loss = -11440.816620228015
Iteration 8300: Loss = -11440.81677436156
1
Iteration 8400: Loss = -11440.816526154285
Iteration 8500: Loss = -11440.816375731458
Iteration 8600: Loss = -11440.822818734152
1
Iteration 8700: Loss = -11440.816845042254
2
Iteration 8800: Loss = -11440.816515773316
3
Iteration 8900: Loss = -11440.875191709694
4
Iteration 9000: Loss = -11440.823915234987
5
Iteration 9100: Loss = -11440.859470727068
6
Iteration 9200: Loss = -11440.832091449598
7
Iteration 9300: Loss = -11440.815885263448
Iteration 9400: Loss = -11440.816750460344
1
Iteration 9500: Loss = -11440.989213492885
2
Iteration 9600: Loss = -11440.821792637913
3
Iteration 9700: Loss = -11440.817853596738
4
Iteration 9800: Loss = -11440.81757999288
5
Iteration 9900: Loss = -11440.815680076508
Iteration 10000: Loss = -11440.815910419959
1
Iteration 10100: Loss = -11440.816019758822
2
Iteration 10200: Loss = -11440.81901914458
3
Iteration 10300: Loss = -11440.816219777575
4
Iteration 10400: Loss = -11440.816676236409
5
Iteration 10500: Loss = -11440.815609124742
Iteration 10600: Loss = -11440.860317374405
1
Iteration 10700: Loss = -11440.815661337812
2
Iteration 10800: Loss = -11440.815515759637
Iteration 10900: Loss = -11440.83450936426
1
Iteration 11000: Loss = -11440.815574457813
2
Iteration 11100: Loss = -11440.816265378771
3
Iteration 11200: Loss = -11440.823799822047
4
Iteration 11300: Loss = -11440.81569620584
5
Iteration 11400: Loss = -11440.813231954922
Iteration 11500: Loss = -11440.840208781494
1
Iteration 11600: Loss = -11440.860991937701
2
Iteration 11700: Loss = -11440.820782801686
3
Iteration 11800: Loss = -11440.813005588501
Iteration 11900: Loss = -11440.878333412993
1
Iteration 12000: Loss = -11440.829219681102
2
Iteration 12100: Loss = -11440.853524621285
3
Iteration 12200: Loss = -11440.82427464407
4
Iteration 12300: Loss = -11440.810017288371
Iteration 12400: Loss = -11440.833133531403
1
Iteration 12500: Loss = -11440.809978078627
Iteration 12600: Loss = -11440.810875834935
1
Iteration 12700: Loss = -11440.914955555127
2
Iteration 12800: Loss = -11440.81055171288
3
Iteration 12900: Loss = -11440.809930360829
Iteration 13000: Loss = -11440.814126320394
1
Iteration 13100: Loss = -11440.818751305935
2
Iteration 13200: Loss = -11440.809962698979
3
Iteration 13300: Loss = -11440.809872805203
Iteration 13400: Loss = -11440.810809319682
1
Iteration 13500: Loss = -11440.809985853351
2
Iteration 13600: Loss = -11440.81264110293
3
Iteration 13700: Loss = -11440.814617577214
4
Iteration 13800: Loss = -11440.81098798578
5
Iteration 13900: Loss = -11440.811030054672
6
Iteration 14000: Loss = -11440.825069004215
7
Iteration 14100: Loss = -11440.819327229938
8
Iteration 14200: Loss = -11440.810041645216
9
Iteration 14300: Loss = -11440.842234667241
10
Stopping early at iteration 14300 due to no improvement.
tensor([[  4.7282,  -6.9679],
        [  6.3955,  -7.7867],
        [ -9.8950,   6.7305],
        [ -9.7199,   6.6254],
        [  5.7829,  -7.2468],
        [  4.5580,  -6.0466],
        [ -2.3746,   0.2868],
        [  8.0676, -11.3412],
        [-10.8163,   7.8282],
        [  3.2999,  -4.8411],
        [  3.9014,  -5.8621],
        [ -7.5144,   6.0815],
        [-10.9009,   7.9203],
        [  7.7779,  -9.4988],
        [  2.8113,  -4.1998],
        [  5.9367,  -7.3311],
        [  0.6705,  -2.1753],
        [  5.1179,  -6.5768],
        [ -6.1150,   3.8818],
        [ -6.1030,   4.0570],
        [  5.3828,  -6.8584],
        [ -5.8325,   4.4129],
        [  5.9487,  -7.8020],
        [  4.8198,  -6.3494],
        [ -4.9562,   3.5695],
        [  3.8105,  -5.2098],
        [  5.0160,  -6.4048],
        [  3.5853,  -5.0833],
        [  4.0920,  -5.8078],
        [ -8.1890,   6.7206],
        [  6.9113,  -9.0259],
        [  6.3718,  -7.8653],
        [  1.4453,  -3.6039],
        [  5.6308,  -8.4407],
        [  3.1914,  -4.6052],
        [ -4.1545,   2.0298],
        [  4.8926,  -6.7159],
        [ -4.8697,   3.3787],
        [  6.7710,  -8.2668],
        [ -8.7554,   7.2931],
        [  4.9364,  -6.4162],
        [  4.1897,  -6.3794],
        [ -6.6041,   5.2088],
        [ -3.8994,   2.2549],
        [  3.2667,  -6.5121],
        [ -3.1555,   1.3472],
        [  5.0773,  -8.5686],
        [  5.2083,  -6.8372],
        [  6.1259,  -7.9175],
        [  4.7052,  -6.1205],
        [ -0.6482,  -0.7831],
        [ -9.1259,   6.8412],
        [ -9.5195,   7.2925],
        [  5.7927,  -7.2054],
        [ -7.2696,   4.6438],
        [  3.7689,  -5.8086],
        [  3.8153,  -6.8546],
        [ -9.0277,   4.9852],
        [ -6.1071,   4.0169],
        [  6.4822,  -8.2516],
        [ -1.0312,  -0.5127],
        [ -9.0789,   7.3717],
        [-10.9580,   6.3427],
        [  4.8093,  -6.2141],
        [ -8.5478,   7.1325],
        [  7.3622, -11.9774],
        [  6.6574,  -8.3412],
        [  3.9349,  -7.0430],
        [  6.9224,  -8.4724],
        [  1.4430,  -4.0605],
        [  0.9218,  -2.5378],
        [  6.3076, -10.8395],
        [  6.2075,  -7.7684],
        [ -8.9307,   7.4465],
        [ -7.5357,   6.1239],
        [ -4.7804,   3.1075],
        [-10.9501,   7.8865],
        [ -5.9629,   4.1473],
        [ -9.6418,   7.8146],
        [ -8.3345,   6.6969],
        [-11.1521,   7.1969],
        [ -5.4361,   2.9924],
        [ -6.6237,   2.0085],
        [  5.0401,  -9.6553],
        [  3.4421,  -6.2668],
        [ -6.7137,   5.0731],
        [  3.8115,  -5.2072],
        [  4.4951,  -6.0249],
        [ -9.0510,   7.0492],
        [  5.0172,  -6.5493],
        [  7.0115,  -9.3213],
        [  5.6576,  -8.1716],
        [ -9.1508,   7.0076],
        [ -7.2732,   5.7976],
        [  5.1455,  -6.5731],
        [  6.3937,  -8.1406],
        [ -7.1991,   5.7832],
        [  5.4357,  -6.8442],
        [ -9.0021,   7.5976],
        [ -8.1999,   6.5470]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7400, 0.2600],
        [0.2564, 0.7436]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5670, 0.4330], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1939, 0.1095],
         [0.5939, 0.3980]],

        [[0.2670, 0.1019],
         [0.0633, 0.3257]],

        [[0.4524, 0.1043],
         [0.0648, 0.2763]],

        [[0.3236, 0.0996],
         [0.5202, 0.3044]],

        [[0.1478, 0.0934],
         [0.7372, 0.8254]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320648505942
Average Adjusted Rand Index: 0.9839994671371317
Iteration 0: Loss = -36022.33205892765
Iteration 10: Loss = -11455.704375630949
Iteration 20: Loss = -11443.301036431321
Iteration 30: Loss = -11443.300954583117
Iteration 40: Loss = -11443.300961226116
1
Iteration 50: Loss = -11443.300961224948
2
Iteration 60: Loss = -11443.300961224955
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7356, 0.2644],
        [0.2612, 0.7388]], dtype=torch.float64)
alpha: tensor([0.5262, 0.4738])
beta: tensor([[[0.1898, 0.1101],
         [0.5733, 0.3912]],

        [[0.0462, 0.1024],
         [0.4313, 0.8941]],

        [[0.8626, 0.1044],
         [0.3137, 0.4170]],

        [[0.9308, 0.0998],
         [0.2391, 0.4751]],

        [[0.2957, 0.0934],
         [0.6753, 0.6820]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961941296443
Average Adjusted Rand Index: 0.9761612713656115
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36022.289092604224
Iteration 100: Loss = -12145.288608472718
Iteration 200: Loss = -11969.035932996496
Iteration 300: Loss = -11848.370425910949
Iteration 400: Loss = -11812.901799434905
Iteration 500: Loss = -11779.17716414357
Iteration 600: Loss = -11766.833055142723
Iteration 700: Loss = -11763.37301533384
Iteration 800: Loss = -11761.697166574297
Iteration 900: Loss = -11761.481740992
Iteration 1000: Loss = -11761.284689059516
Iteration 1100: Loss = -11751.199913622577
Iteration 1200: Loss = -11751.086714728848
Iteration 1300: Loss = -11746.408330900696
Iteration 1400: Loss = -11746.291943502287
Iteration 1500: Loss = -11746.16007651267
Iteration 1600: Loss = -11739.497946133499
Iteration 1700: Loss = -11739.46033254628
Iteration 1800: Loss = -11739.430187087059
Iteration 1900: Loss = -11739.404948578262
Iteration 2000: Loss = -11739.38340151881
Iteration 2100: Loss = -11739.364810144476
Iteration 2200: Loss = -11739.348612316926
Iteration 2300: Loss = -11739.334215451781
Iteration 2400: Loss = -11739.321141395629
Iteration 2500: Loss = -11739.305267094851
Iteration 2600: Loss = -11721.5513899129
Iteration 2700: Loss = -11721.535980581046
Iteration 2800: Loss = -11721.525895676654
Iteration 2900: Loss = -11721.517819821516
Iteration 3000: Loss = -11721.510563456975
Iteration 3100: Loss = -11721.50339690428
Iteration 3200: Loss = -11721.481607270756
Iteration 3300: Loss = -11712.674030555796
Iteration 3400: Loss = -11712.667538551981
Iteration 3500: Loss = -11712.660638123012
Iteration 3600: Loss = -11712.653886529035
Iteration 3700: Loss = -11712.649606885398
Iteration 3800: Loss = -11712.64600005848
Iteration 3900: Loss = -11712.642696882667
Iteration 4000: Loss = -11712.639640955642
Iteration 4100: Loss = -11712.636783606235
Iteration 4200: Loss = -11712.64393694751
1
Iteration 4300: Loss = -11712.631543139407
Iteration 4400: Loss = -11712.628965988288
Iteration 4500: Loss = -11712.626092716453
Iteration 4600: Loss = -11711.888964364893
Iteration 4700: Loss = -11706.893702992344
Iteration 4800: Loss = -11706.888732019735
Iteration 4900: Loss = -11703.793100634237
Iteration 5000: Loss = -11696.38844623817
Iteration 5100: Loss = -11696.382880364821
Iteration 5200: Loss = -11696.192258829618
Iteration 5300: Loss = -11691.404503999764
Iteration 5400: Loss = -11687.534545495879
Iteration 5500: Loss = -11681.865346937617
Iteration 5600: Loss = -11681.850058165186
Iteration 5700: Loss = -11676.237669055748
Iteration 5800: Loss = -11676.216236383032
Iteration 5900: Loss = -11667.638263920544
Iteration 6000: Loss = -11660.466604268979
Iteration 6100: Loss = -11656.688175778443
Iteration 6200: Loss = -11654.456330005254
Iteration 6300: Loss = -11654.421513176967
Iteration 6400: Loss = -11649.074138125075
Iteration 6500: Loss = -11647.605255981764
Iteration 6600: Loss = -11631.900835079738
Iteration 6700: Loss = -11623.461370132161
Iteration 6800: Loss = -11598.693688826748
Iteration 6900: Loss = -11569.633473633809
Iteration 7000: Loss = -11563.139819620646
Iteration 7100: Loss = -11528.636303835268
Iteration 7200: Loss = -11528.519950532365
Iteration 7300: Loss = -11515.082051550844
Iteration 7400: Loss = -11507.718822541761
Iteration 7500: Loss = -11507.720714281002
1
Iteration 7600: Loss = -11494.397060543592
Iteration 7700: Loss = -11481.86331031465
Iteration 7800: Loss = -11481.641541866775
Iteration 7900: Loss = -11481.63767811194
Iteration 8000: Loss = -11481.63847973913
1
Iteration 8100: Loss = -11461.920420632385
Iteration 8200: Loss = -11461.901393912281
Iteration 8300: Loss = -11461.901815826228
1
Iteration 8400: Loss = -11461.89227277512
Iteration 8500: Loss = -11461.894123457652
1
Iteration 8600: Loss = -11461.894939122432
2
Iteration 8700: Loss = -11461.897245881484
3
Iteration 8800: Loss = -11461.888300096169
Iteration 8900: Loss = -11461.89213826439
1
Iteration 9000: Loss = -11461.88731100247
Iteration 9100: Loss = -11461.888502553511
1
Iteration 9200: Loss = -11461.88870168088
2
Iteration 9300: Loss = -11461.886515878163
Iteration 9400: Loss = -11461.886623899507
1
Iteration 9500: Loss = -11461.886172643366
Iteration 9600: Loss = -11461.88568669157
Iteration 9700: Loss = -11461.889585283998
1
Iteration 9800: Loss = -11461.884989555314
Iteration 9900: Loss = -11461.824942657342
Iteration 10000: Loss = -11456.764055523228
Iteration 10100: Loss = -11456.775328225545
1
Iteration 10200: Loss = -11456.779036240061
2
Iteration 10300: Loss = -11456.791640581889
3
Iteration 10400: Loss = -11456.768560860823
4
Iteration 10500: Loss = -11456.766632931189
5
Iteration 10600: Loss = -11456.763181197439
Iteration 10700: Loss = -11456.769249553208
1
Iteration 10800: Loss = -11456.763953117492
2
Iteration 10900: Loss = -11456.763383500043
3
Iteration 11000: Loss = -11456.83325514963
4
Iteration 11100: Loss = -11456.779747479655
5
Iteration 11200: Loss = -11456.771757749271
6
Iteration 11300: Loss = -11456.76321050014
7
Iteration 11400: Loss = -11456.763567818329
8
Iteration 11500: Loss = -11456.870002480546
9
Iteration 11600: Loss = -11441.386168370249
Iteration 11700: Loss = -11441.378730135657
Iteration 11800: Loss = -11441.380055990778
1
Iteration 11900: Loss = -11441.379324550824
2
Iteration 12000: Loss = -11441.392695766452
3
Iteration 12100: Loss = -11441.381125912372
4
Iteration 12200: Loss = -11441.38649275691
5
Iteration 12300: Loss = -11441.387879903807
6
Iteration 12400: Loss = -11441.384662009064
7
Iteration 12500: Loss = -11441.379139156716
8
Iteration 12600: Loss = -11441.409951703166
9
Iteration 12700: Loss = -11441.38193355245
10
Stopping early at iteration 12700 due to no improvement.
tensor([[  4.9360,  -6.4747],
        [  6.3135,  -7.7820],
        [ -8.3012,   6.8947],
        [ -9.1676,   7.1271],
        [  5.3981,  -7.3113],
        [  4.3543,  -5.9192],
        [ -2.7779,   0.0922],
        [  5.8928,  -7.3501],
        [ -9.0066,   7.5694],
        [  3.0185,  -4.7774],
        [  4.0171,  -5.4329],
        [ -7.3289,   5.9396],
        [ -9.1916,   7.5451],
        [  7.3649,  -8.7939],
        [  2.4324,  -4.3761],
        [  4.3326,  -8.6871],
        [  0.4081,  -2.0890],
        [  4.8101,  -6.6223],
        [ -7.3677,   2.7525],
        [ -6.1573,   4.2012],
        [  4.4451,  -8.0841],
        [ -5.9644,   4.5072],
        [  5.6791,  -7.3952],
        [  4.6589,  -6.1752],
        [ -5.5953,   3.1830],
        [  3.8818,  -5.3527],
        [  4.8597,  -7.3212],
        [  1.8934,  -6.5087],
        [  4.0940,  -5.4857],
        [ -8.0675,   5.8718],
        [  6.5581,  -7.9730],
        [  6.3078,  -7.8256],
        [  1.2558,  -4.0861],
        [  6.1320,  -7.6519],
        [  3.0853,  -4.9322],
        [ -4.2712,   1.5600],
        [  8.5010, -10.1527],
        [ -7.5555,   6.0728],
        [  6.0925,  -7.8014],
        [ -8.5606,   5.7986],
        [  4.8059,  -6.2275],
        [  4.1867,  -6.0864],
        [ -7.1556,   5.7566],
        [ -3.3563,   1.8785],
        [  4.2005,  -5.8138],
        [ -3.3398,   1.4052],
        [  4.8168,  -9.3689],
        [  5.2797,  -6.6840],
        [  5.8217,  -7.7023],
        [  3.8646,  -6.6038],
        [  6.8998,  -9.3217],
        [ -8.0029,   6.5700],
        [ -6.9529,   5.5663],
        [  5.7994,  -7.2669],
        [ -6.4755,   5.0250],
        [  3.9504,  -5.3368],
        [  4.4394,  -5.9296],
        [ -8.2792,   5.9154],
        [ -5.9475,   4.3291],
        [  6.2408,  -8.7220],
        [ -0.5238,  -0.9764],
        [ -8.3457,   6.6519],
        [ -8.8973,   7.4249],
        [  4.6663,  -6.0745],
        [ -8.4684,   6.9355],
        [  6.1809,  -7.7261],
        [  6.2010,  -8.0929],
        [  4.8788,  -6.3192],
        [  6.2166,  -8.2929],
        [  2.2096,  -3.5974],
        [ -0.2061,  -3.2901],
        [  6.5382, -10.8730],
        [  6.3409,  -7.9236],
        [ -7.9599,   6.5611],
        [ -7.0688,   5.5432],
        [ -4.7553,   3.3622],
        [ -8.7427,   7.3564],
        [ -5.9655,   4.3554],
        [ -9.5585,   8.1565],
        [ -7.8334,   6.1855],
        [ -8.4751,   6.7685],
        [ -5.2801,   3.4437],
        [ -5.4335,   3.4149],
        [  6.0035,  -8.5342],
        [  4.0037,  -5.4458],
        [ -6.7294,   5.2119],
        [  3.6916,  -5.0888],
        [  4.3633,  -5.9130],
        [ -8.6481,   6.7299],
        [  4.9231,  -6.3098],
        [  6.7431,  -8.1936],
        [  7.7351,  -9.1287],
        [ -8.9298,   6.8539],
        [ -6.8665,   5.4590],
        [  5.0480,  -6.4448],
        [  6.0530,  -7.4867],
        [ -8.1153,   3.5001],
        [  5.1985,  -6.7776],
        [ -8.6423,   6.9485],
        [ -7.8546,   6.1954]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7380, 0.2620],
        [0.2542, 0.7458]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5761, 0.4239], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1937, 0.1105],
         [0.5733, 0.4005]],

        [[0.0462, 0.1022],
         [0.4313, 0.8941]],

        [[0.8626, 0.1043],
         [0.3137, 0.4170]],

        [[0.9308, 0.0995],
         [0.2391, 0.4751]],

        [[0.2957, 0.0929],
         [0.6753, 0.6820]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961941296443
Average Adjusted Rand Index: 0.9761603187969168
Iteration 0: Loss = -35240.69245541681
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.8774,    nan]],

        [[0.9702,    nan],
         [0.0919, 0.4163]],

        [[0.5011,    nan],
         [0.9588, 0.4479]],

        [[0.0190,    nan],
         [0.0598, 0.4526]],

        [[0.1784,    nan],
         [0.6380, 0.1414]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35236.665826469754
Iteration 100: Loss = -12169.255494335894
Iteration 200: Loss = -12165.760267877438
Iteration 300: Loss = -12163.968938715721
Iteration 400: Loss = -12162.825364519602
Iteration 500: Loss = -12161.947504582593
Iteration 600: Loss = -12160.642362070996
Iteration 700: Loss = -12156.40920210888
Iteration 800: Loss = -12152.861466126196
Iteration 900: Loss = -12144.563507313753
Iteration 1000: Loss = -12141.73293724103
Iteration 1100: Loss = -12140.035914019509
Iteration 1200: Loss = -12138.276892483702
Iteration 1300: Loss = -12134.866506368367
Iteration 1400: Loss = -12119.346200228778
Iteration 1500: Loss = -12079.91984289418
Iteration 1600: Loss = -12007.033385103137
Iteration 1700: Loss = -11938.281173488353
Iteration 1800: Loss = -11893.96041705089
Iteration 1900: Loss = -11873.5693371021
Iteration 2000: Loss = -11851.823776923173
Iteration 2100: Loss = -11849.360651450728
Iteration 2200: Loss = -11841.265496432481
Iteration 2300: Loss = -11836.932822559425
Iteration 2400: Loss = -11829.477769568286
Iteration 2500: Loss = -11829.267157033017
Iteration 2600: Loss = -11829.147110787942
Iteration 2700: Loss = -11829.062047116724
Iteration 2800: Loss = -11828.987452164878
Iteration 2900: Loss = -11814.710253261166
Iteration 3000: Loss = -11814.650258484306
Iteration 3100: Loss = -11809.394483004031
Iteration 3200: Loss = -11809.30333970668
Iteration 3300: Loss = -11803.823314844707
Iteration 3400: Loss = -11803.724167474931
Iteration 3500: Loss = -11792.678133224907
Iteration 3600: Loss = -11792.644971587268
Iteration 3700: Loss = -11792.617255276597
Iteration 3800: Loss = -11792.593006980333
Iteration 3900: Loss = -11792.570601615158
Iteration 4000: Loss = -11792.593482774573
1
Iteration 4100: Loss = -11792.519195498422
Iteration 4200: Loss = -11792.487054664336
Iteration 4300: Loss = -11792.45578793259
Iteration 4400: Loss = -11792.083049483193
Iteration 4500: Loss = -11783.560110768758
Iteration 4600: Loss = -11783.51655885332
Iteration 4700: Loss = -11783.484966826858
Iteration 4800: Loss = -11783.458477999991
Iteration 4900: Loss = -11783.437000882643
Iteration 5000: Loss = -11783.416352846249
Iteration 5100: Loss = -11783.39468379792
Iteration 5200: Loss = -11783.149492498644
Iteration 5300: Loss = -11783.139824259139
Iteration 5400: Loss = -11783.088134625272
Iteration 5500: Loss = -11783.058322137154
Iteration 5600: Loss = -11783.05105252761
Iteration 5700: Loss = -11783.04727016294
Iteration 5800: Loss = -11783.03641509989
Iteration 5900: Loss = -11783.026802149949
Iteration 6000: Loss = -11782.90110481918
Iteration 6100: Loss = -11772.980957681659
Iteration 6200: Loss = -11772.976873725045
Iteration 6300: Loss = -11772.97303704197
Iteration 6400: Loss = -11759.419914272155
Iteration 6500: Loss = -11754.831127290068
Iteration 6600: Loss = -11753.182373808786
Iteration 6700: Loss = -11753.176660768368
Iteration 6800: Loss = -11753.180717107769
1
Iteration 6900: Loss = -11753.191775086763
2
Iteration 7000: Loss = -11753.18434764443
3
Iteration 7100: Loss = -11753.16728448433
Iteration 7200: Loss = -11753.164953502888
Iteration 7300: Loss = -11753.112034691823
Iteration 7400: Loss = -11753.155991231028
1
Iteration 7500: Loss = -11753.104862493761
Iteration 7600: Loss = -11753.10468489183
Iteration 7700: Loss = -11753.102552804972
Iteration 7800: Loss = -11753.103303530906
1
Iteration 7900: Loss = -11753.100543899382
Iteration 8000: Loss = -11753.095069219306
Iteration 8100: Loss = -11753.094138279012
Iteration 8200: Loss = -11753.093344318473
Iteration 8300: Loss = -11753.092640138802
Iteration 8400: Loss = -11753.092145562276
Iteration 8500: Loss = -11753.091276118646
Iteration 8600: Loss = -11753.090664265785
Iteration 8700: Loss = -11750.708999601786
Iteration 8800: Loss = -11750.707859855613
Iteration 8900: Loss = -11750.707016112325
Iteration 9000: Loss = -11750.707418295959
1
Iteration 9100: Loss = -11750.67693980228
Iteration 9200: Loss = -11750.675474613405
Iteration 9300: Loss = -11750.674940354742
Iteration 9400: Loss = -11750.674322360443
Iteration 9500: Loss = -11750.685116977844
1
Iteration 9600: Loss = -11750.673446366543
Iteration 9700: Loss = -11750.784429076355
1
Iteration 9800: Loss = -11750.673188837758
Iteration 9900: Loss = -11750.680805042697
1
Iteration 10000: Loss = -11750.787931424755
2
Iteration 10100: Loss = -11750.67362546811
3
Iteration 10200: Loss = -11750.689007697678
4
Iteration 10300: Loss = -11750.671228035115
Iteration 10400: Loss = -11750.66879717818
Iteration 10500: Loss = -11750.650175921954
Iteration 10600: Loss = -11750.59342917915
Iteration 10700: Loss = -11750.593906498392
1
Iteration 10800: Loss = -11750.593082340858
Iteration 10900: Loss = -11736.16398270685
Iteration 11000: Loss = -11736.204291254633
1
Iteration 11100: Loss = -11736.159443823792
Iteration 11200: Loss = -11736.162478757755
1
Iteration 11300: Loss = -11736.166529317656
2
Iteration 11400: Loss = -11736.154450569082
Iteration 11500: Loss = -11736.163673887735
1
Iteration 11600: Loss = -11736.153944246294
Iteration 11700: Loss = -11735.503967978662
Iteration 11800: Loss = -11735.497397866544
Iteration 11900: Loss = -11735.492903368422
Iteration 12000: Loss = -11735.492785649347
Iteration 12100: Loss = -11735.494290571516
1
Iteration 12200: Loss = -11735.507127306113
2
Iteration 12300: Loss = -11735.491886717715
Iteration 12400: Loss = -11735.492159154355
1
Iteration 12500: Loss = -11735.491649346992
Iteration 12600: Loss = -11735.496158737733
1
Iteration 12700: Loss = -11735.491675798976
2
Iteration 12800: Loss = -11735.494811803024
3
Iteration 12900: Loss = -11735.500033639328
4
Iteration 13000: Loss = -11735.49140886411
Iteration 13100: Loss = -11735.491237024025
Iteration 13200: Loss = -11728.447084010173
Iteration 13300: Loss = -11728.136161902401
Iteration 13400: Loss = -11728.1140258646
Iteration 13500: Loss = -11728.100023744018
Iteration 13600: Loss = -11728.100180205684
1
Iteration 13700: Loss = -11728.09487181261
Iteration 13800: Loss = -11728.095523900985
1
Iteration 13900: Loss = -11728.093604027841
Iteration 14000: Loss = -11728.093546764641
Iteration 14100: Loss = -11728.132522952754
1
Iteration 14200: Loss = -11728.094642515285
2
Iteration 14300: Loss = -11722.464428768973
Iteration 14400: Loss = -11722.461899481112
Iteration 14500: Loss = -11722.455534985473
Iteration 14600: Loss = -11722.477769883537
1
Iteration 14700: Loss = -11722.455722441238
2
Iteration 14800: Loss = -11722.488311767029
3
Iteration 14900: Loss = -11722.455392672246
Iteration 15000: Loss = -11722.455001194572
Iteration 15100: Loss = -11722.461047179413
1
Iteration 15200: Loss = -11722.454837412504
Iteration 15300: Loss = -11722.455565817761
1
Iteration 15400: Loss = -11722.454730155867
Iteration 15500: Loss = -11722.45604236637
1
Iteration 15600: Loss = -11722.456363222575
2
Iteration 15700: Loss = -11722.463609587605
3
Iteration 15800: Loss = -11722.453607851636
Iteration 15900: Loss = -11722.439764827523
Iteration 16000: Loss = -11722.463899693117
1
Iteration 16100: Loss = -11722.448175132418
2
Iteration 16200: Loss = -11722.442354529865
3
Iteration 16300: Loss = -11709.038865920627
Iteration 16400: Loss = -11709.039490522959
1
Iteration 16500: Loss = -11709.039103619047
2
Iteration 16600: Loss = -11709.038893794856
3
Iteration 16700: Loss = -11709.0620395391
4
Iteration 16800: Loss = -11709.038529244563
Iteration 16900: Loss = -11709.038952748779
1
Iteration 17000: Loss = -11709.049463187881
2
Iteration 17100: Loss = -11709.136133702968
3
Iteration 17200: Loss = -11708.779763973913
Iteration 17300: Loss = -11708.763301777442
Iteration 17400: Loss = -11708.757121988547
Iteration 17500: Loss = -11708.782839463629
1
Iteration 17600: Loss = -11708.756983413457
Iteration 17700: Loss = -11708.759422792202
1
Iteration 17800: Loss = -11708.865594636298
2
Iteration 17900: Loss = -11708.756635771404
Iteration 18000: Loss = -11708.760186754507
1
Iteration 18100: Loss = -11708.756693951353
2
Iteration 18200: Loss = -11708.754449496482
Iteration 18300: Loss = -11708.755104024496
1
Iteration 18400: Loss = -11708.293571104776
Iteration 18500: Loss = -11708.30017068347
1
Iteration 18600: Loss = -11708.377093140076
2
Iteration 18700: Loss = -11708.279386802742
Iteration 18800: Loss = -11708.26929447047
Iteration 18900: Loss = -11708.272419244899
1
Iteration 19000: Loss = -11708.277098387998
2
Iteration 19100: Loss = -11708.26912006627
Iteration 19200: Loss = -11708.268762941727
Iteration 19300: Loss = -11708.275126007251
1
Iteration 19400: Loss = -11708.269442424144
2
Iteration 19500: Loss = -11708.269274780594
3
Iteration 19600: Loss = -11708.376307133081
4
Iteration 19700: Loss = -11708.268735952948
Iteration 19800: Loss = -11708.273900671695
1
Iteration 19900: Loss = -11708.308757870176
2
tensor([[  2.4099,  -3.8658],
        [ -0.3129,  -1.2235],
        [  6.8801,  -8.4312],
        [  7.2972,  -8.7467],
        [  2.3619,  -3.7497],
        [  7.1708,  -9.1483],
        [  2.7834,  -4.4407],
        [  2.7491,  -4.5409],
        [  7.5541,  -9.0643],
        [  2.7508,  -4.8357],
        [  1.2537,  -2.8753],
        [  5.0795,  -6.7340],
        [  7.2062,  -8.9512],
        [ -4.1114,   2.6284],
        [  0.0722,  -1.4619],
        [ -0.7467,  -1.1265],
        [  5.9936,  -9.0740],
        [  1.4845,  -3.3766],
        [  7.2501, -11.2013],
        [  1.9700,  -6.5852],
        [  1.3210,  -2.8274],
        [  4.2936,  -5.8872],
        [  2.4999,  -4.2425],
        [  1.8683,  -3.3981],
        [  5.2514,  -7.0143],
        [  3.4152,  -4.8084],
        [  3.9790,  -6.1643],
        [  1.5716,  -3.9886],
        [  1.8627,  -3.2802],
        [  5.9854,  -8.0003],
        [ -0.1957,  -1.1907],
        [ -3.7600,   1.2437],
        [ -1.6818,  -1.4443],
        [  1.5482,  -3.8040],
        [  1.5213,  -3.2534],
        [  5.3077,  -6.7077],
        [  1.6404,  -3.0267],
        [  6.4535,  -7.9052],
        [ -0.6648,  -1.1716],
        [  6.4104,  -7.9037],
        [  1.7173,  -3.6024],
        [  0.5166,  -2.1465],
        [  5.6938,  -7.1359],
        [  2.6693,  -4.0562],
        [  1.8069,  -3.4363],
        [  2.9298,  -4.9767],
        [  2.1852,  -3.7238],
        [  2.2564,  -3.9676],
        [ -4.0332,   2.6459],
        [ -0.0945,  -1.3479],
        [  0.8140,  -2.2708],
        [  6.9614,  -8.3734],
        [  5.2140,  -6.7560],
        [  1.1063,  -2.5117],
        [  4.8687,  -6.5225],
        [ -3.1017,   1.3880],
        [  2.8287,  -5.2034],
        [  8.6863, -10.1502],
        [  4.4719,  -6.2085],
        [ -4.1656,   2.3808],
        [  3.9604,  -6.3396],
        [  2.8279,  -4.4868],
        [  6.7266,  -8.2612],
        [  2.4984,  -6.3861],
        [  7.0364,  -8.5679],
        [  2.8565,  -4.8916],
        [ -3.2589,   0.2629],
        [  2.0918,  -3.4782],
        [  2.6620,  -4.3375],
        [  2.7380,  -4.1973],
        [  3.1817,  -4.6838],
        [  2.7771,  -4.1644],
        [ -4.0346,   2.4682],
        [  2.8783,  -5.9119],
        [  5.2705,  -6.6756],
        [  2.2940,  -4.1510],
        [  7.3017, -10.0118],
        [  5.7382,  -8.5203],
        [  7.0934,  -8.8515],
        [  4.9195,  -7.0139],
        [  6.0237,  -7.5027],
        [  6.9230,  -9.2279],
        [  4.2030,  -6.3335],
        [  0.1940,  -1.9933],
        [ -2.9486,   0.7656],
        [  4.2963,  -5.6842],
        [ -3.3712,   1.9718],
        [ -5.3100,   2.5815],
        [  6.2431,  -7.8627],
        [  1.8420,  -3.3637],
        [  2.5471,  -4.4432],
        [  0.5782,  -1.9834],
        [  6.3617,  -7.8509],
        [  7.3086,  -8.6949],
        [  1.0352,  -2.4233],
        [  0.6843,  -3.0734],
        [  5.1523,  -7.0236],
        [  0.0783,  -2.3289],
        [  5.4089,  -8.7882],
        [  4.9614,  -6.9484]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7188, 0.2812],
        [0.3149, 0.6851]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8722, 0.1278], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2101, 0.1086],
         [0.8774, 0.3935]],

        [[0.9702, 0.0923],
         [0.0919, 0.4163]],

        [[0.5011, 0.1157],
         [0.9588, 0.4479]],

        [[0.0190, 0.1002],
         [0.0598, 0.4526]],

        [[0.1784, 0.0934],
         [0.6380, 0.1414]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.0026936560438907487
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 36
Adjusted Rand Index: 0.07167087033623441
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080875752406894
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.25252550236966104
Average Adjusted Rand Index: 0.5754129579066066
Iteration 0: Loss = -35341.5887350298
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.9112,    nan]],

        [[0.8660,    nan],
         [0.6917, 0.4377]],

        [[0.2724,    nan],
         [0.6417, 0.7803]],

        [[0.9009,    nan],
         [0.7857, 0.1326]],

        [[0.8747,    nan],
         [0.8264, 0.8694]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35340.64624409856
Iteration 100: Loss = -12166.729473953057
Iteration 200: Loss = -12162.968435997243
Iteration 300: Loss = -12161.547459177871
Iteration 400: Loss = -12160.755254005377
Iteration 500: Loss = -12159.273548442201
Iteration 600: Loss = -12138.508356127142
Iteration 700: Loss = -12104.251811091952
Iteration 800: Loss = -12041.879234124008
Iteration 900: Loss = -11916.239584371897
Iteration 1000: Loss = -11832.708067217638
Iteration 1100: Loss = -11781.618843731114
Iteration 1200: Loss = -11742.188347787065
Iteration 1300: Loss = -11735.669238850523
Iteration 1400: Loss = -11732.365182144822
Iteration 1500: Loss = -11718.304657148869
Iteration 1600: Loss = -11716.002177476577
Iteration 1700: Loss = -11715.828099226224
Iteration 1800: Loss = -11715.688577334498
Iteration 1900: Loss = -11715.524847032804
Iteration 2000: Loss = -11715.253555418776
Iteration 2100: Loss = -11714.861005849443
Iteration 2200: Loss = -11714.758131700939
Iteration 2300: Loss = -11714.706445408849
Iteration 2400: Loss = -11714.605055732756
Iteration 2500: Loss = -11714.351301113154
Iteration 2600: Loss = -11713.21003316487
Iteration 2700: Loss = -11713.029123812128
Iteration 2800: Loss = -11712.978009278664
Iteration 2900: Loss = -11712.941250004407
Iteration 3000: Loss = -11712.888577713646
Iteration 3100: Loss = -11711.987986382033
Iteration 3200: Loss = -11704.773189964928
Iteration 3300: Loss = -11694.398564484283
Iteration 3400: Loss = -11694.320247700205
Iteration 3500: Loss = -11691.916384203012
Iteration 3600: Loss = -11686.16852028956
Iteration 3700: Loss = -11685.9205971208
Iteration 3800: Loss = -11685.891932612105
Iteration 3900: Loss = -11684.23788354723
Iteration 4000: Loss = -11684.031863859132
Iteration 4100: Loss = -11684.019479098157
Iteration 4200: Loss = -11684.009348528585
Iteration 4300: Loss = -11683.690061165598
Iteration 4400: Loss = -11678.835906860024
Iteration 4500: Loss = -11678.82304063176
Iteration 4600: Loss = -11678.783968244594
Iteration 4700: Loss = -11678.542169725994
Iteration 4800: Loss = -11678.526290838858
Iteration 4900: Loss = -11678.095486033371
Iteration 5000: Loss = -11676.9563946392
Iteration 5100: Loss = -11668.748748859805
Iteration 5200: Loss = -11668.741303313414
Iteration 5300: Loss = -11668.734361048084
Iteration 5400: Loss = -11668.348185101118
Iteration 5500: Loss = -11667.215720750613
Iteration 5600: Loss = -11660.789164222668
Iteration 5700: Loss = -11658.08365738069
Iteration 5800: Loss = -11658.072234544821
Iteration 5900: Loss = -11658.067065621191
Iteration 6000: Loss = -11658.049613176503
Iteration 6100: Loss = -11658.011891376875
Iteration 6200: Loss = -11657.96814083647
Iteration 6300: Loss = -11650.44835403807
Iteration 6400: Loss = -11644.453072458238
Iteration 6500: Loss = -11644.42684762911
Iteration 6600: Loss = -11644.424218746033
Iteration 6700: Loss = -11633.103767670047
Iteration 6800: Loss = -11633.0864387574
Iteration 6900: Loss = -11621.706589996733
Iteration 7000: Loss = -11621.71617814217
1
Iteration 7100: Loss = -11621.701546988455
Iteration 7200: Loss = -11621.704395952258
1
Iteration 7300: Loss = -11621.7024187174
2
Iteration 7400: Loss = -11621.69881325181
Iteration 7500: Loss = -11621.697723196105
Iteration 7600: Loss = -11621.716164852462
1
Iteration 7700: Loss = -11621.697239757465
Iteration 7800: Loss = -11621.699329229476
1
Iteration 7900: Loss = -11621.694556942575
Iteration 8000: Loss = -11606.70643174647
Iteration 8100: Loss = -11606.708175364825
1
Iteration 8200: Loss = -11606.705482812216
Iteration 8300: Loss = -11606.702839751391
Iteration 8400: Loss = -11606.701268005638
Iteration 8500: Loss = -11606.245831730526
Iteration 8600: Loss = -11606.188927865518
Iteration 8700: Loss = -11605.349736938513
Iteration 8800: Loss = -11605.348575207981
Iteration 8900: Loss = -11605.353058495795
1
Iteration 9000: Loss = -11605.347271654331
Iteration 9100: Loss = -11593.520275781999
Iteration 9200: Loss = -11593.393681583404
Iteration 9300: Loss = -11593.39514760444
1
Iteration 9400: Loss = -11593.393976768744
2
Iteration 9500: Loss = -11593.419374960577
3
Iteration 9600: Loss = -11593.416915201262
4
Iteration 9700: Loss = -11593.431910682033
5
Iteration 9800: Loss = -11593.399762643468
6
Iteration 9900: Loss = -11586.662508954383
Iteration 10000: Loss = -11578.920252932017
Iteration 10100: Loss = -11578.917853915844
Iteration 10200: Loss = -11578.901878900595
Iteration 10300: Loss = -11578.904208097223
1
Iteration 10400: Loss = -11578.890101641964
Iteration 10500: Loss = -11578.853418968723
Iteration 10600: Loss = -11578.857468051467
1
Iteration 10700: Loss = -11578.852586204106
Iteration 10800: Loss = -11578.851990150763
Iteration 10900: Loss = -11578.855113862603
1
Iteration 11000: Loss = -11578.86214594975
2
Iteration 11100: Loss = -11578.869519968042
3
Iteration 11200: Loss = -11578.87159620187
4
Iteration 11300: Loss = -11578.857217856406
5
Iteration 11400: Loss = -11578.865494030564
6
Iteration 11500: Loss = -11578.89095210495
7
Iteration 11600: Loss = -11578.926081879934
8
Iteration 11700: Loss = -11578.863971713541
9
Iteration 11800: Loss = -11578.87555637239
10
Stopping early at iteration 11800 due to no improvement.
tensor([[ 3.4321e+00, -4.8283e+00],
        [ 4.4163e+00, -6.0800e+00],
        [-8.5932e+00,  5.2349e+00],
        [-6.0475e+00,  4.4345e+00],
        [ 3.6994e+00, -5.0889e+00],
        [ 3.3674e+00, -6.0999e+00],
        [ 5.4120e+00, -7.7671e+00],
        [ 3.1090e+00, -6.3009e+00],
        [-7.8755e+00,  6.4599e+00],
        [ 7.0441e+00, -8.5878e+00],
        [ 4.0009e+00, -5.4140e+00],
        [-8.7515e+00,  6.2726e+00],
        [-8.0175e+00,  6.4237e+00],
        [ 5.8491e+00, -7.6412e+00],
        [ 1.9936e+00, -3.6783e+00],
        [ 2.9283e+00, -6.0335e+00],
        [ 3.5435e-01, -2.1641e+00],
        [ 4.6588e+00, -6.9245e+00],
        [-3.1051e+00,  1.6974e+00],
        [-5.1288e+00,  3.7410e+00],
        [ 4.1803e+00, -5.8393e+00],
        [-3.4170e+00,  5.6222e-01],
        [ 3.1309e+00, -7.7461e+00],
        [ 3.1357e+00, -4.8107e+00],
        [-3.1233e+00,  1.6497e+00],
        [ 2.2033e+00, -4.2841e+00],
        [ 4.3279e+00, -5.7635e+00],
        [ 1.6883e+00, -4.0925e+00],
        [ 5.6178e+00, -7.3775e+00],
        [-5.8657e+00,  3.4659e+00],
        [ 4.2061e+00, -7.1211e+00],
        [ 4.0638e+00, -6.4224e+00],
        [ 2.7785e+00, -4.1723e+00],
        [ 4.9584e+00, -6.6435e+00],
        [ 3.0294e+00, -4.8986e+00],
        [-1.6506e+00,  2.1801e-01],
        [ 6.1115e+00, -9.4557e+00],
        [-1.0638e+00, -6.2143e-01],
        [ 4.7948e+00, -6.1830e+00],
        [-6.4003e+00,  3.3092e+00],
        [ 2.8841e+00, -4.4213e+00],
        [ 3.8675e+00, -7.4399e+00],
        [-3.2354e+00,  1.5628e+00],
        [-1.5061e+00, -6.3047e-03],
        [ 4.1618e+00, -5.5489e+00],
        [-3.2371e+00,  1.4827e+00],
        [ 4.6873e+00, -6.1105e+00],
        [ 2.3144e+00, -6.4092e+00],
        [ 4.4431e+00, -6.2956e+00],
        [ 3.1467e+00, -4.5551e+00],
        [ 8.9410e-01, -2.2807e+00],
        [-6.8951e+00,  5.3372e+00],
        [ 3.9502e+00, -7.6841e+00],
        [ 5.8825e+00, -7.2954e+00],
        [-5.0646e+00,  2.6231e+00],
        [ 3.3727e+00, -4.7991e+00],
        [ 2.7635e+00, -4.1594e+00],
        [-5.5461e+00,  3.3289e+00],
        [-9.3210e+00,  4.7058e+00],
        [ 3.7195e+00, -8.3347e+00],
        [ 6.2240e-01, -2.0279e+00],
        [-6.6605e+00,  5.1793e+00],
        [-8.1637e+00,  4.8213e+00],
        [ 2.9820e+00, -4.6403e+00],
        [ 6.4753e+00, -8.6095e+00],
        [ 5.0229e+00, -6.6528e+00],
        [ 4.9334e+00, -6.4740e+00],
        [ 4.3202e+00, -5.7649e+00],
        [ 4.0980e+00, -6.5018e+00],
        [ 2.3169e+00, -3.7245e+00],
        [ 1.2095e+00, -2.7084e+00],
        [ 5.5597e+00, -6.9834e+00],
        [ 5.5843e+00, -7.0308e+00],
        [-5.3950e+00,  3.2446e+00],
        [ 5.4474e+00, -9.4428e+00],
        [-7.8493e+00,  6.2956e+00],
        [-7.3480e+00,  5.9611e+00],
        [-3.7784e+00,  1.9288e+00],
        [-7.5892e+00,  6.1919e+00],
        [-6.5724e+00,  4.8512e+00],
        [-6.9347e+00,  5.2193e+00],
        [-5.2256e+00,  1.5058e+00],
        [-3.4834e+00,  1.6146e+00],
        [ 4.8089e+00, -6.2039e+00],
        [ 2.4778e+00, -4.3870e+00],
        [-4.6823e+00,  2.8143e+00],
        [ 2.7266e+00, -4.1130e+00],
        [ 4.1256e+00, -5.5149e+00],
        [ 6.0785e+00, -8.0357e+00],
        [ 2.3550e+00, -6.5323e+00],
        [ 5.6026e+00, -7.2305e+00],
        [ 4.1374e+00, -5.5243e+00],
        [-6.6923e+00,  5.2764e+00],
        [-3.7873e+00,  1.0814e+00],
        [ 3.2140e+00, -5.8051e+00],
        [ 6.1210e+00, -7.5105e+00],
        [-4.8768e+00,  2.9482e+00],
        [ 6.3162e+00, -7.8719e+00],
        [-7.1311e+00,  5.6784e+00],
        [-6.2702e+00,  4.7121e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7029, 0.2971],
        [0.3053, 0.6947]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6346, 0.3654], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1856, 0.1350],
         [0.9112, 0.4033]],

        [[0.8660, 0.1027],
         [0.6917, 0.4377]],

        [[0.2724, 0.1276],
         [0.6417, 0.7803]],

        [[0.9009, 0.1077],
         [0.7857, 0.1326]],

        [[0.8747, 0.0935],
         [0.8264, 0.8694]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369016634604504
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448573745636614
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8833667995382055
Average Adjusted Rand Index: 0.8846736823744614
11447.126705591018
new:  [0.9840320648505942, 0.9760961941296443, 0.25252550236966104, 0.8833667995382055] [0.9839994671371317, 0.9761603187969168, 0.5754129579066066, 0.8846736823744614] [11440.842234667241, 11441.38193355245, 11708.254020238304, 11578.87555637239]
prior:  [0.9760961941296443, 0.9760961941296443, 0.0, 0.0] [0.9761612713656115, 0.9761612713656115, 0.0, 0.0] [11443.300961224955, 11443.300961224955, nan, nan]
-----------------------------------------------------------------------------------------
This iteration is 21
True Objective function: Loss = -11586.683428311346
Iteration 0: Loss = -34357.46030250416
Iteration 10: Loss = -12174.495028455107
Iteration 20: Loss = -11777.203120094402
Iteration 30: Loss = -11765.403831634236
Iteration 40: Loss = -11764.151307947332
Iteration 50: Loss = -11764.133817059013
Iteration 60: Loss = -11764.133130288907
Iteration 70: Loss = -11764.133093748727
Iteration 80: Loss = -11764.133089265073
Iteration 90: Loss = -11764.13308599327
Iteration 100: Loss = -11764.133085993328
1
Iteration 110: Loss = -11764.133085993328
2
Iteration 120: Loss = -11764.133085993328
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.4894, 0.5106],
        [0.3584, 0.6416]], dtype=torch.float64)
alpha: tensor([0.4567, 0.5433])
beta: tensor([[[0.3905, 0.0981],
         [0.3323, 0.2163]],

        [[0.3444, 0.0996],
         [0.0422, 0.2355]],

        [[0.9806, 0.1001],
         [0.4980, 0.5728]],

        [[0.9322, 0.0897],
         [0.4057, 0.3125]],

        [[0.1032, 0.1028],
         [0.9267, 0.4499]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 75
Adjusted Rand Index: 0.24424822515894298
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.477824457534413
Average Adjusted Rand Index: 0.832848422525081
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34952.31694166645
Iteration 100: Loss = -12460.141956237789
Iteration 200: Loss = -12457.64392059541
Iteration 300: Loss = -12456.182948119673
Iteration 400: Loss = -12454.808796742369
Iteration 500: Loss = -12440.861698567973
Iteration 600: Loss = -12414.254902723318
Iteration 700: Loss = -12351.200311218727
Iteration 800: Loss = -12179.457972709803
Iteration 900: Loss = -12136.762069052178
Iteration 1000: Loss = -12069.348958424469
Iteration 1100: Loss = -11971.811367453289
Iteration 1200: Loss = -11896.399921362741
Iteration 1300: Loss = -11829.955203944946
Iteration 1400: Loss = -11799.744259546233
Iteration 1500: Loss = -11780.31250210326
Iteration 1600: Loss = -11769.967403410865
Iteration 1700: Loss = -11764.210555969048
Iteration 1800: Loss = -11757.874429304908
Iteration 1900: Loss = -11752.146174584392
Iteration 2000: Loss = -11712.219502867732
Iteration 2100: Loss = -11702.828283096138
Iteration 2200: Loss = -11687.238589453433
Iteration 2300: Loss = -11664.60113428461
Iteration 2400: Loss = -11618.659727274806
Iteration 2500: Loss = -11597.9193138129
Iteration 2600: Loss = -11585.91423631276
Iteration 2700: Loss = -11585.9003880566
Iteration 2800: Loss = -11585.890097769365
Iteration 2900: Loss = -11585.881846779886
Iteration 3000: Loss = -11585.874836068608
Iteration 3100: Loss = -11585.866489571132
Iteration 3200: Loss = -11585.86027034844
Iteration 3300: Loss = -11579.775363763983
Iteration 3400: Loss = -11578.37819917206
Iteration 3500: Loss = -11578.364816912255
Iteration 3600: Loss = -11578.36143929445
Iteration 3700: Loss = -11578.358411866393
Iteration 3800: Loss = -11578.355901388199
Iteration 3900: Loss = -11578.35329185395
Iteration 4000: Loss = -11578.35106436904
Iteration 4100: Loss = -11578.351713311638
1
Iteration 4200: Loss = -11578.34714743826
Iteration 4300: Loss = -11578.345341705017
Iteration 4400: Loss = -11578.343780469675
Iteration 4500: Loss = -11578.347592076743
1
Iteration 4600: Loss = -11578.354053929075
2
Iteration 4700: Loss = -11578.339406403873
Iteration 4800: Loss = -11578.337436852424
Iteration 4900: Loss = -11578.310113836216
Iteration 5000: Loss = -11578.30905861687
Iteration 5100: Loss = -11578.30811600741
Iteration 5200: Loss = -11578.307249527012
Iteration 5300: Loss = -11578.306448227531
Iteration 5400: Loss = -11578.337023333432
1
Iteration 5500: Loss = -11578.305001870522
Iteration 5600: Loss = -11578.304312475555
Iteration 5700: Loss = -11578.304569570068
1
Iteration 5800: Loss = -11578.303073811447
Iteration 5900: Loss = -11578.302441476753
Iteration 6000: Loss = -11578.302912881834
1
Iteration 6100: Loss = -11578.300286924969
Iteration 6200: Loss = -11578.299694329071
Iteration 6300: Loss = -11578.29953285066
Iteration 6400: Loss = -11578.298493702698
Iteration 6500: Loss = -11578.297945927938
Iteration 6600: Loss = -11578.297621267779
Iteration 6700: Loss = -11578.296314921326
Iteration 6800: Loss = -11578.296619968565
1
Iteration 6900: Loss = -11578.295733195551
Iteration 7000: Loss = -11578.298003577007
1
Iteration 7100: Loss = -11578.295200639448
Iteration 7200: Loss = -11578.29501042375
Iteration 7300: Loss = -11578.294734875943
Iteration 7400: Loss = -11578.29506276215
1
Iteration 7500: Loss = -11578.297398654026
2
Iteration 7600: Loss = -11578.294329158787
Iteration 7700: Loss = -11578.324655579083
1
Iteration 7800: Loss = -11578.293787108358
Iteration 7900: Loss = -11578.293927067187
1
Iteration 8000: Loss = -11578.297417249005
2
Iteration 8100: Loss = -11578.293328729866
Iteration 8200: Loss = -11578.33378389601
1
Iteration 8300: Loss = -11578.36077139357
2
Iteration 8400: Loss = -11578.297160616463
3
Iteration 8500: Loss = -11578.294629880484
4
Iteration 8600: Loss = -11578.292769659418
Iteration 8700: Loss = -11578.293420947446
1
Iteration 8800: Loss = -11578.292619993867
Iteration 8900: Loss = -11578.29281590499
1
Iteration 9000: Loss = -11578.296116814818
2
Iteration 9100: Loss = -11578.293960840765
3
Iteration 9200: Loss = -11578.292170450284
Iteration 9300: Loss = -11578.295085159381
1
Iteration 9400: Loss = -11578.320048581028
2
Iteration 9500: Loss = -11578.297337657095
3
Iteration 9600: Loss = -11578.302086320793
4
Iteration 9700: Loss = -11578.29178730008
Iteration 9800: Loss = -11578.29151837201
Iteration 9900: Loss = -11578.289529867236
Iteration 10000: Loss = -11578.29694850151
1
Iteration 10100: Loss = -11578.302416614526
2
Iteration 10200: Loss = -11578.289783185422
3
Iteration 10300: Loss = -11578.289728397998
4
Iteration 10400: Loss = -11578.296802382973
5
Iteration 10500: Loss = -11578.290763691728
6
Iteration 10600: Loss = -11578.295059107786
7
Iteration 10700: Loss = -11578.29605044658
8
Iteration 10800: Loss = -11578.291500064523
9
Iteration 10900: Loss = -11578.290802889735
10
Stopping early at iteration 10900 due to no improvement.
tensor([[  5.3635,  -9.9787],
        [  5.0093,  -9.6245],
        [  5.8488, -10.4640],
        [  5.6954, -10.3107],
        [ -8.9796,   4.3643],
        [  6.6019, -11.2171],
        [ -9.5602,   4.9450],
        [ -5.9044,   1.2892],
        [  5.6364, -10.2517],
        [ -8.4327,   3.8175],
        [ -9.9159,   5.3006],
        [  5.5049, -10.1201],
        [  6.2251, -10.8403],
        [  6.3661, -10.9814],
        [  6.0570, -10.6722],
        [  4.8857,  -9.5009],
        [  5.4335, -10.0487],
        [ -7.1597,   2.5444],
        [  6.1639, -10.7791],
        [  4.6000,  -9.2152],
        [  5.4456, -10.0608],
        [  5.8545, -10.4697],
        [  6.4364, -11.0516],
        [  6.1311, -10.7464],
        [  6.3247, -10.9399],
        [  5.9411, -10.5563],
        [ -7.7472,   3.1320],
        [-10.2894,   5.6741],
        [  6.2330, -10.8482],
        [  5.2370,  -9.8522],
        [  4.8842,  -9.4995],
        [ -9.5382,   4.9230],
        [  5.3539,  -9.9691],
        [  6.2746, -10.8898],
        [  5.6379, -10.2532],
        [-10.1818,   5.5666],
        [ -9.7911,   5.1759],
        [-10.4282,   5.8130],
        [  4.4782,  -9.0934],
        [  6.3251, -10.9403],
        [  4.3961,  -9.0113],
        [ -9.4493,   4.8341],
        [  6.4411, -11.0563],
        [  4.4123,  -9.0275],
        [ -9.4215,   4.8063],
        [ -9.9955,   5.3803],
        [-10.0147,   5.3994],
        [ -9.9468,   5.3316],
        [  4.5146,  -9.1298],
        [  4.6844,  -9.2996],
        [  6.3654, -10.9807],
        [  6.4365, -11.0517],
        [ -7.7472,   3.1320],
        [  6.6164, -11.2316],
        [  3.4738,  -8.0890],
        [  4.8269,  -9.4422],
        [ -9.9419,   5.3267],
        [ -9.9113,   5.2961],
        [  6.1420, -10.7572],
        [  6.7062, -11.3214],
        [  6.5982, -11.2134],
        [  6.4521, -11.0673],
        [  5.6010, -10.2162],
        [-10.0231,   5.4079],
        [ -9.8644,   5.2492],
        [  6.4265, -11.0418],
        [-10.2895,   5.6742],
        [  5.9143, -10.5295],
        [  4.7554,  -9.3706],
        [  4.4744,  -9.0896],
        [  5.9233, -10.5385],
        [ -8.7084,   4.0932],
        [  6.6066, -11.2218],
        [ -9.5436,   4.9284],
        [ -8.9935,   4.3783],
        [  4.5645,  -9.1797],
        [ -3.5911,  -1.0241],
        [ -9.0731,   4.4579],
        [  5.9882, -10.6035],
        [-10.1929,   5.5777],
        [ -6.4916,   1.8763],
        [  5.8459, -10.4611],
        [-10.0204,   5.4052],
        [-10.1658,   5.5506],
        [  6.0290, -10.6442],
        [  6.4170, -11.0323],
        [-10.0929,   5.4777],
        [ -9.4567,   4.8415],
        [  6.5857, -11.2009],
        [  6.0034, -10.6186],
        [  6.6127, -11.2279],
        [  3.7748,  -8.3900],
        [  5.6212, -10.2364],
        [  3.9858,  -8.6010],
        [ -8.7192,   4.1040],
        [ -8.2790,   3.6638],
        [-10.0618,   5.4466],
        [  5.0785,  -9.6938],
        [-10.1197,   5.5045],
        [ -2.4742,  -2.1411]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7418, 0.2582],
        [0.2128, 0.7872]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6148, 0.3852], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4002, 0.0975],
         [0.3323, 0.1995]],

        [[0.3444, 0.1010],
         [0.0422, 0.2355]],

        [[0.9806, 0.1002],
         [0.4980, 0.5728]],

        [[0.9322, 0.0885],
         [0.4057, 0.3125]],

        [[0.1032, 0.1032],
         [0.9267, 0.4499]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9840320329363369
Average Adjusted Rand Index: 0.9839995611635631
Iteration 0: Loss = -26705.629777356946
Iteration 10: Loss = -12311.693059114416
Iteration 20: Loss = -11582.589873724437
Iteration 30: Loss = -11582.457861047787
Iteration 40: Loss = -11582.457843104701
Iteration 50: Loss = -11582.457844642237
1
Iteration 60: Loss = -11582.457844642237
2
Iteration 70: Loss = -11582.457844642237
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7797, 0.2203],
        [0.2679, 0.7321]], dtype=torch.float64)
alpha: tensor([0.4884, 0.5116])
beta: tensor([[[0.1945, 0.0986],
         [0.9237, 0.3944]],

        [[0.9001, 0.1012],
         [0.0734, 0.0632]],

        [[0.2488, 0.1006],
         [0.4656, 0.9597]],

        [[0.6602, 0.0895],
         [0.3609, 0.0017]],

        [[0.3551, 0.1033],
         [0.0828, 0.2263]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26706.09661365377
Iteration 100: Loss = -12461.143349785743
Iteration 200: Loss = -12243.747216226833
Iteration 300: Loss = -12042.077928650915
Iteration 400: Loss = -11913.365691868017
Iteration 500: Loss = -11813.703854413778
Iteration 600: Loss = -11759.411695157412
Iteration 700: Loss = -11718.775990590731
Iteration 800: Loss = -11708.573583671274
Iteration 900: Loss = -11700.760948065636
Iteration 1000: Loss = -11683.829353366125
Iteration 1100: Loss = -11683.53038927896
Iteration 1200: Loss = -11670.494932119846
Iteration 1300: Loss = -11664.501772740425
Iteration 1400: Loss = -11664.40939628414
Iteration 1500: Loss = -11664.307458083482
Iteration 1600: Loss = -11649.686041122895
Iteration 1700: Loss = -11649.637193698338
Iteration 1800: Loss = -11649.594671832616
Iteration 1900: Loss = -11642.168366659016
Iteration 2000: Loss = -11622.40929919147
Iteration 2100: Loss = -11622.370481883341
Iteration 2200: Loss = -11622.346633626761
Iteration 2300: Loss = -11622.327119721878
Iteration 2400: Loss = -11622.310363404604
Iteration 2500: Loss = -11622.29280245166
Iteration 2600: Loss = -11604.441415086585
Iteration 2700: Loss = -11604.423696563657
Iteration 2800: Loss = -11604.410375119774
Iteration 2900: Loss = -11604.398503370508
Iteration 3000: Loss = -11604.374810375994
Iteration 3100: Loss = -11597.918357712548
Iteration 3200: Loss = -11597.909352271783
Iteration 3300: Loss = -11597.9027364459
Iteration 3400: Loss = -11597.896697901368
Iteration 3500: Loss = -11584.721139016634
Iteration 3600: Loss = -11584.692806428637
Iteration 3700: Loss = -11584.68772281129
Iteration 3800: Loss = -11584.683250645676
Iteration 3900: Loss = -11584.679340733755
Iteration 4000: Loss = -11584.675644260304
Iteration 4100: Loss = -11584.672438951493
Iteration 4200: Loss = -11584.669451847694
Iteration 4300: Loss = -11584.669115772409
Iteration 4400: Loss = -11584.66423141844
Iteration 4500: Loss = -11584.661930602459
Iteration 4600: Loss = -11584.659805788997
Iteration 4700: Loss = -11584.657830490143
Iteration 4800: Loss = -11584.65601348826
Iteration 4900: Loss = -11584.654342603695
Iteration 5000: Loss = -11584.65275607959
Iteration 5100: Loss = -11584.654402119744
1
Iteration 5200: Loss = -11584.650002013495
Iteration 5300: Loss = -11584.64872364896
Iteration 5400: Loss = -11584.647628414254
Iteration 5500: Loss = -11584.657747327483
1
Iteration 5600: Loss = -11584.645481078082
Iteration 5700: Loss = -11584.650755559225
1
Iteration 5800: Loss = -11584.643653962352
Iteration 5900: Loss = -11584.642830247729
Iteration 6000: Loss = -11584.642008197656
Iteration 6100: Loss = -11584.641314119324
Iteration 6200: Loss = -11584.64061735677
Iteration 6300: Loss = -11584.639849365634
Iteration 6400: Loss = -11584.638173644858
Iteration 6500: Loss = -11578.779501073652
Iteration 6600: Loss = -11578.778288633828
Iteration 6700: Loss = -11578.7782101401
Iteration 6800: Loss = -11578.77751316839
Iteration 6900: Loss = -11578.778805171134
1
Iteration 7000: Loss = -11578.776287721194
Iteration 7100: Loss = -11578.775812676087
Iteration 7200: Loss = -11578.775493581335
Iteration 7300: Loss = -11578.775036563762
Iteration 7400: Loss = -11578.781500723282
1
Iteration 7500: Loss = -11578.775106314331
2
Iteration 7600: Loss = -11578.777574603397
3
Iteration 7700: Loss = -11578.799923155586
4
Iteration 7800: Loss = -11578.773491874415
Iteration 7900: Loss = -11578.777333717877
1
Iteration 8000: Loss = -11578.774842959889
2
Iteration 8100: Loss = -11578.788988919077
3
Iteration 8200: Loss = -11578.846061800668
4
Iteration 8300: Loss = -11578.77247593233
Iteration 8400: Loss = -11578.772293136544
Iteration 8500: Loss = -11578.772001416362
Iteration 8600: Loss = -11578.771798758651
Iteration 8700: Loss = -11578.773555144278
1
Iteration 8800: Loss = -11578.771478433862
Iteration 8900: Loss = -11578.838020340532
1
Iteration 9000: Loss = -11578.773676835997
2
Iteration 9100: Loss = -11578.773659990695
3
Iteration 9200: Loss = -11578.779736201324
4
Iteration 9300: Loss = -11578.773218192087
5
Iteration 9400: Loss = -11578.772243755657
6
Iteration 9500: Loss = -11578.771073726024
Iteration 9600: Loss = -11578.772543113522
1
Iteration 9700: Loss = -11578.842201576323
2
Iteration 9800: Loss = -11578.785894272592
3
Iteration 9900: Loss = -11578.770804933225
Iteration 10000: Loss = -11578.782761570688
1
Iteration 10100: Loss = -11578.771334954541
2
Iteration 10200: Loss = -11578.771283594882
3
Iteration 10300: Loss = -11578.771669837837
4
Iteration 10400: Loss = -11578.770422453026
Iteration 10500: Loss = -11578.770184519502
Iteration 10600: Loss = -11578.780065564359
1
Iteration 10700: Loss = -11578.858570763814
2
Iteration 10800: Loss = -11578.841001126362
3
Iteration 10900: Loss = -11578.779897921611
4
Iteration 11000: Loss = -11578.773271285867
5
Iteration 11100: Loss = -11578.774065817543
6
Iteration 11200: Loss = -11578.302156997608
Iteration 11300: Loss = -11578.282616850269
Iteration 11400: Loss = -11578.28303290109
1
Iteration 11500: Loss = -11578.28146108545
Iteration 11600: Loss = -11578.279309220288
Iteration 11700: Loss = -11578.315344937684
1
Iteration 11800: Loss = -11578.278848623884
Iteration 11900: Loss = -11578.28145508347
1
Iteration 12000: Loss = -11578.282186069204
2
Iteration 12100: Loss = -11578.285129275946
3
Iteration 12200: Loss = -11578.281562026663
4
Iteration 12300: Loss = -11578.281404518055
5
Iteration 12400: Loss = -11578.280485811165
6
Iteration 12500: Loss = -11578.296584923726
7
Iteration 12600: Loss = -11578.287204682912
8
Iteration 12700: Loss = -11578.343947211455
9
Iteration 12800: Loss = -11578.281321358432
10
Stopping early at iteration 12800 due to no improvement.
tensor([[ -8.6423,   6.7798],
        [ -8.5092,   6.9649],
        [ -8.8510,   7.1056],
        [ -9.1592,   7.7624],
        [  5.8528,  -7.2404],
        [ -9.7616,   8.2984],
        [  6.3238,  -8.2084],
        [  1.3149,  -5.8750],
        [ -8.3723,   6.6104],
        [  5.4262,  -6.8138],
        [  6.7482,  -8.8908],
        [ -8.5806,   7.1657],
        [ -9.2231,   7.7101],
        [ -9.7307,   8.1722],
        [ -8.3619,   6.6641],
        [ -8.8490,   6.7333],
        [-10.4026,   6.2084],
        [  3.7450,  -6.0010],
        [-11.4175,   6.9935],
        [ -8.8215,   5.9016],
        [ -9.4195,   7.7867],
        [ -9.9401,   7.1336],
        [ -9.2875,   7.7241],
        [ -9.9272,   7.4825],
        [-10.0177,   8.3609],
        [ -3.6172,   1.3950],
        [  4.5678,  -6.3609],
        [  7.1494,  -8.5774],
        [ -8.6026,   6.8185],
        [ -8.8805,   7.1827],
        [ -9.7993,   8.4129],
        [  7.5299,  -8.9681],
        [ -9.3129,   7.3137],
        [ -9.2908,   7.7381],
        [ -9.1251,   7.6938],
        [  8.2099,  -9.7136],
        [  6.5944,  -8.2008],
        [  7.1846,  -8.6423],
        [ -8.7952,   6.7324],
        [ -9.4164,   7.6303],
        [ -9.7356,   8.3086],
        [  6.5525,  -7.9957],
        [ -9.4600,   7.7740],
        [ -7.5235,   6.0664],
        [  6.4831,  -7.9163],
        [  8.5509, -10.1361],
        [  6.9372,  -8.3653],
        [  6.3044,  -9.1205],
        [ -9.9876,   6.8543],
        [ -8.3915,   5.9089],
        [ -9.6044,   7.9372],
        [-11.1539,   8.4599],
        [  7.0865,  -8.4944],
        [ -9.9216,   8.3792],
        [ -8.6316,   4.0164],
        [ -3.1791,   1.5693],
        [  6.6307,  -8.4979],
        [  6.8333,  -8.3537],
        [ -8.0380,   6.6133],
        [ -9.5644,   7.6651],
        [ -9.5664,   8.1564],
        [ -9.7728,   7.6857],
        [ -8.7576,   6.9848],
        [  6.9020,  -8.3737],
        [  6.4974,  -9.7048],
        [-10.8504,   8.2143],
        [  6.9588,  -9.0457],
        [ -9.0533,   7.6177],
        [ -8.0274,   6.1210],
        [ -9.8644,   8.0638],
        [ -9.0858,   7.6170],
        [  5.7312,  -7.1237],
        [-11.3226,   8.6492],
        [  5.7279,  -7.7674],
        [  5.7740,  -7.3773],
        [ -8.2072,   6.6396],
        [  0.5825,  -1.9818],
        [  5.9404,  -7.3267],
        [ -9.1269,   7.7024],
        [  6.5495,  -9.1441],
        [  3.4104,  -4.9708],
        [ -9.8392,   7.0156],
        [  7.0058,  -9.9902],
        [  6.7692,  -8.2302],
        [ -9.4817,   6.9013],
        [ -7.7150,   5.8685],
        [  6.6411,  -8.2503],
        [  6.1510,  -8.5812],
        [ -7.8358,   6.3779],
        [ -4.0792,   2.4937],
        [-11.2847,   8.1278],
        [ -7.7554,   6.3581],
        [-10.9027,   7.4648],
        [ -7.0068,   5.6146],
        [  6.3981,  -8.6033],
        [  5.2470,  -7.3057],
        [  6.6921,  -8.2478],
        [-10.5267,   7.8496],
        [  6.7514,  -8.2334],
        [ -0.5462,  -0.8965]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7860, 0.2140],
        [0.2624, 0.7376]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3853, 0.6147], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1995, 0.0975],
         [0.9237, 0.4002]],

        [[0.9001, 0.1010],
         [0.0734, 0.0632]],

        [[0.2488, 0.1001],
         [0.4656, 0.9597]],

        [[0.6602, 0.0886],
         [0.3609, 0.0017]],

        [[0.3551, 0.1032],
         [0.0828, 0.2263]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9840320329363369
Average Adjusted Rand Index: 0.9839995611635631
Iteration 0: Loss = -21813.071245439074
Iteration 10: Loss = -11829.684650228344
Iteration 20: Loss = -11824.39925521673
Iteration 30: Loss = -11798.886336020598
Iteration 40: Loss = -11781.599010891627
Iteration 50: Loss = -11780.005130075875
Iteration 60: Loss = -11779.883485577562
Iteration 70: Loss = -11779.884321349584
1
Iteration 80: Loss = -11779.88547801443
2
Iteration 90: Loss = -11779.885669511164
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.6034, 0.3966],
        [0.4919, 0.5081]], dtype=torch.float64)
alpha: tensor([0.5166, 0.4834])
beta: tensor([[[0.2128, 0.0977],
         [0.6269, 0.3816]],

        [[0.2253, 0.1013],
         [0.4353, 0.1012]],

        [[0.6766, 0.0998],
         [0.4057, 0.3086]],

        [[0.5568, 0.0982],
         [0.9543, 0.9414]],

        [[0.3580, 0.1025],
         [0.3135, 0.9201]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 26
Adjusted Rand Index: 0.22262626262626262
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.4833821812780762
Average Adjusted Rand Index: 0.8285240300185448
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21812.520073571966
Iteration 100: Loss = -12079.216069896649
Iteration 200: Loss = -11927.509669954252
Iteration 300: Loss = -11920.79262770663
Iteration 400: Loss = -11907.234488347689
Iteration 500: Loss = -11855.602963834943
Iteration 600: Loss = -11813.745279204666
Iteration 700: Loss = -11796.095888603315
Iteration 800: Loss = -11795.362268082783
Iteration 900: Loss = -11795.265886899311
Iteration 1000: Loss = -11795.208049878329
Iteration 1100: Loss = -11795.167523039992
Iteration 1200: Loss = -11795.137434307224
Iteration 1300: Loss = -11795.114308954107
Iteration 1400: Loss = -11795.095938005185
Iteration 1500: Loss = -11795.081035491163
Iteration 1600: Loss = -11795.068730840892
Iteration 1700: Loss = -11795.057895726042
Iteration 1800: Loss = -11776.633226470609
Iteration 1900: Loss = -11776.617274970244
Iteration 2000: Loss = -11776.608660016855
Iteration 2100: Loss = -11776.601759082947
Iteration 2200: Loss = -11776.596011835934
Iteration 2300: Loss = -11776.591121333375
Iteration 2400: Loss = -11776.586787740767
Iteration 2500: Loss = -11776.58300208256
Iteration 2600: Loss = -11776.579677403695
Iteration 2700: Loss = -11776.576701521759
Iteration 2800: Loss = -11776.574075898176
Iteration 2900: Loss = -11776.571644957326
Iteration 3000: Loss = -11776.56944836366
Iteration 3100: Loss = -11776.567357867154
Iteration 3200: Loss = -11776.56539264399
Iteration 3300: Loss = -11776.563456780801
Iteration 3400: Loss = -11776.561741434705
Iteration 3500: Loss = -11776.560331448396
Iteration 3600: Loss = -11776.557184662232
Iteration 3700: Loss = -11769.592143812832
Iteration 3800: Loss = -11769.62959466792
1
Iteration 3900: Loss = -11769.589077325638
Iteration 4000: Loss = -11769.587724039051
Iteration 4100: Loss = -11769.586390754472
Iteration 4200: Loss = -11769.58463002386
Iteration 4300: Loss = -11769.57993016155
Iteration 4400: Loss = -11769.57201685581
Iteration 4500: Loss = -11769.570943902141
Iteration 4600: Loss = -11769.571006673004
1
Iteration 4700: Loss = -11769.569359847688
Iteration 4800: Loss = -11762.77827315983
Iteration 4900: Loss = -11762.775185990122
Iteration 5000: Loss = -11762.774596028557
Iteration 5100: Loss = -11762.774237349075
Iteration 5200: Loss = -11762.785472669508
1
Iteration 5300: Loss = -11762.773290029641
Iteration 5400: Loss = -11762.77805031001
1
Iteration 5500: Loss = -11762.77373256903
2
Iteration 5600: Loss = -11762.772787690155
Iteration 5700: Loss = -11762.773516345496
1
Iteration 5800: Loss = -11762.771847859714
Iteration 5900: Loss = -11762.771525376462
Iteration 6000: Loss = -11762.781656017578
1
Iteration 6100: Loss = -11762.771017406218
Iteration 6200: Loss = -11762.771105910715
1
Iteration 6300: Loss = -11762.771346778713
2
Iteration 6400: Loss = -11762.77212045643
3
Iteration 6500: Loss = -11762.770292076628
Iteration 6600: Loss = -11762.770089595524
Iteration 6700: Loss = -11762.77216086807
1
Iteration 6800: Loss = -11762.776037952224
2
Iteration 6900: Loss = -11762.770055819135
Iteration 7000: Loss = -11762.775075615687
1
Iteration 7100: Loss = -11762.769495285875
Iteration 7200: Loss = -11762.76989477009
1
Iteration 7300: Loss = -11762.769334535877
Iteration 7400: Loss = -11762.77036673471
1
Iteration 7500: Loss = -11762.771088101634
2
Iteration 7600: Loss = -11762.770462571145
3
Iteration 7700: Loss = -11762.783306123078
4
Iteration 7800: Loss = -11762.768831877387
Iteration 7900: Loss = -11762.769067125302
1
Iteration 8000: Loss = -11762.768702102245
Iteration 8100: Loss = -11762.76864946263
Iteration 8200: Loss = -11762.768496761784
Iteration 8300: Loss = -11762.768697384687
1
Iteration 8400: Loss = -11762.768406115301
Iteration 8500: Loss = -11762.76860849745
1
Iteration 8600: Loss = -11762.904532401766
2
Iteration 8700: Loss = -11762.768172419277
Iteration 8800: Loss = -11762.769858628126
1
Iteration 8900: Loss = -11762.768082096085
Iteration 9000: Loss = -11762.773471506305
1
Iteration 9100: Loss = -11762.768012877983
Iteration 9200: Loss = -11762.773137580636
1
Iteration 9300: Loss = -11762.767949440746
Iteration 9400: Loss = -11762.79831001243
1
Iteration 9500: Loss = -11762.767877889852
Iteration 9600: Loss = -11762.767874902009
Iteration 9700: Loss = -11762.767861302915
Iteration 9800: Loss = -11762.767780952092
Iteration 9900: Loss = -11762.853264908636
1
Iteration 10000: Loss = -11762.7677257126
Iteration 10100: Loss = -11762.767708978716
Iteration 10200: Loss = -11762.767868814819
1
Iteration 10300: Loss = -11762.770747632303
2
Iteration 10400: Loss = -11762.772742533742
3
Iteration 10500: Loss = -11762.767655349657
Iteration 10600: Loss = -11762.785411645084
1
Iteration 10700: Loss = -11762.767301616166
Iteration 10800: Loss = -11762.767442932578
1
Iteration 10900: Loss = -11762.767167025433
Iteration 11000: Loss = -11762.767298173601
1
Iteration 11100: Loss = -11762.767069402775
Iteration 11200: Loss = -11762.771325786347
1
Iteration 11300: Loss = -11762.766594243672
Iteration 11400: Loss = -11762.781783264822
1
Iteration 11500: Loss = -11762.766533603075
Iteration 11600: Loss = -11762.79862295098
1
Iteration 11700: Loss = -11762.766414631116
Iteration 11800: Loss = -11762.766864359484
1
Iteration 11900: Loss = -11762.765719775114
Iteration 12000: Loss = -11762.765686813358
Iteration 12100: Loss = -11762.776503815941
1
Iteration 12200: Loss = -11762.765421908865
Iteration 12300: Loss = -11762.83401305717
1
Iteration 12400: Loss = -11762.764723099637
Iteration 12500: Loss = -11762.785780502736
1
Iteration 12600: Loss = -11762.76471070523
Iteration 12700: Loss = -11762.780020711278
1
Iteration 12800: Loss = -11762.767506312308
2
Iteration 12900: Loss = -11762.768063769116
3
Iteration 13000: Loss = -11762.764723689988
4
Iteration 13100: Loss = -11762.764838431329
5
Iteration 13200: Loss = -11762.76956931187
6
Iteration 13300: Loss = -11762.76537974675
7
Iteration 13400: Loss = -11762.762496148607
Iteration 13500: Loss = -11762.76262878711
1
Iteration 13600: Loss = -11762.762672936326
2
Iteration 13700: Loss = -11762.771416176674
3
Iteration 13800: Loss = -11762.79422772589
4
Iteration 13900: Loss = -11762.77545265092
5
Iteration 14000: Loss = -11762.76252155473
6
Iteration 14100: Loss = -11762.762602721397
7
Iteration 14200: Loss = -11762.762647843227
8
Iteration 14300: Loss = -11762.762465888673
Iteration 14400: Loss = -11762.762409362042
Iteration 14500: Loss = -11762.764279141738
1
Iteration 14600: Loss = -11762.762401312571
Iteration 14700: Loss = -11762.804213772817
1
Iteration 14800: Loss = -11762.762408856774
2
Iteration 14900: Loss = -11762.762385903676
Iteration 15000: Loss = -11762.762560561821
1
Iteration 15100: Loss = -11762.76327754742
2
Iteration 15200: Loss = -11762.762429075803
3
Iteration 15300: Loss = -11761.26630444112
Iteration 15400: Loss = -11761.314940699416
1
Iteration 15500: Loss = -11761.259488935139
Iteration 15600: Loss = -11761.264435045314
1
Iteration 15700: Loss = -11761.25679984151
Iteration 15800: Loss = -11761.256732598564
Iteration 15900: Loss = -11761.256677342688
Iteration 16000: Loss = -11761.256583341356
Iteration 16100: Loss = -11761.256866739423
1
Iteration 16200: Loss = -11761.420783344669
2
Iteration 16300: Loss = -11761.2565542874
Iteration 16400: Loss = -11761.297411985228
1
Iteration 16500: Loss = -11761.27352833562
2
Iteration 16600: Loss = -11761.256595787861
3
Iteration 16700: Loss = -11761.264523971719
4
Iteration 16800: Loss = -11761.267746097954
5
Iteration 16900: Loss = -11761.25650852848
Iteration 17000: Loss = -11761.258105991708
1
Iteration 17100: Loss = -11761.261970091371
2
Iteration 17200: Loss = -11761.256442380338
Iteration 17300: Loss = -11761.256863142311
1
Iteration 17400: Loss = -11761.25644430615
2
Iteration 17500: Loss = -11761.259948460629
3
Iteration 17600: Loss = -11761.256431165903
Iteration 17700: Loss = -11761.258686207819
1
Iteration 17800: Loss = -11761.276441819118
2
Iteration 17900: Loss = -11761.256490124186
3
Iteration 18000: Loss = -11761.256747712461
4
Iteration 18100: Loss = -11761.256482145322
5
Iteration 18200: Loss = -11761.2566647462
6
Iteration 18300: Loss = -11761.290230536226
7
Iteration 18400: Loss = -11761.256630800284
8
Iteration 18500: Loss = -11761.262512397663
9
Iteration 18600: Loss = -11761.2925784785
10
Stopping early at iteration 18600 due to no improvement.
tensor([[-10.5923,   8.9058],
        [ -4.4131,   2.7420],
        [-10.1488,   8.6577],
        [ -9.8353,   8.3046],
        [  3.8028,  -6.9753],
        [-11.3908,   8.7372],
        [  6.2921,  -8.4793],
        [  1.5330,  -2.9471],
        [-10.8523,   9.0144],
        [  4.8921,  -6.2820],
        [  7.8502,  -9.2518],
        [-10.3077,   8.9199],
        [-11.0559,   9.6622],
        [-11.2745,   9.2021],
        [-11.3855,   9.5210],
        [-10.1777,   7.9578],
        [-11.0525,   9.5647],
        [  2.7854,  -4.5755],
        [-10.6015,   9.1885],
        [ -8.6983,   6.9907],
        [ -9.9355,   8.5194],
        [-11.2736,   9.4455],
        [-10.7666,   8.9764],
        [ -9.2347,   7.6966],
        [-10.8259,   8.3395],
        [ -9.9113,   8.4650],
        [  3.3578,  -5.6357],
        [  8.3580, -10.0261],
        [-11.8213,   9.0281],
        [-10.2822,   8.8957],
        [-11.6205,   9.2590],
        [  8.6748, -11.6620],
        [-11.1120,   9.3942],
        [-11.3957,   8.9304],
        [-11.2385,   8.2113],
        [  8.7836, -10.8351],
        [  6.9591,  -8.3575],
        [  7.4260, -12.0412],
        [-10.7682,   9.3654],
        [-12.3708,   7.7556],
        [-10.7868,   9.0953],
        [  5.2455,  -7.9101],
        [-11.5595,   8.8949],
        [ -9.3526,   6.7764],
        [  5.2949,  -8.3486],
        [  8.9263, -11.0104],
        [  6.9656, -10.5719],
        [  8.5103, -12.4079],
        [-10.2644,   8.5757],
        [ -9.2406,   7.8208],
        [-10.3322,   7.9720],
        [-11.2405,   9.8541],
        [  3.2686,  -5.7553],
        [-12.8919,   8.2767],
        [ -8.2934,   5.3068],
        [ -2.9593,   1.4560],
        [  6.5210,  -9.7189],
        [  7.6980,  -9.4627],
        [-11.1480,   8.9063],
        [-11.2909,   9.6648],
        [-11.6337,   9.5619],
        [-10.9598,   9.1173],
        [-10.6671,   9.0071],
        [  8.3056, -10.0147],
        [  8.3193, -10.4475],
        [-10.2462,   8.7706],
        [  8.9763, -10.3849],
        [-10.5817,   9.0062],
        [ -9.7102,   7.8868],
        [-11.0115,   9.6242],
        [-11.1277,   9.1587],
        [  4.4100,  -6.2195],
        [-11.1902,   9.7874],
        [  8.1404,  -9.5526],
        [  5.1694,  -6.5561],
        [-10.5650,   8.7098],
        [ -0.2693,  -1.1359],
        [  9.1423, -10.8738],
        [-10.5593,   9.1728],
        [  8.5293, -10.0044],
        [  2.2169,  -3.9054],
        [-12.5675,   7.9522],
        [  8.5816, -10.0749],
        [  7.8687, -10.0801],
        [-10.9927,   9.5595],
        [-10.1322,   8.7453],
        [  7.8341,  -9.8684],
        [  5.7838,  -7.7225],
        [-11.1254,   9.2786],
        [-11.2616,   9.1790],
        [-10.9278,   9.4512],
        [ -8.9683,   6.8765],
        [-11.5559,   8.7073],
        [ -8.0223,   5.6433],
        [  7.0958,  -8.4979],
        [  4.2088,  -5.7765],
        [  8.6498, -10.0688],
        [-11.1613,   9.7087],
        [  7.6931,  -9.6785],
        [ -0.8303,  -0.9344]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5953, 0.4047],
        [0.4907, 0.5093]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3799, 0.6201], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2193, 0.0964],
         [0.6269, 0.3874]],

        [[0.2253, 0.1010],
         [0.4353, 0.1012]],

        [[0.6766, 0.0992],
         [0.4057, 0.3086]],

        [[0.5568, 0.0927],
         [0.9543, 0.9414]],

        [[0.3580, 0.1022],
         [0.3135, 0.9201]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 23
Adjusted Rand Index: 0.28444444444444444
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.4667883594148197
Average Adjusted Rand Index: 0.8408876663821813
Iteration 0: Loss = -29140.387823446956
Iteration 10: Loss = -12457.021782381591
Iteration 20: Loss = -12438.338937185696
Iteration 30: Loss = -12124.582002697192
Iteration 40: Loss = -11606.004326624148
Iteration 50: Loss = -11582.458073582082
Iteration 60: Loss = -11582.457841073521
Iteration 70: Loss = -11582.45783768577
Iteration 80: Loss = -11582.457837685764
Iteration 90: Loss = -11582.457837685764
1
Iteration 100: Loss = -11582.457837685764
2
Iteration 110: Loss = -11582.457837685764
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.7321, 0.2679],
        [0.2203, 0.7797]], dtype=torch.float64)
alpha: tensor([0.5116, 0.4884])
beta: tensor([[[0.3944, 0.0986],
         [0.0615, 0.1945]],

        [[0.6688, 0.1012],
         [0.4214, 0.1707]],

        [[0.5701, 0.1006],
         [0.7819, 0.1466]],

        [[0.8446, 0.0895],
         [0.0287, 0.4227]],

        [[0.9674, 0.1033],
         [0.3721, 0.9642]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29136.554627310827
Iteration 100: Loss = -12463.717810519734
Iteration 200: Loss = -12438.354784238467
Iteration 300: Loss = -12377.412054413739
Iteration 400: Loss = -12279.855730545472
Iteration 500: Loss = -12117.624297617978
Iteration 600: Loss = -12023.705520607526
Iteration 700: Loss = -11947.9422953816
Iteration 800: Loss = -11914.157552113076
Iteration 900: Loss = -11906.88959446982
Iteration 1000: Loss = -11905.865803029208
Iteration 1100: Loss = -11905.377504045544
Iteration 1200: Loss = -11905.061712485454
Iteration 1300: Loss = -11904.838120276025
Iteration 1400: Loss = -11904.670930632717
Iteration 1500: Loss = -11904.54075329115
Iteration 1600: Loss = -11904.435322157116
Iteration 1700: Loss = -11904.271573719561
Iteration 1800: Loss = -11894.961589974273
Iteration 1900: Loss = -11894.87657092137
Iteration 2000: Loss = -11894.780216368254
Iteration 2100: Loss = -11894.68637739008
Iteration 2200: Loss = -11893.61707073739
Iteration 2300: Loss = -11890.883595231575
Iteration 2400: Loss = -11890.78404636024
Iteration 2500: Loss = -11890.743968716402
Iteration 2600: Loss = -11890.689848033744
Iteration 2700: Loss = -11890.542506540918
Iteration 2800: Loss = -11890.522295339946
Iteration 2900: Loss = -11890.504482258122
Iteration 3000: Loss = -11890.449392947448
Iteration 3100: Loss = -11883.870431782845
Iteration 3200: Loss = -11870.5807325346
Iteration 3300: Loss = -11866.646010985727
Iteration 3400: Loss = -11866.612053809786
Iteration 3500: Loss = -11860.419916321835
Iteration 3600: Loss = -11860.523294207562
1
Iteration 3700: Loss = -11860.36223796252
Iteration 3800: Loss = -11860.356299754452
Iteration 3900: Loss = -11860.336551684577
Iteration 4000: Loss = -11860.32066981312
Iteration 4100: Loss = -11854.265002825761
Iteration 4200: Loss = -11842.853304770806
Iteration 4300: Loss = -11842.85162269097
Iteration 4400: Loss = -11842.838738578592
Iteration 4500: Loss = -11842.8328460866
Iteration 4600: Loss = -11842.833028302117
1
Iteration 4700: Loss = -11842.905483900984
2
Iteration 4800: Loss = -11842.823521260918
Iteration 4900: Loss = -11842.816623711036
Iteration 5000: Loss = -11842.811638372583
Iteration 5100: Loss = -11842.808269013618
Iteration 5200: Loss = -11842.854603114141
1
Iteration 5300: Loss = -11842.802465553876
Iteration 5400: Loss = -11842.800583294496
Iteration 5500: Loss = -11842.797703676762
Iteration 5600: Loss = -11842.795604815621
Iteration 5700: Loss = -11842.794361549632
Iteration 5800: Loss = -11842.794764372671
1
Iteration 5900: Loss = -11842.789959428095
Iteration 6000: Loss = -11842.788686143047
Iteration 6100: Loss = -11842.78677243921
Iteration 6200: Loss = -11842.798408851277
1
Iteration 6300: Loss = -11842.783917768867
Iteration 6400: Loss = -11842.799328893965
1
Iteration 6500: Loss = -11842.781136298054
Iteration 6600: Loss = -11842.783837253559
1
Iteration 6700: Loss = -11837.996774533822
Iteration 6800: Loss = -11835.773136173202
Iteration 6900: Loss = -11835.773443186368
1
Iteration 7000: Loss = -11835.772323900743
Iteration 7100: Loss = -11835.769141166218
Iteration 7200: Loss = -11835.767915546952
Iteration 7300: Loss = -11835.777283731926
1
Iteration 7400: Loss = -11835.767017833452
Iteration 7500: Loss = -11835.759550613604
Iteration 7600: Loss = -11824.47771296854
Iteration 7700: Loss = -11824.498134958645
1
Iteration 7800: Loss = -11824.460446439836
Iteration 7900: Loss = -11824.462948717897
1
Iteration 8000: Loss = -11824.480876833535
2
Iteration 8100: Loss = -11824.462042901874
3
Iteration 8200: Loss = -11824.457939115022
Iteration 8300: Loss = -11816.78026718517
Iteration 8400: Loss = -11816.773694212572
Iteration 8500: Loss = -11816.772512274893
Iteration 8600: Loss = -11816.793840095239
1
Iteration 8700: Loss = -11816.7701823663
Iteration 8800: Loss = -11816.761078042771
Iteration 8900: Loss = -11816.768113200364
1
Iteration 9000: Loss = -11816.758585630669
Iteration 9100: Loss = -11816.758341197992
Iteration 9200: Loss = -11816.967254283778
1
Iteration 9300: Loss = -11816.757656438103
Iteration 9400: Loss = -11816.810530671539
1
Iteration 9500: Loss = -11816.757155891584
Iteration 9600: Loss = -11816.756787264105
Iteration 9700: Loss = -11808.913540532449
Iteration 9800: Loss = -11798.75368869036
Iteration 9900: Loss = -11798.902394826098
1
Iteration 10000: Loss = -11798.744469068848
Iteration 10100: Loss = -11786.330840387449
Iteration 10200: Loss = -11786.262698462035
Iteration 10300: Loss = -11786.331405845456
1
Iteration 10400: Loss = -11786.262162879188
Iteration 10500: Loss = -11786.277326962507
1
Iteration 10600: Loss = -11786.261774909968
Iteration 10700: Loss = -11786.281296549389
1
Iteration 10800: Loss = -11786.265377791864
2
Iteration 10900: Loss = -11786.266362263781
3
Iteration 11000: Loss = -11786.281151290834
4
Iteration 11100: Loss = -11786.26137336407
Iteration 11200: Loss = -11786.261084308711
Iteration 11300: Loss = -11786.324935983725
1
Iteration 11400: Loss = -11786.260765140236
Iteration 11500: Loss = -11775.2550616052
Iteration 11600: Loss = -11775.18820797669
Iteration 11700: Loss = -11775.181911003716
Iteration 11800: Loss = -11775.182883151696
1
Iteration 11900: Loss = -11775.181803704718
Iteration 12000: Loss = -11775.182420551826
1
Iteration 12100: Loss = -11775.1858876119
2
Iteration 12200: Loss = -11775.199841977445
3
Iteration 12300: Loss = -11775.25847631504
4
Iteration 12400: Loss = -11775.181204176273
Iteration 12500: Loss = -11775.18120228148
Iteration 12600: Loss = -11775.290288103517
1
Iteration 12700: Loss = -11775.18141284114
2
Iteration 12800: Loss = -11775.18023378827
Iteration 12900: Loss = -11775.204476599141
1
Iteration 13000: Loss = -11775.180081058084
Iteration 13100: Loss = -11772.077583505597
Iteration 13200: Loss = -11772.073928975733
Iteration 13300: Loss = -11772.088232240107
1
Iteration 13400: Loss = -11772.073697274504
Iteration 13500: Loss = -11772.071936975939
Iteration 13600: Loss = -11772.279626872052
1
Iteration 13700: Loss = -11772.071212190558
Iteration 13800: Loss = -11772.074653061974
1
Iteration 13900: Loss = -11772.07156639979
2
Iteration 14000: Loss = -11772.071292562372
3
Iteration 14100: Loss = -11772.075644391221
4
Iteration 14200: Loss = -11772.129776664167
5
Iteration 14300: Loss = -11772.14001637399
6
Iteration 14400: Loss = -11772.071127416551
Iteration 14500: Loss = -11772.07276477882
1
Iteration 14600: Loss = -11772.07132175682
2
Iteration 14700: Loss = -11772.071200029392
3
Iteration 14800: Loss = -11772.09255899006
4
Iteration 14900: Loss = -11772.070601990079
Iteration 15000: Loss = -11771.687872440149
Iteration 15100: Loss = -11771.687655856953
Iteration 15200: Loss = -11771.499313196284
Iteration 15300: Loss = -11771.503384268439
1
Iteration 15400: Loss = -11771.589747933534
2
Iteration 15500: Loss = -11771.497936994921
Iteration 15600: Loss = -11771.49869776228
1
Iteration 15700: Loss = -11771.509485598017
2
Iteration 15800: Loss = -11770.538458346846
Iteration 15900: Loss = -11767.443906898312
Iteration 16000: Loss = -11767.446273896818
1
Iteration 16100: Loss = -11767.443755923985
Iteration 16200: Loss = -11767.444208096673
1
Iteration 16300: Loss = -11766.0333780475
Iteration 16400: Loss = -11766.02860297539
Iteration 16500: Loss = -11765.830029936482
Iteration 16600: Loss = -11765.82474808499
Iteration 16700: Loss = -11765.815567429763
Iteration 16800: Loss = -11765.82141088073
1
Iteration 16900: Loss = -11765.815966555963
2
Iteration 17000: Loss = -11765.822190750285
3
Iteration 17100: Loss = -11765.826896666145
4
Iteration 17200: Loss = -11765.85148186654
5
Iteration 17300: Loss = -11765.834178592899
6
Iteration 17400: Loss = -11765.814379676613
Iteration 17500: Loss = -11765.815309004985
1
Iteration 17600: Loss = -11765.814351256342
Iteration 17700: Loss = -11765.814891358357
1
Iteration 17800: Loss = -11765.818625838685
2
Iteration 17900: Loss = -11765.814359117581
3
Iteration 18000: Loss = -11765.81600960145
4
Iteration 18100: Loss = -11765.819533214639
5
Iteration 18200: Loss = -11765.817194166822
6
Iteration 18300: Loss = -11765.820456102052
7
Iteration 18400: Loss = -11765.814660206874
8
Iteration 18500: Loss = -11765.817958555117
9
Iteration 18600: Loss = -11765.8147023319
10
Stopping early at iteration 18600 due to no improvement.
tensor([[  7.5649,  -9.8697],
        [  2.3975,  -4.3930],
        [  6.0480,  -7.4364],
        [  7.4303,  -8.8167],
        [ -6.4380,   4.7558],
        [  8.4553, -13.0705],
        [ -7.9745,   6.5557],
        [ -3.1910,   1.6949],
        [  7.9610,  -9.5726],
        [ -6.9477,   4.2908],
        [-10.6528,   6.0376],
        [  8.1089, -10.1754],
        [  7.7152,  -9.1082],
        [  8.6305, -10.0412],
        [  5.1837,  -7.1932],
        [  7.1737,  -9.0706],
        [  8.9626, -12.9247],
        [ -4.7587,   2.9570],
        [  8.2391, -10.1520],
        [  6.6793,  -8.4774],
        [  7.2506,  -9.4346],
        [  8.0632, -10.3646],
        [  8.4160,  -9.8212],
        [  7.0370,  -8.6223],
        [  7.8387,  -9.6373],
        [  0.1335,  -4.2396],
        [ -5.3134,   3.9266],
        [ -9.2840,   7.8945],
        [  8.2261,  -9.7052],
        [  6.9475, -10.8529],
        [  8.4293,  -9.8470],
        [-10.6730,   8.3440],
        [  8.5631, -11.3400],
        [  8.1786,  -9.5827],
        [  7.7060,  -9.5094],
        [-11.9557,   7.3405],
        [ -8.3221,   6.6526],
        [ -9.9616,   7.1825],
        [  7.9992, -10.0605],
        [  7.8776,  -9.2649],
        [  8.5511, -10.0398],
        [ -7.5781,   6.1145],
        [  8.3922,  -9.7785],
        [  6.8223,  -8.4447],
        [ -7.7714,   6.0833],
        [-12.2634,   7.6482],
        [ -8.9065,   7.2960],
        [ -9.9676,   8.1284],
        [  8.1036, -11.7111],
        [  7.0525,  -8.4418],
        [  7.4704, -11.1722],
        [  8.7608, -11.7442],
        [ -5.5710,   3.7413],
        [  8.3210,  -9.7926],
        [  5.8438,  -7.7380],
        [  1.2187,  -3.0373],
        [ -8.6944,   6.8652],
        [ -9.8195,   6.9307],
        [  7.3789,  -8.7851],
        [  7.8792,  -9.3169],
        [  8.8452, -10.8453],
        [  8.1278, -11.0399],
        [  8.0664,  -9.4666],
        [-10.7535,   6.5055],
        [ -8.8667,   7.4621],
        [  8.2384, -11.6307],
        [-10.8214,   6.8175],
        [  8.1337,  -9.5691],
        [  7.0436,  -9.1926],
        [  9.0569, -11.6996],
        [  8.9142, -10.5797],
        [ -6.3141,   4.6510],
        [  8.0246,  -9.4894],
        [ -8.2304,   6.8441],
        [ -7.2744,   4.6865],
        [  7.4969, -10.3662],
        [ -2.0579,  -0.9988],
        [-11.1174,   8.9370],
        [  7.2828, -10.5587],
        [ -9.3185,   7.6495],
        [ -3.9352,   2.5472],
        [  8.0574,  -9.4468],
        [ -9.1080,   7.4404],
        [ -9.5210,   7.4933],
        [  9.1902, -10.5907],
        [  5.9337,  -8.1270],
        [ -9.2047,   7.3571],
        [ -7.9610,   5.8003],
        [  6.5877,  -8.2528],
        [  7.8797,  -9.2869],
        [  8.4176, -12.9340],
        [  6.7852,  -8.3412],
        [  8.0350,  -9.4322],
        [  6.0197,  -7.5714],
        [ -9.9522,   5.7290],
        [ -6.0586,   4.3098],
        [-10.3343,   7.9867],
        [  8.4009,  -9.8234],
        [ -9.6361,   7.4906],
        [ -1.1081,  -0.6349]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4991, 0.5009],
        [0.3928, 0.6072]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6161, 0.3839], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3924, 0.0973],
         [0.0615, 0.2144]],

        [[0.6688, 0.1012],
         [0.4214, 0.1707]],

        [[0.5701, 0.0998],
         [0.7819, 0.1466]],

        [[0.8446, 0.0961],
         [0.0287, 0.4227]],

        [[0.9674, 0.1026],
         [0.3721, 0.9642]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 71
Adjusted Rand Index: 0.16808080808080808
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.5002647731045401
Average Adjusted Rand Index: 0.817614939109454
Iteration 0: Loss = -24090.81267982832
Iteration 10: Loss = -11582.471657211687
Iteration 20: Loss = -11582.457833886938
Iteration 30: Loss = -11582.457840206092
1
Iteration 40: Loss = -11582.457844024204
2
Iteration 50: Loss = -11582.457844024204
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7797, 0.2203],
        [0.2679, 0.7321]], dtype=torch.float64)
alpha: tensor([0.4884, 0.5116])
beta: tensor([[[0.1945, 0.0986],
         [0.5752, 0.3944]],

        [[0.5309, 0.1012],
         [0.6228, 0.9325]],

        [[0.1084, 0.1006],
         [0.2417, 0.7006]],

        [[0.1367, 0.0895],
         [0.9927, 0.9622]],

        [[0.8769, 0.1033],
         [0.2620, 0.2763]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24090.467294850932
Iteration 100: Loss = -12497.450911285907
Iteration 200: Loss = -12207.44241638815
Iteration 300: Loss = -12022.55777099524
Iteration 400: Loss = -11996.962867712467
Iteration 500: Loss = -11921.183513250417
Iteration 600: Loss = -11841.873563457959
Iteration 700: Loss = -11827.7701444628
Iteration 800: Loss = -11826.105025235154
Iteration 900: Loss = -11817.89232616506
Iteration 1000: Loss = -11816.561273293824
Iteration 1100: Loss = -11816.280260058356
Iteration 1200: Loss = -11815.391274217469
Iteration 1300: Loss = -11814.15537059725
Iteration 1400: Loss = -11813.600295234906
Iteration 1500: Loss = -11811.157533752777
Iteration 1600: Loss = -11807.115906740042
Iteration 1700: Loss = -11806.027240382198
Iteration 1800: Loss = -11805.91794987378
Iteration 1900: Loss = -11801.326285458244
Iteration 2000: Loss = -11801.276535990426
Iteration 2100: Loss = -11801.241583668914
Iteration 2200: Loss = -11801.206120128269
Iteration 2300: Loss = -11801.178478135524
Iteration 2400: Loss = -11801.156891564182
Iteration 2500: Loss = -11801.13798099815
Iteration 2600: Loss = -11801.10580766017
Iteration 2700: Loss = -11796.260727118508
Iteration 2800: Loss = -11796.24872287791
Iteration 2900: Loss = -11796.241671363368
Iteration 3000: Loss = -11796.23543730324
Iteration 3100: Loss = -11796.247918881596
1
Iteration 3200: Loss = -11795.907962386584
Iteration 3300: Loss = -11794.113547290703
Iteration 3400: Loss = -11794.107634115779
Iteration 3500: Loss = -11794.10373678513
Iteration 3600: Loss = -11794.08156982014
Iteration 3700: Loss = -11788.017788306472
Iteration 3800: Loss = -11783.606963138574
Iteration 3900: Loss = -11776.171725748933
Iteration 4000: Loss = -11775.994154722686
Iteration 4100: Loss = -11772.353831631533
Iteration 4200: Loss = -11772.349099050862
Iteration 4300: Loss = -11772.343405669517
Iteration 4400: Loss = -11766.326759918453
Iteration 4500: Loss = -11766.21748417725
Iteration 4600: Loss = -11766.211640368701
Iteration 4700: Loss = -11766.207377233532
Iteration 4800: Loss = -11766.103797993452
Iteration 4900: Loss = -11762.813221344732
Iteration 5000: Loss = -11762.819795621866
1
Iteration 5100: Loss = -11761.762420391848
Iteration 5200: Loss = -11754.806983925875
Iteration 5300: Loss = -11754.784773904898
Iteration 5400: Loss = -11754.782268826268
Iteration 5500: Loss = -11754.778790992063
Iteration 5600: Loss = -11746.444701626044
Iteration 5700: Loss = -11735.124187670885
Iteration 5800: Loss = -11735.119036247024
Iteration 5900: Loss = -11732.364445853978
Iteration 6000: Loss = -11732.335686289012
Iteration 6100: Loss = -11732.317844224566
Iteration 6200: Loss = -11732.3167624896
Iteration 6300: Loss = -11732.338097362257
1
Iteration 6400: Loss = -11732.312700175959
Iteration 6500: Loss = -11725.909216636139
Iteration 6600: Loss = -11725.899067590577
Iteration 6700: Loss = -11725.899907798817
1
Iteration 6800: Loss = -11725.897334942823
Iteration 6900: Loss = -11725.90232840088
1
Iteration 7000: Loss = -11717.700157105533
Iteration 7100: Loss = -11717.731113039876
1
Iteration 7200: Loss = -11717.700303912987
2
Iteration 7300: Loss = -11717.713632424886
3
Iteration 7400: Loss = -11717.700238879152
4
Iteration 7500: Loss = -11697.68847187643
Iteration 7600: Loss = -11697.678681992908
Iteration 7700: Loss = -11697.688190300396
1
Iteration 7800: Loss = -11697.69063651007
2
Iteration 7900: Loss = -11697.680369680409
3
Iteration 8000: Loss = -11690.371239790958
Iteration 8100: Loss = -11682.895161244256
Iteration 8200: Loss = -11682.898999655175
1
Iteration 8300: Loss = -11682.898407183682
2
Iteration 8400: Loss = -11682.90642039845
3
Iteration 8500: Loss = -11682.893857740686
Iteration 8600: Loss = -11682.892506565868
Iteration 8700: Loss = -11682.896807328189
1
Iteration 8800: Loss = -11682.892433382547
Iteration 8900: Loss = -11682.902949354988
1
Iteration 9000: Loss = -11682.891909438775
Iteration 9100: Loss = -11682.891888918424
Iteration 9200: Loss = -11682.910695112974
1
Iteration 9300: Loss = -11682.902603780047
2
Iteration 9400: Loss = -11682.94067096401
3
Iteration 9500: Loss = -11682.890575828591
Iteration 9600: Loss = -11672.079439295934
Iteration 9700: Loss = -11672.084377976018
1
Iteration 9800: Loss = -11672.17038482371
2
Iteration 9900: Loss = -11672.078746118481
Iteration 10000: Loss = -11672.080177594107
1
Iteration 10100: Loss = -11672.123128784187
2
Iteration 10200: Loss = -11672.213885742301
3
Iteration 10300: Loss = -11672.0779488918
Iteration 10400: Loss = -11672.079130886867
1
Iteration 10500: Loss = -11672.081039753566
2
Iteration 10600: Loss = -11672.084555511943
3
Iteration 10700: Loss = -11672.227578713171
4
Iteration 10800: Loss = -11672.07379038824
Iteration 10900: Loss = -11667.18757201741
Iteration 11000: Loss = -11667.199165522814
1
Iteration 11100: Loss = -11667.186404572274
Iteration 11200: Loss = -11667.186476875395
1
Iteration 11300: Loss = -11654.42526581144
Iteration 11400: Loss = -11654.426656211223
1
Iteration 11500: Loss = -11654.395199884513
Iteration 11600: Loss = -11654.397156583409
1
Iteration 11700: Loss = -11654.401359134567
2
Iteration 11800: Loss = -11654.398507279819
3
Iteration 11900: Loss = -11654.400017257338
4
Iteration 12000: Loss = -11654.402677470986
5
Iteration 12100: Loss = -11654.3936537119
Iteration 12200: Loss = -11654.398535107968
1
Iteration 12300: Loss = -11654.433837855735
2
Iteration 12400: Loss = -11654.3982800437
3
Iteration 12500: Loss = -11654.394375539936
4
Iteration 12600: Loss = -11654.394321501843
5
Iteration 12700: Loss = -11642.500538862123
Iteration 12800: Loss = -11642.512684833755
1
Iteration 12900: Loss = -11642.49818805547
Iteration 13000: Loss = -11642.499724158746
1
Iteration 13100: Loss = -11642.49978492907
2
Iteration 13200: Loss = -11642.49929426882
3
Iteration 13300: Loss = -11642.579859771311
4
Iteration 13400: Loss = -11642.513058064802
5
Iteration 13500: Loss = -11642.511914713366
6
Iteration 13600: Loss = -11642.499401117224
7
Iteration 13700: Loss = -11642.499544383665
8
Iteration 13800: Loss = -11642.499831676474
9
Iteration 13900: Loss = -11642.508760233231
10
Stopping early at iteration 13900 due to no improvement.
tensor([[ -9.4111,   8.0216],
        [ -6.6661,   2.8704],
        [ -8.4790,   7.0900],
        [-10.1547,   8.4267],
        [  5.3539,  -7.7608],
        [-10.0508,   7.9499],
        [  6.7944,  -8.9098],
        [  2.6597,  -4.1144],
        [-10.2138,   7.7115],
        [  5.5145,  -7.2173],
        [  7.8895,  -9.2975],
        [ -9.4735,   8.0867],
        [ -9.2199,   7.7447],
        [-10.2741,   8.2697],
        [ -6.9019,   5.4108],
        [-10.1096,   7.8783],
        [ -9.5572,   8.1425],
        [-10.6445,   6.6291],
        [-10.8954,   7.3470],
        [ -8.5771,   5.5735],
        [ -9.5580,   7.9990],
        [-10.1066,   8.4838],
        [-10.0314,   8.5758],
        [ -9.7738,   6.4914],
        [-10.8306,   7.1106],
        [ -4.1913,   2.2984],
        [  4.6832,  -6.7711],
        [  8.5524, -10.1272],
        [ -9.9162,   8.5280],
        [ -9.8038,   7.0774],
        [ -9.8031,   8.2840],
        [  7.2234,  -8.6711],
        [ -9.8401,   7.7817],
        [ -9.7779,   6.6405],
        [ -9.4074,   8.0210],
        [  7.1234,  -9.6752],
        [  7.1532,  -8.5718],
        [  8.2069,  -9.6265],
        [ -9.8223,   8.2786],
        [ -8.9016,   7.3856],
        [ -9.4798,   8.0208],
        [  6.7380,  -8.1435],
        [-10.9398,   8.1496],
        [ -7.7532,   5.5466],
        [  6.9147,  -9.0585],
        [  6.8833,  -8.2755],
        [  7.0036,  -9.6555],
        [  7.7149,  -9.1097],
        [ -9.8365,   8.1685],
        [ -8.0977,   6.7084],
        [-10.0847,   8.5534],
        [-10.3461,   8.8966],
        [  5.0164,  -6.5043],
        [-11.3003,   8.2654],
        [ -8.7245,   5.6918],
        [ -3.2032,   0.8468],
        [  7.6679,  -9.8156],
        [  7.6518,  -9.1462],
        [ -9.4748,   7.2521],
        [ -9.7979,   8.1319],
        [-12.0384,   8.6971],
        [ -9.2367,   7.8287],
        [ -8.9338,   7.5470],
        [  6.1150, -10.4886],
        [  7.4560,  -8.9847],
        [ -9.4096,   7.5581],
        [  8.6572, -10.3018],
        [ -9.9936,   8.2670],
        [ -8.9335,   6.1772],
        [-10.6453,   7.6217],
        [-10.5966,   7.6913],
        [  4.5828,  -6.2593],
        [-10.6918,   9.2164],
        [  7.6696,  -9.1708],
        [  8.3836, -10.0081],
        [ -9.2966,   7.8980],
        [ -0.8073,  -1.0002],
        [  6.6316,  -8.0475],
        [-10.7676,   8.9546],
        [  8.1389,  -9.6242],
        [  3.6236,  -5.3678],
        [-10.3893,   7.5313],
        [  7.2564,  -9.1928],
        [  6.9660,  -8.3997],
        [-10.0766,   8.5622],
        [ -7.7164,   5.7308],
        [  7.9721,  -9.3828],
        [  6.9535,  -8.3401],
        [ -8.3724,   6.9255],
        [-10.3656,   7.3080],
        [-10.1913,   8.6080],
        [ -8.4475,   7.0385],
        [ -9.1422,   7.7482],
        [ -8.1287,   6.7168],
        [  6.5620, -10.2472],
        [  5.3534,  -6.8029],
        [  7.7407,  -9.2586],
        [ -9.5789,   7.9764],
        [  6.9387,  -8.7108],
        [ -0.0843,  -1.3183]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7754, 0.2246],
        [0.2892, 0.7108]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3743, 0.6257], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1949, 0.0968],
         [0.5752, 0.4016]],

        [[0.5309, 0.1010],
         [0.6228, 0.9325]],

        [[0.1084, 0.1240],
         [0.2417, 0.7006]],

        [[0.1367, 0.0890],
         [0.9927, 0.9622]],

        [[0.8769, 0.1036],
         [0.2620, 0.2763]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721314419105764
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9291540882844886
Average Adjusted Rand Index: 0.9304109020488591
11586.683428311346
new:  [0.9840320329363369, 0.4667883594148197, 0.5002647731045401, 0.9291540882844886] [0.9839995611635631, 0.8408876663821813, 0.817614939109454, 0.9304109020488591] [11578.281321358432, 11761.2925784785, 11765.8147023319, 11642.508760233231]
prior:  [0.99199998169963, 0.4833821812780762, 0.99199998169963, 0.99199998169963] [0.9919995611635631, 0.8285240300185448, 0.9919995611635631, 0.9919995611635631] [11582.457844642237, 11779.885669511164, 11582.457837685764, 11582.457844024204]
-----------------------------------------------------------------------------------------
This iteration is 22
True Objective function: Loss = -11746.239318135997
Iteration 0: Loss = -31675.505965144715
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.5191,    nan]],

        [[0.6232,    nan],
         [0.7021, 0.4765]],

        [[0.5726,    nan],
         [0.2224, 0.3704]],

        [[0.8199,    nan],
         [0.8116, 0.0537]],

        [[0.9612,    nan],
         [0.6570, 0.2721]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|       | 23/100 [14:18:04<49:57:35, 2335.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|       | 24/100 [14:55:46<48:50:53, 2313.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|       | 25/100 [15:27:24<45:36:12, 2188.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|       | 26/100 [16:05:36<45:38:05, 2220.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|       | 27/100 [16:41:17<44:31:52, 2196.06s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|       | 28/100 [17:10:13<41:09:51, 2058.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|       | 29/100 [17:50:41<42:46:49, 2169.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|       | 30/100 [18:24:27<41:20:34, 2126.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 31%|       | 31/100 [18:55:08<39:06:36, 2040.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 32%|      | 32/100 [19:26:43<37:43:04, 1996.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 33%|      | 33/100 [19:52:56<34:48:04, 1869.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32109.752295704646
Iteration 100: Loss = -12668.403402943395
Iteration 200: Loss = -12659.522298223177
Iteration 300: Loss = -12626.256383241996
Iteration 400: Loss = -12591.447605578154
Iteration 500: Loss = -12548.125658596278
Iteration 600: Loss = -12483.578978078336
Iteration 700: Loss = -12392.106699897487
Iteration 800: Loss = -12220.575885599712
Iteration 900: Loss = -12099.818946797124
Iteration 1000: Loss = -12050.615803617593
Iteration 1100: Loss = -11987.153956826585
Iteration 1200: Loss = -11946.76394110281
Iteration 1300: Loss = -11918.466632674455
Iteration 1400: Loss = -11896.647804367398
Iteration 1500: Loss = -11893.352478883451
Iteration 1600: Loss = -11888.237602360572
Iteration 1700: Loss = -11866.08341589298
Iteration 1800: Loss = -11855.8821713116
Iteration 1900: Loss = -11844.03820467621
Iteration 2000: Loss = -11840.97402703846
Iteration 2100: Loss = -11822.82505832432
Iteration 2200: Loss = -11822.61469287162
Iteration 2300: Loss = -11822.48776673238
Iteration 2400: Loss = -11822.223803105535
Iteration 2500: Loss = -11812.908920693073
Iteration 2600: Loss = -11812.835451742068
Iteration 2700: Loss = -11812.784491071734
Iteration 2800: Loss = -11812.74228495551
Iteration 2900: Loss = -11812.703088852333
Iteration 3000: Loss = -11812.657082570418
Iteration 3100: Loss = -11812.477714246714
Iteration 3200: Loss = -11804.913734577787
Iteration 3300: Loss = -11792.77954310508
Iteration 3400: Loss = -11792.727106520906
Iteration 3500: Loss = -11792.599703735565
Iteration 3600: Loss = -11781.268714255859
Iteration 3700: Loss = -11766.80085497622
Iteration 3800: Loss = -11766.77028715028
Iteration 3900: Loss = -11766.748176200468
Iteration 4000: Loss = -11766.728309760965
Iteration 4100: Loss = -11766.704839374828
Iteration 4200: Loss = -11766.635628772454
Iteration 4300: Loss = -11753.607148589244
Iteration 4400: Loss = -11742.353438523016
Iteration 4500: Loss = -11742.340999389522
Iteration 4600: Loss = -11742.33122562093
Iteration 4700: Loss = -11742.322972126469
Iteration 4800: Loss = -11742.316473746148
Iteration 4900: Loss = -11742.309668861499
Iteration 5000: Loss = -11742.304113490725
Iteration 5100: Loss = -11742.325587218407
1
Iteration 5200: Loss = -11742.294576099033
Iteration 5300: Loss = -11742.29048149698
Iteration 5400: Loss = -11742.286762319924
Iteration 5500: Loss = -11742.284445712528
Iteration 5600: Loss = -11742.28018283524
Iteration 5700: Loss = -11742.277257589867
Iteration 5800: Loss = -11742.274556632723
Iteration 5900: Loss = -11742.272197671151
Iteration 6000: Loss = -11742.284456476773
1
Iteration 6100: Loss = -11742.26763918082
Iteration 6200: Loss = -11742.270657616857
1
Iteration 6300: Loss = -11742.263731535617
Iteration 6400: Loss = -11742.262003241824
Iteration 6500: Loss = -11742.264376080195
1
Iteration 6600: Loss = -11742.258944669073
Iteration 6700: Loss = -11742.25739588877
Iteration 6800: Loss = -11742.25603663423
Iteration 6900: Loss = -11742.257006303473
1
Iteration 7000: Loss = -11742.25352505655
Iteration 7100: Loss = -11742.252355303051
Iteration 7200: Loss = -11742.284618191603
1
Iteration 7300: Loss = -11742.305129004018
2
Iteration 7400: Loss = -11742.248984093767
Iteration 7500: Loss = -11742.247683213374
Iteration 7600: Loss = -11742.245979636587
Iteration 7700: Loss = -11742.242694718885
Iteration 7800: Loss = -11738.225967079896
Iteration 7900: Loss = -11738.22294286483
Iteration 8000: Loss = -11738.230234582501
1
Iteration 8100: Loss = -11738.228034769885
2
Iteration 8200: Loss = -11738.216178366718
Iteration 8300: Loss = -11738.215260576033
Iteration 8400: Loss = -11738.219692463823
1
Iteration 8500: Loss = -11738.213734791063
Iteration 8600: Loss = -11738.216612334747
1
Iteration 8700: Loss = -11738.212638272236
Iteration 8800: Loss = -11738.21121194374
Iteration 8900: Loss = -11738.205499289501
Iteration 9000: Loss = -11738.260625564499
1
Iteration 9100: Loss = -11738.216021693954
2
Iteration 9200: Loss = -11738.20472424936
Iteration 9300: Loss = -11738.223763015292
1
Iteration 9400: Loss = -11738.20249166396
Iteration 9500: Loss = -11738.20174408222
Iteration 9600: Loss = -11738.258556529952
1
Iteration 9700: Loss = -11738.199676322836
Iteration 9800: Loss = -11738.208332537708
1
Iteration 9900: Loss = -11738.198572393043
Iteration 10000: Loss = -11738.203358062521
1
Iteration 10100: Loss = -11738.197039962251
Iteration 10200: Loss = -11738.193338293033
Iteration 10300: Loss = -11738.187080323847
Iteration 10400: Loss = -11738.183780808233
Iteration 10500: Loss = -11738.18214029199
Iteration 10600: Loss = -11738.192828922603
1
Iteration 10700: Loss = -11738.181826070619
Iteration 10800: Loss = -11738.223366322405
1
Iteration 10900: Loss = -11738.182557953089
2
Iteration 11000: Loss = -11738.184381470448
3
Iteration 11100: Loss = -11738.182402943099
4
Iteration 11200: Loss = -11738.181863990225
5
Iteration 11300: Loss = -11738.181149132904
Iteration 11400: Loss = -11738.185820702332
1
Iteration 11500: Loss = -11738.18246991392
2
Iteration 11600: Loss = -11738.193370203177
3
Iteration 11700: Loss = -11738.186169106879
4
Iteration 11800: Loss = -11738.18386306012
5
Iteration 11900: Loss = -11738.182025430244
6
Iteration 12000: Loss = -11738.195718874524
7
Iteration 12100: Loss = -11738.180814104277
Iteration 12200: Loss = -11738.205101917476
1
Iteration 12300: Loss = -11738.182075518504
2
Iteration 12400: Loss = -11738.183620313317
3
Iteration 12500: Loss = -11738.290819845008
4
Iteration 12600: Loss = -11738.180738280153
Iteration 12700: Loss = -11738.180203784723
Iteration 12800: Loss = -11738.210270496753
1
Iteration 12900: Loss = -11738.18074433124
2
Iteration 13000: Loss = -11738.18507868791
3
Iteration 13100: Loss = -11738.308548045037
4
Iteration 13200: Loss = -11738.183650936737
5
Iteration 13300: Loss = -11738.194646486263
6
Iteration 13400: Loss = -11738.217716956686
7
Iteration 13500: Loss = -11738.17983826826
Iteration 13600: Loss = -11738.180700630825
1
Iteration 13700: Loss = -11738.180034765155
2
Iteration 13800: Loss = -11738.179817513364
Iteration 13900: Loss = -11738.179764958333
Iteration 14000: Loss = -11738.180814960879
1
Iteration 14100: Loss = -11738.180907410864
2
Iteration 14200: Loss = -11738.181988177383
3
Iteration 14300: Loss = -11738.179934447076
4
Iteration 14400: Loss = -11738.180363171676
5
Iteration 14500: Loss = -11738.214625450466
6
Iteration 14600: Loss = -11738.182770684722
7
Iteration 14700: Loss = -11738.179502881105
Iteration 14800: Loss = -11738.18054605215
1
Iteration 14900: Loss = -11738.188581061886
2
Iteration 15000: Loss = -11738.240327558373
3
Iteration 15100: Loss = -11738.183136726204
4
Iteration 15200: Loss = -11738.19466588603
5
Iteration 15300: Loss = -11738.188457214537
6
Iteration 15400: Loss = -11738.179414272381
Iteration 15500: Loss = -11738.182407551198
1
Iteration 15600: Loss = -11738.195145076455
2
Iteration 15700: Loss = -11738.186518832628
3
Iteration 15800: Loss = -11738.181216414803
4
Iteration 15900: Loss = -11738.181361073557
5
Iteration 16000: Loss = -11738.195786474696
6
Iteration 16100: Loss = -11738.179383828983
Iteration 16200: Loss = -11738.19101424691
1
Iteration 16300: Loss = -11738.179245144496
Iteration 16400: Loss = -11738.27484151205
1
Iteration 16500: Loss = -11738.17993857774
2
Iteration 16600: Loss = -11738.179877242628
3
Iteration 16700: Loss = -11738.180009161819
4
Iteration 16800: Loss = -11738.19269742735
5
Iteration 16900: Loss = -11738.179211730678
Iteration 17000: Loss = -11738.216580401908
1
Iteration 17100: Loss = -11738.181161133749
2
Iteration 17200: Loss = -11738.181002463212
3
Iteration 17300: Loss = -11738.180501899717
4
Iteration 17400: Loss = -11738.179225354752
5
Iteration 17500: Loss = -11738.310055219225
6
Iteration 17600: Loss = -11738.181352603082
7
Iteration 17700: Loss = -11738.180767072132
8
Iteration 17800: Loss = -11738.189846241292
9
Iteration 17900: Loss = -11738.179199435257
Iteration 18000: Loss = -11738.193262539302
1
Iteration 18100: Loss = -11738.19159617189
2
Iteration 18200: Loss = -11738.18268074184
3
Iteration 18300: Loss = -11738.17954564392
4
Iteration 18400: Loss = -11738.203905318747
5
Iteration 18500: Loss = -11738.197639306862
6
Iteration 18600: Loss = -11738.180188010478
7
Iteration 18700: Loss = -11738.180772933456
8
Iteration 18800: Loss = -11738.184165486022
9
Iteration 18900: Loss = -11738.189468069739
10
Stopping early at iteration 18900 due to no improvement.
tensor([[ -9.2090,   4.5938],
        [ -8.6043,   3.9891],
        [-11.8108,   7.1956],
        [  0.0821,  -4.6973],
        [-11.9300,   7.3148],
        [  6.4608, -11.0760],
        [  5.8567, -10.4719],
        [  4.2980,  -8.9132],
        [  6.5768, -11.1920],
        [  5.5425, -10.1577],
        [  5.1074,  -9.7226],
        [-11.0270,   6.4118],
        [  6.0750, -10.6902],
        [ -7.7946,   3.1793],
        [  5.5541, -10.1693],
        [  6.5981, -11.2133],
        [  5.1343,  -9.7496],
        [-10.2713,   5.6561],
        [  2.4418,  -7.0571],
        [-10.7178,   6.1026],
        [  6.7059, -11.3211],
        [  5.1023,  -9.7175],
        [-11.0375,   6.4223],
        [  6.8809, -11.4961],
        [  6.0846, -10.6999],
        [  6.6605, -11.2757],
        [  4.2867,  -8.9019],
        [ -9.8889,   5.2737],
        [-10.8390,   6.2238],
        [-10.8582,   6.2430],
        [-10.5850,   5.9698],
        [ -7.2957,   2.6804],
        [  7.0452, -11.6604],
        [  5.5361, -10.1513],
        [  6.4997, -11.1149],
        [-10.8782,   6.2630],
        [-10.3534,   5.7382],
        [-10.4031,   5.7878],
        [-10.9975,   6.3823],
        [-10.1355,   5.5203],
        [-10.8279,   6.2126],
        [-10.8524,   6.2372],
        [-11.0428,   6.4276],
        [-10.9969,   6.3817],
        [  6.6096, -11.2249],
        [-10.8912,   6.2760],
        [ -9.8343,   5.2191],
        [  7.0200, -11.6353],
        [  5.5245, -10.1398],
        [  6.1166, -10.7318],
        [ -8.1413,   3.5261],
        [-10.1039,   5.4886],
        [-10.8671,   6.2519],
        [ -9.4637,   4.8485],
        [ -8.7142,   4.0990],
        [-11.1151,   6.4999],
        [-10.3675,   5.7523],
        [  5.7588, -10.3740],
        [  6.5795, -11.1947],
        [-10.2879,   5.6727],
        [-10.6758,   6.0606],
        [  6.2690, -10.8842],
        [  4.1752,  -8.7904],
        [  5.5232, -10.1384],
        [-10.0215,   5.4063],
        [  5.5302, -10.1454],
        [-10.2974,   5.6822],
        [  6.8365, -11.4517],
        [-10.8373,   6.2221],
        [-10.7617,   6.1465],
        [-11.6495,   7.0343],
        [ -7.7855,   3.1703],
        [-11.4558,   6.8406],
        [-11.1710,   6.5558],
        [  5.8570, -10.4722],
        [  5.2265,  -9.8417],
        [ -8.1168,   3.5016],
        [-10.8428,   6.2276],
        [-10.2080,   5.5928],
        [  6.0866, -10.7018],
        [  1.8373,  -6.4525],
        [  4.2766,  -8.8918],
        [-11.5177,   6.9025],
        [ -4.3988,  -0.2164],
        [  4.1854,  -8.8006],
        [-11.0972,   6.4819],
        [  5.4165, -10.0317],
        [  0.5665,  -5.1817],
        [  5.2993,  -9.9145],
        [-10.5369,   5.9216],
        [  4.1853,  -8.8005],
        [-11.2947,   6.6795],
        [  3.7071,  -8.3224],
        [  6.3796, -10.9948],
        [ -9.0268,   4.4116],
        [-12.1231,   7.5078],
        [-10.9216,   6.3064],
        [-10.2918,   5.6766],
        [-11.1085,   6.4933],
        [  5.5521, -10.1673]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7687, 0.2313],
        [0.2006, 0.7994]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4492, 0.5508], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.0910],
         [0.5191, 0.3946]],

        [[0.6232, 0.0985],
         [0.7021, 0.4765]],

        [[0.5726, 0.0987],
         [0.2224, 0.3704]],

        [[0.8199, 0.0980],
         [0.8116, 0.0537]],

        [[0.9612, 0.1061],
         [0.6570, 0.2721]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919995362962712
Average Adjusted Rand Index: 0.9919993417272899
Iteration 0: Loss = -14468.986501905958
Iteration 10: Loss = -11739.615414629741
Iteration 20: Loss = -11739.615417792093
1
Iteration 30: Loss = -11739.615417792093
2
Iteration 40: Loss = -11739.615417792093
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7677, 0.2323],
        [0.2007, 0.7993]], dtype=torch.float64)
alpha: tensor([0.4513, 0.5487])
beta: tensor([[[0.1982, 0.0911],
         [0.4820, 0.3871]],

        [[0.6797, 0.0988],
         [0.3404, 0.3717]],

        [[0.4894, 0.0986],
         [0.2548, 0.9668]],

        [[0.0436, 0.0981],
         [0.4757, 0.9321]],

        [[0.6194, 0.1059],
         [0.2309, 0.4153]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919995362962712
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14468.774541301807
Iteration 100: Loss = -11808.797870137725
Iteration 200: Loss = -11740.030808616562
Iteration 300: Loss = -11739.263869348033
Iteration 400: Loss = -11738.94460796669
Iteration 500: Loss = -11738.772905254606
Iteration 600: Loss = -11738.669280922786
Iteration 700: Loss = -11738.601542354274
Iteration 800: Loss = -11738.554210436676
Iteration 900: Loss = -11738.519648529498
Iteration 1000: Loss = -11738.49353201945
Iteration 1100: Loss = -11738.473295409727
Iteration 1200: Loss = -11738.457212066485
Iteration 1300: Loss = -11738.444228972403
Iteration 1400: Loss = -11738.43334376848
Iteration 1500: Loss = -11738.42417247355
Iteration 1600: Loss = -11738.415925295441
Iteration 1700: Loss = -11738.408435559826
Iteration 1800: Loss = -11738.399584650655
Iteration 1900: Loss = -11738.224819569192
Iteration 2000: Loss = -11738.21932757096
Iteration 2100: Loss = -11738.215659244686
Iteration 2200: Loss = -11738.213443028924
Iteration 2300: Loss = -11738.20970708161
Iteration 2400: Loss = -11738.207207781448
Iteration 2500: Loss = -11738.206787635507
Iteration 2600: Loss = -11738.203064417441
Iteration 2700: Loss = -11738.201235872699
Iteration 2800: Loss = -11738.281796675936
1
Iteration 2900: Loss = -11738.198197486303
Iteration 3000: Loss = -11738.197004392134
Iteration 3100: Loss = -11738.195663882258
Iteration 3200: Loss = -11738.194582772376
Iteration 3300: Loss = -11738.193651664118
Iteration 3400: Loss = -11738.192681253497
Iteration 3500: Loss = -11738.192060217658
Iteration 3600: Loss = -11738.197981815947
1
Iteration 3700: Loss = -11738.194041703371
2
Iteration 3800: Loss = -11738.1900074492
Iteration 3900: Loss = -11738.189157131526
Iteration 4000: Loss = -11738.209818926241
1
Iteration 4100: Loss = -11738.188013209043
Iteration 4200: Loss = -11738.187499926553
Iteration 4300: Loss = -11738.187656054633
1
Iteration 4400: Loss = -11738.186665303954
Iteration 4500: Loss = -11738.191525421287
1
Iteration 4600: Loss = -11738.186822478148
2
Iteration 4700: Loss = -11738.1912099699
3
Iteration 4800: Loss = -11738.185269773881
Iteration 4900: Loss = -11738.185039944345
Iteration 5000: Loss = -11738.216100701584
1
Iteration 5100: Loss = -11738.187449307878
2
Iteration 5200: Loss = -11738.184408075023
Iteration 5300: Loss = -11738.194894850682
1
Iteration 5400: Loss = -11738.196533517748
2
Iteration 5500: Loss = -11738.200077915333
3
Iteration 5600: Loss = -11738.183348337341
Iteration 5700: Loss = -11738.198785858292
1
Iteration 5800: Loss = -11738.182984575053
Iteration 5900: Loss = -11738.18593711988
1
Iteration 6000: Loss = -11738.182680248607
Iteration 6100: Loss = -11738.18464509154
1
Iteration 6200: Loss = -11738.18243488808
Iteration 6300: Loss = -11738.191273806027
1
Iteration 6400: Loss = -11738.182197106262
Iteration 6500: Loss = -11738.184631656684
1
Iteration 6600: Loss = -11738.182039180834
Iteration 6700: Loss = -11738.183187111537
1
Iteration 6800: Loss = -11738.181820487083
Iteration 6900: Loss = -11738.182471649738
1
Iteration 7000: Loss = -11738.181853579452
2
Iteration 7100: Loss = -11738.181657899813
Iteration 7200: Loss = -11738.190595708687
1
Iteration 7300: Loss = -11738.186747793596
2
Iteration 7400: Loss = -11738.18139176389
Iteration 7500: Loss = -11738.212480021708
1
Iteration 7600: Loss = -11738.182881556635
2
Iteration 7700: Loss = -11738.181368608073
Iteration 7800: Loss = -11738.184046579303
1
Iteration 7900: Loss = -11738.194384054841
2
Iteration 8000: Loss = -11738.182748143532
3
Iteration 8100: Loss = -11738.181322090739
Iteration 8200: Loss = -11738.181233006291
Iteration 8300: Loss = -11738.216710279283
1
Iteration 8400: Loss = -11738.180885484306
Iteration 8500: Loss = -11738.184363869454
1
Iteration 8600: Loss = -11738.181248934969
2
Iteration 8700: Loss = -11738.1832593673
3
Iteration 8800: Loss = -11738.215563050133
4
Iteration 8900: Loss = -11738.182691517912
5
Iteration 9000: Loss = -11738.182049103629
6
Iteration 9100: Loss = -11738.19481123289
7
Iteration 9200: Loss = -11738.189863781052
8
Iteration 9300: Loss = -11738.183641764044
9
Iteration 9400: Loss = -11738.206797515719
10
Stopping early at iteration 9400 due to no improvement.
tensor([[ -7.4181,   6.0317],
        [ -6.9985,   5.3759],
        [ -8.7128,   7.1244],
        [  1.1096,  -3.6676],
        [ -8.7346,   7.3272],
        [  6.9545,  -8.4458],
        [  6.4390,  -8.0693],
        [  5.4731,  -7.5113],
        [  7.2008,  -9.2253],
        [  6.4362,  -7.8943],
        [  5.7641,  -8.3053],
        [-10.5299,   5.9147],
        [  6.2270,  -8.5675],
        [ -6.6063,   4.3733],
        [  6.4877,  -7.8748],
        [  6.1190,  -9.9207],
        [  6.1547,  -7.6896],
        [ -8.1400,   6.6906],
        [  3.2297,  -6.2675],
        [ -8.8386,   7.0518],
        [  6.3143,  -9.5935],
        [  5.1949,  -9.0604],
        [ -8.8838,   7.2180],
        [  6.7218,  -8.4380],
        [  6.7251,  -8.1247],
        [  7.0033,  -8.6678],
        [  5.7645,  -7.2704],
        [ -9.6380,   5.0228],
        [ -9.2978,   7.0959],
        [ -8.0071,   6.6177],
        [ -8.5776,   6.9741],
        [ -5.8695,   4.1083],
        [  7.1356,  -8.8298],
        [  6.3325,  -7.8593],
        [  7.2988,  -8.7137],
        [ -8.7314,   7.1598],
        [ -8.1609,   6.2014],
        [ -8.7584,   6.6642],
        [ -9.0180,   7.5759],
        [ -8.4368,   5.3512],
        [ -9.8455,   6.3629],
        [ -8.5045,   6.0087],
        [ -8.8288,   7.4415],
        [ -8.9832,   7.3240],
        [  6.4613,  -9.6832],
        [ -8.7792,   7.2585],
        [ -8.1314,   6.7091],
        [  6.8434,  -9.4322],
        [  5.9322,  -7.9153],
        [  6.4967,  -8.0452],
        [ -7.4825,   4.2001],
        [ -8.4749,   6.2306],
        [ -8.9895,   7.2258],
        [ -7.9350,   6.0189],
        [ -7.1373,   5.3908],
        [ -8.3528,   6.8739],
        [ -9.0243,   6.0044],
        [  6.5648,  -8.7134],
        [  6.8175,  -8.3387],
        [ -8.5167,   6.9038],
        [ -8.7309,   6.7228],
        [  6.8788,  -8.2652],
        [  5.7189,  -7.1271],
        [  6.0427,  -8.3136],
        [ -8.5766,   6.2424],
        [  6.5398,  -7.9670],
        [ -7.9539,   6.3254],
        [  7.1743,  -8.7662],
        [ -8.4356,   6.8204],
        [ -8.5611,   6.8414],
        [ -8.8351,   6.3355],
        [ -6.9630,   3.9945],
        [ -8.6650,   6.7138],
        [ -8.6785,   7.2921],
        [  6.4920,  -7.9337],
        [  5.8858,  -8.1398],
        [ -6.8981,   4.7161],
        [ -9.0197,   6.8275],
        [ -8.9379,   5.7093],
        [  6.3567,  -8.7428],
        [  3.3400,  -4.9517],
        [  5.7646,  -7.3531],
        [ -7.8824,   6.4365],
        [ -3.0476,   1.1358],
        [  5.7046,  -7.1054],
        [ -9.0373,   7.1142],
        [  6.1256,  -8.3886],
        [  2.1742,  -3.5736],
        [  6.2647,  -8.6890],
        [ -8.4469,   6.4451],
        [  5.5869,  -7.2938],
        [ -8.6311,   6.6595],
        [  5.1863,  -6.8418],
        [  6.4919,  -9.1575],
        [ -7.9984,   4.8014],
        [ -8.5961,   7.0414],
        [ -8.6580,   7.2381],
        [ -7.3504,   5.2816],
        [ -8.8370,   7.3440],
        [  5.9350,  -8.1872]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7686, 0.2314],
        [0.2007, 0.7993]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4504, 0.5496], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2024, 0.0910],
         [0.4820, 0.3950]],

        [[0.6797, 0.0989],
         [0.3404, 0.3717]],

        [[0.4894, 0.0986],
         [0.2548, 0.9668]],

        [[0.0436, 0.0980],
         [0.4757, 0.9321]],

        [[0.6194, 0.1062],
         [0.2309, 0.4153]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919995362962712
Average Adjusted Rand Index: 0.9919993417272899
Iteration 0: Loss = -16127.631550067634
Iteration 10: Loss = -11739.615703734415
Iteration 20: Loss = -11739.615417791994
Iteration 30: Loss = -11739.615417792093
1
Iteration 40: Loss = -11739.615417792093
2
Iteration 50: Loss = -11739.615417792093
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7677, 0.2323],
        [0.2007, 0.7993]], dtype=torch.float64)
alpha: tensor([0.4513, 0.5487])
beta: tensor([[[0.1982, 0.0911],
         [0.4680, 0.3871]],

        [[0.9491, 0.0988],
         [0.5707, 0.6702]],

        [[0.8754, 0.0986],
         [0.1580, 0.3244]],

        [[0.9786, 0.0981],
         [0.0999, 0.9286]],

        [[0.4074, 0.1059],
         [0.3073, 0.5213]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919995362962712
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16127.366498910953
Iteration 100: Loss = -11947.27490651422
Iteration 200: Loss = -11747.367217900011
Iteration 300: Loss = -11740.503328937519
Iteration 400: Loss = -11739.673880721977
Iteration 500: Loss = -11739.27144976956
Iteration 600: Loss = -11739.031498982602
Iteration 700: Loss = -11738.875122398022
Iteration 800: Loss = -11738.768610382931
Iteration 900: Loss = -11738.693254658565
Iteration 1000: Loss = -11738.63600091351
Iteration 1100: Loss = -11738.589422140985
Iteration 1200: Loss = -11738.538155848186
Iteration 1300: Loss = -11738.36123027434
Iteration 1400: Loss = -11738.336755200473
Iteration 1500: Loss = -11738.317500023231
Iteration 1600: Loss = -11738.30162077758
Iteration 1700: Loss = -11738.288287846883
Iteration 1800: Loss = -11738.277063483145
Iteration 1900: Loss = -11738.267318686598
Iteration 2000: Loss = -11738.25895359483
Iteration 2100: Loss = -11738.251671036642
Iteration 2200: Loss = -11738.245239644073
Iteration 2300: Loss = -11738.239529318578
Iteration 2400: Loss = -11738.234551574173
Iteration 2500: Loss = -11738.230126507853
Iteration 2600: Loss = -11738.226021509086
Iteration 2700: Loss = -11738.222416302076
Iteration 2800: Loss = -11738.221197705618
Iteration 2900: Loss = -11738.216235236026
Iteration 3000: Loss = -11738.213579873429
Iteration 3100: Loss = -11738.213938856028
1
Iteration 3200: Loss = -11738.208941416984
Iteration 3300: Loss = -11738.206926454324
Iteration 3400: Loss = -11738.205971450352
Iteration 3500: Loss = -11738.203353458892
Iteration 3600: Loss = -11738.201811133798
Iteration 3700: Loss = -11738.20035864236
Iteration 3800: Loss = -11738.19910435273
Iteration 3900: Loss = -11738.197817015714
Iteration 4000: Loss = -11738.196699315946
Iteration 4100: Loss = -11738.196631000168
Iteration 4200: Loss = -11738.194683033997
Iteration 4300: Loss = -11738.193781054166
Iteration 4400: Loss = -11738.195102355216
1
Iteration 4500: Loss = -11738.19213505471
Iteration 4600: Loss = -11738.191406718173
Iteration 4700: Loss = -11738.19367910756
1
Iteration 4800: Loss = -11738.192040866748
2
Iteration 4900: Loss = -11738.18950136237
Iteration 5000: Loss = -11738.188974469538
Iteration 5100: Loss = -11738.188483151016
Iteration 5200: Loss = -11738.188408358732
Iteration 5300: Loss = -11738.197826949874
1
Iteration 5400: Loss = -11738.192634312196
2
Iteration 5500: Loss = -11738.186633920659
Iteration 5600: Loss = -11738.18861023585
1
Iteration 5700: Loss = -11738.18998775343
2
Iteration 5800: Loss = -11738.185556384096
Iteration 5900: Loss = -11738.197177829992
1
Iteration 6000: Loss = -11738.189885313934
2
Iteration 6100: Loss = -11738.184678060921
Iteration 6200: Loss = -11738.189549412267
1
Iteration 6300: Loss = -11738.184321239269
Iteration 6400: Loss = -11738.184406882297
1
Iteration 6500: Loss = -11738.183774070641
Iteration 6600: Loss = -11738.184368861705
1
Iteration 6700: Loss = -11738.183983620733
2
Iteration 6800: Loss = -11738.185724386922
3
Iteration 6900: Loss = -11738.183525313598
Iteration 7000: Loss = -11738.187703418216
1
Iteration 7100: Loss = -11738.182863508413
Iteration 7200: Loss = -11738.183002909129
1
Iteration 7300: Loss = -11738.189975902382
2
Iteration 7400: Loss = -11738.182901962553
3
Iteration 7500: Loss = -11738.194640283966
4
Iteration 7600: Loss = -11738.199915174704
5
Iteration 7700: Loss = -11738.182359119603
Iteration 7800: Loss = -11738.203555544169
1
Iteration 7900: Loss = -11738.18259378326
2
Iteration 8000: Loss = -11738.194669624747
3
Iteration 8100: Loss = -11738.190127142416
4
Iteration 8200: Loss = -11738.181647558691
Iteration 8300: Loss = -11738.180586115861
Iteration 8400: Loss = -11738.201102862462
1
Iteration 8500: Loss = -11738.180363822632
Iteration 8600: Loss = -11738.188151355906
1
Iteration 8700: Loss = -11738.180916422727
2
Iteration 8800: Loss = -11738.181771669388
3
Iteration 8900: Loss = -11738.201983033196
4
Iteration 9000: Loss = -11738.184679058108
5
Iteration 9100: Loss = -11738.183453008694
6
Iteration 9200: Loss = -11738.19880744679
7
Iteration 9300: Loss = -11738.17995097618
Iteration 9400: Loss = -11738.181424910801
1
Iteration 9500: Loss = -11738.181190043175
2
Iteration 9600: Loss = -11738.18082072677
3
Iteration 9700: Loss = -11738.285322723676
4
Iteration 9800: Loss = -11738.181191386677
5
Iteration 9900: Loss = -11738.18462337785
6
Iteration 10000: Loss = -11738.180098830297
7
Iteration 10100: Loss = -11738.18301643365
8
Iteration 10200: Loss = -11738.260279563954
9
Iteration 10300: Loss = -11738.290280101777
10
Stopping early at iteration 10300 due to no improvement.
tensor([[ -7.9629,   5.5836],
        [ -6.8149,   5.4174],
        [ -9.2079,   7.8216],
        [  1.1598,  -3.6193],
        [ -8.8315,   7.3469],
        [  6.2839,  -7.6945],
        [  5.9427,  -7.6647],
        [  5.6386,  -7.0935],
        [  6.1309,  -7.9977],
        [  5.9052,  -7.5463],
        [  6.1270,  -7.5363],
        [ -8.8726,   7.4417],
        [  6.0087,  -8.0610],
        [ -6.6663,   4.3137],
        [  5.9666,  -9.1866],
        [  6.4113,  -8.5885],
        [  5.5255,  -7.2431],
        [ -8.0319,   6.5180],
        [  3.4360,  -6.0599],
        [ -8.2859,   6.8916],
        [  6.8466,  -9.0922],
        [  5.7871,  -7.8053],
        [ -9.3890,   7.9723],
        [  6.5719,  -8.2911],
        [  5.7456,  -8.3333],
        [  6.7611,  -9.3443],
        [  5.2889,  -7.4162],
        [ -9.6776,   5.8793],
        [ -9.2054,   7.8081],
        [ -8.1883,   6.7414],
        [ -9.0182,   7.3407],
        [ -6.4659,   3.5298],
        [  7.6059,  -9.1480],
        [  6.3127,  -8.3851],
        [  6.4849,  -7.8854],
        [ -9.0073,   7.4943],
        [ -8.6701,   6.1236],
        [ -9.7100,   6.8706],
        [ -9.7640,   7.3597],
        [ -7.9448,   6.4020],
        [ -8.6601,   7.2328],
        [ -7.7497,   6.2178],
        [ -9.2116,   7.7879],
        [ -9.4037,   7.9345],
        [  6.2990,  -9.3724],
        [ -9.4917,   8.1053],
        [ -7.5321,   6.1038],
        [  7.1445,  -8.5833],
        [  6.4397,  -8.1363],
        [  6.3265,  -8.7295],
        [ -9.0081,   6.6793],
        [ -8.3681,   6.9145],
        [ -8.4546,   7.0680],
        [ -8.2164,   5.7858],
        [ -8.0602,   4.8789],
        [ -8.1553,   6.7498],
        [ -7.9101,   6.5238],
        [  6.4247,  -7.8506],
        [  6.8856,  -9.3091],
        [ -8.2405,   6.8407],
        [-10.0077,   5.3925],
        [  5.9013,  -8.3623],
        [  5.0563,  -7.9130],
        [  6.1395,  -7.6603],
        [ -7.8227,   6.4042],
        [  5.9660,  -7.3650],
        [ -7.8014,   6.3856],
        [  5.4192,  -9.7941],
        [ -7.5650,   6.1727],
        [ -8.7546,   7.1816],
        [ -8.4853,   6.9893],
        [ -6.1965,   4.7653],
        [ -8.3163,   6.2226],
        [ -9.3266,   7.8801],
        [  6.2200,  -7.6996],
        [  7.0134,  -8.5107],
        [ -6.7078,   4.8876],
        [ -9.4487,   7.5168],
        [ -7.8317,   6.3016],
        [  5.7633,  -8.1036],
        [  2.3780,  -5.9123],
        [  5.5328,  -7.0629],
        [ -7.4810,   5.8979],
        [ -3.0763,   1.1145],
        [  5.4495,  -7.0796],
        [-10.9713,   8.1495],
        [  6.0244,  -7.8311],
        [  2.1543,  -3.5939],
        [  5.4385,  -8.4501],
        [ -8.0408,   6.6413],
        [  5.6185,  -7.0504],
        [ -8.7162,   7.3109],
        [  5.0375,  -6.8589],
        [  6.0882,  -8.3076],
        [ -7.5894,   5.2303],
        [ -8.3438,   6.9526],
        [ -8.8448,   7.0528],
        [ -7.4460,   6.0530],
        [-10.3673,   8.4583],
        [  6.0239,  -7.4729]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7717, 0.2283],
        [0.2017, 0.7983]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4543, 0.5457], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2016, 0.0910],
         [0.4680, 0.3965]],

        [[0.9491, 0.0992],
         [0.5707, 0.6702]],

        [[0.8754, 0.0985],
         [0.1580, 0.3244]],

        [[0.9786, 0.0980],
         [0.0999, 0.9286]],

        [[0.4074, 0.1068],
         [0.3073, 0.5213]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919995362962712
Average Adjusted Rand Index: 0.9919993417272899
Iteration 0: Loss = -44644.068340071186
Iteration 10: Loss = -12658.759978911405
Iteration 20: Loss = -12328.199136333893
Iteration 30: Loss = -11739.615760539415
Iteration 40: Loss = -11739.61542228003
Iteration 50: Loss = -11739.61542228003
1
Iteration 60: Loss = -11739.61542228003
2
Iteration 70: Loss = -11739.61542228003
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7993, 0.2007],
        [0.2323, 0.7677]], dtype=torch.float64)
alpha: tensor([0.5487, 0.4513])
beta: tensor([[[0.3871, 0.0911],
         [0.9401, 0.1982]],

        [[0.6967, 0.0988],
         [0.7358, 0.2357]],

        [[0.5010, 0.0986],
         [0.4046, 0.8394]],

        [[0.7292, 0.0981],
         [0.0080, 0.5624]],

        [[0.6937, 0.1059],
         [0.3166, 0.0837]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919995362962712
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44643.3777566178
Iteration 100: Loss = -12696.84764927388
Iteration 200: Loss = -12673.496057114551
Iteration 300: Loss = -12668.905339480996
Iteration 400: Loss = -12666.936786748125
Iteration 500: Loss = -12660.994551639169
Iteration 600: Loss = -12653.772374527618
Iteration 700: Loss = -12640.580503105257
Iteration 800: Loss = -12589.003150643028
Iteration 900: Loss = -12545.180963289644
Iteration 1000: Loss = -12498.991202985044
Iteration 1100: Loss = -12445.195188153133
Iteration 1200: Loss = -12337.50191909355
Iteration 1300: Loss = -12270.191929400204
Iteration 1400: Loss = -12234.275552587125
Iteration 1500: Loss = -12217.749170911828
Iteration 1600: Loss = -12208.853911878843
Iteration 1700: Loss = -12170.794449840829
Iteration 1800: Loss = -12151.114714065732
Iteration 1900: Loss = -12139.487700024305
Iteration 2000: Loss = -12123.282155076402
Iteration 2100: Loss = -12120.353403759744
Iteration 2200: Loss = -12120.253031299573
Iteration 2300: Loss = -12118.205259963323
Iteration 2400: Loss = -12105.238825109955
Iteration 2500: Loss = -12094.315414997813
Iteration 2600: Loss = -12089.928741352003
Iteration 2700: Loss = -12088.584106033626
Iteration 2800: Loss = -12075.966452418052
Iteration 2900: Loss = -12073.906164004638
Iteration 3000: Loss = -12061.679867560191
Iteration 3100: Loss = -12055.016511680724
Iteration 3200: Loss = -12050.635821900312
Iteration 3300: Loss = -12050.539782243586
Iteration 3400: Loss = -12050.516781483384
Iteration 3500: Loss = -12050.5070906526
Iteration 3600: Loss = -12050.487819312755
Iteration 3700: Loss = -12050.476561372285
Iteration 3800: Loss = -12050.466661993762
Iteration 3900: Loss = -12050.45781358939
Iteration 4000: Loss = -12050.449982015574
Iteration 4100: Loss = -12050.443059918549
Iteration 4200: Loss = -12050.436316349242
Iteration 4300: Loss = -12050.430325873578
Iteration 4400: Loss = -12050.424737855406
Iteration 4500: Loss = -12050.422938026015
Iteration 4600: Loss = -12050.414457615787
Iteration 4700: Loss = -12050.410008371575
Iteration 4800: Loss = -12050.405938865402
Iteration 4900: Loss = -12050.403748449926
Iteration 5000: Loss = -12050.398092375264
Iteration 5100: Loss = -12050.393937283072
Iteration 5200: Loss = -12050.4227269663
1
Iteration 5300: Loss = -12050.384990757559
Iteration 5400: Loss = -12050.381716249558
Iteration 5500: Loss = -12050.378267396289
Iteration 5600: Loss = -12050.37611278919
Iteration 5700: Loss = -12050.373949235029
Iteration 5800: Loss = -12050.37119795118
Iteration 5900: Loss = -12049.326509223241
Iteration 6000: Loss = -12038.483769894812
Iteration 6100: Loss = -12038.480884508097
Iteration 6200: Loss = -12038.478551885204
Iteration 6300: Loss = -12038.475533393525
Iteration 6400: Loss = -12037.997727646978
Iteration 6500: Loss = -12029.749179888784
Iteration 6600: Loss = -12029.740374182626
Iteration 6700: Loss = -12027.451445045474
Iteration 6800: Loss = -12027.443218443013
Iteration 6900: Loss = -12017.473021508
Iteration 7000: Loss = -12008.263600758351
Iteration 7100: Loss = -12008.26065624143
Iteration 7200: Loss = -12008.255993510862
Iteration 7300: Loss = -12004.751298258689
Iteration 7400: Loss = -12004.748253787628
Iteration 7500: Loss = -12004.746309808947
Iteration 7600: Loss = -12004.744302903953
Iteration 7700: Loss = -12004.743075448594
Iteration 7800: Loss = -12004.74439441355
1
Iteration 7900: Loss = -12004.738625222788
Iteration 8000: Loss = -12004.728193366052
Iteration 8100: Loss = -12004.727363189184
Iteration 8200: Loss = -12004.740235406925
1
Iteration 8300: Loss = -11992.142624994518
Iteration 8400: Loss = -11992.140661989095
Iteration 8500: Loss = -11992.147187207749
1
Iteration 8600: Loss = -11992.14496278419
2
Iteration 8700: Loss = -11992.1391805579
Iteration 8800: Loss = -11992.139316424735
1
Iteration 8900: Loss = -11992.13842618546
Iteration 9000: Loss = -11992.143472472315
1
Iteration 9100: Loss = -11992.144738950563
2
Iteration 9200: Loss = -11984.915580865638
Iteration 9300: Loss = -11984.914638971528
Iteration 9400: Loss = -11984.91337083636
Iteration 9500: Loss = -11970.47406912031
Iteration 9600: Loss = -11970.652619714747
1
Iteration 9700: Loss = -11970.471832684934
Iteration 9800: Loss = -11970.48325000708
1
Iteration 9900: Loss = -11962.019553005992
Iteration 10000: Loss = -11962.019321892903
Iteration 10100: Loss = -11962.028352707854
1
Iteration 10200: Loss = -11962.023408892777
2
Iteration 10300: Loss = -11962.043743146136
3
Iteration 10400: Loss = -11962.018183025966
Iteration 10500: Loss = -11962.017837132735
Iteration 10600: Loss = -11962.082841428853
1
Iteration 10700: Loss = -11962.017513518358
Iteration 10800: Loss = -11962.021487567614
1
Iteration 10900: Loss = -11962.021502194702
2
Iteration 11000: Loss = -11962.018298824505
3
Iteration 11100: Loss = -11962.017594979166
4
Iteration 11200: Loss = -11962.017371257654
Iteration 11300: Loss = -11962.022697185444
1
Iteration 11400: Loss = -11962.036729261172
2
Iteration 11500: Loss = -11962.006182828522
Iteration 11600: Loss = -11962.002103648927
Iteration 11700: Loss = -11962.025621019924
1
Iteration 11800: Loss = -11961.99763141026
Iteration 11900: Loss = -11961.993090647815
Iteration 12000: Loss = -11961.995631908605
1
Iteration 12100: Loss = -11949.742494048514
Iteration 12200: Loss = -11949.725892778824
Iteration 12300: Loss = -11949.492619209639
Iteration 12400: Loss = -11942.588024386265
Iteration 12500: Loss = -11942.59259726393
1
Iteration 12600: Loss = -11942.58754712879
Iteration 12700: Loss = -11942.63302525177
1
Iteration 12800: Loss = -11942.590223684972
2
Iteration 12900: Loss = -11942.59909524326
3
Iteration 13000: Loss = -11942.588792511224
4
Iteration 13100: Loss = -11942.611405756907
5
Iteration 13200: Loss = -11942.604003229479
6
Iteration 13300: Loss = -11942.58935658024
7
Iteration 13400: Loss = -11942.627418501326
8
Iteration 13500: Loss = -11942.587134953808
Iteration 13600: Loss = -11942.586997574404
Iteration 13700: Loss = -11942.588049616235
1
Iteration 13800: Loss = -11942.580211297716
Iteration 13900: Loss = -11942.585151302843
1
Iteration 14000: Loss = -11942.579705470454
Iteration 14100: Loss = -11942.584646644322
1
Iteration 14200: Loss = -11942.596017525884
2
Iteration 14300: Loss = -11942.583242719744
3
Iteration 14400: Loss = -11942.580326624307
4
Iteration 14500: Loss = -11942.57914015114
Iteration 14600: Loss = -11942.581484929846
1
Iteration 14700: Loss = -11942.738868961436
2
Iteration 14800: Loss = -11942.57877048228
Iteration 14900: Loss = -11942.598521387645
1
Iteration 15000: Loss = -11942.585767866063
2
Iteration 15100: Loss = -11942.5794881766
3
Iteration 15200: Loss = -11942.578751379557
Iteration 15300: Loss = -11942.585154238679
1
Iteration 15400: Loss = -11927.353013990249
Iteration 15500: Loss = -11927.199096595814
Iteration 15600: Loss = -11927.198981198884
Iteration 15700: Loss = -11927.198698356788
Iteration 15800: Loss = -11927.208085868595
1
Iteration 15900: Loss = -11927.326997819582
2
Iteration 16000: Loss = -11927.198189802113
Iteration 16100: Loss = -11927.200579577226
1
Iteration 16200: Loss = -11927.210417104836
2
Iteration 16300: Loss = -11927.199044010116
3
Iteration 16400: Loss = -11927.204397373129
4
Iteration 16500: Loss = -11927.229209709823
5
Iteration 16600: Loss = -11927.238339462565
6
Iteration 16700: Loss = -11927.221136187914
7
Iteration 16800: Loss = -11927.198382657147
8
Iteration 16900: Loss = -11927.30198411397
9
Iteration 17000: Loss = -11927.201368209988
10
Stopping early at iteration 17000 due to no improvement.
tensor([[  5.3250,  -9.9402],
        [  7.1618,  -8.8056],
        [  7.7322,  -9.5796],
        [ -3.4197,   1.3442],
        [  7.9557,  -9.3455],
        [ -9.9049,   7.1946],
        [ -8.8614,   5.9691],
        [ -9.2467,   7.6111],
        [ -8.8532,   7.1771],
        [ -7.8721,   6.4785],
        [ -8.6074,   5.5174],
        [  7.8020, -10.0747],
        [ -8.1951,   6.6103],
        [  5.2515,  -6.9957],
        [ -8.0726,   6.4841],
        [ -8.7093,   7.3216],
        [ -7.7422,   6.2170],
        [  8.3615, -10.9883],
        [ -9.8865,   7.3030],
        [  8.4925,  -9.9844],
        [ -8.5051,   7.0752],
        [ -7.7636,   6.3773],
        [  7.9646,  -9.3539],
        [ -8.7530,   7.3404],
        [ -9.3616,   6.7194],
        [ -9.3520,   7.4853],
        [ -9.4639,   7.5556],
        [  8.7059, -10.2732],
        [  7.4286,  -9.3001],
        [  7.1627,  -8.9080],
        [  7.7854, -10.3251],
        [  5.0171,  -6.4038],
        [ -9.2855,   7.8947],
        [ -8.2167,   5.9832],
        [ -8.4655,   7.0605],
        [  8.7215, -10.8913],
        [  8.3272,  -9.7833],
        [  8.1711,  -9.6378],
        [  8.3818, -10.0131],
        [  8.4940,  -9.8968],
        [  7.9846,  -9.3731],
        [  7.3415,  -8.7301],
        [  6.7739, -11.1346],
        [  8.1712,  -9.6022],
        [ -8.7105,   7.2080],
        [  8.2797, -10.3518],
        [  7.3954,  -9.4200],
        [ -8.8846,   7.4842],
        [ -8.2331,   6.3575],
        [ -9.4703,   6.4384],
        [  8.3439, -11.3170],
        [  7.7952,  -9.2189],
        [  8.1148,  -9.5546],
        [  6.6947,  -8.4886],
        [  6.4015,  -7.8144],
        [  7.3009, -10.1165],
        [  7.8532, -10.5080],
        [ -8.9721,   7.4159],
        [ -9.0772,   6.9183],
        [  7.9061,  -9.3629],
        [  7.1343,  -8.7672],
        [-11.2484,   6.6332],
        [ -6.9760,   5.5858],
        [ -8.2698,   6.7284],
        [  7.9909,  -9.4452],
        [ -7.9236,   6.4671],
        [  7.8497,  -9.2434],
        [ -9.4026,   7.5274],
        [  7.3618,  -8.7745],
        [  8.4226, -10.9466],
        [  8.0927,  -9.5464],
        [  4.9521,  -7.5148],
        [  7.4015,  -8.8513],
        [  8.2875, -10.6937],
        [ -8.2612,   6.8149],
        [ -9.7960,   8.0214],
        [  7.7807,  -9.4793],
        [  7.9375,  -9.5759],
        [  7.9917,  -9.5220],
        [ -8.4210,   6.8518],
        [ -4.2062,   2.5381],
        [-10.6973,   6.0821],
        [  7.7318, -12.3471],
        [  7.0075,  -9.2454],
        [ -6.8358,   5.0354],
        [  7.7884,  -9.1784],
        [ -9.9188,   7.9359],
        [ -9.5537,   8.0508],
        [ -9.3461,   7.8994],
        [  8.3461, -11.3453],
        [ -6.6656,   5.1554],
        [  6.8709,  -8.9808],
        [ -6.3766,   4.3790],
        [ -9.0182,   7.2201],
        [  6.8340, -11.1970],
        [  8.2552, -10.2641],
        [  8.1960,  -9.5954],
        [  4.8777,  -6.2810],
        [  8.0136,  -9.4108],
        [ -8.8513,   5.7234]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6130, 0.3870],
        [0.3252, 0.6748]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5499, 0.4501], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3931, 0.0909],
         [0.9401, 0.2239]],

        [[0.6967, 0.1029],
         [0.7358, 0.2357]],

        [[0.5010, 0.1095],
         [0.4046, 0.8394]],

        [[0.7292, 0.0985],
         [0.0080, 0.5624]],

        [[0.6937, 0.1022],
         [0.3166, 0.0837]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 3
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 80
Adjusted Rand Index: 0.354551002206262
Global Adjusted Rand Index: 0.4451228568732583
Average Adjusted Rand Index: 0.8470712889023412
Iteration 0: Loss = -23601.510251448384
Iteration 10: Loss = -12387.366639705526
Iteration 20: Loss = -11739.615428246545
Iteration 30: Loss = -11739.615417792093
Iteration 40: Loss = -11739.615417792093
1
Iteration 50: Loss = -11739.615417792093
2
Iteration 60: Loss = -11739.615417792093
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7677, 0.2323],
        [0.2007, 0.7993]], dtype=torch.float64)
alpha: tensor([0.4513, 0.5487])
beta: tensor([[[0.1982, 0.0911],
         [0.1947, 0.3871]],

        [[0.8388, 0.0988],
         [0.8968, 0.5845]],

        [[0.2285, 0.0986],
         [0.7826, 0.2959]],

        [[0.8857, 0.0981],
         [0.4530, 0.5326]],

        [[0.0213, 0.1059],
         [0.9187, 0.7081]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919995362962712
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23408.98485762157
Iteration 100: Loss = -12597.052474683971
Iteration 200: Loss = -12210.948643518323
Iteration 300: Loss = -12117.568318632415
Iteration 400: Loss = -12046.46232656572
Iteration 500: Loss = -11973.163108857168
Iteration 600: Loss = -11956.150448693686
Iteration 700: Loss = -11942.874302516337
Iteration 800: Loss = -11942.51413912586
Iteration 900: Loss = -11942.301723825898
Iteration 1000: Loss = -11942.155055221541
Iteration 1100: Loss = -11942.0475926584
Iteration 1200: Loss = -11941.965786414372
Iteration 1300: Loss = -11941.902455161817
Iteration 1400: Loss = -11941.852498102891
Iteration 1500: Loss = -11941.812091099975
Iteration 1600: Loss = -11941.778729418236
Iteration 1700: Loss = -11941.750738431483
Iteration 1800: Loss = -11941.727002143341
Iteration 1900: Loss = -11941.706587331799
Iteration 2000: Loss = -11941.688917978532
Iteration 2100: Loss = -11941.673253231185
Iteration 2200: Loss = -11941.659086004802
Iteration 2300: Loss = -11941.644492157622
Iteration 2400: Loss = -11941.581838314856
Iteration 2500: Loss = -11941.489591822896
Iteration 2600: Loss = -11941.480296203876
Iteration 2700: Loss = -11941.472519926005
Iteration 2800: Loss = -11941.465684026998
Iteration 2900: Loss = -11941.459495934449
Iteration 3000: Loss = -11941.453890415756
Iteration 3100: Loss = -11941.448714282247
Iteration 3200: Loss = -11941.44407447601
Iteration 3300: Loss = -11941.43957369816
Iteration 3400: Loss = -11941.43543420993
Iteration 3500: Loss = -11941.431519751313
Iteration 3600: Loss = -11941.427258165077
Iteration 3700: Loss = -11941.421407969057
Iteration 3800: Loss = -11941.396384634967
Iteration 3900: Loss = -11941.380140779542
Iteration 4000: Loss = -11941.377226351276
Iteration 4100: Loss = -11941.374819809858
Iteration 4200: Loss = -11941.372665241894
Iteration 4300: Loss = -11941.37073534504
Iteration 4400: Loss = -11941.36897328812
Iteration 4500: Loss = -11941.367437417382
Iteration 4600: Loss = -11941.365818116485
Iteration 4700: Loss = -11941.36439208488
Iteration 4800: Loss = -11941.364948008888
1
Iteration 4900: Loss = -11941.361928040855
Iteration 5000: Loss = -11941.360743779927
Iteration 5100: Loss = -11941.361721337396
1
Iteration 5200: Loss = -11941.35872676766
Iteration 5300: Loss = -11941.35775885723
Iteration 5400: Loss = -11941.356862289018
Iteration 5500: Loss = -11941.356232236243
Iteration 5600: Loss = -11941.354977492134
Iteration 5700: Loss = -11941.350129819739
Iteration 5800: Loss = -11939.3651946367
Iteration 5900: Loss = -11939.366261031299
1
Iteration 6000: Loss = -11939.360971803233
Iteration 6100: Loss = -11939.360255954074
Iteration 6200: Loss = -11939.359573733198
Iteration 6300: Loss = -11939.372196046552
1
Iteration 6400: Loss = -11939.358579978649
Iteration 6500: Loss = -11939.358000056427
Iteration 6600: Loss = -11939.377115140182
1
Iteration 6700: Loss = -11939.357139953541
Iteration 6800: Loss = -11939.35666413836
Iteration 6900: Loss = -11939.357941681335
1
Iteration 7000: Loss = -11939.355445799827
Iteration 7100: Loss = -11939.323834745355
Iteration 7200: Loss = -11939.326404708268
1
Iteration 7300: Loss = -11939.323022676637
Iteration 7400: Loss = -11939.323465560516
1
Iteration 7500: Loss = -11939.322580784603
Iteration 7600: Loss = -11939.325750796648
1
Iteration 7700: Loss = -11939.344111684437
2
Iteration 7800: Loss = -11939.321729191168
Iteration 7900: Loss = -11939.322721907025
1
Iteration 8000: Loss = -11939.359226940607
2
Iteration 8100: Loss = -11939.321873119452
3
Iteration 8200: Loss = -11939.321296109292
Iteration 8300: Loss = -11939.32969899297
1
Iteration 8400: Loss = -11939.320708451369
Iteration 8500: Loss = -11939.320797900216
1
Iteration 8600: Loss = -11939.32397186667
2
Iteration 8700: Loss = -11939.320400345885
Iteration 8800: Loss = -11939.322977958986
1
Iteration 8900: Loss = -11939.328011490008
2
Iteration 9000: Loss = -11939.32612624358
3
Iteration 9100: Loss = -11939.319942576754
Iteration 9200: Loss = -11939.325766486345
1
Iteration 9300: Loss = -11939.321736548858
2
Iteration 9400: Loss = -11939.328730539271
3
Iteration 9500: Loss = -11939.324820738311
4
Iteration 9600: Loss = -11939.319776259585
Iteration 9700: Loss = -11939.335749950778
1
Iteration 9800: Loss = -11939.320776723693
2
Iteration 9900: Loss = -11939.337757243256
3
Iteration 10000: Loss = -11939.32278187676
4
Iteration 10100: Loss = -11939.319260293134
Iteration 10200: Loss = -11939.334550262452
1
Iteration 10300: Loss = -11939.324209052216
2
Iteration 10400: Loss = -11939.32210426131
3
Iteration 10500: Loss = -11939.323881122704
4
Iteration 10600: Loss = -11939.321954847554
5
Iteration 10700: Loss = -11939.319202334596
Iteration 10800: Loss = -11939.323158794705
1
Iteration 10900: Loss = -11939.327655930952
2
Iteration 11000: Loss = -11939.319575082422
3
Iteration 11100: Loss = -11939.322418271739
4
Iteration 11200: Loss = -11939.318830088647
Iteration 11300: Loss = -11939.318868437484
1
Iteration 11400: Loss = -11939.328035597602
2
Iteration 11500: Loss = -11939.320934876514
3
Iteration 11600: Loss = -11939.356139112257
4
Iteration 11700: Loss = -11939.319580223575
5
Iteration 11800: Loss = -11939.332612390246
6
Iteration 11900: Loss = -11939.3186606475
Iteration 12000: Loss = -11939.321091148222
1
Iteration 12100: Loss = -11939.32157669419
2
Iteration 12200: Loss = -11939.357965945339
3
Iteration 12300: Loss = -11939.324465167627
4
Iteration 12400: Loss = -11939.324050559922
5
Iteration 12500: Loss = -11939.319657750628
6
Iteration 12600: Loss = -11939.318664891092
7
Iteration 12700: Loss = -11939.322338720647
8
Iteration 12800: Loss = -11939.325864389817
9
Iteration 12900: Loss = -11939.324373265616
10
Stopping early at iteration 12900 due to no improvement.
tensor([[ -7.6330,   6.1520],
        [ -8.0183,   4.2301],
        [ -9.6193,   7.8720],
        [  1.4462,  -3.9331],
        [ -8.6568,   7.2667],
        [  6.4272,  -7.9997],
        [  6.1420,  -7.6805],
        [  6.0249,  -7.4563],
        [  6.9704,  -8.3612],
        [  6.3799,  -8.2715],
        [  6.7941,  -8.1954],
        [ -9.9481,   8.4769],
        [  6.5386,  -8.1308],
        [ -7.1479,   5.6273],
        [  6.0537,  -7.7300],
        [  6.7129,  -8.1105],
        [  6.6535,  -8.3600],
        [ -8.3989,   6.2567],
        [  4.7132,  -6.1156],
        [ -8.8920,   7.4637],
        [  6.6681,  -8.2806],
        [  6.2478,  -8.1085],
        [-10.3208,   8.4843],
        [  7.2422,  -8.6298],
        [  6.8472,  -9.1041],
        [  6.2512,  -9.4125],
        [  6.0858,  -8.3680],
        [ -7.7236,   6.2716],
        [ -8.8210,   7.4186],
        [ -9.6869,   7.8294],
        [ -8.6807,   7.2528],
        [ -6.7426,   5.1230],
        [  7.4254,  -8.8460],
        [  6.2918,  -8.3557],
        [  6.5233,  -7.9451],
        [ -9.2636,   7.4288],
        [ -7.9177,   6.5273],
        [ -8.1728,   6.6918],
        [ -8.8593,   7.4692],
        [ -8.2957,   6.8618],
        [ -9.4186,   7.0867],
        [ -8.3092,   6.9122],
        [ -8.8041,   7.1887],
        [ -9.2207,   7.8081],
        [  6.5578,  -8.8913],
        [ -9.3501,   7.9454],
        [ -7.7982,   6.3920],
        [  7.1202,  -8.8309],
        [  6.5418,  -7.9287],
        [  6.3654,  -8.7742],
        [ -6.7098,   4.1637],
        [-10.3393,   5.7240],
        [ -9.2564,   7.8190],
        [ -8.7781,   5.9264],
        [ -7.8964,   6.5074],
        [ -8.9468,   7.0173],
        [ -8.5372,   7.1509],
        [  6.7041,  -8.1501],
        [  6.9392,  -8.6414],
        [ -9.3913,   6.5182],
        [ -8.5001,   6.6603],
        [  6.7963,  -8.4799],
        [  5.3223,  -6.7610],
        [  6.5771,  -9.7366],
        [ -8.9219,   6.6911],
        [  7.2501,  -9.2164],
        [ -8.5298,   6.6609],
        [  7.3058,  -9.9036],
        [ -8.4599,   6.9054],
        [ -8.5948,   7.0280],
        [ -8.5026,   6.7663],
        [ -7.2900,   5.7399],
        [ -8.2390,   6.7237],
        [ -9.9713,   7.1066],
        [  6.2541,  -7.6456],
        [  6.4400,  -9.1447],
        [ -6.2711,   4.7684],
        [ -8.9861,   7.4090],
        [ -8.0565,   6.6702],
        [  6.6942,  -8.1061],
        [  2.3083,  -3.7032],
        [  5.4162,  -9.2512],
        [ -7.7379,   6.1111],
        [ -2.7027,   0.7019],
        [  6.0906,  -9.6149],
        [ -9.1603,   7.6715],
        [  6.4779,  -7.8660],
        [  2.5935,  -4.4824],
        [  6.5285,  -7.9204],
        [ -8.2981,   6.8569],
        [  4.8638,  -6.3813],
        [ -9.5761,   7.4330],
        [  4.3494,  -5.8290],
        [  7.3111,  -8.7962],
        [ -7.1993,   5.7200],
        [ -8.7304,   7.3440],
        [ -9.9940,   7.4570],
        [ -6.5486,   5.1615],
        [ -9.2537,   7.7430],
        [  5.6768,  -8.3114]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5409, 0.4591],
        [0.4981, 0.5019]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4501, 0.5499], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2242, 0.0910],
         [0.1947, 0.3924]],

        [[0.8388, 0.0919],
         [0.8968, 0.5845]],

        [[0.2285, 0.1098],
         [0.7826, 0.2959]],

        [[0.8857, 0.0981],
         [0.4530, 0.5326]],

        [[0.0213, 0.1052],
         [0.9187, 0.7081]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 23
Adjusted Rand Index: 0.2859858582610519
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.4558962364399039
Average Adjusted Rand Index: 0.825519519615806
11746.239318135997
new:  [0.9919995362962712, 0.9919995362962712, 0.4451228568732583, 0.4558962364399039] [0.9919993417272899, 0.9919993417272899, 0.8470712889023412, 0.825519519615806] [11738.206797515719, 11738.290280101777, 11927.201368209988, 11939.324373265616]
prior:  [0.9919995362962712, 0.9919995362962712, 0.9919995362962712, 0.9919995362962712] [0.9919993417272899, 0.9919993417272899, 0.9919993417272899, 0.9919993417272899] [11739.615417792093, 11739.615417792093, 11739.61542228003, 11739.615417792093]
-----------------------------------------------------------------------------------------
This iteration is 23
True Objective function: Loss = -11450.026058082021
Iteration 0: Loss = -41877.828986397675
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6893,    nan]],

        [[0.0368,    nan],
         [0.2193, 0.9365]],

        [[0.8336,    nan],
         [0.1346, 0.2440]],

        [[0.2419,    nan],
         [0.8217, 0.9697]],

        [[0.3665,    nan],
         [0.4765, 0.5763]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40615.533875741196
Iteration 100: Loss = -12198.026387507742
Iteration 200: Loss = -12195.356902149131
Iteration 300: Loss = -12194.316737149735
Iteration 400: Loss = -12193.867563359652
Iteration 500: Loss = -12193.48383171797
Iteration 600: Loss = -12190.35273926145
Iteration 700: Loss = -12094.384461271155
Iteration 800: Loss = -11899.129121165248
Iteration 900: Loss = -11799.187446026204
Iteration 1000: Loss = -11797.895627799297
Iteration 1100: Loss = -11782.06264070656
Iteration 1200: Loss = -11781.540804233877
Iteration 1300: Loss = -11774.521535064498
Iteration 1400: Loss = -11774.170632371086
Iteration 1500: Loss = -11769.509600273586
Iteration 1600: Loss = -11769.297061617362
Iteration 1700: Loss = -11769.115694613
Iteration 1800: Loss = -11769.014095412193
Iteration 1900: Loss = -11768.926797820082
Iteration 2000: Loss = -11762.202289361669
Iteration 2100: Loss = -11762.059893785223
Iteration 2200: Loss = -11761.987570796846
Iteration 2300: Loss = -11761.535725056981
Iteration 2400: Loss = -11761.264235191338
Iteration 2500: Loss = -11760.302542379406
Iteration 2600: Loss = -11754.402821366033
Iteration 2700: Loss = -11751.055519727553
Iteration 2800: Loss = -11744.438678451057
Iteration 2900: Loss = -11741.091178675722
Iteration 3000: Loss = -11740.469265971702
Iteration 3100: Loss = -11740.018819271301
Iteration 3200: Loss = -11739.860597222423
Iteration 3300: Loss = -11737.256511479136
Iteration 3400: Loss = -11734.428505833686
Iteration 3500: Loss = -11733.277792026902
Iteration 3600: Loss = -11724.9166559455
Iteration 3700: Loss = -11721.810004304372
Iteration 3800: Loss = -11720.128460091833
Iteration 3900: Loss = -11720.064063633914
Iteration 4000: Loss = -11719.879694489906
Iteration 4100: Loss = -11719.793761582172
Iteration 4200: Loss = -11719.782919018366
Iteration 4300: Loss = -11719.761160165697
Iteration 4400: Loss = -11719.750674189892
Iteration 4500: Loss = -11719.743359132282
Iteration 4600: Loss = -11719.730854518963
Iteration 4700: Loss = -11719.716155183281
Iteration 4800: Loss = -11719.499098452567
Iteration 4900: Loss = -11719.31003381548
Iteration 5000: Loss = -11719.185139400819
Iteration 5100: Loss = -11718.368835885753
Iteration 5200: Loss = -11718.220920647713
Iteration 5300: Loss = -11718.122427052123
Iteration 5400: Loss = -11717.89687041048
Iteration 5500: Loss = -11717.890480249333
Iteration 5600: Loss = -11717.687845495328
Iteration 5700: Loss = -11717.534418287565
Iteration 5800: Loss = -11717.531132213284
Iteration 5900: Loss = -11717.528039487604
Iteration 6000: Loss = -11717.52532405287
Iteration 6100: Loss = -11717.522910460168
Iteration 6200: Loss = -11717.52954345015
1
Iteration 6300: Loss = -11717.518242690589
Iteration 6400: Loss = -11717.520168583722
1
Iteration 6500: Loss = -11717.51190414153
Iteration 6600: Loss = -11717.505547704091
Iteration 6700: Loss = -11717.49662980543
Iteration 6800: Loss = -11717.490252446876
Iteration 6900: Loss = -11717.468690852838
Iteration 7000: Loss = -11717.459693647217
Iteration 7100: Loss = -11717.581766921987
1
Iteration 7200: Loss = -11717.457579701624
Iteration 7300: Loss = -11717.456885973414
Iteration 7400: Loss = -11717.457524994272
1
Iteration 7500: Loss = -11717.46409765404
2
Iteration 7600: Loss = -11717.455629074992
Iteration 7700: Loss = -11717.45427666975
Iteration 7800: Loss = -11717.460458248977
1
Iteration 7900: Loss = -11717.45276893869
Iteration 8000: Loss = -11717.454924317073
1
Iteration 8100: Loss = -11717.437937856943
Iteration 8200: Loss = -11716.322187830861
Iteration 8300: Loss = -11716.314960801246
Iteration 8400: Loss = -11716.304352500078
Iteration 8500: Loss = -11716.303846689805
Iteration 8600: Loss = -11716.332878309997
1
Iteration 8700: Loss = -11716.301146679525
Iteration 8800: Loss = -11716.300521446406
Iteration 8900: Loss = -11716.298931766001
Iteration 9000: Loss = -11716.31947852339
1
Iteration 9100: Loss = -11716.308118814253
2
Iteration 9200: Loss = -11716.297116117392
Iteration 9300: Loss = -11716.29678946189
Iteration 9400: Loss = -11716.297685737694
1
Iteration 9500: Loss = -11716.297332051028
2
Iteration 9600: Loss = -11716.296026267877
Iteration 9700: Loss = -11716.29623227282
1
Iteration 9800: Loss = -11716.320875086341
2
Iteration 9900: Loss = -11716.295814648312
Iteration 10000: Loss = -11716.295240676202
Iteration 10100: Loss = -11716.300506093565
1
Iteration 10200: Loss = -11716.30812037125
2
Iteration 10300: Loss = -11716.297995739451
3
Iteration 10400: Loss = -11716.294784557851
Iteration 10500: Loss = -11716.296066555935
1
Iteration 10600: Loss = -11716.295691639385
2
Iteration 10700: Loss = -11716.294537719483
Iteration 10800: Loss = -11716.294739098461
1
Iteration 10900: Loss = -11716.295547720116
2
Iteration 11000: Loss = -11716.298226174607
3
Iteration 11100: Loss = -11716.297527923796
4
Iteration 11200: Loss = -11716.298283953724
5
Iteration 11300: Loss = -11716.29408579094
Iteration 11400: Loss = -11716.294322109277
1
Iteration 11500: Loss = -11716.320981998502
2
Iteration 11600: Loss = -11716.309654199631
3
Iteration 11700: Loss = -11716.295364595471
4
Iteration 11800: Loss = -11716.294903435028
5
Iteration 11900: Loss = -11716.29430929956
6
Iteration 12000: Loss = -11716.29434953183
7
Iteration 12100: Loss = -11716.29450922917
8
Iteration 12200: Loss = -11716.294016512746
Iteration 12300: Loss = -11716.305061322262
1
Iteration 12400: Loss = -11716.298519738268
2
Iteration 12500: Loss = -11716.293500894613
Iteration 12600: Loss = -11716.294677282509
1
Iteration 12700: Loss = -11716.29602041499
2
Iteration 12800: Loss = -11716.29348696559
Iteration 12900: Loss = -11716.400044510652
1
Iteration 13000: Loss = -11716.296042793036
2
Iteration 13100: Loss = -11716.308593868762
3
Iteration 13200: Loss = -11716.300137568365
4
Iteration 13300: Loss = -11716.308806353436
5
Iteration 13400: Loss = -11716.298213015725
6
Iteration 13500: Loss = -11716.295085743304
7
Iteration 13600: Loss = -11716.41031829172
8
Iteration 13700: Loss = -11716.293379969738
Iteration 13800: Loss = -11716.293396599774
1
Iteration 13900: Loss = -11716.315039578914
2
Iteration 14000: Loss = -11716.29343535761
3
Iteration 14100: Loss = -11716.297449745762
4
Iteration 14200: Loss = -11716.3041759706
5
Iteration 14300: Loss = -11716.299035646247
6
Iteration 14400: Loss = -11716.307846334123
7
Iteration 14500: Loss = -11716.294239922841
8
Iteration 14600: Loss = -11716.293733385057
9
Iteration 14700: Loss = -11716.295398076983
10
Stopping early at iteration 14700 due to no improvement.
tensor([[  5.1637,  -9.7790],
        [  5.8561, -10.4714],
        [  5.0092,  -9.6244],
        [  0.3837,  -4.9989],
        [ -4.3159,  -0.2994],
        [ -1.0676,  -3.5476],
        [  0.7886,  -5.4038],
        [  4.9646,  -9.5798],
        [  0.8893,  -5.5045],
        [  1.5375,  -6.1528],
        [ -6.2727,   1.6575],
        [  5.6727, -10.2879],
        [  6.1951, -10.8104],
        [ -3.2533,  -1.3619],
        [  4.7494,  -9.3647],
        [  5.0775,  -9.6927],
        [  4.4510,  -9.0663],
        [  4.2233,  -8.8386],
        [  5.4411, -10.0563],
        [  4.9516,  -9.5668],
        [  5.0260,  -9.6412],
        [ -6.1494,   1.5342],
        [  5.5061, -10.1214],
        [  4.7914,  -9.4066],
        [  4.5272,  -9.1425],
        [ -2.5161,  -2.0991],
        [ -5.1373,   0.5221],
        [ -4.7985,   0.1833],
        [ -8.0844,   3.4692],
        [ -3.9704,  -0.6448],
        [  1.4843,  -6.0996],
        [ -5.9729,   1.3577],
        [  0.9701,  -5.5853],
        [ -5.9014,   1.2862],
        [  2.4475,  -7.0628],
        [ -5.0574,   0.4422],
        [  0.9575,  -5.5727],
        [  0.0611,  -4.6763],
        [  4.5306,  -9.1458],
        [ -5.6216,   1.0064],
        [  6.0586, -10.6738],
        [  5.2673,  -9.8825],
        [  5.0413,  -9.6565],
        [  4.2238,  -8.8390],
        [  4.4314,  -9.0466],
        [ -5.8928,   1.2776],
        [ -9.0102,   4.3950],
        [ -4.0849,  -0.5303],
        [  4.1359,  -8.7511],
        [  6.0494, -10.6647],
        [  4.4310,  -9.0462],
        [ -4.5285,  -0.0868],
        [  4.5599,  -9.1751],
        [  4.9627,  -9.5779],
        [  6.3544, -10.9696],
        [  5.4311, -10.0463],
        [  4.6138,  -9.2291],
        [ -2.7737,  -1.8416],
        [ -0.4172,  -4.1981],
        [  5.1774,  -9.7926],
        [  5.2062,  -9.8214],
        [  5.6348, -10.2500],
        [ -3.9665,  -0.6487],
        [ -5.6289,   1.0136],
        [  4.5194,  -9.1347],
        [ -3.8629,  -0.7523],
        [  0.2223,  -4.8375],
        [ -1.5418,  -3.0734],
        [  4.8905,  -9.5057],
        [  5.4656, -10.0809],
        [ -1.0800,  -3.5353],
        [ -0.7850,  -3.8302],
        [ -1.2433,  -3.3720],
        [  4.9478,  -9.5630],
        [  6.0303, -10.6455],
        [  0.7175,  -5.3327],
        [  3.9234,  -8.5386],
        [  6.0932, -10.7084],
        [ -3.9469,  -0.6683],
        [  5.3392,  -9.9544],
        [  0.7634,  -5.3786],
        [ -8.4130,   3.7978],
        [ -3.9088,  -0.7064],
        [  3.3989,  -8.0141],
        [  4.7968,  -9.4120],
        [ -4.9984,   0.3832],
        [  1.8194,  -6.4346],
        [  5.0855,  -9.7007],
        [ -7.8470,   3.2318],
        [  5.8208, -10.4360],
        [  2.8798,  -7.4950],
        [  2.2239,  -6.8391],
        [ -2.2074,  -2.4078],
        [  6.1162, -10.7314],
        [ -1.3147,  -3.3006],
        [  3.6160,  -8.2313],
        [  4.6680,  -9.2832],
        [ -4.2753,  -0.3400],
        [ -6.5885,   1.9733],
        [  3.4220,  -8.0372]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6926, 0.3074],
        [0.3758, 0.6242]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7200, 0.2800], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2373, 0.0969],
         [0.6893, 0.3682]],

        [[0.0368, 0.0965],
         [0.2193, 0.9365]],

        [[0.8336, 0.0975],
         [0.1346, 0.2440]],

        [[0.2419, 0.1041],
         [0.8217, 0.9697]],

        [[0.3665, 0.0997],
         [0.4765, 0.5763]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 22
Adjusted Rand Index: 0.3079630444346678
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 24
Adjusted Rand Index: 0.2641881425151387
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.14268764389778754
Average Adjusted Rand Index: 0.7064297985535244
Iteration 0: Loss = -25873.25558703139
Iteration 10: Loss = -11448.75964959736
Iteration 20: Loss = -11448.757520339226
Iteration 30: Loss = -11448.75752358139
1
Iteration 40: Loss = -11448.75752358139
2
Iteration 50: Loss = -11448.75752358139
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7565, 0.2435],
        [0.2755, 0.7245]], dtype=torch.float64)
alpha: tensor([0.5228, 0.4772])
beta: tensor([[[0.1985, 0.0948],
         [0.6294, 0.3897]],

        [[0.2091, 0.0975],
         [0.1211, 0.5765]],

        [[0.2535, 0.0984],
         [0.7950, 0.8205]],

        [[0.3886, 0.1045],
         [0.9962, 0.0245]],

        [[0.6794, 0.0997],
         [0.8195, 0.5412]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25872.970777181545
Iteration 100: Loss = -12298.169428962718
Iteration 200: Loss = -12246.178716869128
Iteration 300: Loss = -12218.039363837688
Iteration 400: Loss = -12024.164488385652
Iteration 500: Loss = -11973.39833301591
Iteration 600: Loss = -11882.150811467782
Iteration 700: Loss = -11851.24260832082
Iteration 800: Loss = -11850.68092601058
Iteration 900: Loss = -11850.382232694314
Iteration 1000: Loss = -11850.136992899697
Iteration 1100: Loss = -11849.257656775244
Iteration 1200: Loss = -11788.52586975243
Iteration 1300: Loss = -11599.67174232422
Iteration 1400: Loss = -11495.423267211541
Iteration 1500: Loss = -11495.121937663856
Iteration 1600: Loss = -11481.034072747061
Iteration 1700: Loss = -11471.529971455
Iteration 1800: Loss = -11471.464700281831
Iteration 1900: Loss = -11471.415216387706
Iteration 2000: Loss = -11462.597567905155
Iteration 2100: Loss = -11462.567570261834
Iteration 2200: Loss = -11462.545462338656
Iteration 2300: Loss = -11462.527399329476
Iteration 2400: Loss = -11462.51225448078
Iteration 2500: Loss = -11462.499206844834
Iteration 2600: Loss = -11462.487508087368
Iteration 2700: Loss = -11462.47539905387
Iteration 2800: Loss = -11462.345226550082
Iteration 2900: Loss = -11447.752640927662
Iteration 3000: Loss = -11447.745293623435
Iteration 3100: Loss = -11447.739003689749
Iteration 3200: Loss = -11447.733509284599
Iteration 3300: Loss = -11447.728664403287
Iteration 3400: Loss = -11447.724341608038
Iteration 3500: Loss = -11447.720521593676
Iteration 3600: Loss = -11447.717016776913
Iteration 3700: Loss = -11447.713876359552
Iteration 3800: Loss = -11447.711023020021
Iteration 3900: Loss = -11447.708438188572
Iteration 4000: Loss = -11447.70605324538
Iteration 4100: Loss = -11447.703861363372
Iteration 4200: Loss = -11447.701852348282
Iteration 4300: Loss = -11447.700033188066
Iteration 4400: Loss = -11447.698330169063
Iteration 4500: Loss = -11447.696774334556
Iteration 4600: Loss = -11447.69533117064
Iteration 4700: Loss = -11447.693974934831
Iteration 4800: Loss = -11447.692714498184
Iteration 4900: Loss = -11447.691565176383
Iteration 5000: Loss = -11447.690514293427
Iteration 5100: Loss = -11447.689528919513
Iteration 5200: Loss = -11447.688565164466
Iteration 5300: Loss = -11447.68767383694
Iteration 5400: Loss = -11447.686872128224
Iteration 5500: Loss = -11447.68605523549
Iteration 5600: Loss = -11447.685219014133
Iteration 5700: Loss = -11447.684190866396
Iteration 5800: Loss = -11447.674226165158
Iteration 5900: Loss = -11447.667280424788
Iteration 6000: Loss = -11447.666647699416
Iteration 6100: Loss = -11447.687605049136
1
Iteration 6200: Loss = -11447.665643121365
Iteration 6300: Loss = -11447.665341362694
Iteration 6400: Loss = -11447.666993096642
1
Iteration 6500: Loss = -11447.66435637288
Iteration 6600: Loss = -11447.673951210554
1
Iteration 6700: Loss = -11447.663580034612
Iteration 6800: Loss = -11447.666135786709
1
Iteration 6900: Loss = -11447.663451114366
Iteration 7000: Loss = -11447.663862591653
1
Iteration 7100: Loss = -11447.674063796814
2
Iteration 7200: Loss = -11447.700997441005
3
Iteration 7300: Loss = -11447.668083450628
4
Iteration 7400: Loss = -11447.684549670379
5
Iteration 7500: Loss = -11447.666251007955
6
Iteration 7600: Loss = -11447.664871883953
7
Iteration 7700: Loss = -11447.665910716341
8
Iteration 7800: Loss = -11447.66145515115
Iteration 7900: Loss = -11447.66994072028
1
Iteration 8000: Loss = -11447.675703408666
2
Iteration 8100: Loss = -11447.692517728905
3
Iteration 8200: Loss = -11447.660481938976
Iteration 8300: Loss = -11447.685807534852
1
Iteration 8400: Loss = -11447.674671565612
2
Iteration 8500: Loss = -11447.68024205068
3
Iteration 8600: Loss = -11447.673890612159
4
Iteration 8700: Loss = -11447.666091332827
5
Iteration 8800: Loss = -11447.661927469988
6
Iteration 8900: Loss = -11447.663867597239
7
Iteration 9000: Loss = -11447.664029500602
8
Iteration 9100: Loss = -11447.668858280376
9
Iteration 9200: Loss = -11447.661362894687
10
Stopping early at iteration 9200 due to no improvement.
tensor([[ 5.5825, -7.2438],
        [ 6.0797, -7.5551],
        [ 5.2531, -7.5112],
        [-6.0406,  4.6234],
        [-5.0469,  3.3378],
        [-7.5818,  5.6295],
        [-7.3259,  4.0568],
        [ 5.9895, -7.8664],
        [-6.4196,  3.7022],
        [-6.4119,  1.7967],
        [-6.9990,  5.5894],
        [ 5.9200, -7.3095],
        [ 7.1583, -8.5599],
        [-6.7177,  4.9170],
        [ 6.2709, -7.7360],
        [ 6.3037, -8.9827],
        [ 5.3163, -7.3053],
        [ 4.5212, -6.7053],
        [ 6.0109, -7.7643],
        [ 5.7616, -8.2088],
        [ 5.1027, -8.0161],
        [-7.3564,  5.7736],
        [ 5.5919, -6.9950],
        [ 4.9731, -7.4815],
        [ 5.1635, -6.6045],
        [-7.2558,  5.6173],
        [-7.5443,  5.2864],
        [-7.4141,  5.8151],
        [-8.5989,  3.9837],
        [-5.8474,  4.4144],
        [-6.5798,  5.0421],
        [-6.9227,  5.3160],
        [-6.6257,  5.1497],
        [-6.4929,  4.5659],
        [ 1.8742, -3.8185],
        [-7.3783,  5.9892],
        [-7.1414,  5.6397],
        [-7.6281,  4.9481],
        [ 5.4934, -6.8906],
        [-8.4457,  4.1158],
        [ 5.7195, -7.2471],
        [ 5.4200, -7.7946],
        [ 5.8253, -7.4045],
        [ 5.0528, -7.2459],
        [ 5.3155, -7.0886],
        [-7.5208,  5.6224],
        [-7.4616,  6.0285],
        [-5.6281,  2.8580],
        [ 3.1306, -6.5695],
        [ 6.3735, -7.7606],
        [ 6.0849, -7.8877],
        [-7.5779,  6.0466],
        [ 4.3975, -6.0103],
        [ 5.8650, -8.2644],
        [ 6.4038, -7.8042],
        [ 5.6590, -7.2641],
        [ 4.9736, -7.0456],
        [-7.7216,  5.4223],
        [-6.9675,  5.3488],
        [ 6.0293, -7.4192],
        [ 4.9382, -7.3021],
        [ 5.4223, -6.8299],
        [-6.9404,  5.4064],
        [-7.1300,  5.1934],
        [ 4.5806, -6.2706],
        [-6.9488,  5.3809],
        [-6.0380,  4.2991],
        [-6.7009,  5.3141],
        [ 5.2093, -7.9767],
        [ 6.8702, -8.3325],
        [-5.8467,  4.2115],
        [-6.6060,  5.1346],
        [-7.0827,  5.6923],
        [ 6.0421, -7.7515],
        [ 6.7187, -8.5625],
        [-4.9753,  3.4159],
        [ 5.5779, -7.0769],
        [ 6.0408, -7.6425],
        [-7.6666,  6.2795],
        [ 6.6517, -8.0695],
        [-8.4850,  3.8698],
        [-7.8168,  5.9856],
        [-7.2654,  5.7844],
        [-6.7694,  5.2896],
        [ 5.4259, -7.0887],
        [-6.8946,  5.3178],
        [-6.0953,  3.8029],
        [ 6.0513, -7.4801],
        [-7.6006,  5.0944],
        [ 6.0271, -8.7723],
        [ 4.4972, -5.9439],
        [-4.7699,  2.8241],
        [-5.8216,  3.4077],
        [ 6.0664, -7.7476],
        [-9.9200,  5.6361],
        [ 4.4614, -6.1593],
        [ 5.8028, -7.4446],
        [-7.2702,  4.8782],
        [-7.2517,  5.0787],
        [ 3.6869, -5.0888]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7256, 0.2744],
        [0.2418, 0.7582]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5000, 0.5000], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3990, 0.0948],
         [0.6294, 0.2021]],

        [[0.2091, 0.0971],
         [0.1211, 0.5765]],

        [[0.2535, 0.0999],
         [0.7950, 0.8205]],

        [[0.3886, 0.1044],
         [0.9962, 0.0245]],

        [[0.6794, 0.0997],
         [0.8195, 0.5412]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919998119331364
Iteration 0: Loss = -28545.334304771644
Iteration 10: Loss = -12192.951427614307
Iteration 20: Loss = -12182.724953217186
Iteration 30: Loss = -11449.578229729581
Iteration 40: Loss = -11448.757511161524
Iteration 50: Loss = -11448.757522056974
1
Iteration 60: Loss = -11448.757522056974
2
Iteration 70: Loss = -11448.757522056974
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7245, 0.2755],
        [0.2435, 0.7565]], dtype=torch.float64)
alpha: tensor([0.4772, 0.5228])
beta: tensor([[[0.3897, 0.0948],
         [0.0469, 0.1985]],

        [[0.8590, 0.0975],
         [0.9611, 0.4763]],

        [[0.4767, 0.0984],
         [0.8296, 0.0116]],

        [[0.9980, 0.1045],
         [0.9501, 0.9043]],

        [[0.2630, 0.0997],
         [0.0773, 0.1238]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28545.675850579806
Iteration 100: Loss = -12275.604853155744
Iteration 200: Loss = -12234.409522236101
Iteration 300: Loss = -12165.559880506906
Iteration 400: Loss = -12021.60895457506
Iteration 500: Loss = -11936.867328229284
Iteration 600: Loss = -11846.483220626877
Iteration 700: Loss = -11799.636278910526
Iteration 800: Loss = -11754.674492094486
Iteration 900: Loss = -11694.396787406715
Iteration 1000: Loss = -11685.071583883828
Iteration 1100: Loss = -11666.790810545514
Iteration 1200: Loss = -11666.435185274495
Iteration 1300: Loss = -11665.27299303899
Iteration 1400: Loss = -11663.844508811513
Iteration 1500: Loss = -11657.577812579635
Iteration 1600: Loss = -11653.90956208254
Iteration 1700: Loss = -11650.479611760778
Iteration 1800: Loss = -11647.055130805335
Iteration 1900: Loss = -11645.979059148312
Iteration 2000: Loss = -11645.87110955144
Iteration 2100: Loss = -11645.73862174161
Iteration 2200: Loss = -11644.265815350434
Iteration 2300: Loss = -11639.801633997784
Iteration 2400: Loss = -11635.524840046932
Iteration 2500: Loss = -11632.975455376021
Iteration 2600: Loss = -11631.83790556664
Iteration 2700: Loss = -11631.622046195436
Iteration 2800: Loss = -11629.85728778945
Iteration 2900: Loss = -11629.193798616541
Iteration 3000: Loss = -11627.226170330445
Iteration 3100: Loss = -11624.42070568141
Iteration 3200: Loss = -11624.396626551461
Iteration 3300: Loss = -11624.330840598497
Iteration 3400: Loss = -11615.880155083392
Iteration 3500: Loss = -11615.759783259566
Iteration 3600: Loss = -11615.724942764615
Iteration 3700: Loss = -11615.696370499436
Iteration 3800: Loss = -11613.1919810661
Iteration 3900: Loss = -11613.14788997218
Iteration 4000: Loss = -11613.139349409375
Iteration 4100: Loss = -11613.133846477876
Iteration 4200: Loss = -11613.128703982622
Iteration 4300: Loss = -11613.123559553287
Iteration 4400: Loss = -11613.124934431822
1
Iteration 4500: Loss = -11613.11688909528
Iteration 4600: Loss = -11613.09405894162
Iteration 4700: Loss = -11613.035552271493
Iteration 4800: Loss = -11613.029835733565
Iteration 4900: Loss = -11613.024624527025
Iteration 5000: Loss = -11612.987942902368
Iteration 5100: Loss = -11599.245870826027
Iteration 5200: Loss = -11599.237207475464
Iteration 5300: Loss = -11599.218140800509
Iteration 5400: Loss = -11587.8077518345
Iteration 5500: Loss = -11587.801141878535
Iteration 5600: Loss = -11587.797128585182
Iteration 5700: Loss = -11587.80237106276
1
Iteration 5800: Loss = -11587.786913877444
Iteration 5900: Loss = -11587.415158898015
Iteration 6000: Loss = -11582.122830960798
Iteration 6100: Loss = -11582.136757019873
1
Iteration 6200: Loss = -11582.115577176508
Iteration 6300: Loss = -11582.119218313577
1
Iteration 6400: Loss = -11582.11189592729
Iteration 6500: Loss = -11582.11448296321
1
Iteration 6600: Loss = -11582.106044933125
Iteration 6700: Loss = -11577.116166361227
Iteration 6800: Loss = -11577.111019951531
Iteration 6900: Loss = -11577.109721039305
Iteration 7000: Loss = -11577.108973382226
Iteration 7100: Loss = -11577.109028924691
1
Iteration 7200: Loss = -11577.10820839735
Iteration 7300: Loss = -11564.873159896884
Iteration 7400: Loss = -11564.807072039832
Iteration 7500: Loss = -11564.807596349017
1
Iteration 7600: Loss = -11564.804813944342
Iteration 7700: Loss = -11564.804605552838
Iteration 7800: Loss = -11564.805374685318
1
Iteration 7900: Loss = -11564.714346113507
Iteration 8000: Loss = -11564.564316653874
Iteration 8100: Loss = -11564.56089490325
Iteration 8200: Loss = -11564.561989156093
1
Iteration 8300: Loss = -11564.559127067205
Iteration 8400: Loss = -11564.564167451006
1
Iteration 8500: Loss = -11564.644620317018
2
Iteration 8600: Loss = -11564.557969505717
Iteration 8700: Loss = -11564.559770646298
1
Iteration 8800: Loss = -11564.571326566233
2
Iteration 8900: Loss = -11564.56009544488
3
Iteration 9000: Loss = -11564.556172285516
Iteration 9100: Loss = -11564.557912690687
1
Iteration 9200: Loss = -11564.581185492923
2
Iteration 9300: Loss = -11555.418124128584
Iteration 9400: Loss = -11555.41800106654
Iteration 9500: Loss = -11555.401197227086
Iteration 9600: Loss = -11555.408312333853
1
Iteration 9700: Loss = -11555.398446229043
Iteration 9800: Loss = -11555.415782415228
1
Iteration 9900: Loss = -11555.400407682659
2
Iteration 10000: Loss = -11555.416254811256
3
Iteration 10100: Loss = -11555.397844492121
Iteration 10200: Loss = -11555.420674007371
1
Iteration 10300: Loss = -11555.399755988466
2
Iteration 10400: Loss = -11555.405351817757
3
Iteration 10500: Loss = -11555.488572540837
4
Iteration 10600: Loss = -11542.419136857312
Iteration 10700: Loss = -11542.40805064811
Iteration 10800: Loss = -11542.39222158256
Iteration 10900: Loss = -11530.106755430139
Iteration 11000: Loss = -11530.105555494138
Iteration 11100: Loss = -11515.85454393165
Iteration 11200: Loss = -11515.862835952452
1
Iteration 11300: Loss = -11515.923898929099
2
Iteration 11400: Loss = -11515.86481433416
3
Iteration 11500: Loss = -11515.853653676017
Iteration 11600: Loss = -11503.252799666987
Iteration 11700: Loss = -11503.264209656092
1
Iteration 11800: Loss = -11503.25758767804
2
Iteration 11900: Loss = -11503.255227225261
3
Iteration 12000: Loss = -11503.25482693591
4
Iteration 12100: Loss = -11503.25373865535
5
Iteration 12200: Loss = -11503.262879867734
6
Iteration 12300: Loss = -11503.251301515851
Iteration 12400: Loss = -11503.313894291723
1
Iteration 12500: Loss = -11503.2489137826
Iteration 12600: Loss = -11503.255203768556
1
Iteration 12700: Loss = -11503.248715002928
Iteration 12800: Loss = -11503.25719162505
1
Iteration 12900: Loss = -11503.249049666938
2
Iteration 13000: Loss = -11503.250989266162
3
Iteration 13100: Loss = -11503.267160804462
4
Iteration 13200: Loss = -11503.255553140927
5
Iteration 13300: Loss = -11503.254745944116
6
Iteration 13400: Loss = -11503.323128086236
7
Iteration 13500: Loss = -11503.250082835864
8
Iteration 13600: Loss = -11503.24849750713
Iteration 13700: Loss = -11503.268392439666
1
Iteration 13800: Loss = -11503.249683105189
2
Iteration 13900: Loss = -11503.247962283102
Iteration 14000: Loss = -11503.262013867528
1
Iteration 14100: Loss = -11503.271605016154
2
Iteration 14200: Loss = -11503.253735079687
3
Iteration 14300: Loss = -11503.248023265649
4
Iteration 14400: Loss = -11503.248447419961
5
Iteration 14500: Loss = -11503.416716039594
6
Iteration 14600: Loss = -11503.2490591755
7
Iteration 14700: Loss = -11489.47560393875
Iteration 14800: Loss = -11489.515426572212
1
Iteration 14900: Loss = -11489.643353685915
2
Iteration 15000: Loss = -11489.474437070678
Iteration 15100: Loss = -11489.473779409718
Iteration 15200: Loss = -11489.478109904532
1
Iteration 15300: Loss = -11489.483177433378
2
Iteration 15400: Loss = -11489.476797836587
3
Iteration 15500: Loss = -11489.473362598193
Iteration 15600: Loss = -11489.472673926177
Iteration 15700: Loss = -11489.475235831218
1
Iteration 15800: Loss = -11489.564605549318
2
Iteration 15900: Loss = -11489.47395320683
3
Iteration 16000: Loss = -11489.472752376083
4
Iteration 16100: Loss = -11489.474517357847
5
Iteration 16200: Loss = -11489.602635962949
6
Iteration 16300: Loss = -11489.503251169228
7
Iteration 16400: Loss = -11489.472381313166
Iteration 16500: Loss = -11489.474334196992
1
Iteration 16600: Loss = -11489.47619790292
2
Iteration 16700: Loss = -11489.474049605387
3
Iteration 16800: Loss = -11489.642855868
4
Iteration 16900: Loss = -11473.701630818226
Iteration 17000: Loss = -11473.706579186042
1
Iteration 17100: Loss = -11473.68890292247
Iteration 17200: Loss = -11473.669944831452
Iteration 17300: Loss = -11473.703757636056
1
Iteration 17400: Loss = -11473.677443453573
2
Iteration 17500: Loss = -11473.669345648028
Iteration 17600: Loss = -11473.660047188665
Iteration 17700: Loss = -11473.666970752214
1
Iteration 17800: Loss = -11473.808180573882
2
Iteration 17900: Loss = -11473.652509421165
Iteration 18000: Loss = -11473.652677769918
1
Iteration 18100: Loss = -11473.670370021624
2
Iteration 18200: Loss = -11473.655512682815
3
Iteration 18300: Loss = -11473.65804010617
4
Iteration 18400: Loss = -11473.659719484674
5
Iteration 18500: Loss = -11473.678215993938
6
Iteration 18600: Loss = -11473.652666873593
7
Iteration 18700: Loss = -11473.652560654078
8
Iteration 18800: Loss = -11473.654899572099
9
Iteration 18900: Loss = -11473.653360109685
10
Stopping early at iteration 18900 due to no improvement.
tensor([[  9.0882, -11.1037],
        [  8.8662, -11.1225],
        [  7.7423,  -9.5943],
        [ -9.9929,   7.6101],
        [ -4.8480,   3.4584],
        [-10.8549,   7.7646],
        [ -5.8737,   3.2920],
        [  7.4374, -10.7670],
        [ -6.0073,   4.2734],
        [-11.4153,   7.4353],
        [-10.0052,   8.6184],
        [  9.4460, -11.4857],
        [  9.1449, -11.2868],
        [ -8.1772,   3.5620],
        [  8.8705, -11.1589],
        [  9.3774, -12.1149],
        [  6.3263,  -9.5081],
        [  4.9451,  -6.3336],
        [  8.2815,  -9.8009],
        [  8.2297, -10.9733],
        [  8.2057,  -9.7236],
        [-10.5497,   8.3101],
        [  7.0584,  -9.2020],
        [  8.9487, -10.5008],
        [  5.1475,  -7.0902],
        [ -8.5053,   7.1190],
        [ -9.6255,   7.6929],
        [-10.5214,   7.0446],
        [-10.4245,   7.2630],
        [ -6.0853,   4.1195],
        [-10.9535,   8.9205],
        [ -8.4804,   7.0677],
        [ -8.6115,   6.0155],
        [ -7.8737,   3.7872],
        [  0.5716,  -5.1868],
        [ -9.7134,   7.9516],
        [ -7.7626,   6.0255],
        [ -9.8359,   7.9507],
        [  6.4063,  -8.7510],
        [-10.0098,   7.4350],
        [  7.6874,  -9.4495],
        [  9.2242, -11.5057],
        [  8.5583, -11.2048],
        [  8.9234, -10.3097],
        [  6.7098,  -8.6328],
        [-10.1964,   8.8101],
        [-10.7967,   8.2985],
        [ -4.9703,   3.4020],
        [  4.1190,  -5.6133],
        [  8.5398,  -9.9536],
        [  8.7344, -10.1278],
        [ -9.1600,   7.7728],
        [  8.8017, -10.5562],
        [  8.6877, -10.6812],
        [  8.2510, -11.1663],
        [  7.7291,  -9.6398],
        [  5.2769,  -6.8853],
        [ -8.5000,   6.4235],
        [ -7.5481,   4.9915],
        [  8.6352, -10.0290],
        [  5.4869,  -6.8979],
        [  7.3506, -11.5953],
        [ -7.7131,   6.2050],
        [ -9.0725,   7.6121],
        [  3.5196,  -7.9038],
        [ -8.8018,   7.4122],
        [ -5.9164,   4.3628],
        [ -8.1749,   6.5262],
        [  7.9503, -10.1912],
        [  9.4799, -12.5214],
        [ -5.7452,   4.2215],
        [-11.0644,   7.9304],
        [ -9.7917,   5.9366],
        [  9.8972, -12.1684],
        [  8.3426,  -9.7865],
        [ -4.8443,   3.4398],
        [  8.8611, -11.8654],
        [  7.7159,  -9.3065],
        [ -8.3278,   6.4738],
        [  8.2292, -10.8095],
        [ -7.1646,   5.0329],
        [ -9.4166,   7.8145],
        [-10.4953,   9.0397],
        [ -7.6761,   6.0566],
        [  6.1451,  -8.0626],
        [ -9.1714,   5.5979],
        [ -5.8479,   4.0560],
        [  8.0267,  -9.4286],
        [ -9.6347,   7.8311],
        [  8.4024,  -9.8568],
        [  3.8323,  -6.7914],
        [ -4.8491,   2.6349],
        [ -5.9099,   3.2681],
        [  8.9118, -11.4842],
        [ -9.6463,   8.0769],
        [  4.3586,  -6.4093],
        [  5.2023,  -8.0007],
        [ -8.8272,   7.2369],
        [ -9.4760,   7.0427],
        [  6.5220,  -7.9257]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7184, 0.2816],
        [0.2455, 0.7545]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4999, 0.5001], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3967, 0.0948],
         [0.0469, 0.2018]],

        [[0.8590, 0.0969],
         [0.9611, 0.4763]],

        [[0.4767, 0.1065],
         [0.8296, 0.0116]],

        [[0.9980, 0.1044],
         [0.9501, 0.9043]],

        [[0.2630, 0.0996],
         [0.0773, 0.1238]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320648505942
Average Adjusted Rand Index: 0.9841616161616162
Iteration 0: Loss = -18742.648054399833
Iteration 10: Loss = -11448.757324642977
Iteration 20: Loss = -11448.757523947419
1
Iteration 30: Loss = -11448.75752358139
2
Iteration 40: Loss = -11448.75752358139
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7565, 0.2435],
        [0.2755, 0.7245]], dtype=torch.float64)
alpha: tensor([0.5228, 0.4772])
beta: tensor([[[0.1985, 0.0948],
         [0.0253, 0.3897]],

        [[0.6669, 0.0975],
         [0.3362, 0.3461]],

        [[0.1345, 0.0984],
         [0.7981, 0.5405]],

        [[0.4770, 0.1045],
         [0.3060, 0.5168]],

        [[0.7691, 0.0997],
         [0.3076, 0.4123]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18742.296469513025
Iteration 100: Loss = -12173.532226254365
Iteration 200: Loss = -12012.045654445397
Iteration 300: Loss = -11639.690147016157
Iteration 400: Loss = -11489.301057279601
Iteration 500: Loss = -11480.975708250382
Iteration 600: Loss = -11465.20906988888
Iteration 700: Loss = -11465.022474147443
Iteration 800: Loss = -11464.909421912855
Iteration 900: Loss = -11464.833601759437
Iteration 1000: Loss = -11464.779394021924
Iteration 1100: Loss = -11464.73867847899
Iteration 1200: Loss = -11464.706731952898
Iteration 1300: Loss = -11464.675144889985
Iteration 1400: Loss = -11447.26187231426
Iteration 1500: Loss = -11447.237410480246
Iteration 1600: Loss = -11447.21928988803
Iteration 1700: Loss = -11447.203743830247
Iteration 1800: Loss = -11447.17020628007
Iteration 1900: Loss = -11447.16064004832
Iteration 2000: Loss = -11447.152598106719
Iteration 2100: Loss = -11447.145669276668
Iteration 2200: Loss = -11447.139685319144
Iteration 2300: Loss = -11447.134558901264
Iteration 2400: Loss = -11447.130048403817
Iteration 2500: Loss = -11447.126065820783
Iteration 2600: Loss = -11447.122580067267
Iteration 2700: Loss = -11447.119404266316
Iteration 2800: Loss = -11447.116604129242
Iteration 2900: Loss = -11447.114093939828
Iteration 3000: Loss = -11447.111787294274
Iteration 3100: Loss = -11447.109711426243
Iteration 3200: Loss = -11447.10784631031
Iteration 3300: Loss = -11447.106120626046
Iteration 3400: Loss = -11447.104529053137
Iteration 3500: Loss = -11447.103048881487
Iteration 3600: Loss = -11447.101738227595
Iteration 3700: Loss = -11447.100537509406
Iteration 3800: Loss = -11447.099426481103
Iteration 3900: Loss = -11447.098374288842
Iteration 4000: Loss = -11447.097441182616
Iteration 4100: Loss = -11447.09652327152
Iteration 4200: Loss = -11447.095706795515
Iteration 4300: Loss = -11447.09493122497
Iteration 4400: Loss = -11447.094199000037
Iteration 4500: Loss = -11447.093583750286
Iteration 4600: Loss = -11447.097664599512
1
Iteration 4700: Loss = -11447.092336045864
Iteration 4800: Loss = -11447.091820839234
Iteration 4900: Loss = -11447.09138436949
Iteration 5000: Loss = -11447.090877966015
Iteration 5100: Loss = -11447.09042710979
Iteration 5200: Loss = -11447.089998067766
Iteration 5300: Loss = -11447.089660597698
Iteration 5400: Loss = -11447.089271343397
Iteration 5500: Loss = -11447.088916305785
Iteration 5600: Loss = -11447.088625348144
Iteration 5700: Loss = -11447.088305891164
Iteration 5800: Loss = -11447.08801971253
Iteration 5900: Loss = -11447.087775114354
Iteration 6000: Loss = -11447.098721018148
1
Iteration 6100: Loss = -11447.087605353916
Iteration 6200: Loss = -11447.087092800057
Iteration 6300: Loss = -11447.088474986303
1
Iteration 6400: Loss = -11447.086715298523
Iteration 6500: Loss = -11447.096611772515
1
Iteration 6600: Loss = -11447.086394906722
Iteration 6700: Loss = -11447.118093398529
1
Iteration 6800: Loss = -11447.087324513212
2
Iteration 6900: Loss = -11447.085865054014
Iteration 7000: Loss = -11447.085763330042
Iteration 7100: Loss = -11447.086599871393
1
Iteration 7200: Loss = -11447.09000452805
2
Iteration 7300: Loss = -11447.087712556928
3
Iteration 7400: Loss = -11447.085798366961
4
Iteration 7500: Loss = -11447.08516949289
Iteration 7600: Loss = -11447.094609618453
1
Iteration 7700: Loss = -11447.086951934949
2
Iteration 7800: Loss = -11447.085473372172
3
Iteration 7900: Loss = -11447.088907483783
4
Iteration 8000: Loss = -11447.085748577443
5
Iteration 8100: Loss = -11447.08497854931
Iteration 8200: Loss = -11447.084852303997
Iteration 8300: Loss = -11447.084840537687
Iteration 8400: Loss = -11447.162029072653
1
Iteration 8500: Loss = -11447.086011913827
2
Iteration 8600: Loss = -11447.086093091202
3
Iteration 8700: Loss = -11447.086236326559
4
Iteration 8800: Loss = -11447.08447184702
Iteration 8900: Loss = -11447.087076114189
1
Iteration 9000: Loss = -11447.08784298467
2
Iteration 9100: Loss = -11447.084932862952
3
Iteration 9200: Loss = -11447.092544528923
4
Iteration 9300: Loss = -11447.092305189133
5
Iteration 9400: Loss = -11447.087030757035
6
Iteration 9500: Loss = -11447.09663284215
7
Iteration 9600: Loss = -11447.084486226688
8
Iteration 9700: Loss = -11447.0848188345
9
Iteration 9800: Loss = -11447.086440602137
10
Stopping early at iteration 9800 due to no improvement.
tensor([[ -7.2681,   5.8766],
        [ -8.1341,   6.6924],
        [ -6.8653,   5.4730],
        [  4.6865,  -6.1485],
        [  3.4556,  -4.8826],
        [  7.1737, -10.3873],
        [  6.5604,  -7.9807],
        [-10.3825,   7.7734],
        [  6.0312,  -8.3733],
        [  3.0919,  -5.0768],
        [  5.7373,  -7.6322],
        [ -8.1693,   6.6087],
        [ -8.7115,   7.2131],
        [  5.1520,  -6.5714],
        [ -7.3831,   5.7977],
        [ -9.5513,   7.2478],
        [ -8.9468,   5.2330],
        [ -9.1168,   7.1181],
        [ -8.2410,   6.2923],
        [ -7.7862,   6.3805],
        [ -8.5990,   6.8340],
        [  6.8659,  -8.3332],
        [-10.0664,   6.6083],
        [ -7.6231,   6.0372],
        [ -7.9081,   6.2993],
        [  6.6592,  -8.9458],
        [  7.1715, -10.7021],
        [  5.0606,  -9.1432],
        [  6.9661,  -8.3624],
        [  6.6691,  -9.4419],
        [  5.7355,  -7.8330],
        [  6.6220,  -8.6828],
        [  5.8974,  -7.2850],
        [  5.0183,  -6.5783],
        [ -3.9437,   1.7933],
        [  6.8910,  -8.7017],
        [  5.8487,  -7.3143],
        [  6.8448, -11.4600],
        [-10.9729,   6.3577],
        [  6.7010,  -8.1270],
        [ -9.1203,   6.2966],
        [-10.3740,   5.7587],
        [ -8.0892,   6.6706],
        [ -7.1486,   4.9829],
        [ -7.1919,   5.7972],
        [  6.7558,  -8.3344],
        [  6.8990,  -8.8178],
        [  6.5243,  -8.9709],
        [ -5.8975,   3.8728],
        [ -8.3642,   6.3885],
        [ -8.6482,   6.4613],
        [  6.4045,  -7.8285],
        [ -6.6362,   4.1201],
        [ -7.9791,   6.4943],
        [ -9.2611,   6.6512],
        [ -8.6096,   7.0440],
        [-10.5065,   5.8913],
        [  5.7313,  -9.2929],
        [  5.8857,  -7.2734],
        [ -8.4184,   6.6279],
        [ -7.0297,   5.3662],
        [ -7.5347,   5.9335],
        [  6.7996,  -8.1872],
        [  7.4341,  -9.8084],
        [ -7.1052,   4.4951],
        [  6.4350,  -9.1776],
        [  6.5449,  -9.5034],
        [  6.9100,  -8.2979],
        [ -8.9674,   7.2462],
        [ -8.3325,   6.7239],
        [  6.4737,  -7.9097],
        [  5.1722,  -6.6584],
        [  5.8908, -10.3342],
        [ -8.0779,   6.5897],
        [ -8.2272,   6.2264],
        [  6.6640,  -8.1577],
        [ -7.2188,   5.7665],
        [ -7.9578,   6.3209],
        [  5.2009,  -7.3209],
        [ -7.8243,   6.3573],
        [  6.6207,  -8.3771],
        [  6.7751,  -8.2389],
        [  6.5263,  -8.6232],
        [  5.9779,  -8.0612],
        [ -9.3758,   7.1239],
        [  6.6163,  -8.0965],
        [  4.2705,  -5.6738],
        [ -9.3414,   7.1734],
        [  5.9235,  -8.0324],
        [ -7.4917,   6.1019],
        [ -7.3123,   3.5042],
        [  6.8032,  -8.3258],
        [  3.8351,  -5.4188],
        [ -8.5218,   6.6983],
        [  6.3197,  -7.8781],
        [ -6.1626,   4.6322],
        [ -8.8255,   6.3444],
        [  6.9672,  -8.6425],
        [  6.3818,  -9.2236],
        [ -5.3186,   3.4888]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7576, 0.2424],
        [0.2738, 0.7262]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5015, 0.4985], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.0948],
         [0.0253, 0.3979]],

        [[0.6669, 0.0971],
         [0.3362, 0.3461]],

        [[0.1345, 0.0990],
         [0.7981, 0.5405]],

        [[0.4770, 0.1043],
         [0.3060, 0.5168]],

        [[0.7691, 0.0997],
         [0.3076, 0.4123]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919998119331364
Iteration 0: Loss = -22021.294505098664
Iteration 10: Loss = -11448.761875557404
Iteration 20: Loss = -11448.757502686294
Iteration 30: Loss = -11448.757503178267
1
Iteration 40: Loss = -11448.757503178267
2
Iteration 50: Loss = -11448.757503178267
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7245, 0.2755],
        [0.2435, 0.7565]], dtype=torch.float64)
alpha: tensor([0.4772, 0.5228])
beta: tensor([[[0.3897, 0.0948],
         [0.1229, 0.1985]],

        [[0.6254, 0.0975],
         [0.4693, 0.7174]],

        [[0.1714, 0.0984],
         [0.9417, 0.8156]],

        [[0.4198, 0.1045],
         [0.1041, 0.5817]],

        [[0.8959, 0.0997],
         [0.0226, 0.2227]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22021.11656844419
Iteration 100: Loss = -12204.49177670591
Iteration 200: Loss = -12186.298952554815
Iteration 300: Loss = -12180.765863052682
Iteration 400: Loss = -12170.770713788257
Iteration 500: Loss = -12058.124940487487
Iteration 600: Loss = -11862.19015082295
Iteration 700: Loss = -11809.967887314535
Iteration 800: Loss = -11804.050946543703
Iteration 900: Loss = -11778.799639065172
Iteration 1000: Loss = -11773.077391161463
Iteration 1100: Loss = -11767.534505311789
Iteration 1200: Loss = -11766.405587324642
Iteration 1300: Loss = -11766.324795289227
Iteration 1400: Loss = -11766.254352540602
Iteration 1500: Loss = -11766.216228961053
Iteration 1600: Loss = -11766.189119085213
Iteration 1700: Loss = -11766.166658639815
Iteration 1800: Loss = -11766.148809357226
Iteration 1900: Loss = -11766.133700303586
Iteration 2000: Loss = -11766.096087890715
Iteration 2100: Loss = -11761.64269308891
Iteration 2200: Loss = -11761.540020779637
Iteration 2300: Loss = -11761.531314421696
Iteration 2400: Loss = -11761.523111454104
Iteration 2500: Loss = -11761.513074602399
Iteration 2600: Loss = -11761.462903070244
Iteration 2700: Loss = -11761.454774753363
Iteration 2800: Loss = -11761.448979380788
Iteration 2900: Loss = -11760.693100537042
Iteration 3000: Loss = -11758.47421902856
Iteration 3100: Loss = -11758.46784174757
Iteration 3200: Loss = -11758.363663281094
Iteration 3300: Loss = -11750.908581960972
Iteration 3400: Loss = -11750.904924048225
Iteration 3500: Loss = -11750.901459438308
Iteration 3600: Loss = -11750.835569460169
Iteration 3700: Loss = -11750.831030024452
Iteration 3800: Loss = -11750.827425534868
Iteration 3900: Loss = -11739.116003517052
Iteration 4000: Loss = -11739.083925774981
Iteration 4100: Loss = -11739.080546722636
Iteration 4200: Loss = -11739.076079096716
Iteration 4300: Loss = -11739.074236362234
Iteration 4400: Loss = -11739.072933618016
Iteration 4500: Loss = -11739.07214108385
Iteration 4600: Loss = -11739.070753614793
Iteration 4700: Loss = -11739.069817797379
Iteration 4800: Loss = -11739.068920990503
Iteration 4900: Loss = -11739.067999721978
Iteration 5000: Loss = -11739.067872935653
Iteration 5100: Loss = -11739.072261246823
1
Iteration 5200: Loss = -11739.019467407767
Iteration 5300: Loss = -11739.018649173753
Iteration 5400: Loss = -11739.017955156638
Iteration 5500: Loss = -11739.026996757086
1
Iteration 5600: Loss = -11739.016844080972
Iteration 5700: Loss = -11739.016147603083
Iteration 5800: Loss = -11739.016432438095
1
Iteration 5900: Loss = -11739.014475845097
Iteration 6000: Loss = -11739.01431010142
Iteration 6100: Loss = -11739.014833494291
1
Iteration 6200: Loss = -11739.013429271987
Iteration 6300: Loss = -11739.01559925466
1
Iteration 6400: Loss = -11739.047791630175
2
Iteration 6500: Loss = -11739.01196358026
Iteration 6600: Loss = -11739.008579308942
Iteration 6700: Loss = -11736.456641240351
Iteration 6800: Loss = -11736.448640804489
Iteration 6900: Loss = -11736.448646494391
1
Iteration 7000: Loss = -11736.448653625968
2
Iteration 7100: Loss = -11736.448036354273
Iteration 7200: Loss = -11736.447801967863
Iteration 7300: Loss = -11736.447669822017
Iteration 7400: Loss = -11736.449247109844
1
Iteration 7500: Loss = -11736.448012367879
2
Iteration 7600: Loss = -11736.447148524763
Iteration 7700: Loss = -11736.447082282783
Iteration 7800: Loss = -11736.446945655995
Iteration 7900: Loss = -11736.446767077963
Iteration 8000: Loss = -11736.449592543277
1
Iteration 8100: Loss = -11736.446596813115
Iteration 8200: Loss = -11736.44647299077
Iteration 8300: Loss = -11736.450624274858
1
Iteration 8400: Loss = -11736.446333424314
Iteration 8500: Loss = -11736.446188705106
Iteration 8600: Loss = -11736.46824422495
1
Iteration 8700: Loss = -11736.446022755345
Iteration 8800: Loss = -11736.445936891661
Iteration 8900: Loss = -11736.476240843394
1
Iteration 9000: Loss = -11736.445730473186
Iteration 9100: Loss = -11736.446453488428
1
Iteration 9200: Loss = -11736.445321077963
Iteration 9300: Loss = -11736.442507816115
Iteration 9400: Loss = -11736.454563458647
1
Iteration 9500: Loss = -11736.414954308564
Iteration 9600: Loss = -11736.414923510703
Iteration 9700: Loss = -11736.41505205665
1
Iteration 9800: Loss = -11736.427655402851
2
Iteration 9900: Loss = -11736.42637640133
3
Iteration 10000: Loss = -11736.414747869017
Iteration 10100: Loss = -11736.417436460806
1
Iteration 10200: Loss = -11736.415931034093
2
Iteration 10300: Loss = -11736.415052759488
3
Iteration 10400: Loss = -11736.776067119425
4
Iteration 10500: Loss = -11736.414534677142
Iteration 10600: Loss = -11735.17549659712
Iteration 10700: Loss = -11734.87652183215
Iteration 10800: Loss = -11734.8746565089
Iteration 10900: Loss = -11734.91047004673
1
Iteration 11000: Loss = -11734.874591729988
Iteration 11100: Loss = -11734.875514000696
1
Iteration 11200: Loss = -11734.878481333397
2
Iteration 11300: Loss = -11734.874521578198
Iteration 11400: Loss = -11734.89409001957
1
Iteration 11500: Loss = -11734.874441184755
Iteration 11600: Loss = -11734.87860940666
1
Iteration 11700: Loss = -11734.871084532777
Iteration 11800: Loss = -11734.884459377436
1
Iteration 11900: Loss = -11734.871063643699
Iteration 12000: Loss = -11734.87152911557
1
Iteration 12100: Loss = -11734.881613218191
2
Iteration 12200: Loss = -11734.871935015895
3
Iteration 12300: Loss = -11734.696893299359
Iteration 12400: Loss = -11734.67837451238
Iteration 12500: Loss = -11734.89650588886
1
Iteration 12600: Loss = -11734.677518635903
Iteration 12700: Loss = -11734.68565371973
1
Iteration 12800: Loss = -11734.724068773396
2
Iteration 12900: Loss = -11734.677222902092
Iteration 13000: Loss = -11734.678709437305
1
Iteration 13100: Loss = -11734.688932164203
2
Iteration 13200: Loss = -11734.67657336566
Iteration 13300: Loss = -11734.676837133567
1
Iteration 13400: Loss = -11734.68326521597
2
Iteration 13500: Loss = -11734.676405080229
Iteration 13600: Loss = -11734.678766554602
1
Iteration 13700: Loss = -11734.676396580904
Iteration 13800: Loss = -11735.088324987752
1
Iteration 13900: Loss = -11734.675938084198
Iteration 14000: Loss = -11734.697214296702
1
Iteration 14100: Loss = -11734.676230347966
2
Iteration 14200: Loss = -11734.676881253694
3
Iteration 14300: Loss = -11734.676037271674
4
Iteration 14400: Loss = -11734.681821579723
5
Iteration 14500: Loss = -11734.677999714557
6
Iteration 14600: Loss = -11734.677600082709
7
Iteration 14700: Loss = -11734.742008878206
8
Iteration 14800: Loss = -11734.676681616003
9
Iteration 14900: Loss = -11734.677035689934
10
Stopping early at iteration 14900 due to no improvement.
tensor([[  8.0887,  -9.6504],
        [  7.1462,  -8.5338],
        [  4.3573,  -5.9457],
        [ -9.6470,   6.8126],
        [ -9.3916,   7.2979],
        [ -6.1585,   3.8472],
        [ -4.6801,   1.8954],
        [  6.7178, -11.1411],
        [ -4.1696,   2.4900],
        [ -4.5130,   2.6765],
        [ -9.4930,   8.0600],
        [  6.9631,  -8.3597],
        [  7.3996,  -9.0195],
        [ -9.4352,   8.0486],
        [  7.6071, -10.3046],
        [  8.7584, -10.3163],
        [  5.9029,  -8.0903],
        [  5.1790,  -8.3172],
        [  8.0226,  -9.4439],
        [  8.1722, -10.0343],
        [  7.2337,  -8.8237],
        [ -8.9131,   7.1642],
        [  7.2288,  -8.6252],
        [  7.5326,  -9.9826],
        [  6.0804,  -8.0962],
        [ -6.1857,   4.7906],
        [ -9.2500,   7.0295],
        [ -7.5817,   6.0290],
        [ -8.6032,   6.3195],
        [ -5.5743,   4.0382],
        [ -9.7248,   8.0120],
        [ -7.0986,   4.8713],
        [-10.3871,   8.0590],
        [ -9.1540,   7.6154],
        [  3.3728,  -5.1193],
        [ -8.0930,   6.4766],
        [ -7.4579,   3.4494],
        [ -8.0508,   6.3234],
        [  6.3737,  -8.3039],
        [ -8.1811,   6.6736],
        [  8.6231, -10.3146],
        [  9.0120, -10.6951],
        [  7.0963,  -8.7387],
        [  5.8181,  -7.3608],
        [  6.9073,  -8.3821],
        [ -8.8855,   6.9703],
        [ -9.1028,   7.5300],
        [ -3.6904,   1.6013],
        [  7.6079,  -9.0175],
        [  8.8940, -11.3159],
        [  7.0632,  -8.6144],
        [ -8.5773,   6.0482],
        [  2.8840,  -4.7953],
        [  6.4302,  -8.7966],
        [  7.4723,  -8.9799],
        [  6.2934,  -8.7406],
        [  5.1060,  -6.7266],
        [ -6.4845,   4.1875],
        [ -8.0858,   3.9827],
        [  8.4819,  -9.8736],
        [  7.3025,  -9.5703],
        [  7.5995,  -9.1196],
        [ -5.9623,   4.3335],
        [ -7.7846,   6.2639],
        [  3.8623,  -5.3589],
        [ -8.8421,   6.7586],
        [ -4.0096,   2.6160],
        [ -6.6145,   4.9529],
        [  6.9202,  -8.6420],
        [  9.0760, -10.4753],
        [ -5.0841,   2.7357],
        [ -9.1480,   7.7617],
        [ -7.6885,   4.2699],
        [  7.3400,  -8.7600],
        [  7.7189,  -9.6658],
        [ -4.1130,   1.4390],
        [-10.5798,   5.9646],
        [  6.2394,  -9.4569],
        [ -6.6211,   4.6602],
        [  7.8974,  -9.8237],
        [ -5.0144,   3.0091],
        [ -9.3970,   7.2145],
        [ -8.3766,   6.9861],
        [ -6.4875,   4.4051],
        [  6.3505,  -8.6396],
        [ -8.4982,   5.4464],
        [ -9.5115,   7.7721],
        [  6.5406,  -9.2521],
        [ -5.8742,   3.7390],
        [  7.9881,  -9.6996],
        [  4.3006,  -6.3238],
        [ -3.0178,   1.2695],
        [ -8.9562,   7.4597],
        [  8.3709, -10.9887],
        [ -8.9636,   6.1943],
        [  6.8069, -10.0638],
        [  6.5010,  -7.9808],
        [ -7.5183,   5.4248],
        [ -8.1090,   6.5532],
        [  7.0489,  -8.4357]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3538, 0.6462],
        [0.4108, 0.5892]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4904, 0.5096], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3719, 0.1001],
         [0.1229, 0.2299]],

        [[0.6254, 0.0968],
         [0.4693, 0.7174]],

        [[0.1714, 0.1029],
         [0.9417, 0.8156]],

        [[0.4198, 0.1075],
         [0.1041, 0.5817]],

        [[0.8959, 0.0996],
         [0.0226, 0.2227]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 78
Adjusted Rand Index: 0.30727461959311375
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 71
Adjusted Rand Index: 0.16808080808080808
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.15512798192733412
Average Adjusted Rand Index: 0.6790696435918037
11450.026058082021
new:  [0.9920000001562724, 0.9840320648505942, 0.9920000001562724, 0.15512798192733412] [0.9919998119331364, 0.9841616161616162, 0.9919998119331364, 0.6790696435918037] [11447.661362894687, 11473.653360109685, 11447.086440602137, 11734.677035689934]
prior:  [1.0, 1.0, 1.0, 1.0] [1.0, 1.0, 1.0, 1.0] [11448.75752358139, 11448.757522056974, 11448.75752358139, 11448.757503178267]
-----------------------------------------------------------------------------------------
This iteration is 24
True Objective function: Loss = -11606.693255507842
Iteration 0: Loss = -22917.15221465214
Iteration 10: Loss = -12269.162340645564
Iteration 20: Loss = -11918.965040448653
Iteration 30: Loss = -11918.738655887482
Iteration 40: Loss = -11938.634284116793
1
Iteration 50: Loss = -11947.598418889114
2
Iteration 60: Loss = -11949.463526211573
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.3515, 0.6485],
        [0.4941, 0.5059]], dtype=torch.float64)
alpha: tensor([0.4392, 0.5608])
beta: tensor([[[0.3602, 0.1057],
         [0.6341, 0.2354]],

        [[0.6236, 0.0973],
         [0.6904, 0.2722]],

        [[0.3569, 0.1179],
         [0.8714, 0.0466]],

        [[0.8076, 0.1100],
         [0.8214, 0.5677]],

        [[0.7709, 0.0989],
         [0.1059, 0.2652]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 78
Adjusted Rand Index: 0.30591100834848295
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 82
Adjusted Rand Index: 0.40440205429200293
Global Adjusted Rand Index: 0.12216715899467277
Average Adjusted Rand Index: 0.7262228711361198
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23025.2753414951
Iteration 100: Loss = -12416.479225699444
Iteration 200: Loss = -12368.429788906245
Iteration 300: Loss = -12318.239143145955
Iteration 400: Loss = -12202.160928333993
Iteration 500: Loss = -11961.254128412724
Iteration 600: Loss = -11703.27181724326
Iteration 700: Loss = -11666.227995642244
Iteration 800: Loss = -11658.947464157553
Iteration 900: Loss = -11622.376681400328
Iteration 1000: Loss = -11622.207733172747
Iteration 1100: Loss = -11622.09176596461
Iteration 1200: Loss = -11621.901021395588
Iteration 1300: Loss = -11602.494133404076
Iteration 1400: Loss = -11602.434328205103
Iteration 1500: Loss = -11602.39590029238
Iteration 1600: Loss = -11602.368396641987
Iteration 1700: Loss = -11602.346959300614
Iteration 1800: Loss = -11602.329402509391
Iteration 1900: Loss = -11602.314705611343
Iteration 2000: Loss = -11602.302266541774
Iteration 2100: Loss = -11602.29162943886
Iteration 2200: Loss = -11602.282466873361
Iteration 2300: Loss = -11602.274544517482
Iteration 2400: Loss = -11602.267671346426
Iteration 2500: Loss = -11602.261624752426
Iteration 2600: Loss = -11602.256295103753
Iteration 2700: Loss = -11602.251585504288
Iteration 2800: Loss = -11602.247318972737
Iteration 2900: Loss = -11602.243472494893
Iteration 3000: Loss = -11602.24007393086
Iteration 3100: Loss = -11602.259396068805
1
Iteration 3200: Loss = -11602.235856662817
Iteration 3300: Loss = -11602.246151566462
1
Iteration 3400: Loss = -11602.22891380592
Iteration 3500: Loss = -11602.225644767252
Iteration 3600: Loss = -11602.223599037265
Iteration 3700: Loss = -11602.221831172408
Iteration 3800: Loss = -11602.235855175793
1
Iteration 3900: Loss = -11602.218714989363
Iteration 4000: Loss = -11602.217278043781
Iteration 4100: Loss = -11602.249932507257
1
Iteration 4200: Loss = -11602.214806004078
Iteration 4300: Loss = -11602.213653524497
Iteration 4400: Loss = -11602.212673058386
Iteration 4500: Loss = -11602.211650312325
Iteration 4600: Loss = -11602.210751708186
Iteration 4700: Loss = -11602.209990037029
Iteration 4800: Loss = -11602.209501844922
Iteration 4900: Loss = -11602.208473643348
Iteration 5000: Loss = -11602.262888349556
1
Iteration 5100: Loss = -11602.207122405513
Iteration 5200: Loss = -11602.206640940649
Iteration 5300: Loss = -11602.206012114175
Iteration 5400: Loss = -11602.205461822832
Iteration 5500: Loss = -11602.206360853574
1
Iteration 5600: Loss = -11602.204527661977
Iteration 5700: Loss = -11602.204819547866
1
Iteration 5800: Loss = -11602.20367111563
Iteration 5900: Loss = -11602.316739559645
1
Iteration 6000: Loss = -11602.202941869193
Iteration 6100: Loss = -11602.204105371597
1
Iteration 6200: Loss = -11602.2049255816
2
Iteration 6300: Loss = -11602.201968073152
Iteration 6400: Loss = -11602.203924413478
1
Iteration 6500: Loss = -11602.201404096093
Iteration 6600: Loss = -11602.22558846402
1
Iteration 6700: Loss = -11602.205547799147
2
Iteration 6800: Loss = -11602.231970360588
3
Iteration 6900: Loss = -11602.20243978929
4
Iteration 7000: Loss = -11602.250481630546
5
Iteration 7100: Loss = -11602.20016755425
Iteration 7200: Loss = -11602.201291845888
1
Iteration 7300: Loss = -11602.268237159433
2
Iteration 7400: Loss = -11602.205257716081
3
Iteration 7500: Loss = -11602.200364896777
4
Iteration 7600: Loss = -11602.287061978437
5
Iteration 7700: Loss = -11602.200519217826
6
Iteration 7800: Loss = -11602.20097445558
7
Iteration 7900: Loss = -11602.203257004465
8
Iteration 8000: Loss = -11602.199384107236
Iteration 8100: Loss = -11602.198558514625
Iteration 8200: Loss = -11602.204557572813
1
Iteration 8300: Loss = -11602.197900392799
Iteration 8400: Loss = -11602.238435935911
1
Iteration 8500: Loss = -11602.1977200696
Iteration 8600: Loss = -11602.215510125898
1
Iteration 8700: Loss = -11602.203416953947
2
Iteration 8800: Loss = -11602.200830490674
3
Iteration 8900: Loss = -11602.198935184375
4
Iteration 9000: Loss = -11602.20978917965
5
Iteration 9100: Loss = -11602.203440870466
6
Iteration 9200: Loss = -11602.203316634943
7
Iteration 9300: Loss = -11602.202382215572
8
Iteration 9400: Loss = -11602.199769856752
9
Iteration 9500: Loss = -11602.205460280567
10
Stopping early at iteration 9500 due to no improvement.
tensor([[ -9.2781,   4.6629],
        [ -8.7132,   4.0979],
        [-10.1014,   5.4862],
        [-10.3217,   5.7064],
        [  4.6991,  -9.3143],
        [  5.1369,  -9.7522],
        [  3.5447,  -8.1600],
        [ -5.6426,   1.0273],
        [  3.4903,  -8.1055],
        [-10.0272,   5.4120],
        [  3.2216,  -7.8368],
        [ -5.7519,   1.1366],
        [  5.2467,  -9.8619],
        [  5.0475,  -9.6628],
        [ -9.0413,   4.4261],
        [ -8.7381,   4.1229],
        [  4.2784,  -8.8936],
        [  3.7527,  -8.3679],
        [ -8.7491,   4.1339],
        [ -8.7799,   4.1647],
        [ -8.8148,   4.1996],
        [ -8.8464,   4.2312],
        [  5.1432,  -9.7584],
        [  3.2427,  -7.8579],
        [  3.5408,  -8.1560],
        [ -8.4857,   3.8704],
        [-10.0443,   5.4290],
        [  4.8932,  -9.5084],
        [  4.0898,  -8.7050],
        [  4.2770,  -8.8923],
        [ -9.5727,   4.9575],
        [ -8.5244,   3.9092],
        [ -8.4642,   3.8490],
        [ -7.8626,   3.2473],
        [  2.0175,  -6.6328],
        [  3.5826,  -8.1978],
        [  4.5467,  -9.1619],
        [  3.2658,  -7.8810],
        [ -9.0768,   4.4616],
        [ -5.0737,   0.4584],
        [ -9.6707,   5.0555],
        [  4.2470,  -8.8623],
        [  4.0223,  -8.6375],
        [ -9.2018,   4.5865],
        [  4.9765,  -9.5917],
        [  4.1920,  -8.8072],
        [ -9.5108,   4.8955],
        [  3.8086,  -8.4238],
        [  4.9703,  -9.5855],
        [  4.0331,  -8.6483],
        [  0.9923,  -5.6075],
        [ -9.9028,   5.2876],
        [ -5.5196,   0.9044],
        [ -9.0331,   4.4178],
        [  2.3958,  -7.0111],
        [  3.6753,  -8.2906],
        [ -4.5883,  -0.0270],
        [  3.8650,  -8.4802],
        [  2.0550,  -6.6702],
        [  1.1786,  -5.7939],
        [ -9.0881,   4.4728],
        [  4.0065,  -8.6217],
        [ -9.6715,   5.0562],
        [ -9.1628,   4.5475],
        [  3.8088,  -8.4240],
        [  2.0245,  -6.6398],
        [ -9.8834,   5.2682],
        [  3.6460,  -8.2613],
        [ -3.8965,  -0.7187],
        [ -4.7483,   0.1331],
        [ -9.5773,   4.9621],
        [  4.5132,  -9.1284],
        [  4.2705,  -8.8858],
        [  3.5536,  -8.1688],
        [ -7.2582,   2.6430],
        [ -8.4407,   3.8255],
        [ -8.7849,   4.1697],
        [ -9.7688,   5.1535],
        [ -9.1312,   4.5160],
        [ -9.5406,   4.9254],
        [ -8.8955,   4.2803],
        [ -9.3084,   4.6932],
        [  2.7591,  -7.3743],
        [  4.5806,  -9.1959],
        [  5.3042,  -9.9195],
        [ -8.9649,   4.3497],
        [ -9.1246,   4.5094],
        [ -6.7338,   2.1186],
        [  3.6803,  -8.2955],
        [  3.3707,  -7.9859],
        [  3.9377,  -8.5529],
        [  5.0991,  -9.7143],
        [  3.5150,  -8.1302],
        [ -9.3478,   4.7326],
        [  1.8676,  -6.4828],
        [ -9.1118,   4.4966],
        [  3.8182,  -8.4334],
        [ -9.3477,   4.7325],
        [  4.6822,  -9.2974],
        [  2.2310,  -6.8462]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7634, 0.2366],
        [0.2340, 0.7660]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5103, 0.4897], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1961, 0.1058],
         [0.6341, 0.4106]],

        [[0.6236, 0.1002],
         [0.6904, 0.2722]],

        [[0.3569, 0.0990],
         [0.8714, 0.0466]],

        [[0.8076, 0.1112],
         [0.8214, 0.5677]],

        [[0.7709, 0.1026],
         [0.1059, 0.2652]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -22276.89948083025
Iteration 10: Loss = -11603.905862921907
Iteration 20: Loss = -11603.905833316068
Iteration 30: Loss = -11603.905833316068
1
Iteration 40: Loss = -11603.905833316068
2
Iteration 50: Loss = -11603.905833316068
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7644, 0.2356],
        [0.2354, 0.7646]], dtype=torch.float64)
alpha: tensor([0.5095, 0.4905])
beta: tensor([[[0.1923, 0.1063],
         [0.2437, 0.4019]],

        [[0.1332, 0.0997],
         [0.3993, 0.3304]],

        [[0.8676, 0.0991],
         [0.9906, 0.1337]],

        [[0.8697, 0.1109],
         [0.5207, 0.5468]],

        [[0.2607, 0.1027],
         [0.7422, 0.4687]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22276.788403283834
Iteration 100: Loss = -12405.501126240866
Iteration 200: Loss = -12283.55110679061
Iteration 300: Loss = -12277.549900360413
Iteration 400: Loss = -12269.430263891307
Iteration 500: Loss = -12265.873424815825
Iteration 600: Loss = -12263.567507542348
Iteration 700: Loss = -12262.652475160585
Iteration 800: Loss = -12260.445619916747
Iteration 900: Loss = -12252.455738707838
Iteration 1000: Loss = -12214.472161672176
Iteration 1100: Loss = -12099.859448297675
Iteration 1200: Loss = -12056.538587805862
Iteration 1300: Loss = -12053.728746864048
Iteration 1400: Loss = -11958.9734556927
Iteration 1500: Loss = -11709.095185449136
Iteration 1600: Loss = -11693.552864278932
Iteration 1700: Loss = -11685.791637392558
Iteration 1800: Loss = -11685.679886908665
Iteration 1900: Loss = -11685.606293039531
Iteration 2000: Loss = -11685.548005443305
Iteration 2100: Loss = -11676.190003535134
Iteration 2200: Loss = -11676.158440036797
Iteration 2300: Loss = -11676.133898075255
Iteration 2400: Loss = -11676.114501475362
Iteration 2500: Loss = -11676.09533945327
Iteration 2600: Loss = -11676.073819270057
Iteration 2700: Loss = -11676.075420617626
1
Iteration 2800: Loss = -11676.028415060335
Iteration 2900: Loss = -11676.001474013732
Iteration 3000: Loss = -11676.145681482314
1
Iteration 3100: Loss = -11675.987232687745
Iteration 3200: Loss = -11675.98133092967
Iteration 3300: Loss = -11675.97608147774
Iteration 3400: Loss = -11675.971342113178
Iteration 3500: Loss = -11675.967062191481
Iteration 3600: Loss = -11675.96323094504
Iteration 3700: Loss = -11675.961093137956
Iteration 3800: Loss = -11675.956520567024
Iteration 3900: Loss = -11675.953655605494
Iteration 4000: Loss = -11675.954408200223
1
Iteration 4100: Loss = -11675.948668062782
Iteration 4200: Loss = -11675.946983197051
Iteration 4300: Loss = -11675.94808522032
1
Iteration 4400: Loss = -11675.942743874171
Iteration 4500: Loss = -11675.940840589587
Iteration 4600: Loss = -11675.940745424696
Iteration 4700: Loss = -11675.937776769822
Iteration 4800: Loss = -11675.936433161458
Iteration 4900: Loss = -11675.934922151078
Iteration 5000: Loss = -11675.933654603406
Iteration 5100: Loss = -11675.932414305329
Iteration 5200: Loss = -11675.93139948283
Iteration 5300: Loss = -11675.93030525079
Iteration 5400: Loss = -11675.939856556506
1
Iteration 5500: Loss = -11675.92819313235
Iteration 5600: Loss = -11675.92596668083
Iteration 5700: Loss = -11665.56928179221
Iteration 5800: Loss = -11665.562962391017
Iteration 5900: Loss = -11654.2860444193
Iteration 6000: Loss = -11654.273372330123
Iteration 6100: Loss = -11654.275455502171
1
Iteration 6200: Loss = -11654.272137962673
Iteration 6300: Loss = -11654.27162484461
Iteration 6400: Loss = -11654.273363124792
1
Iteration 6500: Loss = -11654.271152802987
Iteration 6600: Loss = -11654.27064611168
Iteration 6700: Loss = -11654.290602419427
1
Iteration 6800: Loss = -11654.27054256933
Iteration 6900: Loss = -11654.270268209679
Iteration 7000: Loss = -11654.285610033956
1
Iteration 7100: Loss = -11654.293563173855
2
Iteration 7200: Loss = -11654.27723306891
3
Iteration 7300: Loss = -11654.357943862971
4
Iteration 7400: Loss = -11654.267596506554
Iteration 7500: Loss = -11654.267165942612
Iteration 7600: Loss = -11654.272298859394
1
Iteration 7700: Loss = -11654.270030092544
2
Iteration 7800: Loss = -11654.267711590319
3
Iteration 7900: Loss = -11654.266281979711
Iteration 8000: Loss = -11654.267443705874
1
Iteration 8100: Loss = -11654.265955765224
Iteration 8200: Loss = -11654.26596148678
1
Iteration 8300: Loss = -11654.266108524118
2
Iteration 8400: Loss = -11654.281232032967
3
Iteration 8500: Loss = -11654.266175185747
4
Iteration 8600: Loss = -11654.271087952997
5
Iteration 8700: Loss = -11654.267061449118
6
Iteration 8800: Loss = -11654.316189152154
7
Iteration 8900: Loss = -11654.288699976449
8
Iteration 9000: Loss = -11654.29951406907
9
Iteration 9100: Loss = -11654.291808512682
10
Stopping early at iteration 9100 due to no improvement.
tensor([[-6.9879,  5.1384],
        [-5.7247,  3.4391],
        [-7.7526,  6.2500],
        [-8.7555,  5.3014],
        [ 3.8057, -5.1937],
        [ 5.1878, -6.6063],
        [ 4.8811, -6.4404],
        [-4.5636,  2.4975],
        [ 4.5828, -6.1422],
        [-8.9086,  6.3889],
        [ 3.5018, -7.1141],
        [-4.5634,  2.9645],
        [ 5.0196, -6.6629],
        [ 5.2523, -6.9211],
        [-7.3282,  5.8769],
        [-7.3057,  4.5077],
        [ 5.1541, -6.5407],
        [ 4.9185, -7.0599],
        [-8.6625,  5.2574],
        [-6.8917,  4.8164],
        [-6.9146,  5.4824],
        [-7.1190,  5.3798],
        [ 4.3678, -6.8312],
        [ 3.1964, -5.1681],
        [ 4.3118, -5.7693],
        [-6.4875,  5.0599],
        [-7.8254,  6.1156],
        [ 5.5173, -7.6987],
        [ 5.2947, -8.3443],
        [ 4.7633, -6.6358],
        [-7.0362,  5.6348],
        [-5.4713,  4.0772],
        [-6.8457,  4.4347],
        [-6.6910,  4.6418],
        [ 3.0556, -4.7893],
        [ 4.7293, -6.1969],
        [ 5.0409, -6.6374],
        [ 4.2345, -6.0465],
        [-9.5365,  5.9035],
        [-4.3720,  1.4137],
        [-8.3738,  6.3864],
        [ 4.7927, -6.6532],
        [ 5.3140, -6.7003],
        [-7.5870,  6.0984],
        [ 4.2382, -7.1457],
        [ 5.2454, -6.6536],
        [-6.9987,  5.5141],
        [ 5.0748, -6.7691],
        [ 4.1750, -7.4397],
        [ 4.3295, -8.9447],
        [ 1.8518, -4.0773],
        [-8.1323,  6.0953],
        [-4.2819,  2.6184],
        [-4.8494,  3.0506],
        [ 3.5914, -5.0057],
        [ 4.1396, -6.8332],
        [-3.1338,  1.7323],
        [ 5.3397, -7.1893],
        [ 2.4092, -5.4717],
        [ 2.0010, -4.4389],
        [-6.9847,  5.2148],
        [ 5.5475, -7.6447],
        [-7.5429,  5.1837],
        [-7.3829,  5.9691],
        [ 5.1759, -6.6210],
        [ 2.7925, -5.0607],
        [-7.2744,  5.6058],
        [ 4.1130, -6.9455],
        [-2.7561,  0.9164],
        [-3.3895,  1.6892],
        [-7.8349,  5.9537],
        [ 5.3738, -6.7821],
        [ 5.7520, -8.4390],
        [ 4.9358, -6.5063],
        [-5.8578,  4.1550],
        [-6.4953,  5.0797],
        [-7.4444,  5.6238],
        [-8.0028,  6.2213],
        [-7.1936,  5.4603],
        [-8.2728,  4.2190],
        [-7.4919,  4.7752],
        [-8.2279,  5.6747],
        [ 3.9339, -5.4366],
        [ 5.4332, -6.8209],
        [ 5.0601, -6.5107],
        [-8.6830,  5.9954],
        [-7.1204,  5.1002],
        [-5.5886,  3.4457],
        [ 4.7776, -6.3943],
        [ 4.3052, -5.8671],
        [ 5.1897, -6.5972],
        [ 4.0184, -5.7514],
        [ 4.6988, -6.1945],
        [-7.2785,  5.5981],
        [ 3.7389, -5.3170],
        [-6.6240,  5.1751],
        [ 4.9382, -8.1397],
        [-7.2053,  5.5309],
        [ 5.8039, -7.2926],
        [ 3.3622, -5.0145]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7384, 0.2616],
        [0.2498, 0.7502]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5104, 0.4896], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.1058],
         [0.2437, 0.3969]],

        [[0.1332, 0.1005],
         [0.3993, 0.3304]],

        [[0.8676, 0.1004],
         [0.9906, 0.1337]],

        [[0.8697, 0.1071],
         [0.5207, 0.5468]],

        [[0.2607, 0.1026],
         [0.7422, 0.4687]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448543354594036
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446731746254289
Average Adjusted Rand Index: 0.9451299031072133
Iteration 0: Loss = -16852.318434161498
Iteration 10: Loss = -11603.905840538597
Iteration 20: Loss = -11603.905833316068
Iteration 30: Loss = -11603.905833316068
1
Iteration 40: Loss = -11603.905833316068
2
Iteration 50: Loss = -11603.905833316068
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7644, 0.2356],
        [0.2354, 0.7646]], dtype=torch.float64)
alpha: tensor([0.5095, 0.4905])
beta: tensor([[[0.1923, 0.1063],
         [0.6143, 0.4019]],

        [[0.5143, 0.0997],
         [0.9261, 0.7692]],

        [[0.3840, 0.0991],
         [0.8228, 0.8796]],

        [[0.5660, 0.1109],
         [0.6650, 0.5981]],

        [[0.8449, 0.1027],
         [0.6327, 0.1564]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16851.902681717464
Iteration 100: Loss = -12309.545151678092
Iteration 200: Loss = -11632.981159308278
Iteration 300: Loss = -11621.11867018378
Iteration 400: Loss = -11616.377122801327
Iteration 500: Loss = -11616.15697194496
Iteration 600: Loss = -11616.03576366349
Iteration 700: Loss = -11615.958867011654
Iteration 800: Loss = -11615.906076676725
Iteration 900: Loss = -11615.867376704511
Iteration 1000: Loss = -11615.836542413113
Iteration 1100: Loss = -11615.808591548188
Iteration 1200: Loss = -11615.789665715922
Iteration 1300: Loss = -11615.776154373678
Iteration 1400: Loss = -11615.765091372112
Iteration 1500: Loss = -11615.755863957465
Iteration 1600: Loss = -11615.747917960474
Iteration 1700: Loss = -11615.740895735924
Iteration 1800: Loss = -11615.73193163633
Iteration 1900: Loss = -11612.837167478549
Iteration 2000: Loss = -11612.831491683673
Iteration 2100: Loss = -11612.827411946037
Iteration 2200: Loss = -11602.27682712776
Iteration 2300: Loss = -11602.259896722077
Iteration 2400: Loss = -11602.256889257013
Iteration 2500: Loss = -11602.254424638924
Iteration 2600: Loss = -11602.252270336268
Iteration 2700: Loss = -11602.250356858905
Iteration 2800: Loss = -11602.24869632602
Iteration 2900: Loss = -11602.247166284538
Iteration 3000: Loss = -11602.245754014073
Iteration 3100: Loss = -11602.244487072478
Iteration 3200: Loss = -11602.243312709397
Iteration 3300: Loss = -11602.242156303519
Iteration 3400: Loss = -11602.240886030246
Iteration 3500: Loss = -11602.239014006751
Iteration 3600: Loss = -11602.19953849418
Iteration 3700: Loss = -11602.198731887012
Iteration 3800: Loss = -11602.201545877364
1
Iteration 3900: Loss = -11602.197438497631
Iteration 4000: Loss = -11602.196848558508
Iteration 4100: Loss = -11602.196338391579
Iteration 4200: Loss = -11602.195871116304
Iteration 4300: Loss = -11602.195420958971
Iteration 4400: Loss = -11602.194940848374
Iteration 4500: Loss = -11602.195181099823
1
Iteration 4600: Loss = -11602.194148261415
Iteration 4700: Loss = -11602.194134686535
Iteration 4800: Loss = -11602.193425166926
Iteration 4900: Loss = -11602.193145562334
Iteration 5000: Loss = -11602.194730276009
1
Iteration 5100: Loss = -11602.192602536357
Iteration 5200: Loss = -11602.192399225753
Iteration 5300: Loss = -11602.19307770177
1
Iteration 5400: Loss = -11602.191969961983
Iteration 5500: Loss = -11602.191872178073
Iteration 5600: Loss = -11602.192222939482
1
Iteration 5700: Loss = -11602.191600458116
Iteration 5800: Loss = -11602.208776532385
1
Iteration 5900: Loss = -11602.235168588184
2
Iteration 6000: Loss = -11602.191911478185
3
Iteration 6100: Loss = -11602.190850429053
Iteration 6200: Loss = -11602.19080505052
Iteration 6300: Loss = -11602.197552332533
1
Iteration 6400: Loss = -11602.201825099448
2
Iteration 6500: Loss = -11602.193180404534
3
Iteration 6600: Loss = -11602.194044551017
4
Iteration 6700: Loss = -11602.190757033637
Iteration 6800: Loss = -11602.192922066495
1
Iteration 6900: Loss = -11602.192581305972
2
Iteration 7000: Loss = -11602.196484422297
3
Iteration 7100: Loss = -11602.192597616506
4
Iteration 7200: Loss = -11602.194781530046
5
Iteration 7300: Loss = -11602.190798862677
6
Iteration 7400: Loss = -11602.189896687829
Iteration 7500: Loss = -11602.191737616056
1
Iteration 7600: Loss = -11602.328262098643
2
Iteration 7700: Loss = -11602.189950505299
3
Iteration 7800: Loss = -11602.191519132944
4
Iteration 7900: Loss = -11602.191316713841
5
Iteration 8000: Loss = -11602.255432171476
6
Iteration 8100: Loss = -11602.191018349979
7
Iteration 8200: Loss = -11602.19937168378
8
Iteration 8300: Loss = -11602.188818078639
Iteration 8400: Loss = -11602.224845676228
1
Iteration 8500: Loss = -11602.188655214164
Iteration 8600: Loss = -11602.196893175818
1
Iteration 8700: Loss = -11602.203921583983
2
Iteration 8800: Loss = -11602.203160398145
3
Iteration 8900: Loss = -11602.189491244184
4
Iteration 9000: Loss = -11602.188759994686
5
Iteration 9100: Loss = -11602.193434544117
6
Iteration 9200: Loss = -11602.201009828106
7
Iteration 9300: Loss = -11602.199657215828
8
Iteration 9400: Loss = -11602.213083793205
9
Iteration 9500: Loss = -11602.190112872386
10
Stopping early at iteration 9500 due to no improvement.
tensor([[ -7.8328,   6.3421],
        [ -5.2656,   3.3763],
        [ -9.0143,   6.6547],
        [ -8.7915,   7.2765],
        [  2.6950,  -7.3103],
        [  5.0999,  -9.7151],
        [  5.7321,  -7.1639],
        [ -4.0341,   2.6360],
        [  5.0249,  -6.6524],
        [ -8.6640,   7.1309],
        [  4.8563,  -6.4235],
        [ -4.6266,   2.2450],
        [  6.1709,  -7.6286],
        [  6.6307,  -8.1424],
        [ -8.0831,   6.6412],
        [ -7.6289,   6.2385],
        [  6.2857,  -7.6845],
        [  5.6149,  -7.5234],
        [ -8.7739,   6.3642],
        [ -8.4848,   5.9574],
        [ -8.2510,   5.6176],
        [ -7.4338,   5.7721],
        [  4.5516,  -7.6596],
        [  4.9692,  -6.5012],
        [  5.3536,  -7.2058],
        [ -8.0684,   4.9688],
        [ -8.4083,   6.7907],
        [  6.9624,  -8.5254],
        [  6.5565,  -8.0487],
        [  5.9651,  -8.1363],
        [ -9.9759,   5.4263],
        [ -5.5105,   3.7401],
        [ -6.8782,   5.4030],
        [ -7.5350,   3.5838],
        [  2.8078,  -5.8424],
        [  4.8595,  -7.9676],
        [  6.8693,  -8.3813],
        [  4.8767,  -6.2771],
        [ -8.2745,   6.5835],
        [ -3.7521,   1.7800],
        [ -9.5925,   6.4274],
        [  5.8744,  -8.2245],
        [  5.2636,  -9.8788],
        [ -8.2938,   6.8570],
        [  6.5511,  -7.9674],
        [  6.0522,  -7.7069],
        [ -7.7800,   6.3613],
        [  5.5584,  -7.7669],
        [  6.6982,  -8.1006],
        [  6.7158,  -8.7778],
        [  2.3968,  -4.2040],
        [ -9.6558,   6.7411],
        [ -3.9341,   2.4912],
        [ -4.4995,   3.0298],
        [  3.8293,  -5.5766],
        [  5.8395,  -7.3293],
        [ -3.0389,   1.5213],
        [  6.1768,  -8.2945],
        [  3.2840,  -5.4416],
        [  2.7652,  -4.2086],
        [ -7.0576,   5.6711],
        [  6.4593,  -8.2581],
        [ -9.8868,   5.6310],
        [-10.3064,   6.0699],
        [  6.0038,  -7.9625],
        [  3.6172,  -5.0486],
        [ -8.0214,   6.6182],
        [  5.8032,  -7.2010],
        [ -2.4075,   0.7673],
        [ -3.2961,   1.5843],
        [ -9.0115,   6.3707],
        [  6.7805,  -8.4342],
        [  6.6961,  -8.0982],
        [  5.7717,  -7.1652],
        [ -7.5633,   6.1580],
        [ -7.5960,   5.9340],
        [ -7.8393,   6.0471],
        [ -8.3706,   6.8285],
        [ -8.6639,   6.9053],
        [ -8.5767,   6.2667],
        [ -7.8130,   6.3470],
        [ -7.9539,   6.5669],
        [  3.8380,  -6.3309],
        [  6.4265,  -7.8326],
        [  6.1504,  -7.6331],
        [ -7.9769,   6.5231],
        [ -7.8180,   6.2638],
        [ -5.1262,   3.7291],
        [  5.0058,  -7.3094],
        [  5.1759,  -6.6050],
        [  6.4658,  -8.2535],
        [  4.6621,  -6.0941],
        [  5.5042,  -7.0948],
        [ -8.5090,   6.8234],
        [  3.2555,  -5.0935],
        [ -7.0523,   5.6290],
        [  6.4465,  -7.8918],
        [ -7.5728,   6.1633],
        [  6.8975,  -8.9228],
        [  3.7522,  -5.3260]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7682, 0.2318],
        [0.2342, 0.7658]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5080, 0.4920], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1960, 0.1058],
         [0.6143, 0.4106]],

        [[0.5143, 0.1002],
         [0.9261, 0.7692]],

        [[0.3840, 0.0990],
         [0.8228, 0.8796]],

        [[0.5660, 0.1110],
         [0.6650, 0.5981]],

        [[0.8449, 0.1027],
         [0.6327, 0.1564]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -23344.022061216976
Iteration 10: Loss = -11615.161710305165
Iteration 20: Loss = -11603.90583458135
Iteration 30: Loss = -11603.905834472018
Iteration 40: Loss = -11603.905834472018
1
Iteration 50: Loss = -11603.905834472018
2
Iteration 60: Loss = -11603.905834472018
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7646, 0.2354],
        [0.2356, 0.7644]], dtype=torch.float64)
alpha: tensor([0.4905, 0.5095])
beta: tensor([[[0.4019, 0.1063],
         [0.0729, 0.1923]],

        [[0.9089, 0.0997],
         [0.4291, 0.0834]],

        [[0.8318, 0.0991],
         [0.4700, 0.6067]],

        [[0.2176, 0.1109],
         [0.8071, 0.8655]],

        [[0.6381, 0.1027],
         [0.7137, 0.1467]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23343.495284814348
Iteration 100: Loss = -12211.949410621695
Iteration 200: Loss = -11921.041496713027
Iteration 300: Loss = -11910.360739048112
Iteration 400: Loss = -11905.716421356874
Iteration 500: Loss = -11893.142392912021
Iteration 600: Loss = -11889.28277723184
Iteration 700: Loss = -11883.71622315866
Iteration 800: Loss = -11881.661205266855
Iteration 900: Loss = -11880.899258968006
Iteration 1000: Loss = -11877.7564672036
Iteration 1100: Loss = -11876.052975326105
Iteration 1200: Loss = -11875.843965371803
Iteration 1300: Loss = -11875.802797713246
Iteration 1400: Loss = -11875.769696427731
Iteration 1500: Loss = -11875.74989848253
Iteration 1600: Loss = -11875.722760281838
Iteration 1700: Loss = -11871.39242639438
Iteration 1800: Loss = -11871.265250691928
Iteration 1900: Loss = -11871.25324682963
Iteration 2000: Loss = -11871.245018885898
Iteration 2100: Loss = -11871.238384836874
Iteration 2200: Loss = -11871.232762916652
Iteration 2300: Loss = -11871.22792310328
Iteration 2400: Loss = -11871.223638835048
Iteration 2500: Loss = -11871.219951101097
Iteration 2600: Loss = -11871.216460738797
Iteration 2700: Loss = -11871.213258733105
Iteration 2800: Loss = -11871.210476987013
Iteration 2900: Loss = -11871.207489353234
Iteration 3000: Loss = -11871.205161372323
Iteration 3100: Loss = -11871.203744995602
Iteration 3200: Loss = -11871.201596036992
Iteration 3300: Loss = -11871.200110843285
Iteration 3400: Loss = -11871.199286827583
Iteration 3500: Loss = -11871.197458392418
Iteration 3600: Loss = -11871.196294096211
Iteration 3700: Loss = -11871.19674044515
1
Iteration 3800: Loss = -11871.194312176216
Iteration 3900: Loss = -11871.1933986252
Iteration 4000: Loss = -11871.193254904218
Iteration 4100: Loss = -11871.223579101303
1
Iteration 4200: Loss = -11871.191112967512
Iteration 4300: Loss = -11871.19043067539
Iteration 4400: Loss = -11871.189797571422
Iteration 4500: Loss = -11871.189145320728
Iteration 4600: Loss = -11871.188569307213
Iteration 4700: Loss = -11871.188070248696
Iteration 4800: Loss = -11871.18874710475
1
Iteration 4900: Loss = -11871.187029001283
Iteration 5000: Loss = -11871.187610928866
1
Iteration 5100: Loss = -11871.143595994883
Iteration 5200: Loss = -11871.134086839984
Iteration 5300: Loss = -11871.133612628248
Iteration 5400: Loss = -11871.135889032901
1
Iteration 5500: Loss = -11871.132985692304
Iteration 5600: Loss = -11871.132752952659
Iteration 5700: Loss = -11871.13304638576
1
Iteration 5800: Loss = -11871.132853868894
2
Iteration 5900: Loss = -11871.13193729235
Iteration 6000: Loss = -11871.131807573825
Iteration 6100: Loss = -11871.132245906107
1
Iteration 6200: Loss = -11871.133103456537
2
Iteration 6300: Loss = -11871.134718603538
3
Iteration 6400: Loss = -11871.130765012675
Iteration 6500: Loss = -11870.991876502729
Iteration 6600: Loss = -11870.989683059252
Iteration 6700: Loss = -11870.989499594029
Iteration 6800: Loss = -11870.989492173163
Iteration 6900: Loss = -11870.98973063525
1
Iteration 7000: Loss = -11871.001557417823
2
Iteration 7100: Loss = -11870.98987676004
3
Iteration 7200: Loss = -11870.996246571502
4
Iteration 7300: Loss = -11870.988914601743
Iteration 7400: Loss = -11870.988964877752
1
Iteration 7500: Loss = -11870.99991288999
2
Iteration 7600: Loss = -11870.990175471416
3
Iteration 7700: Loss = -11870.989196723247
4
Iteration 7800: Loss = -11870.988256979996
Iteration 7900: Loss = -11870.988426663236
1
Iteration 8000: Loss = -11870.990576149663
2
Iteration 8100: Loss = -11867.874106588695
Iteration 8200: Loss = -11867.868931088615
Iteration 8300: Loss = -11867.868628241544
Iteration 8400: Loss = -11867.86814006104
Iteration 8500: Loss = -11867.873683531107
1
Iteration 8600: Loss = -11867.86889368293
2
Iteration 8700: Loss = -11867.87716830838
3
Iteration 8800: Loss = -11867.867983179558
Iteration 8900: Loss = -11867.867519556523
Iteration 9000: Loss = -11867.965443162977
1
Iteration 9100: Loss = -11867.867448223034
Iteration 9200: Loss = -11868.04906203318
1
Iteration 9300: Loss = -11867.867533193335
2
Iteration 9400: Loss = -11867.875894640485
3
Iteration 9500: Loss = -11867.870287518483
4
Iteration 9600: Loss = -11867.86783336569
5
Iteration 9700: Loss = -11867.867856664367
6
Iteration 9800: Loss = -11867.871001446118
7
Iteration 9900: Loss = -11867.873245672334
8
Iteration 10000: Loss = -11867.867067491205
Iteration 10100: Loss = -11867.869555148292
1
Iteration 10200: Loss = -11867.869550316866
2
Iteration 10300: Loss = -11867.916776278582
3
Iteration 10400: Loss = -11867.86835278531
4
Iteration 10500: Loss = -11867.866993667578
Iteration 10600: Loss = -11867.873157206115
1
Iteration 10700: Loss = -11867.866303176685
Iteration 10800: Loss = -11867.876072924406
1
Iteration 10900: Loss = -11867.91376338658
2
Iteration 11000: Loss = -11867.8661277865
Iteration 11100: Loss = -11867.866498142017
1
Iteration 11200: Loss = -11867.866837786645
2
Iteration 11300: Loss = -11867.866163325118
3
Iteration 11400: Loss = -11867.868824059044
4
Iteration 11500: Loss = -11867.908046229755
5
Iteration 11600: Loss = -11867.866449954396
6
Iteration 11700: Loss = -11867.86852252276
7
Iteration 11800: Loss = -11867.908095431936
8
Iteration 11900: Loss = -11868.01653961345
9
Iteration 12000: Loss = -11867.872199041638
10
Stopping early at iteration 12000 due to no improvement.
tensor([[-7.6785,  6.1181],
        [-7.1084,  5.2160],
        [-9.0142,  7.5677],
        [-8.6631,  7.0359],
        [-1.6781,  0.2909],
        [ 3.3054, -5.0383],
        [-3.9642,  1.5224],
        [-9.1573,  4.8113],
        [-6.6680,  5.0318],
        [-8.5358,  7.1493],
        [-4.6423,  1.5744],
        [-6.7225,  4.5659],
        [ 3.8510, -5.3966],
        [ 5.5715, -7.0098],
        [-7.7440,  6.2557],
        [-9.6649,  6.8889],
        [-3.1176,  0.1971],
        [-3.1740,  1.6206],
        [-9.0260,  7.5914],
        [-8.2498,  6.7811],
        [-9.2727,  7.3115],
        [-7.6918,  5.6708],
        [-0.2516, -1.4505],
        [ 2.0210, -4.2661],
        [-3.2705,  1.7694],
        [-7.5316,  6.1252],
        [-8.3667,  6.7635],
        [-1.6969,  0.2694],
        [-9.3567,  7.1073],
        [ 0.5928, -2.5879],
        [-8.5816,  7.1709],
        [-6.4809,  4.9536],
        [-7.7079,  6.2908],
        [-5.8673,  4.0314],
        [-1.2730, -0.3458],
        [ 0.1237, -3.4832],
        [ 4.1748, -6.0391],
        [-4.2947,  1.5630],
        [-9.3810,  7.3945],
        [-8.3511,  6.8468],
        [-8.4311,  6.9722],
        [-1.1382, -1.3661],
        [ 1.2497, -3.7282],
        [-9.2487,  7.0915],
        [-0.7120, -0.6912],
        [ 2.0364, -4.1512],
        [-9.6113,  7.0931],
        [-1.9854,  0.4654],
        [-2.5895,  1.0829],
        [ 2.1456, -3.5601],
        [-1.6269,  0.1783],
        [-8.3925,  7.0037],
        [-8.7411,  6.9135],
        [-6.9144,  5.5122],
        [-4.3917,  2.7893],
        [ 0.3445, -2.1941],
        [-5.4374,  3.9600],
        [-4.5802,  3.1087],
        [-5.3717,  1.8392],
        [ 0.7668, -2.3234],
        [-8.0036,  4.7279],
        [ 0.7397, -2.1918],
        [-9.0271,  7.3613],
        [-8.6835,  7.2061],
        [ 3.1936, -5.3735],
        [-4.4537,  2.9544],
        [-9.7541,  7.0517],
        [-2.3423,  0.2597],
        [-5.8978,  4.4790],
        [-5.8482,  4.2489],
        [-8.3266,  6.4200],
        [ 2.2623, -3.8980],
        [ 2.1530, -4.1116],
        [-5.4038,  2.6823],
        [-6.7253,  5.3367],
        [-9.4335,  7.5441],
        [-7.6192,  6.0481],
        [-9.7130,  7.0004],
        [-9.0016,  7.0409],
        [-7.7725,  5.9934],
        [-8.2496,  6.6421],
        [-8.2186,  6.8044],
        [-6.1236,  3.3710],
        [ 1.1656, -2.8354],
        [ 1.8028, -3.2245],
        [-9.1771,  6.5194],
        [-8.4789,  6.9436],
        [-4.8730,  3.2371],
        [-4.8019,  2.9432],
        [-2.6810,  0.7743],
        [-3.0328,  1.2503],
        [-4.2333,  0.9129],
        [ 1.6915, -3.1699],
        [-7.9687,  6.3665],
        [-1.9019,  0.1244],
        [-8.4771,  6.4692],
        [-2.4007,  0.6197],
        [-8.5286,  7.0715],
        [ 1.9914, -3.5271],
        [-3.9815,  2.5224]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3937, 0.6063],
        [0.5166, 0.4834]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2265, 0.7735], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3969, 0.1060],
         [0.0729, 0.2215]],

        [[0.9089, 0.0984],
         [0.4291, 0.0834]],

        [[0.8318, 0.1080],
         [0.4700, 0.6067]],

        [[0.2176, 0.1106],
         [0.8071, 0.8655]],

        [[0.6381, 0.1027],
         [0.7137, 0.1467]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 71
Adjusted Rand Index: 0.1705473823297185
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 71
Adjusted Rand Index: 0.16587158053629153
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.18158518943707194
Average Adjusted Rand Index: 0.6592830089029313
Iteration 0: Loss = -29285.155101626788
Iteration 10: Loss = -12419.851423526767
Iteration 20: Loss = -12419.849801199734
Iteration 30: Loss = -11635.91509884038
Iteration 40: Loss = -11603.905832663782
Iteration 50: Loss = -11603.905834472018
1
Iteration 60: Loss = -11603.905834472018
2
Iteration 70: Loss = -11603.905834472018
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7646, 0.2354],
        [0.2356, 0.7644]], dtype=torch.float64)
alpha: tensor([0.4905, 0.5095])
beta: tensor([[[0.4019, 0.1063],
         [0.3860, 0.1923]],

        [[0.6218, 0.0997],
         [0.2976, 0.2585]],

        [[0.8741, 0.0991],
         [0.0509, 0.4110]],

        [[0.1215, 0.1109],
         [0.0263, 0.2415]],

        [[0.9477, 0.1027],
         [0.0859, 0.7380]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29282.903583987936
Iteration 100: Loss = -12442.878009551454
Iteration 200: Loss = -12424.713327365625
Iteration 300: Loss = -12421.295607230242
Iteration 400: Loss = -12406.709855409079
Iteration 500: Loss = -12358.754029901345
Iteration 600: Loss = -12279.468014940907
Iteration 700: Loss = -12197.621573824199
Iteration 800: Loss = -12131.38775389572
Iteration 900: Loss = -12076.822948670053
Iteration 1000: Loss = -12036.256591159989
Iteration 1100: Loss = -12023.133052435765
Iteration 1200: Loss = -12008.751355925664
Iteration 1300: Loss = -12007.954386204914
Iteration 1400: Loss = -12007.358529435696
Iteration 1500: Loss = -12007.089116428675
Iteration 1600: Loss = -12006.838420524171
Iteration 1700: Loss = -12006.47995921962
Iteration 1800: Loss = -12003.624067096547
Iteration 1900: Loss = -12003.323256878968
Iteration 2000: Loss = -12002.735606071099
Iteration 2100: Loss = -12002.325583797749
Iteration 2200: Loss = -12000.41627643269
Iteration 2300: Loss = -11999.975558971528
Iteration 2400: Loss = -11997.574930783609
Iteration 2500: Loss = -11997.387993952065
Iteration 2600: Loss = -11994.171627568827
Iteration 2700: Loss = -11993.967979087305
Iteration 2800: Loss = -11991.521799723501
Iteration 2900: Loss = -11983.963605915347
Iteration 3000: Loss = -11982.299917720085
Iteration 3100: Loss = -11981.535334066299
Iteration 3200: Loss = -11981.28415265396
Iteration 3300: Loss = -11980.944622500989
Iteration 3400: Loss = -11980.151008328889
Iteration 3500: Loss = -11979.745453995922
Iteration 3600: Loss = -11979.590160314834
Iteration 3700: Loss = -11977.390255636366
Iteration 3800: Loss = -11977.312200368791
Iteration 3900: Loss = -11977.288927307025
Iteration 4000: Loss = -11977.276149871052
Iteration 4100: Loss = -11977.266115453054
Iteration 4200: Loss = -11977.25590233575
Iteration 4300: Loss = -11977.246709293437
Iteration 4400: Loss = -11977.232936027107
Iteration 4500: Loss = -11977.219325500468
Iteration 4600: Loss = -11977.086911682869
Iteration 4700: Loss = -11970.308285059618
Iteration 4800: Loss = -11970.300561480259
Iteration 4900: Loss = -11970.30871712638
1
Iteration 5000: Loss = -11970.29169139958
Iteration 5100: Loss = -11970.288093044743
Iteration 5200: Loss = -11970.316307118332
1
Iteration 5300: Loss = -11962.735720040417
Iteration 5400: Loss = -11962.727620638558
Iteration 5500: Loss = -11962.797852919846
1
Iteration 5600: Loss = -11962.72127735709
Iteration 5700: Loss = -11962.71886676833
Iteration 5800: Loss = -11962.73121844711
1
Iteration 5900: Loss = -11962.705763048454
Iteration 6000: Loss = -11962.700394699139
Iteration 6100: Loss = -11962.689356180712
Iteration 6200: Loss = -11962.686889229772
Iteration 6300: Loss = -11962.68399957683
Iteration 6400: Loss = -11962.750030583162
1
Iteration 6500: Loss = -11962.688459890493
2
Iteration 6600: Loss = -11962.682073813143
Iteration 6700: Loss = -11962.675041616747
Iteration 6800: Loss = -11962.674375669314
Iteration 6900: Loss = -11962.675696667211
1
Iteration 7000: Loss = -11962.720038525484
2
Iteration 7100: Loss = -11962.71867438043
3
Iteration 7200: Loss = -11962.791631252874
4
Iteration 7300: Loss = -11962.675949255265
5
Iteration 7400: Loss = -11962.670715300297
Iteration 7500: Loss = -11962.682080035156
1
Iteration 7600: Loss = -11962.682567752998
2
Iteration 7700: Loss = -11962.675451369083
3
Iteration 7800: Loss = -11949.565614326097
Iteration 7900: Loss = -11949.553496614231
Iteration 8000: Loss = -11949.550873661734
Iteration 8100: Loss = -11949.549707467842
Iteration 8200: Loss = -11949.549312501838
Iteration 8300: Loss = -11943.811091160753
Iteration 8400: Loss = -11943.829539362447
1
Iteration 8500: Loss = -11943.904949003008
2
Iteration 8600: Loss = -11943.80759712898
Iteration 8700: Loss = -11943.805930711153
Iteration 8800: Loss = -11943.815017931967
1
Iteration 8900: Loss = -11943.805109592466
Iteration 9000: Loss = -11943.808325264597
1
Iteration 9100: Loss = -11943.803166288617
Iteration 9200: Loss = -11926.55028557123
Iteration 9300: Loss = -11926.548561184474
Iteration 9400: Loss = -11926.553894040875
1
Iteration 9500: Loss = -11926.577901058617
2
Iteration 9600: Loss = -11926.547269358955
Iteration 9700: Loss = -11926.55416649535
1
Iteration 9800: Loss = -11919.10841638711
Iteration 9900: Loss = -11918.891214317653
Iteration 10000: Loss = -11918.87971683161
Iteration 10100: Loss = -11918.885750389585
1
Iteration 10200: Loss = -11916.228708259261
Iteration 10300: Loss = -11916.189846896983
Iteration 10400: Loss = -11916.167722575474
Iteration 10500: Loss = -11916.16456654297
Iteration 10600: Loss = -11916.372176117027
1
Iteration 10700: Loss = -11916.16263924855
Iteration 10800: Loss = -11916.159651437792
Iteration 10900: Loss = -11916.213545660006
1
Iteration 11000: Loss = -11916.157400462791
Iteration 11100: Loss = -11901.530569578214
Iteration 11200: Loss = -11901.531353764438
1
Iteration 11300: Loss = -11901.530110356489
Iteration 11400: Loss = -11901.604396682133
1
Iteration 11500: Loss = -11901.53077039279
2
Iteration 11600: Loss = -11890.51464979257
Iteration 11700: Loss = -11890.512262373732
Iteration 11800: Loss = -11890.527442811186
1
Iteration 11900: Loss = -11890.540562599193
2
Iteration 12000: Loss = -11890.512106206288
Iteration 12100: Loss = -11890.512863027421
1
Iteration 12200: Loss = -11890.515024097547
2
Iteration 12300: Loss = -11890.512584510723
3
Iteration 12400: Loss = -11890.511496645953
Iteration 12500: Loss = -11890.518783554908
1
Iteration 12600: Loss = -11890.516289623756
2
Iteration 12700: Loss = -11890.511621235022
3
Iteration 12800: Loss = -11890.512991462816
4
Iteration 12900: Loss = -11890.513460956701
5
Iteration 13000: Loss = -11890.511312499852
Iteration 13100: Loss = -11890.511547389851
1
Iteration 13200: Loss = -11890.511907224887
2
Iteration 13300: Loss = -11890.511769129851
3
Iteration 13400: Loss = -11890.53178324893
4
Iteration 13500: Loss = -11890.525074508958
5
Iteration 13600: Loss = -11890.512521402023
6
Iteration 13700: Loss = -11890.510193989621
Iteration 13800: Loss = -11890.511092627536
1
Iteration 13900: Loss = -11890.510636401295
2
Iteration 14000: Loss = -11890.511512798968
3
Iteration 14100: Loss = -11890.513514771526
4
Iteration 14200: Loss = -11890.511930196953
5
Iteration 14300: Loss = -11890.51819548035
6
Iteration 14400: Loss = -11890.51353946481
7
Iteration 14500: Loss = -11890.577595353465
8
Iteration 14600: Loss = -11890.509980257599
Iteration 14700: Loss = -11890.512069582326
1
Iteration 14800: Loss = -11890.51100008896
2
Iteration 14900: Loss = -11890.511059738183
3
Iteration 15000: Loss = -11890.514549106065
4
Iteration 15100: Loss = -11890.523148208884
5
Iteration 15200: Loss = -11890.514288303042
6
Iteration 15300: Loss = -11890.509970286885
Iteration 15400: Loss = -11890.51154072515
1
Iteration 15500: Loss = -11890.582262674629
2
Iteration 15600: Loss = -11890.522159792976
3
Iteration 15700: Loss = -11890.515312083393
4
Iteration 15800: Loss = -11890.511030795611
5
Iteration 15900: Loss = -11890.516195088712
6
Iteration 16000: Loss = -11890.513530137092
7
Iteration 16100: Loss = -11890.515660374247
8
Iteration 16200: Loss = -11890.53633983004
9
Iteration 16300: Loss = -11890.516062183164
10
Stopping early at iteration 16300 due to no improvement.
tensor([[ -8.0277,   5.8582],
        [ -6.9553,   5.4458],
        [ -9.4536,   8.0394],
        [-10.2033,   8.0960],
        [ -2.7498,  -0.5041],
        [  3.1981,  -4.8521],
        [ -5.1235,   0.5083],
        [ -7.4880,   5.8473],
        [ -9.0016,   7.5687],
        [ -9.5690,   7.6518],
        [ -9.0617,   7.4328],
        [ -6.4333,   4.9922],
        [  3.8501,  -5.2525],
        [  4.8586,  -7.6459],
        [ -9.2708,   5.2606],
        [ -9.8361,   7.2656],
        [ -2.8849,   0.9738],
        [ -3.2595,   1.8707],
        [ -9.4193,   8.0286],
        [ -8.2416,   6.3236],
        [ -9.4335,   7.9867],
        [ -7.5633,   5.9460],
        [ -0.1979,  -1.1914],
        [  1.2867,  -4.7602],
        [ -3.2597,   1.8705],
        [ -9.5199,   7.6612],
        [-10.3388,   7.4323],
        [ -2.5549,  -0.4271],
        [ -8.9601,   7.5736],
        [  0.4408,  -2.7308],
        [ -7.4772,   5.9083],
        [ -9.3132,   7.7934],
        [ -7.5435,   6.1491],
        [ -6.0551,   3.9040],
        [ -1.7074,  -0.5938],
        [ -0.3327,  -3.7416],
        [  4.3681,  -5.8169],
        [ -3.6952,   2.2892],
        [ -8.5941,   6.4295],
        [ -8.7406,   7.0282],
        [ -8.6011,   7.1941],
        [ -1.0264,  -0.9853],
        [  1.6515,  -3.0406],
        [-10.6495,   6.8113],
        [ -0.8421,  -0.6349],
        [  2.1495,  -3.7568],
        [ -7.5940,   6.1441],
        [ -2.2532,   0.5448],
        [ -3.1697,   0.8436],
        [  2.1008,  -3.5285],
        [ -1.5524,   0.1658],
        [ -9.1686,   7.1115],
        [ -8.9416,   7.2976],
        [ -6.9491,   5.5544],
        [ -4.4664,   2.8178],
        [ -1.1111,  -3.5041],
        [ -9.1826,   7.7950],
        [ -4.8358,   3.0814],
        [ -4.5328,   3.0111],
        [ -0.0366,  -3.2413],
        [ -7.1730,   5.6104],
        [  0.5276,  -2.1782],
        [ -9.0613,   7.5717],
        [ -9.3725,   7.9743],
        [  3.4626,  -4.8718],
        [ -6.1265,   1.5671],
        [-10.2458,   8.7160],
        [ -2.3393,   0.5733],
        [ -5.9203,   4.3409],
        [ -5.7322,   4.3334],
        [-10.8051,   6.8504],
        [  2.0282,  -3.7264],
        [  2.3129,  -3.7989],
        [ -5.1443,   3.1240],
        [ -9.3786,   7.6368],
        [ -5.6888,   4.3012],
        [ -7.7945,   5.8568],
        [ -9.4667,   7.4477],
        [-10.8565,   7.0023],
        [ -8.7967,   5.4091],
        [ -9.1155,   7.0605],
        [ -8.3039,   6.8979],
        [ -6.2684,   3.6790],
        [  1.0831,  -2.5165],
        [  1.6896,  -3.0788],
        [ -6.9713,   5.3863],
        [ -9.6770,   8.2535],
        [ -9.5291,   7.2609],
        [ -4.7474,   2.8836],
        [ -4.1111,  -0.5041],
        [ -3.5188,   1.0952],
        [ -3.4471,   1.9117],
        [  1.1303,  -3.4087],
        [-10.4376,   6.5372],
        [ -2.3905,  -0.1831],
        [ -8.2873,   6.4211],
        [ -2.3838,   0.9843],
        [ -9.4763,   7.1571],
        [  0.7343,  -4.4089],
        [ -8.8503,   7.4558]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4075, 0.5925],
        [0.4789, 0.5211]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2232, 0.7768], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4007, 0.1061],
         [0.3860, 0.2195]],

        [[0.6218, 0.0986],
         [0.2976, 0.2585]],

        [[0.8741, 0.1055],
         [0.0509, 0.4110]],

        [[0.1215, 0.1169],
         [0.0263, 0.2415]],

        [[0.9477, 0.1027],
         [0.0859, 0.7380]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 70
Adjusted Rand Index: 0.15421916488112936
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 68
Adjusted Rand Index: 0.11771297362968322
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.1920312335943991
Average Adjusted Rand Index: 0.6383849857591818
11606.693255507842
new:  [0.9446731746254289, 1.0, 0.18158518943707194, 0.1920312335943991] [0.9451299031072133, 1.0, 0.6592830089029313, 0.6383849857591818] [11654.291808512682, 11602.190112872386, 11867.872199041638, 11890.516062183164]
prior:  [1.0, 1.0, 1.0, 1.0] [1.0, 1.0, 1.0, 1.0] [11603.905833316068, 11603.905833316068, 11603.905834472018, 11603.905834472018]
-----------------------------------------------------------------------------------------
This iteration is 25
True Objective function: Loss = -11708.888301198027
Iteration 0: Loss = -25005.618849020382
Iteration 10: Loss = -12416.345390776718
Iteration 20: Loss = -11859.175789123268
Iteration 30: Loss = -11851.447459707126
Iteration 40: Loss = -11868.560388295275
1
Iteration 50: Loss = -11874.471919752237
2
Iteration 60: Loss = -11874.365873969724
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6946, 0.3054],
        [0.3784, 0.6216]], dtype=torch.float64)
alpha: tensor([0.5171, 0.4829])
beta: tensor([[[0.2051, 0.1053],
         [0.9097, 0.3854]],

        [[0.8694, 0.1048],
         [0.5065, 0.1962]],

        [[0.4613, 0.0878],
         [0.1636, 0.7686]],

        [[0.9405, 0.1022],
         [0.5269, 0.3366]],

        [[0.7303, 0.1232],
         [0.7543, 0.4042]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 32
Adjusted Rand Index: 0.12178742089629342
Global Adjusted Rand Index: 0.5290422990301036
Average Adjusted Rand Index: 0.8243574841792587
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25058.099859404916
Iteration 100: Loss = -12564.588671236645
Iteration 200: Loss = -12558.635648536898
Iteration 300: Loss = -12548.157864840807
Iteration 400: Loss = -12538.693061683487
Iteration 500: Loss = -12193.872660014482
Iteration 600: Loss = -12041.496247470717
Iteration 700: Loss = -11955.033673170621
Iteration 800: Loss = -11859.770631637562
Iteration 900: Loss = -11815.679801870809
Iteration 1000: Loss = -11777.815217526988
Iteration 1100: Loss = -11756.62914796621
Iteration 1200: Loss = -11733.69316599307
Iteration 1300: Loss = -11733.505820150602
Iteration 1400: Loss = -11733.390033382817
Iteration 1500: Loss = -11733.30855155456
Iteration 1600: Loss = -11733.247409810667
Iteration 1700: Loss = -11733.199698728986
Iteration 1800: Loss = -11733.161377942715
Iteration 1900: Loss = -11733.129757410388
Iteration 2000: Loss = -11733.102980247006
Iteration 2100: Loss = -11733.07943062298
Iteration 2200: Loss = -11733.057218782968
Iteration 2300: Loss = -11733.035129426622
Iteration 2400: Loss = -11733.001970092773
Iteration 2500: Loss = -11732.864908680527
Iteration 2600: Loss = -11732.788932295007
Iteration 2700: Loss = -11732.77603864714
Iteration 2800: Loss = -11732.766608020189
Iteration 2900: Loss = -11732.758687568541
Iteration 3000: Loss = -11732.751810593852
Iteration 3100: Loss = -11732.74568720873
Iteration 3200: Loss = -11732.740163406921
Iteration 3300: Loss = -11732.73526467803
Iteration 3400: Loss = -11732.730769222113
Iteration 3500: Loss = -11732.726647656033
Iteration 3600: Loss = -11732.722911015124
Iteration 3700: Loss = -11732.71952542064
Iteration 3800: Loss = -11732.716411450985
Iteration 3900: Loss = -11732.713485217359
Iteration 4000: Loss = -11732.710815562628
Iteration 4100: Loss = -11732.708351870155
Iteration 4200: Loss = -11732.706115360485
Iteration 4300: Loss = -11732.703931911923
Iteration 4400: Loss = -11732.701955024779
Iteration 4500: Loss = -11732.70011715209
Iteration 4600: Loss = -11732.699894851192
Iteration 4700: Loss = -11732.693319100783
Iteration 4800: Loss = -11723.866681221767
Iteration 4900: Loss = -11723.865774395223
Iteration 5000: Loss = -11707.420592556902
Iteration 5100: Loss = -11707.41861408824
Iteration 5200: Loss = -11707.416896390787
Iteration 5300: Loss = -11707.415383447344
Iteration 5400: Loss = -11707.413916144158
Iteration 5500: Loss = -11707.412560957451
Iteration 5600: Loss = -11707.411658290923
Iteration 5700: Loss = -11707.419711980365
1
Iteration 5800: Loss = -11707.411559052387
Iteration 5900: Loss = -11707.408444124934
Iteration 6000: Loss = -11707.414158298201
1
Iteration 6100: Loss = -11707.406746686805
Iteration 6200: Loss = -11707.405957336558
Iteration 6300: Loss = -11703.97827826756
Iteration 6400: Loss = -11703.978761912122
1
Iteration 6500: Loss = -11703.976933434824
Iteration 6600: Loss = -11703.98592940284
1
Iteration 6700: Loss = -11703.987650155848
2
Iteration 6800: Loss = -11703.975101933667
Iteration 6900: Loss = -11703.974549720508
Iteration 7000: Loss = -11703.984154834452
1
Iteration 7100: Loss = -11704.058828005811
2
Iteration 7200: Loss = -11703.97279555273
Iteration 7300: Loss = -11703.973820313917
1
Iteration 7400: Loss = -11704.010215347804
2
Iteration 7500: Loss = -11703.969602970728
Iteration 7600: Loss = -11703.998028926651
1
Iteration 7700: Loss = -11703.96853861832
Iteration 7800: Loss = -11703.972396660838
1
Iteration 7900: Loss = -11703.986958093905
2
Iteration 8000: Loss = -11703.975612553775
3
Iteration 8100: Loss = -11703.975969287507
4
Iteration 8200: Loss = -11703.966667829443
Iteration 8300: Loss = -11703.994527542136
1
Iteration 8400: Loss = -11703.966089744095
Iteration 8500: Loss = -11703.963486377126
Iteration 8600: Loss = -11703.938579243028
Iteration 8700: Loss = -11703.936702367571
Iteration 8800: Loss = -11703.936411688746
Iteration 8900: Loss = -11703.93967969608
1
Iteration 9000: Loss = -11703.945714593601
2
Iteration 9100: Loss = -11703.93761638767
3
Iteration 9200: Loss = -11703.94842831069
4
Iteration 9300: Loss = -11703.935922231543
Iteration 9400: Loss = -11703.935974943159
1
Iteration 9500: Loss = -11703.938865821183
2
Iteration 9600: Loss = -11703.937426278746
3
Iteration 9700: Loss = -11703.937560826269
4
Iteration 9800: Loss = -11703.935361546752
Iteration 9900: Loss = -11703.937502950352
1
Iteration 10000: Loss = -11703.935214318612
Iteration 10100: Loss = -11703.93651543901
1
Iteration 10200: Loss = -11703.935532431846
2
Iteration 10300: Loss = -11703.94883639318
3
Iteration 10400: Loss = -11703.935661864538
4
Iteration 10500: Loss = -11703.94183491408
5
Iteration 10600: Loss = -11703.93494681398
Iteration 10700: Loss = -11703.949645202723
1
Iteration 10800: Loss = -11703.969572158154
2
Iteration 10900: Loss = -11703.9347738174
Iteration 11000: Loss = -11703.93478278526
1
Iteration 11100: Loss = -11703.948026708416
2
Iteration 11200: Loss = -11703.939485788187
3
Iteration 11300: Loss = -11703.94466299749
4
Iteration 11400: Loss = -11703.977279085115
5
Iteration 11500: Loss = -11703.939681517651
6
Iteration 11600: Loss = -11703.955086827946
7
Iteration 11700: Loss = -11703.942878583273
8
Iteration 11800: Loss = -11703.933607917612
Iteration 11900: Loss = -11703.934030653121
1
Iteration 12000: Loss = -11703.953076532789
2
Iteration 12100: Loss = -11703.934905742617
3
Iteration 12200: Loss = -11703.934837111121
4
Iteration 12300: Loss = -11703.936257051428
5
Iteration 12400: Loss = -11703.933884778147
6
Iteration 12500: Loss = -11703.97142774534
7
Iteration 12600: Loss = -11703.933404145404
Iteration 12700: Loss = -11703.948129106873
1
Iteration 12800: Loss = -11703.9335085185
2
Iteration 12900: Loss = -11703.93466944304
3
Iteration 13000: Loss = -11703.939974643317
4
Iteration 13100: Loss = -11703.934864977016
5
Iteration 13200: Loss = -11703.9380136865
6
Iteration 13300: Loss = -11703.93448914706
7
Iteration 13400: Loss = -11703.937843701327
8
Iteration 13500: Loss = -11703.933906055343
9
Iteration 13600: Loss = -11703.936953487317
10
Stopping early at iteration 13600 due to no improvement.
tensor([[ -8.7869,   4.1716],
        [  6.2838, -10.8991],
        [  4.2352,  -8.8504],
        [ -7.0470,   2.4318],
        [  5.2224,  -9.8376],
        [ -6.9421,   2.3269],
        [ -4.7099,   0.0947],
        [  3.4041,  -8.0193],
        [  6.2319, -10.8471],
        [ -7.8465,   3.2313],
        [ -9.2797,   4.6645],
        [ -7.8268,   3.2116],
        [  5.1382,  -9.7534],
        [  3.7748,  -8.3901],
        [ -9.6756,   5.0604],
        [ -8.8827,   4.2675],
        [ -9.6832,   5.0680],
        [  5.8646, -10.4799],
        [  5.2914,  -9.9066],
        [ -9.2297,   4.6145],
        [  2.5331,  -7.1484],
        [ -9.0951,   4.4799],
        [ -8.7193,   4.1041],
        [ -7.7122,   3.0970],
        [ -7.7287,   3.1135],
        [ -9.6748,   5.0595],
        [  3.3652,  -7.9804],
        [  5.6748, -10.2900],
        [ -7.0576,   2.4424],
        [ -8.1500,   3.5348],
        [  6.0544, -10.6696],
        [ -7.6310,   3.0158],
        [-10.4239,   5.8087],
        [  5.0087,  -9.6239],
        [  4.7228,  -9.3381],
        [  2.5336,  -7.1488],
        [  0.5060,  -5.1212],
        [  4.4191,  -9.0344],
        [ -9.8583,   5.2431],
        [  1.1672,  -5.7824],
        [ -9.7531,   5.1379],
        [  5.6400, -10.2552],
        [  5.5444, -10.1597],
        [ -9.2880,   4.6728],
        [  2.9230,  -7.5382],
        [ -8.7685,   4.1533],
        [  4.6442,  -9.2595],
        [  5.3738,  -9.9891],
        [ -8.9738,   4.3585],
        [ -9.3721,   4.7568],
        [ -1.7461,  -2.8692],
        [  6.5428, -11.1581],
        [  5.9075, -10.5227],
        [ -7.9177,   3.3025],
        [  0.9266,  -5.5418],
        [ -9.8749,   5.2597],
        [ -8.9321,   4.3169],
        [  0.4086,  -5.0238],
        [  6.0773, -10.6925],
        [ -9.7228,   5.1076],
        [  2.7801,  -7.3953],
        [  6.1802, -10.7954],
        [  4.8872,  -9.5024],
        [ -9.2771,   4.6618],
        [  2.6684,  -7.2836],
        [  5.7234, -10.3387],
        [  0.5922,  -5.2074],
        [ -9.3936,   4.7784],
        [  6.0157, -10.6309],
        [ -5.3426,   0.7274],
        [  4.0948,  -8.7100],
        [  2.1299,  -6.7451],
        [ -7.1935,   2.5782],
        [ -8.0571,   3.4419],
        [ -7.1053,   2.4901],
        [  4.4917,  -9.1069],
        [ -2.6842,  -1.9311],
        [ -8.4854,   3.8702],
        [ -6.2986,   1.6834],
        [  0.6966,  -5.3118],
        [  5.3492,  -9.9645],
        [ -8.9986,   4.3833],
        [  1.0575,  -5.6727],
        [  5.8326, -10.4478],
        [  2.4375,  -7.0527],
        [ -9.0863,   4.4710],
        [-10.1046,   5.4894],
        [  3.9613,  -8.5765],
        [ -8.0443,   3.4291],
        [  6.1426, -10.7578],
        [ -7.3030,   2.6878],
        [ -8.8489,   4.2336],
        [ -9.1858,   4.5706],
        [ -5.4447,   0.8295],
        [  1.6469,  -6.2621],
        [  4.1994,  -8.8147],
        [ -8.4635,   3.8483],
        [ -8.6073,   3.9921],
        [  2.5300,  -7.1452],
        [ -9.4961,   4.8809]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7495, 0.2505],
        [0.2773, 0.7227]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5012, 0.4988], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3969, 0.1057],
         [0.9097, 0.1998]],

        [[0.8694, 0.1049],
         [0.5065, 0.1962]],

        [[0.4613, 0.0881],
         [0.1636, 0.7686]],

        [[0.9405, 0.1019],
         [0.5269, 0.3366]],

        [[0.7303, 0.1010],
         [0.7543, 0.4042]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -20053.3135544319
Iteration 10: Loss = -12524.51953154527
Iteration 20: Loss = -11705.506114942224
Iteration 30: Loss = -11705.508065822543
1
Iteration 40: Loss = -11705.508065822543
2
Iteration 50: Loss = -11705.508065822543
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7205, 0.2795],
        [0.2523, 0.7477]], dtype=torch.float64)
alpha: tensor([0.4716, 0.5284])
beta: tensor([[[0.1953, 0.1054],
         [0.2821, 0.3896]],

        [[0.9678, 0.1048],
         [0.0296, 0.9597]],

        [[0.8777, 0.0880],
         [0.3694, 0.5553]],

        [[0.6765, 0.1022],
         [0.5460, 0.8525]],

        [[0.8020, 0.1011],
         [0.0235, 0.3294]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20053.27060574033
Iteration 100: Loss = -12546.792557724093
Iteration 200: Loss = -12472.409199804628
Iteration 300: Loss = -12160.466537315717
Iteration 400: Loss = -12109.288430847913
Iteration 500: Loss = -12085.547571545347
Iteration 600: Loss = -12061.901401842537
Iteration 700: Loss = -12053.518873417834
Iteration 800: Loss = -12040.697841066665
Iteration 900: Loss = -12036.598557372006
Iteration 1000: Loss = -12035.353239315267
Iteration 1100: Loss = -12008.709840276182
Iteration 1200: Loss = -12008.568181746406
Iteration 1300: Loss = -12008.479783968753
Iteration 1400: Loss = -12008.343849731673
Iteration 1500: Loss = -12007.647837327495
Iteration 1600: Loss = -11999.82593103621
Iteration 1700: Loss = -11997.762971128435
Iteration 1800: Loss = -11992.460163562639
Iteration 1900: Loss = -11992.40050986514
Iteration 2000: Loss = -11992.371375670935
Iteration 2100: Loss = -11990.071117455353
Iteration 2200: Loss = -11982.177125504142
Iteration 2300: Loss = -11982.160109101242
Iteration 2400: Loss = -11982.143536694753
Iteration 2500: Loss = -11982.13242638547
Iteration 2600: Loss = -11982.122740685214
Iteration 2700: Loss = -11982.110033548212
Iteration 2800: Loss = -11982.027605483974
Iteration 2900: Loss = -11962.15074974117
Iteration 3000: Loss = -11961.011933341344
Iteration 3100: Loss = -11960.957303180448
Iteration 3200: Loss = -11960.890504311365
Iteration 3300: Loss = -11945.461199401989
Iteration 3400: Loss = -11944.723495942342
Iteration 3500: Loss = -11944.714653092926
Iteration 3600: Loss = -11944.709849457233
Iteration 3700: Loss = -11944.704912998035
Iteration 3800: Loss = -11944.700968168565
Iteration 3900: Loss = -11944.69757230016
Iteration 4000: Loss = -11944.693626408785
Iteration 4100: Loss = -11944.688331010222
Iteration 4200: Loss = -11934.740989992968
Iteration 4300: Loss = -11934.691744352502
Iteration 4400: Loss = -11934.67901041632
Iteration 4500: Loss = -11916.475426667483
Iteration 4600: Loss = -11916.469742504867
Iteration 4700: Loss = -11916.466766303296
Iteration 4800: Loss = -11916.464629970786
Iteration 4900: Loss = -11916.462568886813
Iteration 5000: Loss = -11916.459709831848
Iteration 5100: Loss = -11916.465705747247
1
Iteration 5200: Loss = -11916.454201429706
Iteration 5300: Loss = -11916.453227908692
Iteration 5400: Loss = -11916.452337872
Iteration 5500: Loss = -11916.451524162441
Iteration 5600: Loss = -11916.450765040741
Iteration 5700: Loss = -11916.447324122824
Iteration 5800: Loss = -11907.705433762247
Iteration 5900: Loss = -11890.464822433649
Iteration 6000: Loss = -11890.449777788353
Iteration 6100: Loss = -11890.452679138354
1
Iteration 6200: Loss = -11890.447854317334
Iteration 6300: Loss = -11890.44723067948
Iteration 6400: Loss = -11890.447278175445
1
Iteration 6500: Loss = -11890.446228119989
Iteration 6600: Loss = -11890.445764166096
Iteration 6700: Loss = -11890.445339472139
Iteration 6800: Loss = -11890.444995079582
Iteration 6900: Loss = -11890.445916924828
1
Iteration 7000: Loss = -11890.444212819768
Iteration 7100: Loss = -11890.448371846425
1
Iteration 7200: Loss = -11890.437638072992
Iteration 7300: Loss = -11878.97587889864
Iteration 7400: Loss = -11878.975169392183
Iteration 7500: Loss = -11878.980387881165
1
Iteration 7600: Loss = -11878.97462738252
Iteration 7700: Loss = -11878.97443856908
Iteration 7800: Loss = -11878.98024887538
1
Iteration 7900: Loss = -11878.974048692966
Iteration 8000: Loss = -11878.974893658558
1
Iteration 8100: Loss = -11878.987626584772
2
Iteration 8200: Loss = -11878.973452379276
Iteration 8300: Loss = -11878.97905374871
1
Iteration 8400: Loss = -11879.022360736992
2
Iteration 8500: Loss = -11878.973045361849
Iteration 8600: Loss = -11878.973301387485
1
Iteration 8700: Loss = -11878.999204519227
2
Iteration 8800: Loss = -11878.990469056082
3
Iteration 8900: Loss = -11879.022516913088
4
Iteration 9000: Loss = -11878.975614137937
5
Iteration 9100: Loss = -11878.972529230885
Iteration 9200: Loss = -11878.97407828265
1
Iteration 9300: Loss = -11878.999514620893
2
Iteration 9400: Loss = -11878.972680546027
3
Iteration 9500: Loss = -11878.982302729168
4
Iteration 9600: Loss = -11878.976243476533
5
Iteration 9700: Loss = -11878.972346706896
Iteration 9800: Loss = -11873.508735222138
Iteration 9900: Loss = -11858.517548669432
Iteration 10000: Loss = -11858.528640360062
1
Iteration 10100: Loss = -11858.51474748222
Iteration 10200: Loss = -11858.511796349867
Iteration 10300: Loss = -11858.511810342288
1
Iteration 10400: Loss = -11858.516796739794
2
Iteration 10500: Loss = -11858.559805902036
3
Iteration 10600: Loss = -11858.516050068734
4
Iteration 10700: Loss = -11858.510097170129
Iteration 10800: Loss = -11858.509610298517
Iteration 10900: Loss = -11858.512791776382
1
Iteration 11000: Loss = -11858.515436831438
2
Iteration 11100: Loss = -11858.644335278843
3
Iteration 11200: Loss = -11858.52302547589
4
Iteration 11300: Loss = -11858.510018128287
5
Iteration 11400: Loss = -11858.522035961478
6
Iteration 11500: Loss = -11858.510036079131
7
Iteration 11600: Loss = -11858.509686254785
8
Iteration 11700: Loss = -11858.51774105283
9
Iteration 11800: Loss = -11858.51690461118
10
Stopping early at iteration 11800 due to no improvement.
tensor([[  5.4173,  -7.3051],
        [-10.2344,   5.6423],
        [ -7.3089,   5.3649],
        [  4.1375,  -5.5241],
        [ -7.9037,   6.4441],
        [  3.8962,  -5.7679],
        [  1.6169,  -3.0063],
        [ -6.6913,   5.1322],
        [ -9.2164,   6.9066],
        [  4.7047,  -6.0969],
        [  6.0362,  -7.4278],
        [  3.3126,  -6.8030],
        [ -8.6199,   5.6688],
        [ -6.6289,   5.1217],
        [  6.4261,  -7.8378],
        [  5.4984,  -7.2885],
        [  6.4019,  -7.8001],
        [ -8.2940,   6.8473],
        [ -7.2459,   4.1543],
        [  5.7334,  -7.1201],
        [ -6.0654,   4.4735],
        [  6.8813,  -8.3570],
        [  5.2902,  -6.8380],
        [  2.7457,  -7.3609],
        [  3.6792,  -7.0885],
        [  5.5073, -10.0160],
        [ -6.6551,   4.9064],
        [ -7.8653,   6.4649],
        [  6.3908,  -8.4167],
        [  3.8533,  -7.3105],
        [ -8.0377,   6.6465],
        [  3.4747,  -6.1108],
        [  6.0378,  -7.4318],
        [ -7.8676,   6.4439],
        [ -7.8584,   5.7692],
        [ -5.6012,   3.9196],
        [ -7.6307,   6.1496],
        [ -7.2008,   5.7602],
        [  5.8977,  -8.8904],
        [ -4.8305,   3.3684],
        [  4.1253,  -8.7405],
        [ -8.5855,   6.8698],
        [ -7.9055,   6.4012],
        [  5.2802,  -6.7917],
        [ -5.6790,   4.1731],
        [  5.2941,  -7.0081],
        [ -7.7044,   6.2741],
        [ -9.3891,   6.5969],
        [  5.5542,  -6.9710],
        [  6.2666,  -7.7886],
        [ -1.8043,   0.4174],
        [ -8.7483,   7.3245],
        [ -8.4625,   5.9012],
        [  6.8612,  -9.5588],
        [ -4.6817,   2.5106],
        [  5.8679,  -7.3914],
        [  5.9940,  -7.3824],
        [ -3.9493,   2.5548],
        [ -8.3327,   6.8732],
        [  4.7370,  -6.3834],
        [ -5.8482,   4.4550],
        [ -9.6343,   7.3151],
        [ -7.3839,   5.9074],
        [  5.8142,  -7.3835],
        [ -5.9816,   3.5614],
        [ -8.1615,   6.3263],
        [ -4.2446,   2.8581],
        [  6.0629,  -8.7339],
        [ -8.0471,   6.4554],
        [  2.3744,  -3.9098],
        [ -8.4307,   5.8914],
        [ -5.6437,   4.2423],
        [  3.4142,  -5.0486],
        [  4.7214,  -6.4178],
        [  3.7641,  -5.4376],
        [ -7.7615,   5.8947],
        [ -0.4205,  -1.4781],
        [  4.8415,  -6.8297],
        [  3.0420,  -5.1563],
        [ -4.1179,   1.9708],
        [ -8.2610,   6.2100],
        [  5.1232,  -8.0756],
        [ -4.1506,   2.7628],
        [ -8.0229,   6.6041],
        [ -5.3660,   3.9372],
        [  5.4319,  -8.7366],
        [  6.2500,  -7.6376],
        [ -7.2163,   5.5694],
        [  4.1239,  -6.0476],
        [ -8.7811,   6.8058],
        [  4.2932,  -6.1669],
        [  4.8168,  -8.1727],
        [  5.7645,  -8.7688],
        [  1.8784,  -3.3856],
        [ -5.5540,   3.4791],
        [ -7.9691,   6.2202],
        [  4.9586,  -6.8331],
        [  5.4991,  -7.0463],
        [ -7.8488,   4.6112],
        [  2.8601,  -5.8182]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6748, 0.3252],
        [0.3706, 0.6294]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4996, 0.5004], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2140, 0.1052],
         [0.2821, 0.3919]],

        [[0.9678, 0.1106],
         [0.0296, 0.9597]],

        [[0.8777, 0.0879],
         [0.3694, 0.5553]],

        [[0.6765, 0.1024],
         [0.5460, 0.8525]],

        [[0.8020, 0.1075],
         [0.0235, 0.3294]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 24
Adjusted Rand Index: 0.26343612334801764
Global Adjusted Rand Index: 0.4778207446667853
Average Adjusted Rand Index: 0.8446865663968935
Iteration 0: Loss = -24866.44456755525
Iteration 10: Loss = -12479.533561043672
Iteration 20: Loss = -11705.508014369136
Iteration 30: Loss = -11705.508065822543
1
Iteration 40: Loss = -11705.508065822543
2
Iteration 50: Loss = -11705.508065822543
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7205, 0.2795],
        [0.2523, 0.7477]], dtype=torch.float64)
alpha: tensor([0.4716, 0.5284])
beta: tensor([[[0.1953, 0.1054],
         [0.6366, 0.3896]],

        [[0.9770, 0.1048],
         [0.7368, 0.2749]],

        [[0.5394, 0.0880],
         [0.4016, 0.5222]],

        [[0.0483, 0.1022],
         [0.4096, 0.1470]],

        [[0.8632, 0.1011],
         [0.0781, 0.7020]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24866.697439805594
Iteration 100: Loss = -12556.209361629331
Iteration 200: Loss = -12500.523084405755
Iteration 300: Loss = -12288.238596609424
Iteration 400: Loss = -12091.58019156019
Iteration 500: Loss = -12075.527378876952
Iteration 600: Loss = -12059.628647405607
Iteration 700: Loss = -12055.789921893978
Iteration 800: Loss = -12043.904572864696
Iteration 900: Loss = -12043.481006161315
Iteration 1000: Loss = -12031.660793287108
Iteration 1100: Loss = -12031.12082927338
Iteration 1200: Loss = -12012.808196770682
Iteration 1300: Loss = -12012.659515932864
Iteration 1400: Loss = -12012.574598585745
Iteration 1500: Loss = -12012.511421824767
Iteration 1600: Loss = -12012.456552072583
Iteration 1700: Loss = -12012.399101909592
Iteration 1800: Loss = -12012.33637517535
Iteration 1900: Loss = -12012.124829930463
Iteration 2000: Loss = -12011.802894625049
Iteration 2100: Loss = -12011.57634665585
Iteration 2200: Loss = -11998.408053787689
Iteration 2300: Loss = -11990.156197029646
Iteration 2400: Loss = -11987.444220412359
Iteration 2500: Loss = -11978.903721390117
Iteration 2600: Loss = -11978.839846207016
Iteration 2700: Loss = -11974.110234116053
Iteration 2800: Loss = -11974.064005440132
Iteration 2900: Loss = -11973.971728728926
Iteration 3000: Loss = -11958.129501085983
Iteration 3100: Loss = -11958.095513316608
Iteration 3200: Loss = -11957.422941647295
Iteration 3300: Loss = -11956.429173426799
Iteration 3400: Loss = -11944.236228295109
Iteration 3500: Loss = -11938.421226376026
Iteration 3600: Loss = -11938.180034291498
Iteration 3700: Loss = -11938.136221511782
Iteration 3800: Loss = -11932.193412511539
Iteration 3900: Loss = -11932.183851555088
Iteration 4000: Loss = -11932.217692876608
1
Iteration 4100: Loss = -11932.173102163262
Iteration 4200: Loss = -11932.18182082248
1
Iteration 4300: Loss = -11932.165919800002
Iteration 4400: Loss = -11932.186811689444
1
Iteration 4500: Loss = -11932.161528377455
Iteration 4600: Loss = -11932.174998160217
1
Iteration 4700: Loss = -11932.155965041838
Iteration 4800: Loss = -11932.177192761115
1
Iteration 4900: Loss = -11932.159063653871
2
Iteration 5000: Loss = -11932.178588473733
3
Iteration 5100: Loss = -11932.148703883673
Iteration 5200: Loss = -11932.147065654553
Iteration 5300: Loss = -11932.178373803596
1
Iteration 5400: Loss = -11932.112768194385
Iteration 5500: Loss = -11923.428929571131
Iteration 5600: Loss = -11923.425137275815
Iteration 5700: Loss = -11923.416287690685
Iteration 5800: Loss = -11923.41493925155
Iteration 5900: Loss = -11923.413201262174
Iteration 6000: Loss = -11923.414073195207
1
Iteration 6100: Loss = -11923.41161667889
Iteration 6200: Loss = -11923.411265661405
Iteration 6300: Loss = -11923.449807472836
1
Iteration 6400: Loss = -11914.05142049737
Iteration 6500: Loss = -11914.000864712194
Iteration 6600: Loss = -11914.000759037404
Iteration 6700: Loss = -11913.994585130855
Iteration 6800: Loss = -11913.996589587387
1
Iteration 6900: Loss = -11913.993192318596
Iteration 7000: Loss = -11913.993669803218
1
Iteration 7100: Loss = -11913.992349808479
Iteration 7200: Loss = -11914.001090097985
1
Iteration 7300: Loss = -11913.998378603781
2
Iteration 7400: Loss = -11914.002030369658
3
Iteration 7500: Loss = -11913.997366204052
4
Iteration 7600: Loss = -11913.99321592963
5
Iteration 7700: Loss = -11914.030901858298
6
Iteration 7800: Loss = -11913.98967995173
Iteration 7900: Loss = -11913.98897528725
Iteration 8000: Loss = -11898.822855639188
Iteration 8100: Loss = -11898.820497991266
Iteration 8200: Loss = -11898.822235149939
1
Iteration 8300: Loss = -11898.860375633061
2
Iteration 8400: Loss = -11898.819210919666
Iteration 8500: Loss = -11898.825942479361
1
Iteration 8600: Loss = -11898.818847376764
Iteration 8700: Loss = -11898.818655998652
Iteration 8800: Loss = -11898.819728563356
1
Iteration 8900: Loss = -11898.821279176687
2
Iteration 9000: Loss = -11898.818207726086
Iteration 9100: Loss = -11898.818328270996
1
Iteration 9200: Loss = -11898.819088138574
2
Iteration 9300: Loss = -11898.819138706996
3
Iteration 9400: Loss = -11898.818693407542
4
Iteration 9500: Loss = -11898.81832583152
5
Iteration 9600: Loss = -11898.818127270593
Iteration 9700: Loss = -11898.817606885992
Iteration 9800: Loss = -11898.817454323163
Iteration 9900: Loss = -11898.820165102541
1
Iteration 10000: Loss = -11898.817319609185
Iteration 10100: Loss = -11898.817382983245
1
Iteration 10200: Loss = -11898.956022899782
2
Iteration 10300: Loss = -11898.81738897517
3
Iteration 10400: Loss = -11898.84414243783
4
Iteration 10500: Loss = -11898.816906177843
Iteration 10600: Loss = -11898.826390547092
1
Iteration 10700: Loss = -11898.984987607146
2
Iteration 10800: Loss = -11898.817836167746
3
Iteration 10900: Loss = -11898.820383179193
4
Iteration 11000: Loss = -11898.820008761659
5
Iteration 11100: Loss = -11898.817402590956
6
Iteration 11200: Loss = -11898.820083259567
7
Iteration 11300: Loss = -11898.82166402004
8
Iteration 11400: Loss = -11898.848885598552
9
Iteration 11500: Loss = -11898.8074052384
Iteration 11600: Loss = -11888.109946563374
Iteration 11700: Loss = -11888.11454394424
1
Iteration 11800: Loss = -11888.119973848126
2
Iteration 11900: Loss = -11863.98331509159
Iteration 12000: Loss = -11863.9687515799
Iteration 12100: Loss = -11863.957957414606
Iteration 12200: Loss = -11863.962936167043
1
Iteration 12300: Loss = -11863.962037810477
2
Iteration 12400: Loss = -11863.965554994236
3
Iteration 12500: Loss = -11863.990749089435
4
Iteration 12600: Loss = -11863.959720257537
5
Iteration 12700: Loss = -11863.957362841147
Iteration 12800: Loss = -11863.962383065023
1
Iteration 12900: Loss = -11863.957145549566
Iteration 13000: Loss = -11863.959220920044
1
Iteration 13100: Loss = -11863.965625604164
2
Iteration 13200: Loss = -11864.080058256279
3
Iteration 13300: Loss = -11863.95960214548
4
Iteration 13400: Loss = -11863.955585418862
Iteration 13500: Loss = -11863.955833888665
1
Iteration 13600: Loss = -11863.955730866986
2
Iteration 13700: Loss = -11863.955786900184
3
Iteration 13800: Loss = -11863.955744079674
4
Iteration 13900: Loss = -11863.95951295969
5
Iteration 14000: Loss = -11864.163459910389
6
Iteration 14100: Loss = -11863.956638408792
7
Iteration 14200: Loss = -11863.957057858519
8
Iteration 14300: Loss = -11863.973368278414
9
Iteration 14400: Loss = -11863.955586209606
10
Stopping early at iteration 14400 due to no improvement.
tensor([[ 5.7195, -7.1712],
        [-9.6696,  7.5814],
        [-6.9999,  5.6126],
        [ 4.1066, -5.6962],
        [-9.7588,  5.1436],
        [ 4.0739, -5.6477],
        [ 1.5126, -3.3454],
        [-8.7050,  5.5230],
        [-8.5488,  6.9041],
        [ 4.4429, -6.5981],
        [ 5.7974, -8.1209],
        [ 4.3716, -6.0542],
        [-8.5215,  5.7857],
        [-7.6034,  6.1904],
        [ 6.7432, -8.1356],
        [ 5.5896, -7.5919],
        [ 6.7164, -8.1746],
        [-9.3114,  6.9894],
        [-6.6294,  4.5794],
        [ 5.5639, -7.4868],
        [-5.8565,  4.4488],
        [ 5.9232, -7.3527],
        [ 5.6599, -7.0583],
        [ 3.8644, -6.4570],
        [ 4.7351, -6.2069],
        [ 6.5210, -8.5148],
        [-7.6503,  3.7610],
        [-8.2551,  6.6606],
        [ 3.9345, -5.9035],
        [ 4.9597, -6.4385],
        [-9.5069,  7.2356],
        [ 4.2174, -5.6044],
        [ 6.3550, -9.8775],
        [-8.1054,  6.6767],
        [-7.9428,  6.1864],
        [-7.9105,  6.1689],
        [-3.6694,  2.0598],
        [-7.3796,  5.9787],
        [ 6.3596, -7.9421],
        [-4.6548,  3.2430],
        [ 5.7080, -7.1314],
        [-9.6684,  6.3428],
        [-8.9479,  5.9353],
        [ 5.6646, -7.0824],
        [-5.8242,  3.6667],
        [ 4.9025, -7.7469],
        [-7.8837,  6.3744],
        [-7.8344,  6.4445],
        [ 5.7826, -7.8916],
        [ 6.5059, -7.9180],
        [-1.7068,  0.2445],
        [-9.0258,  7.4302],
        [-8.4970,  6.9760],
        [ 7.4759, -9.0261],
        [-4.7322,  2.2566],
        [ 6.5005, -7.9538],
        [ 5.8126, -7.9002],
        [-4.5217,  1.7402],
        [-8.9029,  7.4399],
        [ 4.9736, -6.3684],
        [-5.9293,  4.2563],
        [-8.9159,  7.2854],
        [-7.7146,  6.2848],
        [ 7.7991, -9.1966],
        [-5.4901,  4.1034],
        [-8.2409,  6.8090],
        [-4.1254,  2.7196],
        [ 6.5315, -8.1818],
        [-8.9988,  7.4112],
        [ 2.3445, -4.0471],
        [-7.3832,  5.4311],
        [-5.6001,  4.0820],
        [ 3.4160, -5.4712],
        [ 4.4583, -6.8719],
        [ 2.6371, -6.6746],
        [-7.9725,  6.4445],
        [-0.2698, -1.3828],
        [ 5.2881, -6.7234],
        [ 3.2180, -5.1727],
        [-3.9331,  2.0253],
        [-8.7777,  5.6423],
        [ 5.7617, -8.2777],
        [-4.6926,  2.0569],
        [-8.6701,  6.4362],
        [-5.3865,  3.8214],
        [ 6.1913, -7.7846],
        [ 6.3202, -8.1097],
        [-8.9934,  4.4952],
        [ 4.6142, -6.0007],
        [-9.0548,  7.4956],
        [ 4.2436, -6.2580],
        [ 5.7915, -7.3617],
        [ 5.9818, -8.0082],
        [ 1.9921, -3.5051],
        [-5.4259,  3.3583],
        [-7.3896,  5.8281],
        [ 4.8498, -7.0667],
        [ 5.7722, -7.4377],
        [-5.4479,  4.0389],
        [ 3.1113, -5.9907]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7004, 0.2996],
        [0.3855, 0.6145]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4987, 0.5013], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2096, 0.1054],
         [0.6366, 0.3958]],

        [[0.9770, 0.1108],
         [0.7368, 0.2749]],

        [[0.5394, 0.0880],
         [0.4016, 0.5222]],

        [[0.0483, 0.1028],
         [0.4096, 0.1470]],

        [[0.8632, 0.1074],
         [0.0781, 0.7020]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 34
Adjusted Rand Index: 0.09449743213499633
Global Adjusted Rand Index: 0.5348934141166085
Average Adjusted Rand Index: 0.8108988281542893
Iteration 0: Loss = -25410.377342947715
Iteration 10: Loss = -11944.694990230082
Iteration 20: Loss = -11705.508068480623
Iteration 30: Loss = -11705.508065822543
Iteration 40: Loss = -11705.508065822543
1
Iteration 50: Loss = -11705.508065822543
2
Iteration 60: Loss = -11705.508065822543
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7205, 0.2795],
        [0.2523, 0.7477]], dtype=torch.float64)
alpha: tensor([0.4716, 0.5284])
beta: tensor([[[0.1953, 0.1054],
         [0.6422, 0.3896]],

        [[0.1918, 0.1048],
         [0.3342, 0.2524]],

        [[0.0567, 0.0880],
         [0.7406, 0.9417]],

        [[0.8271, 0.1022],
         [0.5898, 0.4318]],

        [[0.5490, 0.1011],
         [0.2588, 0.8979]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25411.749286111994
Iteration 100: Loss = -12552.72503085947
Iteration 200: Loss = -12451.862298744989
Iteration 300: Loss = -12137.455295527892
Iteration 400: Loss = -12064.479962229734
Iteration 500: Loss = -12046.842458666075
Iteration 600: Loss = -12042.370086994808
Iteration 700: Loss = -12035.800155658382
Iteration 800: Loss = -12027.951744560381
Iteration 900: Loss = -12013.758477273886
Iteration 1000: Loss = -12012.846417320497
Iteration 1100: Loss = -11994.489850783862
Iteration 1200: Loss = -11994.350192830181
Iteration 1300: Loss = -11994.235117743703
Iteration 1400: Loss = -11993.977239062153
Iteration 1500: Loss = -11990.213752351443
Iteration 1600: Loss = -11968.522544023599
Iteration 1700: Loss = -11968.284457981872
Iteration 1800: Loss = -11950.989414564567
Iteration 1900: Loss = -11939.653066376328
Iteration 2000: Loss = -11939.60593724171
Iteration 2100: Loss = -11939.569777644345
Iteration 2200: Loss = -11932.308297999632
Iteration 2300: Loss = -11932.270515812508
Iteration 2400: Loss = -11932.251255909028
Iteration 2500: Loss = -11932.236620722004
Iteration 2600: Loss = -11932.224113333914
Iteration 2700: Loss = -11932.213071054557
Iteration 2800: Loss = -11932.202847051865
Iteration 2900: Loss = -11932.194230738505
Iteration 3000: Loss = -11932.186366572972
Iteration 3100: Loss = -11932.178732943139
Iteration 3200: Loss = -11932.167713466602
Iteration 3300: Loss = -11914.16816980487
Iteration 3400: Loss = -11914.096456235142
Iteration 3500: Loss = -11900.139898760839
Iteration 3600: Loss = -11900.1332324331
Iteration 3700: Loss = -11900.127701806323
Iteration 3800: Loss = -11900.120426093246
Iteration 3900: Loss = -11884.672408927932
Iteration 4000: Loss = -11884.663221299334
Iteration 4100: Loss = -11884.657405638845
Iteration 4200: Loss = -11884.653414282773
Iteration 4300: Loss = -11884.649591654419
Iteration 4400: Loss = -11884.645491329955
Iteration 4500: Loss = -11884.636750562746
Iteration 4600: Loss = -11852.321586332162
Iteration 4700: Loss = -11852.21990568968
Iteration 4800: Loss = -11852.21244903155
Iteration 4900: Loss = -11852.206823044835
Iteration 5000: Loss = -11833.89783543369
Iteration 5100: Loss = -11833.855187451034
Iteration 5200: Loss = -11833.852190273961
Iteration 5300: Loss = -11833.850040548097
Iteration 5400: Loss = -11833.84817222154
Iteration 5500: Loss = -11833.846564148367
Iteration 5600: Loss = -11833.844943784135
Iteration 5700: Loss = -11833.843259876121
Iteration 5800: Loss = -11833.840841214145
Iteration 5900: Loss = -11833.839170209872
Iteration 6000: Loss = -11833.83814085219
Iteration 6100: Loss = -11833.837318903685
Iteration 6200: Loss = -11833.83653508393
Iteration 6300: Loss = -11833.835866159818
Iteration 6400: Loss = -11833.835226285035
Iteration 6500: Loss = -11833.839168889881
1
Iteration 6600: Loss = -11833.834019898102
Iteration 6700: Loss = -11833.83346889751
Iteration 6800: Loss = -11833.833010651222
Iteration 6900: Loss = -11833.832550638965
Iteration 7000: Loss = -11833.832145798451
Iteration 7100: Loss = -11833.831962087972
Iteration 7200: Loss = -11833.831349706816
Iteration 7300: Loss = -11833.831017661663
Iteration 7400: Loss = -11833.830924905344
Iteration 7500: Loss = -11833.943322405863
1
Iteration 7600: Loss = -11833.830127047071
Iteration 7700: Loss = -11833.83084461518
1
Iteration 7800: Loss = -11833.829549884089
Iteration 7900: Loss = -11833.831893435448
1
Iteration 8000: Loss = -11833.829103727903
Iteration 8100: Loss = -11833.828897794736
Iteration 8200: Loss = -11833.831831091977
1
Iteration 8300: Loss = -11833.843528534187
2
Iteration 8400: Loss = -11833.92066055036
3
Iteration 8500: Loss = -11833.85171296386
4
Iteration 8600: Loss = -11833.828045468872
Iteration 8700: Loss = -11833.828249591905
1
Iteration 8800: Loss = -11833.82810401413
2
Iteration 8900: Loss = -11833.827687784835
Iteration 9000: Loss = -11833.82740271381
Iteration 9100: Loss = -11833.827656951291
1
Iteration 9200: Loss = -11833.830575775466
2
Iteration 9300: Loss = -11833.827046926266
Iteration 9400: Loss = -11833.827237276462
1
Iteration 9500: Loss = -11833.828077828392
2
Iteration 9600: Loss = -11833.827187278139
3
Iteration 9700: Loss = -11833.826775160956
Iteration 9800: Loss = -11833.829530871048
1
Iteration 9900: Loss = -11833.845366117655
2
Iteration 10000: Loss = -11833.850103142686
3
Iteration 10100: Loss = -11833.838036099374
4
Iteration 10200: Loss = -11833.825912149137
Iteration 10300: Loss = -11833.833794387583
1
Iteration 10400: Loss = -11833.828857267674
2
Iteration 10500: Loss = -11833.826051836533
3
Iteration 10600: Loss = -11833.866383617311
4
Iteration 10700: Loss = -11833.894339525456
5
Iteration 10800: Loss = -11833.82685107806
6
Iteration 10900: Loss = -11833.826851111062
7
Iteration 11000: Loss = -11833.825675840213
Iteration 11100: Loss = -11833.825470739703
Iteration 11200: Loss = -11833.82546554118
Iteration 11300: Loss = -11833.827112407087
1
Iteration 11400: Loss = -11833.825905499903
2
Iteration 11500: Loss = -11833.85252548543
3
Iteration 11600: Loss = -11833.826258295545
4
Iteration 11700: Loss = -11833.825136478425
Iteration 11800: Loss = -11833.82590910589
1
Iteration 11900: Loss = -11833.825051324313
Iteration 12000: Loss = -11833.826924955238
1
Iteration 12100: Loss = -11833.825136850095
2
Iteration 12200: Loss = -11833.826873277769
3
Iteration 12300: Loss = -11833.825844119507
4
Iteration 12400: Loss = -11833.831086120877
5
Iteration 12500: Loss = -11833.852011801568
6
Iteration 12600: Loss = -11833.858999263986
7
Iteration 12700: Loss = -11833.82664354493
8
Iteration 12800: Loss = -11833.827128970357
9
Iteration 12900: Loss = -11833.825776019472
10
Stopping early at iteration 12900 due to no improvement.
tensor([[  2.9537,  -5.4365],
        [  7.9508,  -9.6036],
        [  8.5330, -10.3952],
        [  6.5538,  -8.2476],
        [  6.1429,  -7.7072],
        [  3.4930,  -5.5710],
        [  3.6008,  -5.0046],
        [  4.6253,  -6.3505],
        [  6.8842,  -8.2967],
        [  1.3622,  -3.8772],
        [  2.2937,  -3.9913],
        [  3.6153,  -5.0778],
        [  6.7259,  -8.1822],
        [  7.3447, -10.6044],
        [  0.0847,  -3.3020],
        [  0.2354,  -1.8394],
        [ -4.5091,   2.0455],
        [  6.7842,  -8.2623],
        [  6.6129,  -8.0024],
        [  4.3075,  -7.2890],
        [  6.4098,  -8.5227],
        [  2.6954,  -4.6773],
        [  3.2416,  -5.8010],
        [  3.2507,  -4.6434],
        [  4.6889,  -6.1021],
        [  4.3499,  -6.1277],
        [  6.6118,  -8.4838],
        [  8.4526,  -9.8668],
        [  3.9495,  -7.6925],
        [  2.2627,  -3.7905],
        [  6.4561,  -8.2023],
        [  2.2223,  -3.6172],
        [ -3.4304,   0.7642],
        [  6.4085, -10.0522],
        [  4.4740,  -9.0892],
        [  6.6336,  -8.3691],
        [  7.7349,  -9.2622],
        [  6.7756,  -9.0996],
        [  3.1072,  -5.1325],
        [  5.2814,  -6.9225],
        [  0.8579,  -2.4361],
        [  7.6131,  -9.0306],
        [  6.2288,  -8.2406],
        [ -0.5011,  -2.8505],
        [  4.3361,  -5.8504],
        [ -2.1216,  -1.7603],
        [  4.6920,  -6.6761],
        [  6.6041,  -8.0919],
        [ -0.9768,  -1.2503],
        [  3.0679,  -4.6611],
        [  5.3672,  -7.3169],
        [  7.6295,  -9.0605],
        [  6.0301,  -9.2399],
        [ -4.0230,   2.5255],
        [  6.2658,  -8.5980],
        [  1.1639,  -3.0570],
        [ -0.8210,  -0.9036],
        [  5.5930,  -7.6024],
        [  7.1990,  -9.6070],
        [  5.2588,  -6.7154],
        [  7.5562,  -8.9474],
        [  7.8882,  -9.2840],
        [  6.8345,  -8.3066],
        [  3.0238,  -4.5388],
        [  8.0619,  -9.4581],
        [  6.6166,  -8.1129],
        [  5.3897, -10.0049],
        [ -4.8507,   2.7194],
        [  9.0811, -10.5949],
        [  2.5692,  -3.9647],
        [  5.3144,  -9.9296],
        [  5.8555,  -7.2580],
        [ -3.1424,  -0.9933],
        [  4.1135,  -5.9070],
        [  6.4160,  -9.0249],
        [  5.9061,  -7.9942],
        [  4.8958,  -6.2824],
        [  4.8894,  -6.3005],
        [  7.2957,  -8.6860],
        [  5.6258,  -7.0173],
        [  6.6585,  -8.0477],
        [ -5.8796,   2.5330],
        [  8.0624,  -9.4964],
        [  7.8552, -10.7863],
        [  6.1329, -10.7481],
        [ -1.1797,  -0.2094],
        [  1.8262,  -4.4981],
        [  5.2545,  -6.7406],
        [ -3.7605,   1.9625],
        [  7.0257,  -8.8090],
        [  6.8628, -10.0447],
        [  4.1411,  -6.0301],
        [  4.8280,  -6.2383],
        [  3.2958,  -5.7539],
        [  4.9603,  -6.4712],
        [  6.2185,  -8.5965],
        [  2.7976,  -4.1897],
        [ -1.0643,  -2.3390],
        [  6.7452,  -8.5468],
        [ -0.4125,  -4.2027]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6201, 0.3799],
        [0.2733, 0.7267]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9036, 0.0964], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2087, 0.0816],
         [0.6422, 0.4006]],

        [[0.1918, 0.1108],
         [0.3342, 0.2524]],

        [[0.0567, 0.0880],
         [0.7406, 0.9417]],

        [[0.8271, 0.1021],
         [0.5898, 0.4318]],

        [[0.5490, 0.1011],
         [0.2588, 0.8979]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.02918749315918129
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5767484169735367
Average Adjusted Rand Index: 0.7978368403591263
Iteration 0: Loss = -31455.00826676859
Iteration 10: Loss = -12560.669829000131
Iteration 20: Loss = -12450.917501115717
Iteration 30: Loss = -11851.02454959561
Iteration 40: Loss = -11855.76321839321
1
Iteration 50: Loss = -11873.726902513848
2
Iteration 60: Loss = -11878.089005921622
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6840, 0.3160],
        [0.3639, 0.6361]], dtype=torch.float64)
alpha: tensor([0.5071, 0.4929])
beta: tensor([[[0.2093, 0.1054],
         [0.6930, 0.3812]],

        [[0.4667, 0.1048],
         [0.3533, 0.3381]],

        [[0.0700, 0.0879],
         [0.1307, 0.1042]],

        [[0.7320, 0.1023],
         [0.8834, 0.5240]],

        [[0.1782, 0.1217],
         [0.5819, 0.4881]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 27
Adjusted Rand Index: 0.2042256312680858
Global Adjusted Rand Index: 0.5002654410198497
Average Adjusted Rand Index: 0.8408451262536172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31455.202723080445
Iteration 100: Loss = -12579.763513391032
Iteration 200: Loss = -12546.35661134905
Iteration 300: Loss = -12498.586024116436
Iteration 400: Loss = -12343.466600408821
Iteration 500: Loss = -12107.538585517634
Iteration 600: Loss = -12079.241648036945
Iteration 700: Loss = -12059.700109006222
Iteration 800: Loss = -12049.750351560482
Iteration 900: Loss = -12038.400398647193
Iteration 1000: Loss = -12027.195823639984
Iteration 1100: Loss = -12022.892607785796
Iteration 1200: Loss = -12002.929025271687
Iteration 1300: Loss = -11973.341903569613
Iteration 1400: Loss = -11965.907721615558
Iteration 1500: Loss = -11965.612055576066
Iteration 1600: Loss = -11958.359710050552
Iteration 1700: Loss = -11955.79571870999
Iteration 1800: Loss = -11944.05096837845
Iteration 1900: Loss = -11936.323151866674
Iteration 2000: Loss = -11936.250911298048
Iteration 2100: Loss = -11934.945032699292
Iteration 2200: Loss = -11914.926784056192
Iteration 2300: Loss = -11914.877738969006
Iteration 2400: Loss = -11906.572671965727
Iteration 2500: Loss = -11906.518046903451
Iteration 2600: Loss = -11906.488403520952
Iteration 2700: Loss = -11906.395350953304
Iteration 2800: Loss = -11888.038181606657
Iteration 2900: Loss = -11888.01923917291
Iteration 3000: Loss = -11888.004413721406
Iteration 3100: Loss = -11887.985307552446
Iteration 3200: Loss = -11874.627012287447
Iteration 3300: Loss = -11864.046564182889
Iteration 3400: Loss = -11864.03118592176
Iteration 3500: Loss = -11864.020441719249
Iteration 3600: Loss = -11864.011491798186
Iteration 3700: Loss = -11864.00341022168
Iteration 3800: Loss = -11863.994887441566
Iteration 3900: Loss = -11863.97052222484
Iteration 4000: Loss = -11852.728593698104
Iteration 4100: Loss = -11830.090991248846
Iteration 4200: Loss = -11830.075604076012
Iteration 4300: Loss = -11830.066109359057
Iteration 4400: Loss = -11830.059205239453
Iteration 4500: Loss = -11830.054222923976
Iteration 4600: Loss = -11830.050242396737
Iteration 4700: Loss = -11830.046804157124
Iteration 4800: Loss = -11830.043732724364
Iteration 4900: Loss = -11830.040962484049
Iteration 5000: Loss = -11830.038441236251
Iteration 5100: Loss = -11830.036243804674
Iteration 5200: Loss = -11830.034135714013
Iteration 5300: Loss = -11830.032249681948
Iteration 5400: Loss = -11830.031342247907
Iteration 5500: Loss = -11830.028813897521
Iteration 5600: Loss = -11830.027137062263
Iteration 5700: Loss = -11830.034134205167
1
Iteration 5800: Loss = -11830.023285107682
Iteration 5900: Loss = -11830.017911918778
Iteration 6000: Loss = -11828.813614888812
Iteration 6100: Loss = -11828.80561129855
Iteration 6200: Loss = -11828.806269426892
1
Iteration 6300: Loss = -11828.812608291582
2
Iteration 6400: Loss = -11828.802246412995
Iteration 6500: Loss = -11828.820376003268
1
Iteration 6600: Loss = -11828.80067375999
Iteration 6700: Loss = -11828.799961757453
Iteration 6800: Loss = -11828.799622309338
Iteration 6900: Loss = -11828.798739788685
Iteration 7000: Loss = -11828.79807480374
Iteration 7100: Loss = -11828.801899334623
1
Iteration 7200: Loss = -11828.797221071349
Iteration 7300: Loss = -11828.796517479677
Iteration 7400: Loss = -11828.798358794213
1
Iteration 7500: Loss = -11828.79562895439
Iteration 7600: Loss = -11828.806882617579
1
Iteration 7700: Loss = -11828.794259414904
Iteration 7800: Loss = -11828.793665908624
Iteration 7900: Loss = -11828.792340147382
Iteration 8000: Loss = -11828.795699605298
1
Iteration 8100: Loss = -11828.79345417173
2
Iteration 8200: Loss = -11828.803083409446
3
Iteration 8300: Loss = -11828.812407642374
4
Iteration 8400: Loss = -11828.802563203668
5
Iteration 8500: Loss = -11828.79346748543
6
Iteration 8600: Loss = -11828.7942424503
7
Iteration 8700: Loss = -11828.790639310017
Iteration 8800: Loss = -11828.798599528202
1
Iteration 8900: Loss = -11828.789658007066
Iteration 9000: Loss = -11828.789540335496
Iteration 9100: Loss = -11818.213751538462
Iteration 9200: Loss = -11818.208450920323
Iteration 9300: Loss = -11818.21492677287
1
Iteration 9400: Loss = -11818.208444196665
Iteration 9500: Loss = -11818.26507506547
1
Iteration 9600: Loss = -11818.203438910119
Iteration 9700: Loss = -11817.901368172357
Iteration 9800: Loss = -11817.882406897837
Iteration 9900: Loss = -11805.599577726454
Iteration 10000: Loss = -11805.632712190658
1
Iteration 10100: Loss = -11805.605659946526
2
Iteration 10200: Loss = -11805.595100378192
Iteration 10300: Loss = -11805.595908454548
1
Iteration 10400: Loss = -11805.594233905747
Iteration 10500: Loss = -11805.598280668562
1
Iteration 10600: Loss = -11805.593958628675
Iteration 10700: Loss = -11805.593358919454
Iteration 10800: Loss = -11805.593364672879
1
Iteration 10900: Loss = -11805.6679476594
2
Iteration 11000: Loss = -11794.611719855275
Iteration 11100: Loss = -11794.612369406914
1
Iteration 11200: Loss = -11794.637253637164
2
Iteration 11300: Loss = -11794.623422486255
3
Iteration 11400: Loss = -11794.611850213869
4
Iteration 11500: Loss = -11794.609297149396
Iteration 11600: Loss = -11794.610619719804
1
Iteration 11700: Loss = -11771.675826657402
Iteration 11800: Loss = -11771.674515925257
Iteration 11900: Loss = -11771.684758537187
1
Iteration 12000: Loss = -11771.711808772005
2
Iteration 12100: Loss = -11771.678155580239
3
Iteration 12200: Loss = -11749.413018995792
Iteration 12300: Loss = -11749.413044249242
1
Iteration 12400: Loss = -11749.414551169268
2
Iteration 12500: Loss = -11749.41073164327
Iteration 12600: Loss = -11749.421404079176
1
Iteration 12700: Loss = -11749.420024741425
2
Iteration 12800: Loss = -11749.511470864369
3
Iteration 12900: Loss = -11749.41729729428
4
Iteration 13000: Loss = -11749.445607168429
5
Iteration 13100: Loss = -11749.422627231772
6
Iteration 13200: Loss = -11749.410479321492
Iteration 13300: Loss = -11749.411235461486
1
Iteration 13400: Loss = -11749.422905004743
2
Iteration 13500: Loss = -11749.41045924638
Iteration 13600: Loss = -11749.449913210003
1
Iteration 13700: Loss = -11749.420907803347
2
Iteration 13800: Loss = -11749.437256605277
3
Iteration 13900: Loss = -11749.41968598357
4
Iteration 14000: Loss = -11749.417119195436
5
Iteration 14100: Loss = -11749.414632861768
6
Iteration 14200: Loss = -11749.414230616883
7
Iteration 14300: Loss = -11749.410626186282
8
Iteration 14400: Loss = -11749.410322932528
Iteration 14500: Loss = -11749.410754824097
1
Iteration 14600: Loss = -11749.410770977183
2
Iteration 14700: Loss = -11749.4107997824
3
Iteration 14800: Loss = -11749.413056858779
4
Iteration 14900: Loss = -11749.418986869532
5
Iteration 15000: Loss = -11749.41574016758
6
Iteration 15100: Loss = -11749.425251952898
7
Iteration 15200: Loss = -11749.415704672565
8
Iteration 15300: Loss = -11749.42583404163
9
Iteration 15400: Loss = -11749.445817691094
10
Stopping early at iteration 15400 due to no improvement.
tensor([[  5.9229,  -7.3108],
        [ -9.5480,   8.0482],
        [-12.0730,   8.0558],
        [  7.9655,  -9.3635],
        [ -9.9331,   7.6942],
        [  7.5626,  -8.9490],
        [  7.3033,  -8.8555],
        [ -9.7399,   7.1025],
        [ -9.8043,   7.8698],
        [  7.8161,  -9.2036],
        [  6.7680,  -8.1606],
        [  3.7377,  -7.3700],
        [-10.1192,   8.1303],
        [ -9.5766,   7.9092],
        [  7.2318, -10.7420],
        [  5.7618,  -7.1566],
        [  7.4386,  -9.0089],
        [ -9.8311,   8.4446],
        [ -6.7457,   3.8573],
        [  6.1640,  -7.7529],
        [ -5.8362,   3.7722],
        [  7.5674,  -9.0737],
        [  5.9725,  -7.5044],
        [  4.6389,  -6.1821],
        [  7.9190,  -9.9681],
        [  7.3883,  -8.7882],
        [-11.1352,   6.9100],
        [ -9.4129,   7.8340],
        [  7.6509,  -9.1982],
        [  5.1581,  -6.6571],
        [ -9.8142,   8.3949],
        [  4.6163,  -6.0497],
        [  6.2676,  -9.3260],
        [ -9.5863,   7.6596],
        [ -7.8718,   5.3145],
        [ -9.4333,   7.5039],
        [ -8.8982,   7.4995],
        [ -7.3157,   5.8114],
        [  5.4906, -10.1059],
        [ -4.7895,   2.0965],
        [  5.9229,  -7.8377],
        [ -9.1157,   7.4462],
        [ -8.5066,   7.0841],
        [  6.1614,  -7.7056],
        [ -8.6623,   6.9505],
        [  6.1364,  -7.5314],
        [ -7.7208,   6.1920],
        [ -9.4219,   7.9128],
        [  6.2799,  -8.3406],
        [  6.6972, -10.0872],
        [ -1.9580,  -0.8439],
        [ -9.6410,   8.1913],
        [ -8.9715,   7.4750],
        [  7.2924,  -9.2551],
        [ -3.9171,   2.5099],
        [  7.8817, -10.5650],
        [  5.9912,  -8.2589],
        [ -3.4163,   1.9985],
        [ -9.7417,   7.8943],
        [  5.0654,  -6.4866],
        [ -9.3020,   7.6685],
        [-10.5943,   8.9743],
        [ -9.0483,   7.5769],
        [  7.7454,  -9.9508],
        [-10.0746,   7.5461],
        [ -9.3329,   7.7838],
        [ -3.9480,   1.8436],
        [  7.1764,  -8.5716],
        [ -9.3955,   8.0021],
        [  2.2651,  -3.8732],
        [-10.8958,   7.7046],
        [ -5.1858,   3.6685],
        [  3.4004,  -6.4501],
        [  5.0761,  -6.4652],
        [  4.0650,  -5.5149],
        [ -7.6904,   6.2704],
        [ -0.2974,  -1.1160],
        [  5.5934,  -6.9840],
        [  7.4161,  -8.9458],
        [ -7.5971,   6.2007],
        [ -9.5443,   7.9037],
        [  6.3698,  -7.7660],
        [ -9.9016,   6.9638],
        [-10.3811,   8.1097],
        [ -9.6894,   8.2997],
        [  6.1744,  -7.8773],
        [  8.4523, -10.5180],
        [ -9.3725,   7.8247],
        [  4.6064,  -7.0198],
        [ -9.1261,   7.5852],
        [  7.7026,  -9.1040],
        [  6.0515,  -7.5005],
        [  6.6694,  -8.8524],
        [  2.3503,  -3.9014],
        [ -4.7264,   3.1503],
        [ -7.0845,   5.6665],
        [  5.4323,  -6.8435],
        [  5.3645,  -7.9077],
        [ -8.8366,   7.2898],
        [  3.7595,  -6.4510]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7188, 0.2812],
        [0.2627, 0.7373]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4998, 0.5002], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1984, 0.1056],
         [0.6930, 0.3988]],

        [[0.4667, 0.1049],
         [0.3533, 0.3381]],

        [[0.0700, 0.1038],
         [0.1307, 0.1042]],

        [[0.7320, 0.1017],
         [0.8834, 0.5240]],

        [[0.1782, 0.1004],
         [0.5819, 0.4881]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824283882000855
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961941296443
Average Adjusted Rand Index: 0.976485677640017
11708.888301198027
new:  [0.4778207446667853, 0.5348934141166085, 0.5767484169735367, 0.9760961941296443] [0.8446865663968935, 0.8108988281542893, 0.7978368403591263, 0.976485677640017] [11858.51690461118, 11863.955586209606, 11833.825776019472, 11749.445817691094]
prior:  [1.0, 1.0, 1.0, 0.5002654410198497] [1.0, 1.0, 1.0, 0.8408451262536172] [11705.508065822543, 11705.508065822543, 11705.508065822543, 11878.089005921622]
-----------------------------------------------------------------------------------------
This iteration is 26
True Objective function: Loss = -11685.792700692185
Iteration 0: Loss = -52892.77655318661
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.8897,    nan]],

        [[0.9288,    nan],
         [0.4023, 0.4278]],

        [[0.8452,    nan],
         [0.3181, 0.0985]],

        [[0.3145,    nan],
         [0.0857, 0.7951]],

        [[0.7716,    nan],
         [0.3024, 0.4494]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -50829.74806020384
Iteration 100: Loss = -12496.29827629768
Iteration 200: Loss = -12490.085958031223
Iteration 300: Loss = -12483.286573718486
Iteration 400: Loss = -12475.349137155272
Iteration 500: Loss = -12452.190448495152
Iteration 600: Loss = -12388.235232416557
Iteration 700: Loss = -12244.582191706297
Iteration 800: Loss = -12086.108743750641
Iteration 900: Loss = -11999.928914282162
Iteration 1000: Loss = -11922.891856022132
Iteration 1100: Loss = -11903.927999196045
Iteration 1200: Loss = -11875.52661087346
Iteration 1300: Loss = -11872.774819034388
Iteration 1400: Loss = -11871.045568253243
Iteration 1500: Loss = -11866.673896126878
Iteration 1600: Loss = -11866.3207303623
Iteration 1700: Loss = -11866.075085323977
Iteration 1800: Loss = -11865.888753807267
Iteration 1900: Loss = -11865.742195533057
Iteration 2000: Loss = -11865.621915602183
Iteration 2100: Loss = -11865.523723835675
Iteration 2200: Loss = -11865.452578856271
Iteration 2300: Loss = -11865.372079272724
Iteration 2400: Loss = -11865.312663329785
Iteration 2500: Loss = -11865.261307497782
Iteration 2600: Loss = -11865.216294609572
Iteration 2700: Loss = -11865.192036570585
Iteration 2800: Loss = -11865.142006772036
Iteration 2900: Loss = -11865.110850283398
Iteration 3000: Loss = -11865.0825853133
Iteration 3100: Loss = -11865.058528989293
Iteration 3200: Loss = -11865.034035167646
Iteration 3300: Loss = -11865.013517039548
Iteration 3400: Loss = -11865.00050155427
Iteration 3500: Loss = -11864.97830651318
Iteration 3600: Loss = -11864.96389661949
Iteration 3700: Loss = -11864.949770415695
Iteration 3800: Loss = -11864.936922717172
Iteration 3900: Loss = -11864.925402189452
Iteration 4000: Loss = -11864.915505880384
Iteration 4100: Loss = -11864.904976311453
Iteration 4200: Loss = -11864.89581567507
Iteration 4300: Loss = -11864.890782795012
Iteration 4400: Loss = -11864.884591296212
Iteration 4500: Loss = -11864.871991189584
Iteration 4600: Loss = -11864.881158610504
1
Iteration 4700: Loss = -11864.858478419228
Iteration 4800: Loss = -11864.85212464222
Iteration 4900: Loss = -11864.91370191226
1
Iteration 5000: Loss = -11864.839855379376
Iteration 5100: Loss = -11864.833552964259
Iteration 5200: Loss = -11864.831942041626
Iteration 5300: Loss = -11864.814144940832
Iteration 5400: Loss = -11864.79799405406
Iteration 5500: Loss = -11864.79041378118
Iteration 5600: Loss = -11864.782511765934
Iteration 5700: Loss = -11864.798341992257
1
Iteration 5800: Loss = -11864.773293332333
Iteration 5900: Loss = -11864.770195194804
Iteration 6000: Loss = -11864.775248041016
1
Iteration 6100: Loss = -11864.764474463193
Iteration 6200: Loss = -11864.76730379032
1
Iteration 6300: Loss = -11864.768715990807
2
Iteration 6400: Loss = -11864.758261355219
Iteration 6500: Loss = -11864.759235366884
1
Iteration 6600: Loss = -11864.754040108623
Iteration 6700: Loss = -11864.752074537735
Iteration 6800: Loss = -11864.751279643697
Iteration 6900: Loss = -11864.750500361903
Iteration 7000: Loss = -11864.74719388814
Iteration 7100: Loss = -11864.746769931508
Iteration 7200: Loss = -11864.748322701504
1
Iteration 7300: Loss = -11864.783522150097
2
Iteration 7400: Loss = -11864.741700678973
Iteration 7500: Loss = -11862.255570805155
Iteration 7600: Loss = -11862.179682865262
Iteration 7700: Loss = -11862.178794530073
Iteration 7800: Loss = -11862.187103477152
1
Iteration 7900: Loss = -11862.184303462496
2
Iteration 8000: Loss = -11862.180989076003
3
Iteration 8100: Loss = -11862.174146576592
Iteration 8200: Loss = -11862.174035617658
Iteration 8300: Loss = -11862.205297486105
1
Iteration 8400: Loss = -11862.107937001263
Iteration 8500: Loss = -11862.112639251558
1
Iteration 8600: Loss = -11862.106850724145
Iteration 8700: Loss = -11862.11311129865
1
Iteration 8800: Loss = -11862.105114064621
Iteration 8900: Loss = -11862.104626132841
Iteration 9000: Loss = -11862.107835117231
1
Iteration 9100: Loss = -11862.104690307178
2
Iteration 9200: Loss = -11862.106259150642
3
Iteration 9300: Loss = -11862.12436157784
4
Iteration 9400: Loss = -11862.107144639189
5
Iteration 9500: Loss = -11862.143330958175
6
Iteration 9600: Loss = -11862.101740182114
Iteration 9700: Loss = -11862.10488647267
1
Iteration 9800: Loss = -11862.101504524058
Iteration 9900: Loss = -11862.144310059613
1
Iteration 10000: Loss = -11862.105512085545
2
Iteration 10100: Loss = -11862.105410494087
3
Iteration 10200: Loss = -11862.100116804222
Iteration 10300: Loss = -11862.10234136572
1
Iteration 10400: Loss = -11862.099373443461
Iteration 10500: Loss = -11862.100795487551
1
Iteration 10600: Loss = -11862.105098077782
2
Iteration 10700: Loss = -11862.12815760201
3
Iteration 10800: Loss = -11862.099506357692
4
Iteration 10900: Loss = -11862.111240789982
5
Iteration 11000: Loss = -11862.209978554227
6
Iteration 11100: Loss = -11862.098580878335
Iteration 11200: Loss = -11862.101456581408
1
Iteration 11300: Loss = -11862.101347438087
2
Iteration 11400: Loss = -11862.104333985662
3
Iteration 11500: Loss = -11862.098681790834
4
Iteration 11600: Loss = -11862.124784436004
5
Iteration 11700: Loss = -11862.106261840623
6
Iteration 11800: Loss = -11862.10425980388
7
Iteration 11900: Loss = -11862.102595837523
8
Iteration 12000: Loss = -11862.105753559561
9
Iteration 12100: Loss = -11862.1122012651
10
Stopping early at iteration 12100 due to no improvement.
tensor([[ -9.1403,   4.5250],
        [ -6.0080,   1.3927],
        [  3.7879,  -8.4031],
        [  3.8845,  -8.4997],
        [ -8.4798,   3.8646],
        [  2.7941,  -7.4093],
        [ -9.2776,   4.6623],
        [  4.0998,  -8.7150],
        [ -9.1593,   4.5441],
        [ -8.9250,   4.3097],
        [ -4.9330,   0.3177],
        [ -8.9851,   4.3699],
        [  3.3555,  -7.9707],
        [ -8.8935,   4.2782],
        [ -8.7111,   4.0959],
        [  3.6051,  -8.2204],
        [  4.1654,  -8.7806],
        [  3.7922,  -8.4074],
        [ -8.2865,   3.6712],
        [ -0.3132,  -4.3020],
        [  2.4586,  -7.0738],
        [ -8.9681,   4.3529],
        [ -9.3748,   4.7596],
        [  3.9173,  -8.5325],
        [  3.7483,  -8.3635],
        [  3.6168,  -8.2320],
        [ -9.1319,   4.5167],
        [ -9.4013,   4.7861],
        [  4.3484,  -8.9636],
        [ -8.2151,   3.5999],
        [ -8.8677,   4.2525],
        [  4.2368,  -8.8520],
        [  3.8053,  -8.4205],
        [ -9.2736,   4.6583],
        [ -8.9487,   4.3335],
        [  2.9621,  -7.5773],
        [  3.7888,  -8.4040],
        [ -8.6980,   4.0827],
        [ -8.7564,   4.1412],
        [  3.2669,  -7.8821],
        [ -8.7327,   4.1174],
        [  3.7225,  -8.3377],
        [  4.1456,  -8.7609],
        [ -8.0804,   3.4652],
        [  4.0101,  -8.6253],
        [ -8.9416,   4.3264],
        [  3.2196,  -7.8348],
        [  4.3797,  -8.9949],
        [ -9.1245,   4.5093],
        [ -9.0467,   4.4315],
        [  3.5306,  -8.1459],
        [ -8.0327,   3.4175],
        [  3.4989,  -8.1141],
        [  4.0910,  -8.7062],
        [  2.7602,  -7.3755],
        [  3.9401,  -8.5554],
        [ -8.9317,   4.3165],
        [ -9.0388,   4.4236],
        [ -0.3048,  -4.3104],
        [ -8.6775,   4.0623],
        [  0.8562,  -5.4714],
        [ -9.0884,   4.4732],
        [  3.3489,  -7.9641],
        [ -7.6883,   3.0731],
        [ -8.9834,   4.3682],
        [  0.8712,  -5.4864],
        [ -9.2045,   4.5893],
        [ -8.5337,   3.9185],
        [ -8.4565,   3.8413],
        [  0.1615,  -4.7768],
        [ -9.1601,   4.5449],
        [  3.8130,  -8.4282],
        [ -8.4033,   3.7881],
        [ -9.1358,   4.5206],
        [ -6.8234,   2.2082],
        [  3.4953,  -8.1106],
        [  3.9408,  -8.5560],
        [ -9.3662,   4.7510],
        [ -9.0885,   4.4733],
        [  4.0283,  -8.6435],
        [ -1.2738,  -3.3414],
        [  2.1090,  -6.7242],
        [  2.5442,  -7.1594],
        [ -9.3209,   4.7057],
        [ -8.5479,   3.9326],
        [ -9.1729,   4.5577],
        [ -9.3741,   4.7588],
        [-10.0668,   5.4516],
        [  4.0387,  -8.6539],
        [ -9.1354,   4.5202],
        [  3.8982,  -8.5134],
        [  1.8958,  -6.5111],
        [  3.6591,  -8.2743],
        [ -8.7395,   4.1243],
        [ -9.3215,   4.7062],
        [ -9.1954,   4.5802],
        [  0.9162,  -5.5314],
        [  3.8656,  -8.4809],
        [ -9.8243,   5.2091],
        [ -9.0063,   4.3911]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7050, 0.2950],
        [0.3865, 0.6135]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4721, 0.5279], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2314, 0.0979],
         [0.8897, 0.3806]],

        [[0.9288, 0.1027],
         [0.4023, 0.4278]],

        [[0.8452, 0.1033],
         [0.3181, 0.0985]],

        [[0.3145, 0.1125],
         [0.0857, 0.7951]],

        [[0.7716, 0.0936],
         [0.3024, 0.4494]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 15
Adjusted Rand Index: 0.48510267107807936
Global Adjusted Rand Index: 0.42395188900161884
Average Adjusted Rand Index: 0.8810199073123153
Iteration 0: Loss = -26415.08153876036
Iteration 10: Loss = -12487.173671325469
Iteration 20: Loss = -12487.062521365038
Iteration 30: Loss = -12225.07574444779
Iteration 40: Loss = -11682.053134866086
Iteration 50: Loss = -11682.053167737962
1
Iteration 60: Loss = -11682.053167704898
2
Iteration 70: Loss = -11682.053167704898
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7561, 0.2439],
        [0.2681, 0.7319]], dtype=torch.float64)
alpha: tensor([0.5225, 0.4775])
beta: tensor([[[0.3860, 0.0980],
         [0.8467, 0.1943]],

        [[0.2402, 0.1029],
         [0.5015, 0.5803]],

        [[0.8078, 0.1034],
         [0.5461, 0.6537]],

        [[0.0438, 0.1130],
         [0.6537, 0.7876]],

        [[0.6399, 0.0937],
         [0.4459, 0.4354]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26413.247400841927
Iteration 100: Loss = -12494.609302982926
Iteration 200: Loss = -12485.522057804206
Iteration 300: Loss = -12455.170976591597
Iteration 400: Loss = -12406.942696009432
Iteration 500: Loss = -12229.4978638518
Iteration 600: Loss = -12000.765389968212
Iteration 700: Loss = -11961.695705308119
Iteration 800: Loss = -11947.825977309994
Iteration 900: Loss = -11937.285110340694
Iteration 1000: Loss = -11925.782553380848
Iteration 1100: Loss = -11922.696315767509
Iteration 1200: Loss = -11907.819635264996
Iteration 1300: Loss = -11894.995805195551
Iteration 1400: Loss = -11894.608622292584
Iteration 1500: Loss = -11888.744895259768
Iteration 1600: Loss = -11888.038839616762
Iteration 1700: Loss = -11885.261205981631
Iteration 1800: Loss = -11881.370764195517
Iteration 1900: Loss = -11871.573480938898
Iteration 2000: Loss = -11871.50365464278
Iteration 2100: Loss = -11865.705935111951
Iteration 2200: Loss = -11865.650963352604
Iteration 2300: Loss = -11865.612290731156
Iteration 2400: Loss = -11865.580964864424
Iteration 2500: Loss = -11858.19978452973
Iteration 2600: Loss = -11858.133585554426
Iteration 2700: Loss = -11858.021545948202
Iteration 2800: Loss = -11840.695331101722
Iteration 2900: Loss = -11840.540772481982
Iteration 3000: Loss = -11840.51960828267
Iteration 3100: Loss = -11840.505346884856
Iteration 3200: Loss = -11832.046899247078
Iteration 3300: Loss = -11826.45036266517
Iteration 3400: Loss = -11826.385728232492
Iteration 3500: Loss = -11826.400048531825
1
Iteration 3600: Loss = -11826.346663356304
Iteration 3700: Loss = -11815.136376363751
Iteration 3800: Loss = -11810.497445844274
Iteration 3900: Loss = -11810.551881375257
1
Iteration 4000: Loss = -11810.475754827248
Iteration 4100: Loss = -11810.470730739826
Iteration 4200: Loss = -11810.463304872534
Iteration 4300: Loss = -11810.455373097302
Iteration 4400: Loss = -11810.441063529388
Iteration 4500: Loss = -11802.208758294599
Iteration 4600: Loss = -11802.195852561688
Iteration 4700: Loss = -11802.214772448691
1
Iteration 4800: Loss = -11802.182042663333
Iteration 4900: Loss = -11802.190784432449
1
Iteration 5000: Loss = -11802.173188333622
Iteration 5100: Loss = -11802.167225743211
Iteration 5200: Loss = -11792.531219067636
Iteration 5300: Loss = -11792.536834995793
1
Iteration 5400: Loss = -11792.532156093164
2
Iteration 5500: Loss = -11792.520672157938
Iteration 5600: Loss = -11792.527157416322
1
Iteration 5700: Loss = -11792.516881861016
Iteration 5800: Loss = -11792.5152578096
Iteration 5900: Loss = -11792.513665047316
Iteration 6000: Loss = -11792.5122719937
Iteration 6100: Loss = -11792.511089705322
Iteration 6200: Loss = -11792.51016350888
Iteration 6300: Loss = -11792.508124384494
Iteration 6400: Loss = -11792.506561661476
Iteration 6500: Loss = -11780.762969700785
Iteration 6600: Loss = -11780.727457345678
Iteration 6700: Loss = -11780.741429611713
1
Iteration 6800: Loss = -11780.721805096033
Iteration 6900: Loss = -11780.722104685723
1
Iteration 7000: Loss = -11780.720021472758
Iteration 7100: Loss = -11780.695114117576
Iteration 7200: Loss = -11771.965468358629
Iteration 7300: Loss = -11771.962326443218
Iteration 7400: Loss = -11771.96882015049
1
Iteration 7500: Loss = -11771.958438106658
Iteration 7600: Loss = -11771.986264328934
1
Iteration 7700: Loss = -11771.96567957839
2
Iteration 7800: Loss = -11771.955818290666
Iteration 7900: Loss = -11771.961150179732
1
Iteration 8000: Loss = -11762.129328829356
Iteration 8100: Loss = -11762.13394104739
1
Iteration 8200: Loss = -11762.12780458469
Iteration 8300: Loss = -11762.126365894088
Iteration 8400: Loss = -11762.148921827014
1
Iteration 8500: Loss = -11762.12800254797
2
Iteration 8600: Loss = -11762.127374267811
3
Iteration 8700: Loss = -11762.137312626672
4
Iteration 8800: Loss = -11762.12539318531
Iteration 8900: Loss = -11744.472958940396
Iteration 9000: Loss = -11744.45457080635
Iteration 9100: Loss = -11744.44614926672
Iteration 9200: Loss = -11744.445872975331
Iteration 9300: Loss = -11744.451396445338
1
Iteration 9400: Loss = -11744.444087289587
Iteration 9500: Loss = -11744.444089109238
1
Iteration 9600: Loss = -11744.44310260829
Iteration 9700: Loss = -11744.445962982194
1
Iteration 9800: Loss = -11744.443131664535
2
Iteration 9900: Loss = -11744.442813012993
Iteration 10000: Loss = -11744.449858694314
1
Iteration 10100: Loss = -11744.44199649104
Iteration 10200: Loss = -11744.442539842279
1
Iteration 10300: Loss = -11744.445015312618
2
Iteration 10400: Loss = -11744.442771619733
3
Iteration 10500: Loss = -11744.450789138566
4
Iteration 10600: Loss = -11744.444090278144
5
Iteration 10700: Loss = -11744.442681801087
6
Iteration 10800: Loss = -11744.441367741276
Iteration 10900: Loss = -11744.450675189435
1
Iteration 11000: Loss = -11744.444668817481
2
Iteration 11100: Loss = -11732.545955084253
Iteration 11200: Loss = -11732.477516190655
Iteration 11300: Loss = -11732.487341563301
1
Iteration 11400: Loss = -11732.479269085372
2
Iteration 11500: Loss = -11732.468081859382
Iteration 11600: Loss = -11732.46893325084
1
Iteration 11700: Loss = -11732.469334150373
2
Iteration 11800: Loss = -11732.469078002801
3
Iteration 11900: Loss = -11732.468897676195
4
Iteration 12000: Loss = -11732.47153569149
5
Iteration 12100: Loss = -11732.46715414837
Iteration 12200: Loss = -11732.468117832037
1
Iteration 12300: Loss = -11732.466986044043
Iteration 12400: Loss = -11732.467391135808
1
Iteration 12500: Loss = -11732.473304187131
2
Iteration 12600: Loss = -11732.6725994727
3
Iteration 12700: Loss = -11732.469751814466
4
Iteration 12800: Loss = -11732.481240413554
5
Iteration 12900: Loss = -11732.468978060377
6
Iteration 13000: Loss = -11732.47610418982
7
Iteration 13100: Loss = -11732.466591889472
Iteration 13200: Loss = -11732.472003719575
1
Iteration 13300: Loss = -11732.474738600156
2
Iteration 13400: Loss = -11732.565742442293
3
Iteration 13500: Loss = -11732.477014360102
4
Iteration 13600: Loss = -11732.42617213973
Iteration 13700: Loss = -11732.425843454399
Iteration 13800: Loss = -11732.42730353218
1
Iteration 13900: Loss = -11732.428660041873
2
Iteration 14000: Loss = -11732.437636864677
3
Iteration 14100: Loss = -11732.426282857643
4
Iteration 14200: Loss = -11732.429481729761
5
Iteration 14300: Loss = -11732.671235724725
6
Iteration 14400: Loss = -11732.427460652692
7
Iteration 14500: Loss = -11732.516323680562
8
Iteration 14600: Loss = -11732.429140335616
9
Iteration 14700: Loss = -11732.427900990413
10
Stopping early at iteration 14700 due to no improvement.
tensor([[  7.1559,  -9.0736],
        [  2.1262,  -3.8151],
        [ -7.8918,   6.4639],
        [ -7.9171,   5.8284],
        [  7.2455,  -8.7515],
        [ -6.2416,   4.8257],
        [  7.1956,  -8.6971],
        [ -7.8689,   6.4637],
        [  7.1476,  -8.8799],
        [  7.5502, -10.2531],
        [ -9.4736,   8.0671],
        [  7.1563,  -9.0122],
        [ -6.9956,   5.5977],
        [  6.7248,  -8.4898],
        [  6.4615,  -8.8784],
        [ -8.4454,   5.9401],
        [ -8.8272,   6.8489],
        [ -8.6404,   6.0564],
        [  5.1507,  -8.4873],
        [ -3.2286,   1.7276],
        [ -6.6135,   4.9593],
        [  6.7538,  -8.9502],
        [  6.5382,  -8.8388],
        [ -7.6870,   6.2966],
        [ -8.9529,   6.3071],
        [ -7.9856,   6.3456],
        [  8.2598,  -9.7372],
        [  6.6201, -11.2354],
        [ -8.4454,   6.9635],
        [  6.8160,  -8.3713],
        [  6.6741,  -8.3996],
        [ -8.9220,   7.2937],
        [ -8.0987,   6.4866],
        [  7.3475,  -8.8381],
        [  6.8353,  -8.2334],
        [ -8.5006,   6.9473],
        [ -8.0672,   6.5435],
        [  5.6992,  -9.5928],
        [  7.0839,  -8.5600],
        [ -7.6146,   6.0825],
        [  5.7359,  -8.0954],
        [ -8.1339,   5.7876],
        [ -8.1584,   6.7691],
        [  4.7735,  -6.5128],
        [ -7.6283,   6.2390],
        [  6.5357,  -8.0584],
        [ -8.0155,   6.3410],
        [-10.0928,   7.0701],
        [  7.1651,  -8.8529],
        [  7.3715,  -9.8025],
        [ -8.4447,   5.1947],
        [  6.9210,  -8.7839],
        [ -7.1235,   4.3695],
        [-10.5318,   5.9166],
        [ -6.3137,   4.7626],
        [ -8.8852,   6.7243],
        [  7.2196,  -9.2528],
        [  7.1060,  -8.9353],
        [ -4.3046,   2.8758],
        [  7.9847,  -9.7285],
        [ -5.7235,   4.2951],
        [  7.4286, -10.8799],
        [ -7.7203,   6.3321],
        [  2.8271,  -6.4273],
        [  6.6736,  -8.3598],
        [ -4.6315,   3.1213],
        [  7.7210, -10.3254],
        [  4.4509,  -8.2613],
        [  6.8820,  -8.3013],
        [ -3.6026,   2.1510],
        [  6.8226,  -8.6669],
        [ -8.0466,   6.5638],
        [  6.6394,  -8.0693],
        [  7.3471,  -8.7438],
        [  2.5235,  -5.2144],
        [ -7.9713,   6.4904],
        [-10.0181,   5.4029],
        [  7.7588,  -9.3689],
        [  6.7162,  -9.1612],
        [ -8.2201,   5.9447],
        [ -2.3643,   0.8183],
        [ -6.2218,   4.2947],
        [-12.4944,   7.8792],
        [  7.1405,  -8.6300],
        [  4.1823,  -8.3183],
        [  6.9748,  -8.9312],
        [  7.4863,  -8.9830],
        [  7.5751,  -9.3749],
        [ -8.2914,   6.5095],
        [  7.3080,  -8.7450],
        [ -8.3190,   6.5997],
        [ -6.9677,   4.9445],
        [ -8.4392,   6.7376],
        [  5.9350,  -7.3389],
        [  7.7714,  -9.1866],
        [  8.4366, -12.9805],
        [ -5.6533,   3.2169],
        [ -8.4051,   7.0053],
        [  5.7152,  -8.4107],
        [  6.9874,  -8.3790]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7407, 0.2593],
        [0.2760, 0.7240]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5198, 0.4802], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3963, 0.1005],
         [0.8467, 0.1949]],

        [[0.2402, 0.1219],
         [0.5015, 0.5803]],

        [[0.8078, 0.1039],
         [0.5461, 0.6537]],

        [[0.0438, 0.1127],
         [0.6537, 0.7876]],

        [[0.6399, 0.0936],
         [0.4459, 0.4354]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080682750429576
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.952480934748395
Average Adjusted Rand Index: 0.9536132161721547
Iteration 0: Loss = -31964.586848557316
Iteration 10: Loss = -12487.173671338573
Iteration 20: Loss = -12485.117009928194
Iteration 30: Loss = -12107.0111603232
Iteration 40: Loss = -11682.05318435625
Iteration 50: Loss = -11682.053172591173
Iteration 60: Loss = -11682.053172591173
1
Iteration 70: Loss = -11682.053172591173
2
Iteration 80: Loss = -11682.053172591173
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7561, 0.2439],
        [0.2681, 0.7319]], dtype=torch.float64)
alpha: tensor([0.5225, 0.4775])
beta: tensor([[[0.3860, 0.0980],
         [0.9584, 0.1943]],

        [[0.0166, 0.1029],
         [0.1947, 0.4889]],

        [[0.1191, 0.1034],
         [0.9495, 0.3189]],

        [[0.1186, 0.1130],
         [0.8828, 0.2303]],

        [[0.9678, 0.0937],
         [0.7142, 0.9835]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31963.832639742966
Iteration 100: Loss = -12495.914382252504
Iteration 200: Loss = -12487.368843542805
Iteration 300: Loss = -12478.264236377478
Iteration 400: Loss = -12464.29991063199
Iteration 500: Loss = -12360.274915721666
Iteration 600: Loss = -12118.09092187547
Iteration 700: Loss = -11927.656673386322
Iteration 800: Loss = -11869.445431099497
Iteration 900: Loss = -11835.178756240693
Iteration 1000: Loss = -11806.930889863213
Iteration 1100: Loss = -11793.804464542502
Iteration 1200: Loss = -11791.99976851222
Iteration 1300: Loss = -11787.724084868414
Iteration 1400: Loss = -11781.86211142278
Iteration 1500: Loss = -11781.67687228944
Iteration 1600: Loss = -11781.561417379746
Iteration 1700: Loss = -11781.442543837016
Iteration 1800: Loss = -11774.63642262729
Iteration 1900: Loss = -11774.56169114407
Iteration 2000: Loss = -11764.624878762674
Iteration 2100: Loss = -11754.67621772196
Iteration 2200: Loss = -11754.632795832551
Iteration 2300: Loss = -11754.597511198062
Iteration 2400: Loss = -11754.567399101641
Iteration 2500: Loss = -11754.539767316059
Iteration 2600: Loss = -11754.508075770707
Iteration 2700: Loss = -11752.672099212232
Iteration 2800: Loss = -11751.18142589551
Iteration 2900: Loss = -11751.151849503467
Iteration 3000: Loss = -11751.134317281223
Iteration 3100: Loss = -11751.11955890751
Iteration 3200: Loss = -11750.84894490257
Iteration 3300: Loss = -11739.307216872436
Iteration 3400: Loss = -11739.29413298661
Iteration 3500: Loss = -11739.283534033277
Iteration 3600: Loss = -11739.274577496088
Iteration 3700: Loss = -11739.266818585247
Iteration 3800: Loss = -11739.259667600472
Iteration 3900: Loss = -11739.24996071514
Iteration 4000: Loss = -11723.457320710359
Iteration 4100: Loss = -11723.447634698772
Iteration 4200: Loss = -11723.441572637075
Iteration 4300: Loss = -11723.436533487946
Iteration 4400: Loss = -11723.4319354271
Iteration 4500: Loss = -11723.427888075988
Iteration 4600: Loss = -11723.424174517866
Iteration 4700: Loss = -11723.420713170726
Iteration 4800: Loss = -11723.416735487934
Iteration 4900: Loss = -11719.437086182406
Iteration 5000: Loss = -11719.431101917013
Iteration 5100: Loss = -11719.4274709936
Iteration 5200: Loss = -11719.426063406445
Iteration 5300: Loss = -11719.420889761683
Iteration 5400: Loss = -11719.415838389337
Iteration 5500: Loss = -11709.1213584683
Iteration 5600: Loss = -11709.099896105568
Iteration 5700: Loss = -11709.095607758965
Iteration 5800: Loss = -11709.093341479645
Iteration 5900: Loss = -11709.091233804684
Iteration 6000: Loss = -11709.089702667343
Iteration 6100: Loss = -11709.088336093817
Iteration 6200: Loss = -11709.087042231622
Iteration 6300: Loss = -11709.085844514382
Iteration 6400: Loss = -11709.08472159712
Iteration 6500: Loss = -11709.084129983694
Iteration 6600: Loss = -11709.082643129252
Iteration 6700: Loss = -11700.093724302455
Iteration 6800: Loss = -11700.067939777
Iteration 6900: Loss = -11700.066902813473
Iteration 7000: Loss = -11700.065923035438
Iteration 7100: Loss = -11700.065647263207
Iteration 7200: Loss = -11700.064417269006
Iteration 7300: Loss = -11700.065444992959
1
Iteration 7400: Loss = -11700.063740536454
Iteration 7500: Loss = -11700.066797091273
1
Iteration 7600: Loss = -11700.061820589925
Iteration 7700: Loss = -11700.06137178078
Iteration 7800: Loss = -11700.065756329268
1
Iteration 7900: Loss = -11700.060343890535
Iteration 8000: Loss = -11700.071497987943
1
Iteration 8100: Loss = -11698.69658752894
Iteration 8200: Loss = -11698.690912542494
Iteration 8300: Loss = -11698.490729036914
Iteration 8400: Loss = -11698.490445570513
Iteration 8500: Loss = -11698.490722129322
1
Iteration 8600: Loss = -11698.491661398657
2
Iteration 8700: Loss = -11698.523725545152
3
Iteration 8800: Loss = -11698.489936772638
Iteration 8900: Loss = -11698.509018621398
1
Iteration 9000: Loss = -11698.48845492652
Iteration 9100: Loss = -11698.502736228927
1
Iteration 9200: Loss = -11698.488403163781
Iteration 9300: Loss = -11698.487640639427
Iteration 9400: Loss = -11698.487926488586
1
Iteration 9500: Loss = -11698.493168851157
2
Iteration 9600: Loss = -11698.490817981501
3
Iteration 9700: Loss = -11698.486858697936
Iteration 9800: Loss = -11698.500603413931
1
Iteration 9900: Loss = -11698.492834116627
2
Iteration 10000: Loss = -11698.488315728375
3
Iteration 10100: Loss = -11698.553368698582
4
Iteration 10200: Loss = -11698.486216405929
Iteration 10300: Loss = -11698.488161116638
1
Iteration 10400: Loss = -11698.487766063852
2
Iteration 10500: Loss = -11698.509436920627
3
Iteration 10600: Loss = -11698.522879547392
4
Iteration 10700: Loss = -11698.48866911164
5
Iteration 10800: Loss = -11698.488070007495
6
Iteration 10900: Loss = -11698.487022193982
7
Iteration 11000: Loss = -11698.485285353663
Iteration 11100: Loss = -11698.485100637747
Iteration 11200: Loss = -11698.48568581766
1
Iteration 11300: Loss = -11698.502174053607
2
Iteration 11400: Loss = -11698.537932662564
3
Iteration 11500: Loss = -11694.322559580629
Iteration 11600: Loss = -11694.31719078468
Iteration 11700: Loss = -11694.342613927838
1
Iteration 11800: Loss = -11694.316918267856
Iteration 11900: Loss = -11694.317481444823
1
Iteration 12000: Loss = -11694.330539292601
2
Iteration 12100: Loss = -11694.32172359546
3
Iteration 12200: Loss = -11694.340943258896
4
Iteration 12300: Loss = -11694.331134291653
5
Iteration 12400: Loss = -11694.316974731824
6
Iteration 12500: Loss = -11694.319606161542
7
Iteration 12600: Loss = -11694.318444083732
8
Iteration 12700: Loss = -11694.513333197612
9
Iteration 12800: Loss = -11694.316494090639
Iteration 12900: Loss = -11694.319166571622
1
Iteration 13000: Loss = -11694.332470357898
2
Iteration 13100: Loss = -11694.338981921712
3
Iteration 13200: Loss = -11694.360006949484
4
Iteration 13300: Loss = -11694.320297900766
5
Iteration 13400: Loss = -11694.317687702865
6
Iteration 13500: Loss = -11694.441567374193
7
Iteration 13600: Loss = -11694.31691985602
8
Iteration 13700: Loss = -11694.31821694116
9
Iteration 13800: Loss = -11694.364908527361
10
Stopping early at iteration 13800 due to no improvement.
tensor([[  7.7031,  -9.2262],
        [  2.1603,  -3.6186],
        [ -7.8536,   6.3184],
        [ -7.3473,   5.9016],
        [  6.5945,  -8.3464],
        [ -6.6045,   4.9304],
        [  7.6448,  -9.2409],
        [ -8.8315,   6.7625],
        [  6.9725,  -8.3609],
        [  6.6045,  -8.0945],
        [  5.8951,  -7.3446],
        [  6.6686,  -8.1004],
        [ -7.2116,   5.8242],
        [  6.9527,  -8.3846],
        [  6.6129,  -8.0187],
        [ -7.8447,   6.0951],
        [ -9.6754,   6.3019],
        [ -8.0255,   6.3760],
        [  6.1679,  -7.5650],
        [ -3.3469,   1.8222],
        [ -6.5951,   5.2087],
        [  6.9139,  -8.7569],
        [  6.1381,  -7.7624],
        [ -8.5157,   6.2910],
        [ -7.3519,   5.7813],
        [ -7.9949,   6.5818],
        [  7.3431,  -8.7798],
        [  7.3860,  -8.8690],
        [ -8.9111,   6.9173],
        [  7.2747,  -9.1087],
        [  7.0265,  -8.8442],
        [ -8.8115,   7.3405],
        [ -8.0380,   6.1972],
        [  7.8636, -12.2573],
        [  6.6830,  -8.2196],
        [ -8.7699,   6.9262],
        [ -7.6920,   6.2982],
        [  6.4537,  -8.1342],
        [  7.5519,  -9.2882],
        [ -7.7077,   6.0115],
        [  7.7215, -10.1491],
        [ -7.7379,   6.3369],
        [ -8.2034,   6.3929],
        [  4.6312,  -6.7209],
        [ -8.0409,   6.3441],
        [  5.7243,  -9.6085],
        [ -7.0813,   5.6950],
        [ -8.4222,   6.9264],
        [  7.0805, -10.3039],
        [  7.1052,  -8.5159],
        [ -7.4556,   5.9927],
        [  4.9828,  -6.4727],
        [ -7.3985,   5.7476],
        [ -8.4084,   6.6865],
        [ -7.3012,   4.2600],
        [ -8.7128,   6.4092],
        [  6.8597,  -8.8357],
        [  6.5130,  -8.8970],
        [ -8.2504,   6.4072],
        [  6.0710,  -7.5266],
        [ -7.7735,   4.4373],
        [  7.8514,  -9.5709],
        [ -8.1697,   5.8870],
        [  3.6161,  -5.6004],
        [  6.2366,  -9.9461],
        [ -5.0678,   3.0420],
        [  8.2396,  -9.9601],
        [  7.2688,  -8.7164],
        [  7.0688,  -9.2689],
        [ -3.8130,   2.3535],
        [  7.3205,  -8.8390],
        [ -8.2826,   5.9497],
        [  7.0199,  -8.4588],
        [  8.3488, -10.6386],
        [  6.7114,  -8.1133],
        [ -7.7742,   5.9030],
        [ -7.6837,   6.2377],
        [  6.6656, -10.0762],
        [  5.9545, -10.5697],
        [-11.1897,   6.5745],
        [ -3.0973,   0.4422],
        [ -6.1372,   4.7400],
        [-10.0504,   7.5250],
        [  7.3047,  -9.7006],
        [  5.7914,  -7.6069],
        [  7.9218,  -9.4693],
        [  7.0494,  -8.4435],
        [  7.2038,  -9.0607],
        [ -7.8882,   6.4942],
        [  7.5883,  -8.9885],
        [ -8.9562,   6.1395],
        [ -6.0994,   3.6822],
        [ -9.0649,   7.5988],
        [  7.0062,  -8.9407],
        [  7.2224,  -8.6221],
        [  7.7631,  -9.4984],
        [ -5.8451,   3.3006],
        [ -7.7474,   6.0822],
        [  6.7797,  -8.1729],
        [  7.0112,  -8.3975]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7526, 0.2474],
        [0.2706, 0.7294]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5305, 0.4695], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3936, 0.0969],
         [0.9584, 0.1967]],

        [[0.0166, 0.1029],
         [0.1947, 0.4889]],

        [[0.1191, 0.1089],
         [0.9495, 0.3189]],

        [[0.1186, 0.1136],
         [0.8828, 0.2303]],

        [[0.9678, 0.0936],
         [0.7142, 0.9835]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320615631362
Average Adjusted Rand Index: 0.9839993730966995
Iteration 0: Loss = -36830.076609939926
Iteration 10: Loss = -11693.561685116445
Iteration 20: Loss = -11682.053186729368
Iteration 30: Loss = -11682.053177909784
Iteration 40: Loss = -11682.053177909784
1
Iteration 50: Loss = -11682.053177909784
2
Iteration 60: Loss = -11682.053177909784
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7319, 0.2681],
        [0.2439, 0.7561]], dtype=torch.float64)
alpha: tensor([0.4775, 0.5225])
beta: tensor([[[0.1943, 0.0980],
         [0.2052, 0.3860]],

        [[0.9680, 0.1029],
         [0.7608, 0.9292]],

        [[0.4121, 0.1034],
         [0.3337, 0.9696]],

        [[0.1268, 0.1130],
         [0.9828, 0.3800]],

        [[0.3264, 0.0937],
         [0.2701, 0.9216]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36711.4334700179
Iteration 100: Loss = -12478.22130486055
Iteration 200: Loss = -12387.492848525093
Iteration 300: Loss = -12204.617847656908
Iteration 400: Loss = -12035.333477141428
Iteration 500: Loss = -12009.786084708107
Iteration 600: Loss = -12006.623969489243
Iteration 700: Loss = -12005.567862222375
Iteration 800: Loss = -12004.892227059761
Iteration 900: Loss = -12004.454552931384
Iteration 1000: Loss = -12003.96023776376
Iteration 1100: Loss = -12003.242867789639
Iteration 1200: Loss = -12002.645552015256
Iteration 1300: Loss = -12002.108397009013
Iteration 1400: Loss = -12001.126676956108
Iteration 1500: Loss = -12000.317006506464
Iteration 1600: Loss = -11998.828719654288
Iteration 1700: Loss = -11996.239450106174
Iteration 1800: Loss = -11994.435762727882
Iteration 1900: Loss = -11990.398127057304
Iteration 2000: Loss = -11984.340123758819
Iteration 2100: Loss = -11975.997291457674
Iteration 2200: Loss = -11957.662832298272
Iteration 2300: Loss = -11941.510553439724
Iteration 2400: Loss = -11934.711871798518
Iteration 2500: Loss = -11926.239352017219
Iteration 2600: Loss = -11918.35208968744
Iteration 2700: Loss = -11917.990602582104
Iteration 2800: Loss = -11917.923145866449
Iteration 2900: Loss = -11917.855683531076
Iteration 3000: Loss = -11917.721560685055
Iteration 3100: Loss = -11916.24824902075
Iteration 3200: Loss = -11916.158478546375
Iteration 3300: Loss = -11915.129624969495
Iteration 3400: Loss = -11914.992418797105
Iteration 3500: Loss = -11914.866898036253
Iteration 3600: Loss = -11914.851459349653
Iteration 3700: Loss = -11914.840341498248
Iteration 3800: Loss = -11914.82928762923
Iteration 3900: Loss = -11914.816742717567
Iteration 4000: Loss = -11914.798551553991
Iteration 4100: Loss = -11914.744847143684
Iteration 4200: Loss = -11914.543911524313
Iteration 4300: Loss = -11914.4707672084
Iteration 4400: Loss = -11914.453266454435
Iteration 4500: Loss = -11914.444439378432
Iteration 4600: Loss = -11910.99676125912
Iteration 4700: Loss = -11910.935641194344
Iteration 4800: Loss = -11910.92477537803
Iteration 4900: Loss = -11910.913752564595
Iteration 5000: Loss = -11910.91073259215
Iteration 5100: Loss = -11910.907736013965
Iteration 5200: Loss = -11910.904595909953
Iteration 5300: Loss = -11910.898824304544
Iteration 5400: Loss = -11910.042122998811
Iteration 5500: Loss = -11909.98750436378
Iteration 5600: Loss = -11909.982591864844
Iteration 5700: Loss = -11909.977713759941
Iteration 5800: Loss = -11909.963511277381
Iteration 5900: Loss = -11904.329004444367
Iteration 6000: Loss = -11904.315325511334
Iteration 6100: Loss = -11904.310241083545
Iteration 6200: Loss = -11904.306670678252
Iteration 6300: Loss = -11904.304640668508
Iteration 6400: Loss = -11904.30257931793
Iteration 6500: Loss = -11904.301290071908
Iteration 6600: Loss = -11904.300083129036
Iteration 6700: Loss = -11904.298902336028
Iteration 6800: Loss = -11904.29778616209
Iteration 6900: Loss = -11904.296578117752
Iteration 7000: Loss = -11904.294206198283
Iteration 7100: Loss = -11904.271043234845
Iteration 7200: Loss = -11904.273428069073
1
Iteration 7300: Loss = -11904.269027605482
Iteration 7400: Loss = -11904.267407996977
Iteration 7500: Loss = -11904.265482081102
Iteration 7600: Loss = -11903.329300373702
Iteration 7700: Loss = -11903.316233744219
Iteration 7800: Loss = -11903.315064032848
Iteration 7900: Loss = -11903.313988047874
Iteration 8000: Loss = -11903.31342340421
Iteration 8100: Loss = -11903.312951310983
Iteration 8200: Loss = -11903.312492344758
Iteration 8300: Loss = -11903.318056038994
1
Iteration 8400: Loss = -11903.319596458528
2
Iteration 8500: Loss = -11903.332094805493
3
Iteration 8600: Loss = -11903.309749799197
Iteration 8700: Loss = -11903.303365027932
Iteration 8800: Loss = -11903.3183231879
1
Iteration 8900: Loss = -11903.301326552057
Iteration 9000: Loss = -11903.296988079059
Iteration 9100: Loss = -11903.31790116095
1
Iteration 9200: Loss = -11903.292702994548
Iteration 9300: Loss = -11903.292631560342
Iteration 9400: Loss = -11903.292076464028
Iteration 9500: Loss = -11903.24884172549
Iteration 9600: Loss = -11903.254934416263
1
Iteration 9700: Loss = -11903.24733634431
Iteration 9800: Loss = -11903.246494580371
Iteration 9900: Loss = -11903.251808545818
1
Iteration 10000: Loss = -11903.245890316932
Iteration 10100: Loss = -11903.246112543478
1
Iteration 10200: Loss = -11903.253166489312
2
Iteration 10300: Loss = -11903.243401196376
Iteration 10400: Loss = -11903.278445805912
1
Iteration 10500: Loss = -11903.20169418247
Iteration 10600: Loss = -11903.209220289542
1
Iteration 10700: Loss = -11903.19860585693
Iteration 10800: Loss = -11903.1985051462
Iteration 10900: Loss = -11903.204239669147
1
Iteration 11000: Loss = -11903.198413763643
Iteration 11100: Loss = -11903.21646769453
1
Iteration 11200: Loss = -11903.198234152014
Iteration 11300: Loss = -11903.272419011244
1
Iteration 11400: Loss = -11903.198085981036
Iteration 11500: Loss = -11903.199163023586
1
Iteration 11600: Loss = -11903.206438341027
2
Iteration 11700: Loss = -11903.197658055265
Iteration 11800: Loss = -11903.216311551667
1
Iteration 11900: Loss = -11903.197304182882
Iteration 12000: Loss = -11903.290058567507
1
Iteration 12100: Loss = -11903.196623355652
Iteration 12200: Loss = -11903.198361123488
1
Iteration 12300: Loss = -11903.209835858615
2
Iteration 12400: Loss = -11903.196195067543
Iteration 12500: Loss = -11903.47523566558
1
Iteration 12600: Loss = -11903.196093596156
Iteration 12700: Loss = -11903.199209099721
1
Iteration 12800: Loss = -11903.19610979924
2
Iteration 12900: Loss = -11903.201034519152
3
Iteration 13000: Loss = -11903.200623902492
4
Iteration 13100: Loss = -11903.19614046551
5
Iteration 13200: Loss = -11903.19764836493
6
Iteration 13300: Loss = -11903.203474123722
7
Iteration 13400: Loss = -11903.22308390724
8
Iteration 13500: Loss = -11903.204303796281
9
Iteration 13600: Loss = -11903.199343703764
10
Stopping early at iteration 13600 due to no improvement.
tensor([[ -8.7138,   7.3224],
        [ -5.1013,   3.6386],
        [  6.7802,  -8.2105],
        [  5.5833,  -7.7586],
        [ -7.9501,   6.4406],
        [  6.3201,  -8.4059],
        [ -9.9401,   7.4869],
        [  6.7113,  -8.0976],
        [ -8.9468,   7.1884],
        [ -8.7106,   6.9292],
        [ -3.6216,   1.6236],
        [ -9.1854,   7.1598],
        [  4.9307,  -7.5240],
        [ -8.3289,   6.8554],
        [ -8.8874,   6.4181],
        [  6.4623,  -8.1847],
        [  6.4179,  -8.0019],
        [  5.3959,  -7.9577],
        [ -7.6713,   6.0473],
        [  1.1087,  -2.9477],
        [  5.0195,  -6.4095],
        [-10.2219,   6.4873],
        [ -8.2566,   6.4641],
        [  7.0046,  -8.9772],
        [  6.1150,  -9.0736],
        [  6.1040,  -7.6503],
        [ -8.8522,   6.3792],
        [ -8.7522,   7.2305],
        [  6.7462,  -8.5086],
        [ -7.2346,   5.6745],
        [ -8.5959,   6.6879],
        [  6.3903,  -8.6901],
        [  5.5728,  -6.9946],
        [ -8.8347,   7.3894],
        [ -9.2839,   7.3767],
        [  2.7776,  -6.8136],
        [  5.5180,  -8.0839],
        [ -8.9941,   6.4166],
        [ -8.7762,   5.9818],
        [  4.6807,  -6.2495],
        [ -8.0182,   6.6031],
        [  5.6498,  -7.2152],
        [  6.7358,  -8.1222],
        [ -6.9522,   5.5566],
        [  7.2000,  -9.0325],
        [ -9.8339,   6.8186],
        [  2.9044,  -5.0911],
        [  6.7518,  -8.1721],
        [ -8.9482,   7.5246],
        [ -8.2928,   6.8237],
        [  6.8874,  -8.5052],
        [ -7.2683,   5.0167],
        [  4.6958,  -6.9012],
        [  6.4354,  -7.8486],
        [  3.7235,  -5.2334],
        [  6.0385,  -8.7539],
        [ -8.6305,   6.6570],
        [ -9.3389,   7.1799],
        [  1.2062,  -3.0720],
        [ -8.4960,   6.8303],
        [  2.4298,  -3.9470],
        [-11.2830,   6.6678],
        [  6.1860,  -7.9183],
        [ -6.8575,   5.4618],
        [ -9.2246,   6.0459],
        [  2.4386,  -5.2585],
        [ -8.8656,   7.4178],
        [ -7.8442,   6.4148],
        [ -7.9887,   5.6984],
        [  3.9513,  -5.5465],
        [ -9.4919,   4.9827],
        [  6.8754,  -8.5611],
        [ -8.3864,   5.8481],
        [ -9.7425,   7.3103],
        [ -5.2176,   3.8313],
        [  4.8235,  -6.5524],
        [  6.6565,  -9.1046],
        [ -9.7492,   7.4817],
        [ -8.9358,   7.4923],
        [  6.3139,  -8.4044],
        [  0.5978,  -2.8266],
        [  3.9760,  -5.5786],
        [  4.1312,  -5.6868],
        [ -8.9750,   7.4449],
        [ -8.0005,   6.5919],
        [ -9.1464,   7.5189],
        [ -9.0340,   6.7742],
        [ -9.0648,   7.4254],
        [  5.9715,  -7.7755],
        [-10.1765,   6.9577],
        [  5.7519,  -8.0727],
        [  2.7875,  -4.3492],
        [  5.0926,  -7.1741],
        [ -8.1652,   5.8079],
        [ -8.8073,   6.5083],
        [ -8.5645,   7.1687],
        [  3.2475,  -4.7253],
        [  5.9592,  -7.6975],
        [ -8.9319,   6.9838],
        [ -9.7808,   7.2534]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4073, 0.5927],
        [0.9047, 0.0953]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4700, 0.5300], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2318, 0.0981],
         [0.2052, 0.3831]],

        [[0.9680, 0.1044],
         [0.7608, 0.9292]],

        [[0.4121, 0.1031],
         [0.3337, 0.9696]],

        [[0.1268, 0.1202],
         [0.9828, 0.3800]],

        [[0.3264, 0.0939],
         [0.2701, 0.9216]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 19
Adjusted Rand Index: 0.3789264220967039
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 30
Adjusted Rand Index: 0.15307250954025728
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.15513383116162072
Average Adjusted Rand Index: 0.7063997863273923
Iteration 0: Loss = -23893.062632231202
Iteration 10: Loss = -12192.84414975481
Iteration 20: Loss = -11682.053198499829
Iteration 30: Loss = -11682.053177909784
Iteration 40: Loss = -11682.053177909784
1
Iteration 50: Loss = -11682.053177909784
2
Iteration 60: Loss = -11682.053177909784
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7319, 0.2681],
        [0.2439, 0.7561]], dtype=torch.float64)
alpha: tensor([0.4775, 0.5225])
beta: tensor([[[0.1943, 0.0980],
         [0.6135, 0.3860]],

        [[0.9521, 0.1029],
         [0.1216, 0.5702]],

        [[0.5269, 0.1034],
         [0.1305, 0.8738]],

        [[0.8929, 0.1130],
         [0.2311, 0.0288]],

        [[0.0231, 0.0937],
         [0.1348, 0.4949]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23892.63847751432
Iteration 100: Loss = -12464.207069744114
Iteration 200: Loss = -12207.279367828167
Iteration 300: Loss = -11928.335484965917
Iteration 400: Loss = -11879.9152406385
Iteration 500: Loss = -11850.747215119452
Iteration 600: Loss = -11847.844602729383
Iteration 700: Loss = -11837.31464372603
Iteration 800: Loss = -11837.07185461308
Iteration 900: Loss = -11836.920508407798
Iteration 1000: Loss = -11835.989944169574
Iteration 1100: Loss = -11823.594602716723
Iteration 1200: Loss = -11823.52289447445
Iteration 1300: Loss = -11823.469573222517
Iteration 1400: Loss = -11817.349205443968
Iteration 1500: Loss = -11810.621235493385
Iteration 1600: Loss = -11810.5369533661
Iteration 1700: Loss = -11795.000154262605
Iteration 1800: Loss = -11794.7756807611
Iteration 1900: Loss = -11794.75557889513
Iteration 2000: Loss = -11794.737671020244
Iteration 2100: Loss = -11781.184994250534
Iteration 2200: Loss = -11768.990628808908
Iteration 2300: Loss = -11756.45740233182
Iteration 2400: Loss = -11756.438932662626
Iteration 2500: Loss = -11742.196117864158
Iteration 2600: Loss = -11735.598343633572
Iteration 2700: Loss = -11735.589463962597
Iteration 2800: Loss = -11735.582620011888
Iteration 2900: Loss = -11735.576694943313
Iteration 3000: Loss = -11735.571392905376
Iteration 3100: Loss = -11735.566185809092
Iteration 3200: Loss = -11726.4461775017
Iteration 3300: Loss = -11726.38702476476
Iteration 3400: Loss = -11726.377192104124
Iteration 3500: Loss = -11726.361738474629
Iteration 3600: Loss = -11726.358091522705
Iteration 3700: Loss = -11726.355121043553
Iteration 3800: Loss = -11726.35227649979
Iteration 3900: Loss = -11726.346785019146
Iteration 4000: Loss = -11719.008366382703
Iteration 4100: Loss = -11719.002931047113
Iteration 4200: Loss = -11718.997782727427
Iteration 4300: Loss = -11718.934319874397
Iteration 4400: Loss = -11709.045390965452
Iteration 4500: Loss = -11708.499185466724
Iteration 4600: Loss = -11708.497296385505
Iteration 4700: Loss = -11708.495691396323
Iteration 4800: Loss = -11708.494377166864
Iteration 4900: Loss = -11708.496180763324
1
Iteration 5000: Loss = -11708.492276874898
Iteration 5100: Loss = -11708.491330258943
Iteration 5200: Loss = -11708.490422899806
Iteration 5300: Loss = -11708.489643360797
Iteration 5400: Loss = -11708.488915889717
Iteration 5500: Loss = -11708.48853113538
Iteration 5600: Loss = -11708.48751772853
Iteration 5700: Loss = -11708.486873565722
Iteration 5800: Loss = -11708.502171611884
1
Iteration 5900: Loss = -11708.485569883822
Iteration 6000: Loss = -11708.484805970867
Iteration 6100: Loss = -11708.50962399176
1
Iteration 6200: Loss = -11695.640424512823
Iteration 6300: Loss = -11695.622574618768
Iteration 6400: Loss = -11695.617975366302
Iteration 6500: Loss = -11695.622378495838
1
Iteration 6600: Loss = -11695.617041387444
Iteration 6700: Loss = -11695.620363842836
1
Iteration 6800: Loss = -11695.616725461718
Iteration 6900: Loss = -11695.616059230524
Iteration 7000: Loss = -11695.616493064625
1
Iteration 7100: Loss = -11695.617183504944
2
Iteration 7200: Loss = -11695.615880076793
Iteration 7300: Loss = -11695.6150196816
Iteration 7400: Loss = -11695.620932907694
1
Iteration 7500: Loss = -11695.627186382175
2
Iteration 7600: Loss = -11695.620091161547
3
Iteration 7700: Loss = -11695.613601168818
Iteration 7800: Loss = -11695.621681374947
1
Iteration 7900: Loss = -11695.613042300225
Iteration 8000: Loss = -11695.614201609204
1
Iteration 8100: Loss = -11695.61205721229
Iteration 8200: Loss = -11695.614339658521
1
Iteration 8300: Loss = -11695.613512470605
2
Iteration 8400: Loss = -11695.61294185976
3
Iteration 8500: Loss = -11695.623153471866
4
Iteration 8600: Loss = -11695.612149967867
5
Iteration 8700: Loss = -11695.61211278501
6
Iteration 8800: Loss = -11695.614549061005
7
Iteration 8900: Loss = -11695.618713682104
8
Iteration 9000: Loss = -11695.614070240836
9
Iteration 9100: Loss = -11695.628012528652
10
Stopping early at iteration 9100 due to no improvement.
tensor([[-8.5912,  6.5671],
        [-4.5979,  1.3469],
        [ 5.7999, -7.2265],
        [ 5.3811, -7.1699],
        [-8.2062,  4.6707],
        [ 4.6647, -6.4924],
        [-8.2067,  6.3541],
        [ 5.6804, -7.1652],
        [-7.7446,  6.3089],
        [-7.6934,  6.2286],
        [ 5.5178, -9.5728],
        [-9.0511,  6.9546],
        [ 5.4917, -6.9988],
        [-9.1568,  6.6606],
        [-7.3428,  5.4088],
        [ 5.9404, -7.3507],
        [ 5.8141, -7.2816],
        [ 4.7248, -8.0214],
        [-7.5618,  5.0394],
        [ 5.3993, -7.3532],
        [ 4.9641, -6.4397],
        [-8.7488,  6.5700],
        [-9.2479,  5.6955],
        [ 6.1918, -8.5138],
        [ 5.6448, -7.8705],
        [ 5.8915, -8.3300],
        [-8.2743,  6.8716],
        [-8.6178,  7.0372],
        [ 5.9264, -7.3633],
        [-7.8797,  6.3209],
        [-7.5558,  6.1418],
        [ 6.3253, -7.7278],
        [ 4.8115, -8.4667],
        [-9.9882,  6.0458],
        [-7.8005,  6.4042],
        [ 6.0912, -7.7312],
        [ 5.7049, -7.0959],
        [-8.2116,  6.7088],
        [-8.0156,  6.2500],
        [ 4.9136, -7.6720],
        [-8.5555,  7.0819],
        [ 5.5157, -7.3097],
        [ 6.2472, -7.7972],
        [-6.5114,  5.0628],
        [ 6.0520, -7.8406],
        [-7.9309,  6.5129],
        [ 5.2554, -7.9756],
        [ 6.1515, -7.5565],
        [-7.9300,  6.2759],
        [-8.4943,  6.9875],
        [ 5.4446, -7.1490],
        [-8.9143,  6.7543],
        [ 6.0246, -7.7512],
        [ 6.1899, -7.9050],
        [ 4.5261, -6.6120],
        [ 5.9645, -7.4722],
        [-9.2523,  6.2586],
        [-7.9444,  6.4651],
        [ 2.2138, -4.9513],
        [-9.3587,  6.5828],
        [ 3.0200, -5.8277],
        [-8.9590,  6.3960],
        [ 5.4220, -6.8487],
        [-6.1608,  3.1297],
        [-9.4197,  6.5826],
        [ 3.1947, -4.5868],
        [-8.8134,  7.2474],
        [-7.0310,  5.1403],
        [-8.5087,  6.6965],
        [ 2.2027, -3.5958],
        [-7.7187,  4.4579],
        [ 5.8998, -7.5371],
        [-7.6804,  6.0842],
        [-8.5982,  6.9713],
        [-7.7655,  6.2794],
        [ 5.0863, -7.5363],
        [ 5.7611, -7.1655],
        [-8.7482,  7.3266],
        [-8.1201,  6.7133],
        [ 5.8510, -7.2548],
        [ 0.1095, -3.1058],
        [ 4.4876, -6.0559],
        [ 6.5528, -8.2157],
        [-8.7107,  7.2088],
        [-7.9872,  6.4805],
        [-8.5856,  7.1924],
        [-8.1586,  6.6876],
        [-7.8737,  6.4825],
        [ 5.8156, -8.3010],
        [-8.7007,  6.7876],
        [ 6.0503, -8.2595],
        [ 5.0427, -6.4677],
        [ 4.6623, -7.3856],
        [-7.3621,  4.6941],
        [-8.9931,  6.8266],
        [-8.9737,  7.2728],
        [ 2.7469, -6.1053],
        [ 5.8742, -7.5636],
        [-7.2765,  5.8881],
        [-7.8327,  6.4456]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7261, 0.2739],
        [0.2503, 0.7497]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4797, 0.5203], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1969, 0.1005],
         [0.6135, 0.3954]],

        [[0.9521, 0.1078],
         [0.1216, 0.5702]],

        [[0.5269, 0.1038],
         [0.1305, 0.8738]],

        [[0.8929, 0.1133],
         [0.2311, 0.0288]],

        [[0.0231, 0.0938],
         [0.1348, 0.4949]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320578693613
Average Adjusted Rand Index: 0.983999122327126
11685.792700692185
new:  [0.952480934748395, 0.9840320615631362, 0.15513383116162072, 0.9840320578693613] [0.9536132161721547, 0.9839993730966995, 0.7063997863273923, 0.983999122327126] [11732.427900990413, 11694.364908527361, 11903.199343703764, 11695.628012528652]
prior:  [0.9919999997943784, 0.9919999997943784, 0.9919999997943784, 0.9919999997943784] [0.9919998119331364, 0.9919998119331364, 0.9919998119331364, 0.9919998119331364] [11682.053167704898, 11682.053172591173, 11682.053177909784, 11682.053177909784]
-----------------------------------------------------------------------------------------
This iteration is 27
True Objective function: Loss = -11414.61793705235
Iteration 0: Loss = -28971.123333881776
Iteration 10: Loss = -12009.227818536721
Iteration 20: Loss = -11573.354155544765
Iteration 30: Loss = -11572.425916874752
Iteration 40: Loss = -11573.99595819701
1
Iteration 50: Loss = -11583.483675048545
2
Iteration 60: Loss = -11584.43326130336
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6605, 0.3395],
        [0.5007, 0.4993]], dtype=torch.float64)
alpha: tensor([0.5734, 0.4266])
beta: tensor([[[0.2035, 0.0905],
         [0.1795, 0.3903]],

        [[0.2609, 0.1034],
         [0.2653, 0.3364]],

        [[0.7252, 0.1041],
         [0.2556, 0.9052]],

        [[0.5089, 0.1130],
         [0.2905, 0.4919]],

        [[0.9853, 0.0928],
         [0.8852, 0.9590]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 33
Adjusted Rand Index: 0.10870370747399068
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5348906167448243
Average Adjusted Rand Index: 0.8217407414947981
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28884.068367948985
Iteration 100: Loss = -12179.42601198299
Iteration 200: Loss = -12173.676287404882
Iteration 300: Loss = -12158.480120414446
Iteration 400: Loss = -11932.553735939462
Iteration 500: Loss = -11807.502924869956
Iteration 600: Loss = -11779.544045096982
Iteration 700: Loss = -11757.345347733948
Iteration 800: Loss = -11716.866778975162
Iteration 900: Loss = -11660.684443834423
Iteration 1000: Loss = -11628.864819646898
Iteration 1100: Loss = -11597.170316709242
Iteration 1200: Loss = -11594.535232988072
Iteration 1300: Loss = -11594.152254799159
Iteration 1400: Loss = -11593.42025860557
Iteration 1500: Loss = -11593.265429076408
Iteration 1600: Loss = -11590.677931966651
Iteration 1700: Loss = -11587.067930335776
Iteration 1800: Loss = -11583.09064760074
Iteration 1900: Loss = -11582.965485895116
Iteration 2000: Loss = -11580.902601836275
Iteration 2100: Loss = -11572.467443061674
Iteration 2200: Loss = -11570.260975836145
Iteration 2300: Loss = -11558.416455516135
Iteration 2400: Loss = -11550.458828769326
Iteration 2500: Loss = -11541.359370800319
Iteration 2600: Loss = -11532.98544833924
Iteration 2700: Loss = -11525.02358740609
Iteration 2800: Loss = -11524.988553835192
Iteration 2900: Loss = -11524.958984403638
Iteration 3000: Loss = -11521.963830505912
Iteration 3100: Loss = -11513.48343107474
Iteration 3200: Loss = -11500.730318839442
Iteration 3300: Loss = -11493.374975217626
Iteration 3400: Loss = -11493.345791499756
Iteration 3500: Loss = -11483.332862289124
Iteration 3600: Loss = -11480.583457540952
Iteration 3700: Loss = -11468.688214025517
Iteration 3800: Loss = -11468.039004879085
Iteration 3900: Loss = -11465.765467297899
Iteration 4000: Loss = -11455.154878571979
Iteration 4100: Loss = -11455.148983203377
Iteration 4200: Loss = -11455.144119384455
Iteration 4300: Loss = -11455.139922770115
Iteration 4400: Loss = -11455.136017129595
Iteration 4500: Loss = -11455.132039796956
Iteration 4600: Loss = -11455.126536042131
Iteration 4700: Loss = -11455.101184922956
Iteration 4800: Loss = -11439.717000088987
Iteration 4900: Loss = -11439.705196604065
Iteration 5000: Loss = -11439.654495698152
Iteration 5100: Loss = -11439.65225476093
Iteration 5200: Loss = -11439.650395598866
Iteration 5300: Loss = -11439.648671133556
Iteration 5400: Loss = -11439.647006833571
Iteration 5500: Loss = -11439.645090629729
Iteration 5600: Loss = -11439.64222879811
Iteration 5700: Loss = -11431.065668263464
Iteration 5800: Loss = -11431.048040594478
Iteration 5900: Loss = -11431.046363830877
Iteration 6000: Loss = -11431.04450050838
Iteration 6100: Loss = -11431.039160032133
Iteration 6200: Loss = -11417.019665088832
Iteration 6300: Loss = -11417.006164448401
Iteration 6400: Loss = -11407.634805287345
Iteration 6500: Loss = -11407.634356439192
Iteration 6600: Loss = -11407.632917644318
Iteration 6700: Loss = -11407.63228167171
Iteration 6800: Loss = -11407.631945485951
Iteration 6900: Loss = -11407.636806308656
1
Iteration 7000: Loss = -11407.63091515632
Iteration 7100: Loss = -11407.637681125378
1
Iteration 7200: Loss = -11407.63780776289
2
Iteration 7300: Loss = -11407.656571518077
3
Iteration 7400: Loss = -11407.629275033516
Iteration 7500: Loss = -11407.66741222661
1
Iteration 7600: Loss = -11407.628206157236
Iteration 7700: Loss = -11407.62873508464
1
Iteration 7800: Loss = -11407.642674481129
2
Iteration 7900: Loss = -11407.627404569866
Iteration 8000: Loss = -11407.632153300388
1
Iteration 8100: Loss = -11407.626918576372
Iteration 8200: Loss = -11407.664255416395
1
Iteration 8300: Loss = -11407.626582763773
Iteration 8400: Loss = -11407.673764581736
1
Iteration 8500: Loss = -11407.626229415953
Iteration 8600: Loss = -11407.699373877875
1
Iteration 8700: Loss = -11407.626310373227
2
Iteration 8800: Loss = -11407.641652747769
3
Iteration 8900: Loss = -11407.627849131055
4
Iteration 9000: Loss = -11407.649690222115
5
Iteration 9100: Loss = -11407.625701480141
Iteration 9200: Loss = -11407.647269058916
1
Iteration 9300: Loss = -11407.585901083263
Iteration 9400: Loss = -11407.585049016969
Iteration 9500: Loss = -11407.586180498221
1
Iteration 9600: Loss = -11407.58737658955
2
Iteration 9700: Loss = -11407.585294858773
3
Iteration 9800: Loss = -11407.592222141217
4
Iteration 9900: Loss = -11407.58597521677
5
Iteration 10000: Loss = -11407.585575501442
6
Iteration 10100: Loss = -11407.596363918241
7
Iteration 10200: Loss = -11407.584329024447
Iteration 10300: Loss = -11407.584282638123
Iteration 10400: Loss = -11407.613351560258
1
Iteration 10500: Loss = -11407.586867456701
2
Iteration 10600: Loss = -11407.586305525203
3
Iteration 10700: Loss = -11407.623313010265
4
Iteration 10800: Loss = -11407.591353981614
5
Iteration 10900: Loss = -11407.590872306428
6
Iteration 11000: Loss = -11407.587169095634
7
Iteration 11100: Loss = -11407.592515185184
8
Iteration 11200: Loss = -11407.5925817672
9
Iteration 11300: Loss = -11407.587886835508
10
Stopping early at iteration 11300 due to no improvement.
tensor([[ -9.5550,   4.9398],
        [ -9.1665,   4.5513],
        [ -9.5588,   4.9436],
        [ -9.2736,   4.6584],
        [  4.7639,  -9.3791],
        [ -9.6453,   5.0301],
        [  5.7298, -10.3450],
        [  4.4778,  -9.0930],
        [ -8.9109,   4.2957],
        [  5.1120,  -9.7272],
        [ -8.0795,   3.4643],
        [ -5.8366,   1.2214],
        [ -9.5512,   4.9360],
        [  5.1182,  -9.7334],
        [ -8.9886,   4.3734],
        [  2.7220,  -7.3373],
        [ -9.4251,   4.8099],
        [  5.4891, -10.1044],
        [ -9.4016,   4.7864],
        [ -9.0302,   4.4149],
        [ -9.6464,   5.0312],
        [  2.7556,  -7.3708],
        [  5.0504,  -9.6656],
        [  5.4830, -10.0982],
        [ -1.1917,  -3.4235],
        [ -9.3562,   4.7410],
        [ -9.3472,   4.7320],
        [ -9.2030,   4.5878],
        [ -9.1677,   4.5524],
        [ -9.2525,   4.6373],
        [  5.0775,  -9.6927],
        [  3.1023,  -7.7176],
        [ -8.8582,   4.2430],
        [ -9.2722,   4.6569],
        [ -8.6804,   4.0652],
        [  5.3617,  -9.9769],
        [  5.1350,  -9.7502],
        [  5.2533,  -9.8685],
        [  5.5667, -10.1819],
        [ -9.5551,   4.9398],
        [  5.2557,  -9.8709],
        [ -9.3689,   4.7537],
        [  2.1442,  -6.7594],
        [ -9.4281,   4.8129],
        [  5.3929, -10.0081],
        [  3.3670,  -7.9822],
        [  5.7749, -10.3901],
        [  4.8665,  -9.4817],
        [  3.6249,  -8.2401],
        [ -9.1979,   4.5826],
        [ -8.7116,   4.0964],
        [  5.0012,  -9.6164],
        [ -9.4652,   4.8499],
        [  4.0407,  -8.6559],
        [ -0.1143,  -4.5010],
        [  4.8035,  -9.4187],
        [  3.6082,  -8.2235],
        [  5.6828, -10.2980],
        [ -8.9672,   4.3520],
        [  4.3056,  -8.9208],
        [  5.3272,  -9.9424],
        [  4.4305,  -9.0457],
        [ -9.3981,   4.7829],
        [  4.9850,  -9.6003],
        [ -7.8215,   3.2062],
        [ -9.6777,   5.0625],
        [ -9.3383,   4.7230],
        [  2.8645,  -7.4797],
        [ -9.5621,   4.9469],
        [  5.0587,  -9.6739],
        [ -9.8686,   5.2534],
        [ -9.3476,   4.7324],
        [ -8.8459,   4.2307],
        [ -9.3804,   4.7652],
        [ -9.0219,   4.4067],
        [  5.6379, -10.2531],
        [ -9.4716,   4.8563],
        [  4.9341,  -9.5493],
        [ -9.8468,   5.2316],
        [-10.0623,   5.4471],
        [  3.5210,  -8.1363],
        [  4.9193,  -9.5346],
        [  5.0352,  -9.6504],
        [ -4.8207,   0.2055],
        [-10.0312,   5.4159],
        [ -8.8539,   4.2387],
        [  4.5710,  -9.1862],
        [ -8.7378,   4.1226],
        [ -8.6617,   4.0465],
        [ -9.4238,   4.8086],
        [  4.1961,  -8.8113],
        [  4.8867,  -9.5019],
        [  5.0708,  -9.6860],
        [  4.7014,  -9.3167],
        [  4.1219,  -8.7371],
        [  4.7881,  -9.4034],
        [ -9.2656,   4.6504],
        [  4.7661,  -9.3814],
        [  5.7309, -10.3461],
        [ -9.1325,   4.5173]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7760, 0.2240],
        [0.2735, 0.7265]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5025, 0.4975], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1923, 0.0910],
         [0.1795, 0.4039]],

        [[0.2609, 0.1035],
         [0.2653, 0.3364]],

        [[0.7252, 0.1020],
         [0.2556, 0.9052]],

        [[0.5089, 0.1129],
         [0.2905, 0.4919]],

        [[0.9853, 0.0929],
         [0.8852, 0.9590]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -13753.995132674401
Iteration 10: Loss = -11409.470821428526
Iteration 20: Loss = -11409.468152032443
Iteration 30: Loss = -11409.468153339687
1
Iteration 40: Loss = -11409.468153339687
2
Iteration 50: Loss = -11409.468153339687
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7230, 0.2770],
        [0.2269, 0.7731]], dtype=torch.float64)
alpha: tensor([0.4754, 0.5246])
beta: tensor([[[0.3956, 0.0905],
         [0.8300, 0.1884]],

        [[0.7695, 0.1034],
         [0.3175, 0.3158]],

        [[0.8615, 0.1027],
         [0.0534, 0.0353]],

        [[0.6622, 0.1131],
         [0.1701, 0.0261]],

        [[0.3316, 0.0929],
         [0.9557, 0.6130]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999982810025
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -13753.898993717587
Iteration 100: Loss = -11415.096728355864
Iteration 200: Loss = -11408.541227927342
Iteration 300: Loss = -11408.153376309421
Iteration 400: Loss = -11407.966224378222
Iteration 500: Loss = -11407.854651551732
Iteration 600: Loss = -11407.786301140384
Iteration 700: Loss = -11407.7409128984
Iteration 800: Loss = -11407.709665334842
Iteration 900: Loss = -11407.687005425194
Iteration 1000: Loss = -11407.669902490745
Iteration 1100: Loss = -11407.656621186736
Iteration 1200: Loss = -11407.64609735274
Iteration 1300: Loss = -11407.637561436155
Iteration 1400: Loss = -11407.63056884409
Iteration 1500: Loss = -11407.624770569895
Iteration 1600: Loss = -11407.619911163023
Iteration 1700: Loss = -11407.615798973431
Iteration 1800: Loss = -11407.612256249193
Iteration 1900: Loss = -11407.609190880781
Iteration 2000: Loss = -11407.606535669322
Iteration 2100: Loss = -11407.604232465157
Iteration 2200: Loss = -11407.608648244986
1
Iteration 2300: Loss = -11407.600424456974
Iteration 2400: Loss = -11407.71846160009
1
Iteration 2500: Loss = -11407.597374487425
Iteration 2600: Loss = -11407.596070171934
Iteration 2700: Loss = -11407.594969131276
Iteration 2800: Loss = -11407.594655456938
Iteration 2900: Loss = -11407.592975015878
Iteration 3000: Loss = -11407.592162574778
Iteration 3100: Loss = -11407.621487016479
1
Iteration 3200: Loss = -11407.590644534373
Iteration 3300: Loss = -11407.590025249428
Iteration 3400: Loss = -11407.591556747808
1
Iteration 3500: Loss = -11407.588952784916
Iteration 3600: Loss = -11407.61808594561
1
Iteration 3700: Loss = -11407.588097557144
Iteration 3800: Loss = -11407.595111392928
1
Iteration 3900: Loss = -11407.589532992024
2
Iteration 4000: Loss = -11407.590654621781
3
Iteration 4100: Loss = -11407.586567155397
Iteration 4200: Loss = -11407.587585152844
1
Iteration 4300: Loss = -11407.587623105988
2
Iteration 4400: Loss = -11407.600618063083
3
Iteration 4500: Loss = -11407.585778126557
Iteration 4600: Loss = -11407.586309239516
1
Iteration 4700: Loss = -11407.587261417633
2
Iteration 4800: Loss = -11407.592327406981
3
Iteration 4900: Loss = -11407.60659709406
4
Iteration 5000: Loss = -11407.586205626647
5
Iteration 5100: Loss = -11407.687363552795
6
Iteration 5200: Loss = -11407.58408133909
Iteration 5300: Loss = -11407.591138116286
1
Iteration 5400: Loss = -11407.594475329988
2
Iteration 5500: Loss = -11407.589628039443
3
Iteration 5600: Loss = -11407.597632203739
4
Iteration 5700: Loss = -11407.601290709483
5
Iteration 5800: Loss = -11407.590241817166
6
Iteration 5900: Loss = -11407.588843095571
7
Iteration 6000: Loss = -11407.633836437399
8
Iteration 6100: Loss = -11407.597872179598
9
Iteration 6200: Loss = -11407.583031768776
Iteration 6300: Loss = -11407.589372872675
1
Iteration 6400: Loss = -11407.583707257969
2
Iteration 6500: Loss = -11407.583211248499
3
Iteration 6600: Loss = -11407.584574349925
4
Iteration 6700: Loss = -11407.58334524355
5
Iteration 6800: Loss = -11407.586226120691
6
Iteration 6900: Loss = -11407.616664776724
7
Iteration 7000: Loss = -11407.584288217646
8
Iteration 7100: Loss = -11407.606053318837
9
Iteration 7200: Loss = -11407.588632915735
10
Stopping early at iteration 7200 due to no improvement.
tensor([[  5.2921,  -7.9778],
        [  6.7153,  -8.1476],
        [  6.6092,  -8.1529],
        [  6.0745,  -8.4760],
        [ -7.8665,   5.7213],
        [  5.7283, -10.3435],
        [ -8.1051,   6.5998],
        [ -7.2992,   5.8348],
        [  6.0996,  -8.4552],
        [ -7.2345,   5.8408],
        [  5.2401,  -6.6412],
        [  2.7851,  -4.2749],
        [  7.3375,  -8.7251],
        [ -8.3885,   5.6592],
        [  5.8792,  -8.5210],
        [ -5.8852,   4.2124],
        [  6.8389,  -8.4678],
        [ -7.9688,   6.2198],
        [  6.9954,  -8.4033],
        [  5.9400,  -8.2220],
        [  6.6568,  -8.7014],
        [ -5.9322,   4.2279],
        [ -7.3970,   5.8835],
        [ -7.4735,   5.3752],
        [ -1.9154,   0.3244],
        [  6.1247,  -8.3997],
        [  6.7160,  -8.2492],
        [  6.2395,  -7.6269],
        [  6.5934,  -7.9947],
        [  6.5294,  -7.9820],
        [ -7.8476,   6.4085],
        [ -6.1198,   4.7150],
        [  6.2461,  -8.1566],
        [  6.1238,  -7.7478],
        [  5.8150,  -7.7467],
        [ -7.7816,   6.3949],
        [ -8.3092,   5.8453],
        [ -7.7584,   6.3640],
        [ -7.7405,   6.1605],
        [  6.7735,  -8.9300],
        [ -7.8724,   6.4858],
        [  6.5844,  -8.0162],
        [ -5.4890,   3.4192],
        [  7.2063,  -8.6017],
        [ -7.9760,   6.2202],
        [ -6.8209,   4.5741],
        [ -8.1256,   6.7354],
        [ -8.8034,   5.2723],
        [ -6.8238,   5.0858],
        [  6.4500,  -7.8830],
        [  5.8325,  -7.2245],
        [ -7.5450,   6.1440],
        [  6.4862,  -8.0375],
        [ -7.1987,   5.7026],
        [ -3.7117,   0.6791],
        [ -7.2177,   5.4536],
        [ -6.7482,   5.0862],
        [ -8.3889,   6.4415],
        [  7.8858,  -9.4054],
        [ -7.2919,   5.4570],
        [ -7.6084,   6.1034],
        [ -8.0626,   5.5513],
        [  7.0723,  -8.7806],
        [ -7.5305,   6.0087],
        [  4.6988,  -6.4257],
        [  7.2959,  -8.9152],
        [  6.5444,  -8.3330],
        [ -6.1443,   4.2698],
        [  7.0658,  -8.4564],
        [ -7.4912,   6.0936],
        [  7.3840,  -8.8059],
        [  6.1645,  -7.6175],
        [  5.7974,  -7.3532],
        [  4.1565,  -5.5585],
        [  6.5566,  -7.9951],
        [ -8.5275,   6.7789],
        [  6.7407,  -8.1408],
        [ -7.4350,   5.9748],
        [  6.4467,  -7.9821],
        [  6.8062,  -8.3364],
        [ -6.7704,   4.8356],
        [ -7.7829,   5.9848],
        [ -7.9899,   5.9963],
        [  1.8043,  -3.2202],
        [  6.5500,  -8.3896],
        [  6.1155,  -7.9574],
        [ -7.4699,   5.9328],
        [  6.0194,  -7.6438],
        [  5.8635,  -7.4668],
        [  6.5153,  -8.0575],
        [ -6.8429,   5.2441],
        [ -7.6745,   5.7727],
        [ -7.8616,   6.0438],
        [ -6.8727,   5.4567],
        [ -7.0851,   5.6755],
        [ -9.0382,   5.1543],
        [  6.5286,  -7.9738],
        [ -7.6571,   6.2159],
        [ -7.6636,   5.5156],
        [  3.8452,  -5.2585]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7267, 0.2733],
        [0.2254, 0.7746]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5017, 0.4983], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4040, 0.0902],
         [0.8300, 0.1922]],

        [[0.7695, 0.1035],
         [0.3175, 0.3158]],

        [[0.8615, 0.1020],
         [0.0534, 0.0353]],

        [[0.6622, 0.1128],
         [0.1701, 0.0261]],

        [[0.3316, 0.0925],
         [0.9557, 0.6130]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -30475.93348105487
Iteration 10: Loss = -12175.87686281936
Iteration 20: Loss = -12175.74274910848
Iteration 30: Loss = -11971.459841738313
Iteration 40: Loss = -11409.470710675982
Iteration 50: Loss = -11409.468151703484
Iteration 60: Loss = -11409.46815436612
1
Iteration 70: Loss = -11409.46815436612
2
Iteration 80: Loss = -11409.46815436612
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7731, 0.2269],
        [0.2770, 0.7230]], dtype=torch.float64)
alpha: tensor([0.5246, 0.4754])
beta: tensor([[[0.1884, 0.0905],
         [0.9669, 0.3956]],

        [[0.1009, 0.1034],
         [0.0642, 0.4389]],

        [[0.6051, 0.1027],
         [0.6905, 0.3133]],

        [[0.2138, 0.1131],
         [0.2671, 0.9554]],

        [[0.5198, 0.0929],
         [0.6950, 0.7150]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999982810025
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30474.996683635556
Iteration 100: Loss = -12182.122292725857
Iteration 200: Loss = -12176.884879356037
Iteration 300: Loss = -12168.621062891843
Iteration 400: Loss = -12157.990055611637
Iteration 500: Loss = -12122.669048754178
Iteration 600: Loss = -12079.651330775998
Iteration 700: Loss = -11994.90796326286
Iteration 800: Loss = -11805.80476763935
Iteration 900: Loss = -11764.129223303476
Iteration 1000: Loss = -11753.601462640094
Iteration 1100: Loss = -11741.980906425917
Iteration 1200: Loss = -11717.113386928539
Iteration 1300: Loss = -11715.390809032802
Iteration 1400: Loss = -11704.551048962858
Iteration 1500: Loss = -11695.201353419528
Iteration 1600: Loss = -11690.395619660361
Iteration 1700: Loss = -11681.549793713353
Iteration 1800: Loss = -11681.395657491994
Iteration 1900: Loss = -11681.24970457152
Iteration 2000: Loss = -11677.557697911445
Iteration 2100: Loss = -11676.427391281355
Iteration 2200: Loss = -11676.353055894686
Iteration 2300: Loss = -11676.224487257672
Iteration 2400: Loss = -11670.348603610453
Iteration 2500: Loss = -11670.070699044138
Iteration 2600: Loss = -11670.029280211296
Iteration 2700: Loss = -11669.995674083548
Iteration 2800: Loss = -11669.96623256084
Iteration 2900: Loss = -11669.9393814341
Iteration 3000: Loss = -11669.917252944697
Iteration 3100: Loss = -11669.901117579164
Iteration 3200: Loss = -11669.881159620638
Iteration 3300: Loss = -11669.865767770698
Iteration 3400: Loss = -11669.733973533974
Iteration 3500: Loss = -11668.748213802843
Iteration 3600: Loss = -11666.422901070939
Iteration 3700: Loss = -11666.405032268995
Iteration 3800: Loss = -11666.392792495171
Iteration 3900: Loss = -11666.376165348342
Iteration 4000: Loss = -11662.365660996205
Iteration 4100: Loss = -11656.330888391913
Iteration 4200: Loss = -11656.30195421385
Iteration 4300: Loss = -11656.284812263055
Iteration 4400: Loss = -11656.17038649313
Iteration 4500: Loss = -11634.62696782043
Iteration 4600: Loss = -11634.616552040572
Iteration 4700: Loss = -11634.580965331626
Iteration 4800: Loss = -11634.57143995493
Iteration 4900: Loss = -11634.56257088856
Iteration 5000: Loss = -11634.371088798129
Iteration 5100: Loss = -11626.706040759906
Iteration 5200: Loss = -11615.301350884589
Iteration 5300: Loss = -11615.285667355725
Iteration 5400: Loss = -11615.279498648008
Iteration 5500: Loss = -11615.273081602772
Iteration 5600: Loss = -11599.400920495173
Iteration 5700: Loss = -11599.34320259912
Iteration 5800: Loss = -11599.33758576388
Iteration 5900: Loss = -11599.332352565612
Iteration 6000: Loss = -11599.311592017784
Iteration 6100: Loss = -11581.565308486122
Iteration 6200: Loss = -11581.55677292201
Iteration 6300: Loss = -11581.550155719795
Iteration 6400: Loss = -11581.54661288821
Iteration 6500: Loss = -11581.544017884355
Iteration 6600: Loss = -11581.541819848882
Iteration 6700: Loss = -11581.552566765591
1
Iteration 6800: Loss = -11575.96127447838
Iteration 6900: Loss = -11575.956484178556
Iteration 7000: Loss = -11575.953722674007
Iteration 7100: Loss = -11563.203009512557
Iteration 7200: Loss = -11562.25438382217
Iteration 7300: Loss = -11527.543077152486
Iteration 7400: Loss = -11527.529765012645
Iteration 7500: Loss = -11527.536079984398
1
Iteration 7600: Loss = -11527.491204502508
Iteration 7700: Loss = -11514.271859089224
Iteration 7800: Loss = -11514.276104400129
1
Iteration 7900: Loss = -11514.268006817398
Iteration 8000: Loss = -11514.266907815361
Iteration 8100: Loss = -11502.476543561677
Iteration 8200: Loss = -11502.47531594479
Iteration 8300: Loss = -11498.642542674523
Iteration 8400: Loss = -11498.641508621597
Iteration 8500: Loss = -11498.63991526371
Iteration 8600: Loss = -11498.639160103949
Iteration 8700: Loss = -11481.60776596351
Iteration 8800: Loss = -11481.583628022747
Iteration 8900: Loss = -11462.664846889791
Iteration 9000: Loss = -11462.663758527158
Iteration 9100: Loss = -11462.725089113062
1
Iteration 9200: Loss = -11462.34085120985
Iteration 9300: Loss = -11447.561031039098
Iteration 9400: Loss = -11447.567015740628
1
Iteration 9500: Loss = -11442.986089318647
Iteration 9600: Loss = -11443.038374688485
1
Iteration 9700: Loss = -11442.981517579086
Iteration 9800: Loss = -11442.976017036874
Iteration 9900: Loss = -11442.976188421142
1
Iteration 10000: Loss = -11443.008396475458
2
Iteration 10100: Loss = -11443.002688975406
3
Iteration 10200: Loss = -11442.976356188658
4
Iteration 10300: Loss = -11442.991473796372
5
Iteration 10400: Loss = -11442.998962881613
6
Iteration 10500: Loss = -11443.08172341637
7
Iteration 10600: Loss = -11442.984629536757
8
Iteration 10700: Loss = -11442.975127471345
Iteration 10800: Loss = -11442.974344276301
Iteration 10900: Loss = -11442.984570441778
1
Iteration 11000: Loss = -11442.975326252486
2
Iteration 11100: Loss = -11442.987242676978
3
Iteration 11200: Loss = -11442.975469480332
4
Iteration 11300: Loss = -11442.976897429202
5
Iteration 11400: Loss = -11442.987373914764
6
Iteration 11500: Loss = -11442.98502418793
7
Iteration 11600: Loss = -11443.01190151107
8
Iteration 11700: Loss = -11442.988452495383
9
Iteration 11800: Loss = -11442.986288361952
10
Stopping early at iteration 11800 due to no improvement.
tensor([[ -7.0500,   5.5707],
        [ -9.3550,   6.7586],
        [ -7.6026,   5.8609],
        [ -9.1515,   6.0626],
        [  5.6296,  -7.6397],
        [ -8.3942,   6.2396],
        [  6.5599,  -7.9513],
        [  5.2069,  -6.7201],
        [ -7.0108,   5.6244],
        [  6.1800,  -7.6691],
        [ -7.5972,   4.2083],
        [ -4.9462,   2.0486],
        [ -7.6052,   5.9672],
        [  5.6972,  -9.2393],
        [ -7.4390,   5.8042],
        [  4.1364,  -6.1713],
        [ -7.8457,   6.3819],
        [  5.7677,  -7.1555],
        [ -9.2082,   5.2032],
        [ -7.9765,   6.3720],
        [ -8.0857,   6.4679],
        [  4.3238,  -5.9128],
        [  4.8243,  -8.2088],
        [  6.2486,  -7.6616],
        [  0.4654,  -1.8552],
        [ -8.8453,   6.3833],
        [ -7.8523,   5.6608],
        [ -8.3777,   6.5338],
        [ -6.9603,   5.4192],
        [ -7.7047,   6.1275],
        [  5.6454,  -7.2372],
        [  4.6754,  -6.0899],
        [ -8.3747,   5.8512],
        [ -7.8043,   6.3015],
        [ -7.5902,   5.6028],
        [  5.8519,  -7.6288],
        [  5.8038,  -7.2922],
        [  6.2333,  -8.5590],
        [  6.0984,  -7.9155],
        [ -7.4470,   6.0567],
        [  6.3545,  -7.9134],
        [ -7.4969,   6.0677],
        [  3.3405,  -5.6699],
        [ -9.6678,   5.3133],
        [  5.7130,  -7.2437],
        [  4.8210,  -6.2869],
        [  6.0722,  -7.5079],
        [  7.1750,  -8.6243],
        [  4.8001,  -7.0969],
        [ -8.1908,   6.1620],
        [ -7.5356,   5.4525],
        [  5.5918,  -7.2141],
        [ -7.6541,   6.0197],
        [  5.4419,  -7.5274],
        [  1.5546,  -2.9660],
        [  5.8430,  -7.4525],
        [  5.0206,  -6.4687],
        [  6.0794,  -8.2742],
        [ -9.1984,   5.6930],
        [  6.7434,  -8.2654],
        [  5.9175,  -7.3042],
        [  5.5803,  -9.9270],
        [-10.0606,   6.2015],
        [  5.6891,  -7.2746],
        [ -7.7902,   3.1750],
        [-10.1686,   5.5534],
        [ -7.9565,   6.5701],
        [  4.2716,  -6.1913],
        [ -8.5097,   5.8225],
        [  5.8030,  -7.6906],
        [ -8.3529,   6.6223],
        [ -7.4250,   5.9468],
        [ -7.4471,   5.6015],
        [ -5.4858,   3.9705],
        [ -7.3179,   5.9071],
        [  6.1304,  -7.8969],
        [ -8.7371,   7.0868],
        [  5.6858,  -7.3881],
        [ -9.0459,   5.4420],
        [ -8.2251,   6.8116],
        [  5.0758,  -6.4911],
        [  5.5614,  -9.0535],
        [  5.8136,  -8.2272],
        [ -8.3786,   4.8438],
        [ -8.7984,   5.6434],
        [ -7.1697,   5.7821],
        [  7.2386,  -8.7937],
        [ -6.9779,   5.5000],
        [ -8.1680,   5.3389],
        [ -8.0897,   6.4099],
        [  5.1866,  -6.8738],
        [  5.4552,  -7.2084],
        [  6.1368,  -7.5479],
        [  6.1085,  -7.5271],
        [  4.3370,  -7.6730],
        [  5.8265,  -7.4600],
        [ -7.5657,   5.8991],
        [  5.6726,  -7.1587],
        [  6.3473,  -7.7480],
        [ -6.4003,   2.4537]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7688, 0.2312],
        [0.2915, 0.7085]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4982, 0.5018], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1891, 0.0908],
         [0.9669, 0.4078]],

        [[0.1009, 0.1034],
         [0.0642, 0.4389]],

        [[0.6051, 0.1178],
         [0.6905, 0.3133]],

        [[0.2138, 0.1134],
         [0.2671, 0.9554]],

        [[0.5198, 0.0929],
         [0.6950, 0.7150]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8081003563518231
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603206095034849
Average Adjusted Rand Index: 0.9616200712703646
Iteration 0: Loss = -21263.117087045524
Iteration 10: Loss = -12106.089543672157
Iteration 20: Loss = -11409.469952989903
Iteration 30: Loss = -11409.468152032452
Iteration 40: Loss = -11409.468153339687
1
Iteration 50: Loss = -11409.468153339687
2
Iteration 60: Loss = -11409.468153339687
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7230, 0.2770],
        [0.2269, 0.7731]], dtype=torch.float64)
alpha: tensor([0.4754, 0.5246])
beta: tensor([[[0.3956, 0.0905],
         [0.9868, 0.1884]],

        [[0.7307, 0.1034],
         [0.4057, 0.2429]],

        [[0.7259, 0.1027],
         [0.1203, 0.3023]],

        [[0.5946, 0.1131],
         [0.0865, 0.4766]],

        [[0.6568, 0.0929],
         [0.1072, 0.3007]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999982810025
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21128.379296112314
Iteration 100: Loss = -12040.58270980524
Iteration 200: Loss = -11650.468548569364
Iteration 300: Loss = -11626.530988746192
Iteration 400: Loss = -11601.052456500678
Iteration 500: Loss = -11549.188346572211
Iteration 600: Loss = -11531.094262637202
Iteration 700: Loss = -11520.361229102955
Iteration 800: Loss = -11505.21313339773
Iteration 900: Loss = -11504.102570820094
Iteration 1000: Loss = -11500.247309862498
Iteration 1100: Loss = -11500.054089909938
Iteration 1200: Loss = -11480.685267918443
Iteration 1300: Loss = -11478.539323703255
Iteration 1400: Loss = -11478.457013797793
Iteration 1500: Loss = -11477.664281943022
Iteration 1600: Loss = -11462.239926771219
Iteration 1700: Loss = -11462.204653326764
Iteration 1800: Loss = -11462.17643508533
Iteration 1900: Loss = -11462.152485932655
Iteration 2000: Loss = -11462.12969182264
Iteration 2100: Loss = -11462.09456578548
Iteration 2200: Loss = -11448.699079427002
Iteration 2300: Loss = -11448.68151736651
Iteration 2400: Loss = -11448.669926578965
Iteration 2500: Loss = -11448.66000124625
Iteration 2600: Loss = -11448.651377934575
Iteration 2700: Loss = -11448.643346584047
Iteration 2800: Loss = -11448.632900601591
Iteration 2900: Loss = -11448.61510858961
Iteration 3000: Loss = -11434.736383741454
Iteration 3100: Loss = -11434.730469181675
Iteration 3200: Loss = -11434.725722240664
Iteration 3300: Loss = -11434.721506899663
Iteration 3400: Loss = -11434.717687143697
Iteration 3500: Loss = -11434.71420514696
Iteration 3600: Loss = -11434.711139691
Iteration 3700: Loss = -11434.708271528536
Iteration 3800: Loss = -11434.705635146409
Iteration 3900: Loss = -11434.703242068063
Iteration 4000: Loss = -11434.701014775575
Iteration 4100: Loss = -11434.69887784952
Iteration 4200: Loss = -11434.696474169363
Iteration 4300: Loss = -11434.670084996846
Iteration 4400: Loss = -11427.578183090407
Iteration 4500: Loss = -11427.576516862087
Iteration 4600: Loss = -11427.575050251404
Iteration 4700: Loss = -11427.573698397377
Iteration 4800: Loss = -11427.572511530012
Iteration 4900: Loss = -11427.571387388482
Iteration 5000: Loss = -11427.570353796053
Iteration 5100: Loss = -11427.569365034395
Iteration 5200: Loss = -11427.56846127969
Iteration 5300: Loss = -11427.567548596631
Iteration 5400: Loss = -11427.56661354697
Iteration 5500: Loss = -11427.564928499718
Iteration 5600: Loss = -11421.729843110119
Iteration 5700: Loss = -11421.727426516707
Iteration 5800: Loss = -11421.726565873149
Iteration 5900: Loss = -11421.716633882248
Iteration 6000: Loss = -11420.530620050513
Iteration 6100: Loss = -11420.529957239738
Iteration 6200: Loss = -11420.529459742254
Iteration 6300: Loss = -11420.532649974142
1
Iteration 6400: Loss = -11420.528593917197
Iteration 6500: Loss = -11420.528209121447
Iteration 6600: Loss = -11420.527806686012
Iteration 6700: Loss = -11420.527470919436
Iteration 6800: Loss = -11420.527155079366
Iteration 6900: Loss = -11420.526843682997
Iteration 7000: Loss = -11420.526970064235
1
Iteration 7100: Loss = -11420.547753131816
2
Iteration 7200: Loss = -11420.526061907938
Iteration 7300: Loss = -11420.535489607264
1
Iteration 7400: Loss = -11420.525721231346
Iteration 7500: Loss = -11420.525382914808
Iteration 7600: Loss = -11420.52353261985
Iteration 7700: Loss = -11415.270314260806
Iteration 7800: Loss = -11415.273958513919
1
Iteration 7900: Loss = -11415.2698744736
Iteration 8000: Loss = -11415.2754876813
1
Iteration 8100: Loss = -11415.270058583561
2
Iteration 8200: Loss = -11415.306072492165
3
Iteration 8300: Loss = -11415.2691981921
Iteration 8400: Loss = -11415.26942756373
1
Iteration 8500: Loss = -11415.271248690751
2
Iteration 8600: Loss = -11415.294233345667
3
Iteration 8700: Loss = -11415.273147967244
4
Iteration 8800: Loss = -11415.40292103633
5
Iteration 8900: Loss = -11415.273970764554
6
Iteration 9000: Loss = -11415.299811847202
7
Iteration 9100: Loss = -11415.286030133424
8
Iteration 9200: Loss = -11415.27750877433
9
Iteration 9300: Loss = -11415.301539242093
10
Stopping early at iteration 9300 due to no improvement.
tensor([[ 5.9188, -7.6163],
        [ 5.8645, -7.2554],
        [ 5.9554, -7.9274],
        [ 5.6906, -8.2114],
        [-7.0172,  5.5756],
        [ 6.4511, -8.9676],
        [-7.3482,  5.6392],
        [-7.6952,  4.6998],
        [ 5.6262, -8.0971],
        [-7.7767,  6.2046],
        [ 4.4215, -7.4493],
        [ 2.6982, -4.3380],
        [ 6.1892, -8.0279],
        [-7.2436,  5.7941],
        [ 5.8083, -7.2191],
        [-5.7740,  4.3875],
        [ 6.1388, -8.4243],
        [-7.8808,  5.9135],
        [ 6.0271, -7.5267],
        [ 5.7895, -7.1762],
        [ 6.3952, -8.5981],
        [-6.9607,  3.3428],
        [-7.1424,  5.3252],
        [-8.2455,  6.7973],
        [-2.7913, -0.5314],
        [ 6.1288, -7.5563],
        [ 6.4014, -7.8187],
        [ 5.8620, -7.4508],
        [ 5.4069, -8.3803],
        [ 5.9462, -7.6756],
        [-8.2325,  5.3898],
        [-6.4569,  4.3677],
        [ 5.7632, -7.1572],
        [ 5.9598, -7.3607],
        [ 5.5492, -6.9508],
        [-7.7052,  5.5363],
        [-7.7612,  5.2276],
        [-7.9750,  5.8077],
        [-8.9184,  5.1691],
        [ 6.2157, -7.6037],
        [-7.3141,  5.8156],
        [ 6.1572, -8.4454],
        [-5.3488,  3.6120],
        [ 6.4324, -7.9524],
        [-7.2679,  5.8423],
        [-6.2271,  4.6603],
        [-8.0191,  6.0717],
        [-7.1563,  5.6244],
        [-6.5353,  4.9731],
        [ 5.9194, -7.3791],
        [ 4.9595, -7.6793],
        [-7.0648,  5.6477],
        [ 5.5178, -7.0878],
        [-7.2687,  5.2441],
        [-2.9703,  1.4789],
        [-7.1133,  5.4857],
        [-7.0606,  4.4929],
        [-7.3994,  6.0130],
        [ 5.2803, -8.0091],
        [-6.9059,  5.2116],
        [-7.2302,  5.8439],
        [-7.2983,  5.5522],
        [ 5.7480, -8.4824],
        [-6.8700,  5.4334],
        [ 4.6080, -6.0866],
        [ 6.3444, -7.8366],
        [ 6.1542, -7.7191],
        [-6.1050,  4.3027],
        [ 6.1481, -8.2551],
        [-7.2810,  5.8029],
        [ 6.2193, -7.9086],
        [ 4.6538, -9.2690],
        [ 5.7660, -7.3428],
        [ 3.3325, -6.2726],
        [ 5.6227, -7.5792],
        [-7.6259,  5.9089],
        [ 6.1602, -7.6519],
        [-6.9328,  5.3632],
        [ 6.3019, -7.8148],
        [ 6.2268, -8.9916],
        [-6.6792,  4.4808],
        [-6.9423,  5.3360],
        [-7.1245,  5.4393],
        [ 1.6509, -3.2687],
        [ 5.6401, -7.3483],
        [ 5.5712, -6.9826],
        [-7.4626,  5.6423],
        [ 5.6045, -7.3917],
        [ 5.2602, -7.0188],
        [ 5.3791, -9.5708],
        [-8.1077,  4.7727],
        [-7.1540,  5.4089],
        [-7.4550,  5.4198],
        [-6.9230,  5.0509],
        [-6.3946,  5.0072],
        [-7.9165,  5.7160],
        [ 5.9943, -8.3718],
        [-8.8904,  5.4760],
        [-7.7296,  6.0455],
        [ 3.7357, -5.2427]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7184, 0.2816],
        [0.2288, 0.7712]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4961, 0.5039], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4062, 0.0915],
         [0.9868, 0.1909]],

        [[0.7307, 0.1083],
         [0.4057, 0.2429]],

        [[0.7259, 0.1024],
         [0.1203, 0.3023]],

        [[0.5946, 0.1129],
         [0.0865, 0.4766]],

        [[0.6568, 0.0929],
         [0.1072, 0.3007]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999982810025
Average Adjusted Rand Index: 0.9919998119331364
Iteration 0: Loss = -21359.627393125706
Iteration 10: Loss = -11410.074990304167
Iteration 20: Loss = -11409.468152004294
Iteration 30: Loss = -11409.468154007885
1
Iteration 40: Loss = -11409.468154007885
2
Iteration 50: Loss = -11409.468154007885
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7731, 0.2269],
        [0.2770, 0.7230]], dtype=torch.float64)
alpha: tensor([0.5246, 0.4754])
beta: tensor([[[0.1884, 0.0905],
         [0.4707, 0.3956]],

        [[0.6910, 0.1034],
         [0.0915, 0.2015]],

        [[0.1577, 0.1027],
         [0.6280, 0.3625]],

        [[0.2234, 0.1131],
         [0.4886, 0.5671]],

        [[0.8980, 0.0929],
         [0.2430, 0.6026]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999982810025
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21359.399205659793
Iteration 100: Loss = -12205.684626548647
Iteration 200: Loss = -12149.854076499629
Iteration 300: Loss = -12073.40528111749
Iteration 400: Loss = -12043.29185576716
Iteration 500: Loss = -12020.399988979145
Iteration 600: Loss = -11783.880263914609
Iteration 700: Loss = -11663.424959239808
Iteration 800: Loss = -11541.375933525162
Iteration 900: Loss = -11510.29829806583
Iteration 1000: Loss = -11484.265614455078
Iteration 1100: Loss = -11449.242445331038
Iteration 1200: Loss = -11448.93391007981
Iteration 1300: Loss = -11431.073626909072
Iteration 1400: Loss = -11420.850937161287
Iteration 1500: Loss = -11420.630410537695
Iteration 1600: Loss = -11420.564401096794
Iteration 1700: Loss = -11420.520738247986
Iteration 1800: Loss = -11420.485900194355
Iteration 1900: Loss = -11420.457404862806
Iteration 2000: Loss = -11420.433785409297
Iteration 2100: Loss = -11420.413801552739
Iteration 2200: Loss = -11420.39678170241
Iteration 2300: Loss = -11420.382104384065
Iteration 2400: Loss = -11420.36936716493
Iteration 2500: Loss = -11420.358140731125
Iteration 2600: Loss = -11420.348273239353
Iteration 2700: Loss = -11420.339472457707
Iteration 2800: Loss = -11420.331576054106
Iteration 2900: Loss = -11420.324361051786
Iteration 3000: Loss = -11420.317775738278
Iteration 3100: Loss = -11420.311043101217
Iteration 3200: Loss = -11420.299787317523
Iteration 3300: Loss = -11416.932320928194
Iteration 3400: Loss = -11416.923604827492
Iteration 3500: Loss = -11416.919800019212
Iteration 3600: Loss = -11416.916542837345
Iteration 3700: Loss = -11416.913125594836
Iteration 3800: Loss = -11416.910192682753
Iteration 3900: Loss = -11416.907737506452
Iteration 4000: Loss = -11416.905021310922
Iteration 4100: Loss = -11416.902650014272
Iteration 4200: Loss = -11416.90063230084
Iteration 4300: Loss = -11416.898415378331
Iteration 4400: Loss = -11416.896517408755
Iteration 4500: Loss = -11416.895762300546
Iteration 4600: Loss = -11416.892889744413
Iteration 4700: Loss = -11416.894864044621
1
Iteration 4800: Loss = -11416.888303303924
Iteration 4900: Loss = -11416.873379399323
Iteration 5000: Loss = -11416.860752403556
Iteration 5100: Loss = -11416.859924546108
Iteration 5200: Loss = -11416.865868522587
1
Iteration 5300: Loss = -11407.669169387136
Iteration 5400: Loss = -11407.66701819738
Iteration 5500: Loss = -11407.667309884557
1
Iteration 5600: Loss = -11407.66565063236
Iteration 5700: Loss = -11407.665086662702
Iteration 5800: Loss = -11407.678342430852
1
Iteration 5900: Loss = -11407.669032243912
2
Iteration 6000: Loss = -11407.659675495464
Iteration 6100: Loss = -11407.658691881448
Iteration 6200: Loss = -11407.694965712632
1
Iteration 6300: Loss = -11407.70336230892
2
Iteration 6400: Loss = -11407.711707573957
3
Iteration 6500: Loss = -11407.689709119104
4
Iteration 6600: Loss = -11407.66186666911
5
Iteration 6700: Loss = -11407.690637444712
6
Iteration 6800: Loss = -11407.656465177264
Iteration 6900: Loss = -11407.669613606644
1
Iteration 7000: Loss = -11407.660878804563
2
Iteration 7100: Loss = -11407.666064048448
3
Iteration 7200: Loss = -11407.654922041955
Iteration 7300: Loss = -11407.656228408756
1
Iteration 7400: Loss = -11407.654978912857
2
Iteration 7500: Loss = -11407.656175684204
3
Iteration 7600: Loss = -11407.708115336447
4
Iteration 7700: Loss = -11407.656906841958
5
Iteration 7800: Loss = -11407.653316389366
Iteration 7900: Loss = -11407.654852316175
1
Iteration 8000: Loss = -11407.652192464502
Iteration 8100: Loss = -11407.651621320076
Iteration 8200: Loss = -11407.655604098703
1
Iteration 8300: Loss = -11407.65769390467
2
Iteration 8400: Loss = -11407.714122463609
3
Iteration 8500: Loss = -11407.667304664996
4
Iteration 8600: Loss = -11407.654058433127
5
Iteration 8700: Loss = -11407.651200424905
Iteration 8800: Loss = -11407.657797370643
1
Iteration 8900: Loss = -11407.655610996306
2
Iteration 9000: Loss = -11407.655141877842
3
Iteration 9100: Loss = -11407.650853817651
Iteration 9200: Loss = -11407.654311191161
1
Iteration 9300: Loss = -11407.652743020984
2
Iteration 9400: Loss = -11407.653504583875
3
Iteration 9500: Loss = -11407.657620816975
4
Iteration 9600: Loss = -11407.64992485918
Iteration 9700: Loss = -11407.649892570606
Iteration 9800: Loss = -11407.652130683688
1
Iteration 9900: Loss = -11407.651777001616
2
Iteration 10000: Loss = -11407.661177762866
3
Iteration 10100: Loss = -11407.66142870991
4
Iteration 10200: Loss = -11407.654442827328
5
Iteration 10300: Loss = -11407.649266771568
Iteration 10400: Loss = -11407.69564938132
1
Iteration 10500: Loss = -11407.691968112964
2
Iteration 10600: Loss = -11407.708458494883
3
Iteration 10700: Loss = -11407.687476120162
4
Iteration 10800: Loss = -11407.668985511578
5
Iteration 10900: Loss = -11407.664040794636
6
Iteration 11000: Loss = -11407.703274821399
7
Iteration 11100: Loss = -11407.693889903458
8
Iteration 11200: Loss = -11407.65982914783
9
Iteration 11300: Loss = -11407.661256438221
10
Stopping early at iteration 11300 due to no improvement.
tensor([[ -7.6371,   5.9739],
        [ -7.5742,   6.0995],
        [ -8.3438,   6.7651],
        [ -7.9595,   6.5723],
        [  6.4589,  -7.9187],
        [ -8.3537,   6.6653],
        [  6.6449,  -8.1391],
        [  6.7021,  -8.1295],
        [ -8.0278,   6.6170],
        [  6.3221,  -8.4962],
        [ -6.5706,   5.0688],
        [ -4.5728,   2.4862],
        [ -8.5379,   6.5394],
        [  7.1887, -10.2155],
        [ -7.6200,   6.1886],
        [  3.6802,  -6.3847],
        [ -8.2131,   6.7914],
        [  7.7997,  -9.2022],
        [ -8.0601,   6.2598],
        [ -8.0419,   6.5708],
        [ -7.9952,   6.5423],
        [  3.8966,  -6.2359],
        [  7.3633,  -9.4804],
        [  6.5192,  -8.9326],
        [ -0.0910,  -2.3292],
        [ -8.8658,   6.3627],
        [ -8.9801,   7.4311],
        [ -8.5211,   6.5770],
        [ -8.8981,   6.7237],
        [ -8.3845,   6.4853],
        [  7.1673,  -8.5667],
        [  4.6436,  -6.1918],
        [ -8.1691,   6.5011],
        [ -7.7137,   6.1987],
        [ -7.2665,   5.8060],
        [  7.1431,  -8.7769],
        [  7.8754,  -9.3271],
        [  7.6220,  -9.2642],
        [  7.3557,  -8.9500],
        [ -8.1516,   6.7397],
        [  6.2024,  -9.3483],
        [ -8.6101,   6.9756],
        [  3.6246,  -5.2828],
        [ -8.1007,   6.7087],
        [  7.0109, -11.6261],
        [  7.8844,  -9.6501],
        [  7.4158, -12.0310],
        [  5.8486,  -9.0201],
        [  5.3189,  -7.1578],
        [ -8.4017,   6.4314],
        [ -7.4034,   5.9044],
        [  7.1923,  -8.7383],
        [ -8.8973,   7.4286],
        [  5.7261,  -7.2991],
        [  0.6731,  -3.7171],
        [  6.2246,  -7.6997],
        [  4.9221,  -6.9523],
        [  7.6748,  -9.0660],
        [ -8.7807,   7.0293],
        [  5.5638,  -7.6425],
        [  6.6551,  -9.2512],
        [  6.3959,  -8.0974],
        [-11.2662,   6.6509],
        [  7.1109,  -9.0647],
        [ -7.5401,   3.6403],
        [ -8.7251,   6.8182],
        [ -9.8254,   6.7304],
        [  4.3755,  -6.0020],
        [ -8.1791,   6.7914],
        [  5.9514,  -7.7727],
        [ -8.5202,   7.1315],
        [ -8.7003,   6.5262],
        [ -7.2806,   5.6758],
        [ -7.1574,   2.5422],
        [ -7.5730,   6.1829],
        [  7.5656,  -9.1713],
        [ -8.0835,   6.6787],
        [  7.8706, -10.0589],
        [ -8.4668,   6.6946],
        [ -8.7035,   6.5085],
        [  4.8616,  -6.8210],
        [  7.7103,  -9.2163],
        [  6.4229,  -7.8515],
        [ -3.2675,   1.7592],
        [ -7.8385,   6.4386],
        [ -8.1411,   6.7488],
        [  6.3468,  -7.8786],
        [ -7.1445,   5.7331],
        [ -8.1367,   5.7114],
        [ -8.0353,   6.6300],
        [  5.4442,  -7.2937],
        [  7.2417,  -9.1055],
        [  6.8227,  -8.2441],
        [  5.4920,  -7.0201],
        [  5.9142,  -7.4930],
        [  7.1591,  -8.8754],
        [ -9.0179,   6.7370],
        [  6.3941,  -9.7885],
        [  6.7699,  -8.8132],
        [ -5.5241,   3.5711]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7750, 0.2250],
        [0.2736, 0.7264]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4990, 0.5010], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1925, 0.0905],
         [0.4707, 0.4036]],

        [[0.6910, 0.1032],
         [0.0915, 0.2015]],

        [[0.1577, 0.1022],
         [0.6280, 0.3625]],

        [[0.2234, 0.1138],
         [0.4886, 0.5671]],

        [[0.8980, 0.0928],
         [0.2430, 0.6026]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11414.61793705235
new:  [1.0, 0.9603206095034849, 0.9919999982810025, 1.0] [1.0, 0.9616200712703646, 0.9919998119331364, 1.0] [11407.588632915735, 11442.986288361952, 11415.301539242093, 11407.661256438221]
prior:  [0.9919999982810025, 0.9919999982810025, 0.9919999982810025, 0.9919999982810025] [0.9919998119331364, 0.9919998119331364, 0.9919998119331364, 0.9919998119331364] [11409.468153339687, 11409.46815436612, 11409.468153339687, 11409.468154007885]
-----------------------------------------------------------------------------------------
This iteration is 28
True Objective function: Loss = -11435.40992923985
Iteration 0: Loss = -33090.920966999925
Iteration 10: Loss = -11435.406112307182
Iteration 20: Loss = -11435.40600802896
Iteration 30: Loss = -11435.406008041788
1
Iteration 40: Loss = -11435.406008041788
2
Iteration 50: Loss = -11435.406008041788
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7471, 0.2529],
        [0.2426, 0.7574]], dtype=torch.float64)
alpha: tensor([0.4805, 0.5195])
beta: tensor([[[0.3934, 0.0960],
         [0.7345, 0.1937]],

        [[0.9118, 0.0973],
         [0.1552, 0.2573]],

        [[0.4839, 0.1016],
         [0.9233, 0.0541]],

        [[0.1422, 0.0975],
         [0.8206, 0.2218]],

        [[0.6790, 0.0997],
         [0.3612, 0.3826]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33115.359631472784
Iteration 100: Loss = -12014.868272023838
Iteration 200: Loss = -11999.571743046556
Iteration 300: Loss = -11998.055256121394
Iteration 400: Loss = -11997.280710413745
Iteration 500: Loss = -11996.792118316047
Iteration 600: Loss = -11989.880912187875
Iteration 700: Loss = -11989.623708473806
Iteration 800: Loss = -11987.319248608232
Iteration 900: Loss = -11985.167752771813
Iteration 1000: Loss = -11983.803931576424
Iteration 1100: Loss = -11982.38037334102
Iteration 1200: Loss = -11976.048306577739
Iteration 1300: Loss = -11970.146687650518
Iteration 1400: Loss = -11969.48763519511
Iteration 1500: Loss = -11968.801902527115
Iteration 1600: Loss = -11967.185235779232
Iteration 1700: Loss = -11966.486153337282
Iteration 1800: Loss = -11962.788757522248
Iteration 1900: Loss = -11958.365052031495
Iteration 2000: Loss = -11953.747365313688
Iteration 2100: Loss = -11952.286137742605
Iteration 2200: Loss = -11946.217040277494
Iteration 2300: Loss = -11945.736764987809
Iteration 2400: Loss = -11945.32755955167
Iteration 2500: Loss = -11945.059176687326
Iteration 2600: Loss = -11944.871154424101
Iteration 2700: Loss = -11944.720550571057
Iteration 2800: Loss = -11944.585372615647
Iteration 2900: Loss = -11944.509531596823
Iteration 3000: Loss = -11944.480247383364
Iteration 3100: Loss = -11944.46013878561
Iteration 3200: Loss = -11944.441055261264
Iteration 3300: Loss = -11944.428668431936
Iteration 3400: Loss = -11944.41820704688
Iteration 3500: Loss = -11944.409227877051
Iteration 3600: Loss = -11944.401351915527
Iteration 3700: Loss = -11944.394213212463
Iteration 3800: Loss = -11944.38698200864
Iteration 3900: Loss = -11944.377303541889
Iteration 4000: Loss = -11944.362109889478
Iteration 4100: Loss = -11944.347251706538
Iteration 4200: Loss = -11944.337557473551
Iteration 4300: Loss = -11944.331261636045
Iteration 4400: Loss = -11944.326650546851
Iteration 4500: Loss = -11944.323006020593
Iteration 4600: Loss = -11944.319957691518
Iteration 4700: Loss = -11944.317307637722
Iteration 4800: Loss = -11944.314974879575
Iteration 4900: Loss = -11944.148199180903
Iteration 5000: Loss = -11938.482358528921
Iteration 5100: Loss = -11938.48006026335
Iteration 5200: Loss = -11938.479131881262
Iteration 5300: Loss = -11938.476026536177
Iteration 5400: Loss = -11938.474481859426
Iteration 5500: Loss = -11938.471519296148
Iteration 5600: Loss = -11938.47082395915
Iteration 5700: Loss = -11938.466893344843
Iteration 5800: Loss = -11938.464969491184
Iteration 5900: Loss = -11938.463329218695
Iteration 6000: Loss = -11938.461950591569
Iteration 6100: Loss = -11938.460881653797
Iteration 6200: Loss = -11938.459712362244
Iteration 6300: Loss = -11938.458770569985
Iteration 6400: Loss = -11938.460182912822
1
Iteration 6500: Loss = -11938.45714686117
Iteration 6600: Loss = -11938.456484774471
Iteration 6700: Loss = -11938.53870931882
1
Iteration 6800: Loss = -11938.455142954153
Iteration 6900: Loss = -11938.454524996752
Iteration 7000: Loss = -11938.454005670686
Iteration 7100: Loss = -11938.453463241629
Iteration 7200: Loss = -11938.452888321923
Iteration 7300: Loss = -11938.556107698232
1
Iteration 7400: Loss = -11938.451611776365
Iteration 7500: Loss = -11938.450715400788
Iteration 7600: Loss = -11938.449427072368
Iteration 7700: Loss = -11938.449418280414
Iteration 7800: Loss = -11938.46305212909
1
Iteration 7900: Loss = -11938.584367768335
2
Iteration 8000: Loss = -11938.44568614167
Iteration 8100: Loss = -11938.445527345084
Iteration 8200: Loss = -11938.444869504001
Iteration 8300: Loss = -11938.466828954695
1
Iteration 8400: Loss = -11938.444230344914
Iteration 8500: Loss = -11938.443850006706
Iteration 8600: Loss = -11938.508169207631
1
Iteration 8700: Loss = -11938.443073549732
Iteration 8800: Loss = -11938.442498997234
Iteration 8900: Loss = -11938.507712155979
1
Iteration 9000: Loss = -11938.438033153057
Iteration 9100: Loss = -11938.394390622572
Iteration 9200: Loss = -11938.389636672182
Iteration 9300: Loss = -11938.382522742595
Iteration 9400: Loss = -11938.381706713997
Iteration 9500: Loss = -11938.383233127926
1
Iteration 9600: Loss = -11938.382342380462
2
Iteration 9700: Loss = -11938.381740326602
3
Iteration 9800: Loss = -11938.380903997437
Iteration 9900: Loss = -11938.377156606204
Iteration 10000: Loss = -11930.205744029268
Iteration 10100: Loss = -11930.17116839494
Iteration 10200: Loss = -11930.149317840369
Iteration 10300: Loss = -11930.145623232505
Iteration 10400: Loss = -11930.12771563511
Iteration 10500: Loss = -11930.127636788673
Iteration 10600: Loss = -11930.126664507246
Iteration 10700: Loss = -11930.14430346515
1
Iteration 10800: Loss = -11930.12627269406
Iteration 10900: Loss = -11930.160790819744
1
Iteration 11000: Loss = -11930.148824179161
2
Iteration 11100: Loss = -11930.128153127349
3
Iteration 11200: Loss = -11930.332835275627
4
Iteration 11300: Loss = -11930.137737884113
5
Iteration 11400: Loss = -11930.126610708572
6
Iteration 11500: Loss = -11930.131721413569
7
Iteration 11600: Loss = -11930.131111615365
8
Iteration 11700: Loss = -11930.12782903007
9
Iteration 11800: Loss = -11930.124355448374
Iteration 11900: Loss = -11930.123651481983
Iteration 12000: Loss = -11930.123684385417
1
Iteration 12100: Loss = -11930.148982265435
2
Iteration 12200: Loss = -11930.132088262793
3
Iteration 12300: Loss = -11930.14959956576
4
Iteration 12400: Loss = -11930.123806918846
5
Iteration 12500: Loss = -11930.12674445017
6
Iteration 12600: Loss = -11930.123535758245
Iteration 12700: Loss = -11930.132261122379
1
Iteration 12800: Loss = -11930.253386146696
2
Iteration 12900: Loss = -11930.126929871565
3
Iteration 13000: Loss = -11930.123393215476
Iteration 13100: Loss = -11930.12609785216
1
Iteration 13200: Loss = -11930.123270117718
Iteration 13300: Loss = -11930.123761511455
1
Iteration 13400: Loss = -11930.123247973264
Iteration 13500: Loss = -11930.126767895286
1
Iteration 13600: Loss = -11930.12324545274
Iteration 13700: Loss = -11930.132165334757
1
Iteration 13800: Loss = -11930.12327584308
2
Iteration 13900: Loss = -11930.123240703844
Iteration 14000: Loss = -11930.176029395287
1
Iteration 14100: Loss = -11930.123218085193
Iteration 14200: Loss = -11930.123273790534
1
Iteration 14300: Loss = -11930.123502528568
2
Iteration 14400: Loss = -11930.131996313912
3
Iteration 14500: Loss = -11930.123491754666
4
Iteration 14600: Loss = -11930.148601207893
5
Iteration 14700: Loss = -11930.123186905254
Iteration 14800: Loss = -11930.124032549087
1
Iteration 14900: Loss = -11930.14839285908
2
Iteration 15000: Loss = -11925.741865501113
Iteration 15100: Loss = -11925.741688939572
Iteration 15200: Loss = -11925.74594723951
1
Iteration 15300: Loss = -11925.741625884346
Iteration 15400: Loss = -11925.742445891705
1
Iteration 15500: Loss = -11925.741907498195
2
Iteration 15600: Loss = -11925.741964736737
3
Iteration 15700: Loss = -11925.782385380253
4
Iteration 15800: Loss = -11925.741550833927
Iteration 15900: Loss = -11925.743762815473
1
Iteration 16000: Loss = -11925.748428377048
2
Iteration 16100: Loss = -11925.740467472426
Iteration 16200: Loss = -11925.893824097364
1
Iteration 16300: Loss = -11925.738267434004
Iteration 16400: Loss = -11925.744077303643
1
Iteration 16500: Loss = -11925.738623942316
2
Iteration 16600: Loss = -11925.738344294257
3
Iteration 16700: Loss = -11925.75602389815
4
Iteration 16800: Loss = -11925.745666162247
5
Iteration 16900: Loss = -11925.743656646873
6
Iteration 17000: Loss = -11925.738222381658
Iteration 17100: Loss = -11925.744740709837
1
Iteration 17200: Loss = -11925.738282351316
2
Iteration 17300: Loss = -11925.738559371744
3
Iteration 17400: Loss = -11925.750111902731
4
Iteration 17500: Loss = -11925.738345710055
5
Iteration 17600: Loss = -11925.751477873162
6
Iteration 17700: Loss = -11925.742241687998
7
Iteration 17800: Loss = -11925.738267458592
8
Iteration 17900: Loss = -11925.743650577524
9
Iteration 18000: Loss = -11925.748756829425
10
Stopping early at iteration 18000 due to no improvement.
tensor([[-11.5516,   6.9364],
        [ -1.2080,  -3.4072],
        [-10.2700,   5.6548],
        [-11.1666,   6.5514],
        [-12.1784,   7.5631],
        [-11.2065,   6.5913],
        [  1.0560,  -5.6712],
        [  0.4921,  -5.1073],
        [  1.3735,  -5.9887],
        [-10.4537,   5.8385],
        [ -5.8370,   1.2218],
        [ -0.2346,  -4.3806],
        [  0.8716,  -5.4868],
        [-11.0085,   6.3932],
        [  1.1388,  -5.7540],
        [ -8.3889,   3.7737],
        [ -4.6781,   0.0629],
        [ -1.9420,  -2.6733],
        [-12.1014,   7.4862],
        [-12.1366,   7.5214],
        [ -3.7272,  -0.8880],
        [-10.8129,   6.1976],
        [ -4.5101,  -0.1051],
        [ -5.0012,   0.3860],
        [-11.9246,   7.3094],
        [  0.4029,  -5.0181],
        [-10.6227,   6.0075],
        [ -8.3283,   3.7131],
        [ -9.8655,   5.2503],
        [ -5.3833,   0.7680],
        [ -5.5999,   0.9847],
        [ -0.1141,  -4.5011],
        [-12.3469,   7.7317],
        [ -9.4499,   4.8347],
        [ -5.4445,   0.8293],
        [ -4.2648,  -0.3504],
        [-10.3577,   5.7425],
        [-11.3090,   6.6938],
        [ -4.8219,   0.2067],
        [ -6.7194,   2.1042],
        [-11.8661,   7.2509],
        [  2.4268,  -7.0421],
        [ -5.9902,   1.3750],
        [ -4.3093,  -0.3059],
        [-10.6734,   6.0582],
        [ -5.2735,   0.6583],
        [ -2.3202,  -2.2950],
        [-10.4706,   5.8554],
        [ -4.1209,  -0.4943],
        [-11.1974,   6.5822],
        [ -6.1930,   1.5777],
        [-10.5591,   5.9439],
        [ -0.2505,  -4.3647],
        [-12.2900,   7.6748],
        [  5.2583,  -9.8736],
        [ -6.3007,   1.6855],
        [ -3.4515,  -1.1638],
        [  1.4754,  -6.0906],
        [ -4.1586,  -0.4567],
        [ -8.9803,   4.3650],
        [ -8.6969,   4.0817],
        [  0.5384,  -5.1537],
        [ -3.9241,  -0.6911],
        [-11.7851,   7.1699],
        [-12.2140,   7.5988],
        [ -8.9154,   4.3002],
        [-11.7864,   7.1712],
        [-12.2417,   7.6265],
        [-11.2430,   6.6278],
        [ -9.8444,   5.2291],
        [ -9.9970,   5.3818],
        [-12.8877,   8.2725],
        [ -7.4677,   2.8525],
        [ -6.2576,   1.6424],
        [ -8.0802,   3.4650],
        [-11.1612,   6.5460],
        [ -2.6981,  -1.9172],
        [  0.1525,  -4.7677],
        [  1.5881,  -6.2034],
        [  1.8672,  -6.4825],
        [ -3.6853,  -0.9299],
        [  1.2302,  -5.8454],
        [ -2.2874,  -2.3279],
        [ -3.4470,  -1.1683],
        [-10.9956,   6.3803],
        [-10.4769,   5.8616],
        [ -9.8101,   5.1949],
        [ -3.6109,  -1.0044],
        [ -3.5928,  -1.0224],
        [ -4.8251,   0.2099],
        [-11.3432,   6.7280],
        [  3.4101,  -8.0253],
        [-10.1735,   5.5582],
        [-10.9433,   6.3280],
        [ -5.6453,   1.0301],
        [ -5.0816,   0.4664],
        [-12.1102,   7.4950],
        [-10.9751,   6.3598],
        [-11.6722,   7.0570],
        [ -7.7805,   3.1652]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4976, 0.5024],
        [0.2373, 0.7627]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2141, 0.7859], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3772, 0.0950],
         [0.7345, 0.2181]],

        [[0.9118, 0.0927],
         [0.1552, 0.2573]],

        [[0.4839, 0.9770],
         [0.9233, 0.0541]],

        [[0.1422, 0.0972],
         [0.8206, 0.2218]],

        [[0.6790, 0.1000],
         [0.3612, 0.3826]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 71
Adjusted Rand Index: 0.17084212966917642
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 71
Adjusted Rand Index: 0.17084212966917642
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.061703487591474045
Average Adjusted Rand Index: 0.4683368518676706
Iteration 0: Loss = -18232.219294163016
Iteration 10: Loss = -11435.408866719752
Iteration 20: Loss = -11435.406012269654
Iteration 30: Loss = -11435.406009257613
Iteration 40: Loss = -11435.406009257613
1
Iteration 50: Loss = -11435.406009257613
2
Iteration 60: Loss = -11435.406009257613
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7574, 0.2426],
        [0.2529, 0.7471]], dtype=torch.float64)
alpha: tensor([0.5195, 0.4805])
beta: tensor([[[0.1937, 0.0960],
         [0.8601, 0.3934]],

        [[0.3270, 0.0973],
         [0.0036, 0.0242]],

        [[0.8234, 0.1016],
         [0.2393, 0.9916]],

        [[0.8201, 0.0975],
         [0.5586, 0.5099]],

        [[0.9349, 0.0997],
         [0.7521, 0.8776]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18232.002382724688
Iteration 100: Loss = -12013.414315870084
Iteration 200: Loss = -11712.71437288481
Iteration 300: Loss = -11610.468565325213
Iteration 400: Loss = -11596.2236178867
Iteration 500: Loss = -11585.672783818192
Iteration 600: Loss = -11583.040049626396
Iteration 700: Loss = -11581.30279821479
Iteration 800: Loss = -11574.757347440082
Iteration 900: Loss = -11565.747410273305
Iteration 1000: Loss = -11564.820457075159
Iteration 1100: Loss = -11564.751217595867
Iteration 1200: Loss = -11564.53584163659
Iteration 1300: Loss = -11561.997270856275
Iteration 1400: Loss = -11556.764838937008
Iteration 1500: Loss = -11556.736934785884
Iteration 1600: Loss = -11550.80518683459
Iteration 1700: Loss = -11545.504442254794
Iteration 1800: Loss = -11524.142542331825
Iteration 1900: Loss = -11523.978160983152
Iteration 2000: Loss = -11511.727340847474
Iteration 2100: Loss = -11511.69821047481
Iteration 2200: Loss = -11501.843489108653
Iteration 2300: Loss = -11501.817250106016
Iteration 2400: Loss = -11501.797177725373
Iteration 2500: Loss = -11488.752453819607
Iteration 2600: Loss = -11488.736080150635
Iteration 2700: Loss = -11488.71679166069
Iteration 2800: Loss = -11471.938535653575
Iteration 2900: Loss = -11471.922886830638
Iteration 3000: Loss = -11471.91135984621
Iteration 3100: Loss = -11466.036817789736
Iteration 3200: Loss = -11452.898070696108
Iteration 3300: Loss = -11452.889560288953
Iteration 3400: Loss = -11452.885272318115
Iteration 3500: Loss = -11452.881923360655
Iteration 3600: Loss = -11452.89837477618
1
Iteration 3700: Loss = -11452.876939291644
Iteration 3800: Loss = -11452.87498184768
Iteration 3900: Loss = -11452.874642538643
Iteration 4000: Loss = -11452.871716358748
Iteration 4100: Loss = -11452.870237210669
Iteration 4200: Loss = -11452.870848044322
1
Iteration 4300: Loss = -11452.869204315533
Iteration 4400: Loss = -11452.877292621783
1
Iteration 4500: Loss = -11452.865800278083
Iteration 4600: Loss = -11452.863815705143
Iteration 4700: Loss = -11452.855467850628
Iteration 4800: Loss = -11443.333879366237
Iteration 4900: Loss = -11443.329455236575
Iteration 5000: Loss = -11443.329003682631
Iteration 5100: Loss = -11443.335644691666
1
Iteration 5200: Loss = -11443.327474315525
Iteration 5300: Loss = -11443.329046201723
1
Iteration 5400: Loss = -11443.326359635925
Iteration 5500: Loss = -11443.32586645863
Iteration 5600: Loss = -11443.331923645877
1
Iteration 5700: Loss = -11443.342510437635
2
Iteration 5800: Loss = -11443.33644790229
3
Iteration 5900: Loss = -11443.334419196919
4
Iteration 6000: Loss = -11443.326034304997
5
Iteration 6100: Loss = -11443.326160052187
6
Iteration 6200: Loss = -11443.335114270772
7
Iteration 6300: Loss = -11443.41401774324
8
Iteration 6400: Loss = -11443.324793816993
Iteration 6500: Loss = -11443.322450763111
Iteration 6600: Loss = -11443.337311255857
1
Iteration 6700: Loss = -11443.295225097634
Iteration 6800: Loss = -11443.294590669491
Iteration 6900: Loss = -11443.296360015265
1
Iteration 7000: Loss = -11443.383479675857
2
Iteration 7100: Loss = -11443.30764593263
3
Iteration 7200: Loss = -11443.297075872706
4
Iteration 7300: Loss = -11443.294112338537
Iteration 7400: Loss = -11443.328957496735
1
Iteration 7500: Loss = -11443.305106815067
2
Iteration 7600: Loss = -11443.295838872982
3
Iteration 7700: Loss = -11443.33469967909
4
Iteration 7800: Loss = -11443.298787971888
5
Iteration 7900: Loss = -11443.292568566287
Iteration 8000: Loss = -11443.301942061962
1
Iteration 8100: Loss = -11443.293570434153
2
Iteration 8200: Loss = -11443.329819127826
3
Iteration 8300: Loss = -11443.29128467763
Iteration 8400: Loss = -11443.296572224684
1
Iteration 8500: Loss = -11443.289379909385
Iteration 8600: Loss = -11443.309569276758
1
Iteration 8700: Loss = -11443.301716208514
2
Iteration 8800: Loss = -11443.316649598315
3
Iteration 8900: Loss = -11443.29672024872
4
Iteration 9000: Loss = -11443.29509109245
5
Iteration 9100: Loss = -11443.288505687873
Iteration 9200: Loss = -11443.290215489526
1
Iteration 9300: Loss = -11443.295183028136
2
Iteration 9400: Loss = -11443.307017723137
3
Iteration 9500: Loss = -11443.290781552061
4
Iteration 9600: Loss = -11443.31096424557
5
Iteration 9700: Loss = -11443.3092803599
6
Iteration 9800: Loss = -11443.290329936886
7
Iteration 9900: Loss = -11443.298001309828
8
Iteration 10000: Loss = -11443.290617129089
9
Iteration 10100: Loss = -11443.288828016564
10
Stopping early at iteration 10100 due to no improvement.
tensor([[ -8.4451,   6.8407],
        [  5.9909,  -7.3845],
        [ -7.7476,   6.2365],
        [ -9.2306,   7.4433],
        [ -8.8342,   7.0840],
        [ -8.9082,   7.0812],
        [  6.5801,  -8.4440],
        [  5.7491,  -7.9214],
        [  7.1846,  -8.6648],
        [ -8.0718,   6.5869],
        [  4.1545,  -6.3311],
        [  6.1814,  -8.1569],
        [  7.0490,  -8.7594],
        [ -9.6062,   6.7001],
        [  6.9969,  -8.3845],
        [  3.0347,  -4.4459],
        [  6.5803,  -8.1785],
        [  1.3202,  -2.7563],
        [ -8.5873,   7.0981],
        [ -8.0943,   6.5631],
        [  6.7179,  -8.1169],
        [ -8.6346,   7.2357],
        [  5.9572,  -8.9309],
        [  5.7397,  -9.1570],
        [ -8.9266,   7.3797],
        [  6.6362,  -8.3103],
        [ -6.0691,   4.6742],
        [ -4.5287,   2.7276],
        [ -9.3554,   7.8831],
        [  2.8272,  -4.2798],
        [  6.3154,  -8.3623],
        [  6.4423,  -8.4757],
        [ -8.9304,   7.4783],
        [ -8.9166,   6.7548],
        [  6.6166,  -8.2480],
        [  6.0076,  -7.4556],
        [ -8.3141,   6.9278],
        [ -8.1208,   6.4257],
        [  7.4355,  -9.0143],
        [  3.7128,  -5.5444],
        [ -9.3416,   7.7640],
        [  6.6004,  -8.1994],
        [ -3.1707,   0.3677],
        [  4.6598,  -6.1640],
        [ -8.8588,   7.1475],
        [ -2.7535,   1.3607],
        [  3.0722,  -4.4608],
        [ -7.9039,   6.3355],
        [  2.5151,  -6.2280],
        [ -9.7125,   8.3214],
        [  6.8582,  -8.6435],
        [ -9.0429,   7.1556],
        [  6.9676,  -8.3613],
        [ -9.4020,   7.7132],
        [  7.0741,  -8.4972],
        [  3.7273,  -5.1878],
        [  5.9250,  -7.3733],
        [  5.5143,  -7.5792],
        [  5.6724,  -8.6382],
        [ -8.6264,   6.7070],
        [ -8.0089,   5.7474],
        [  5.6968,  -9.0834],
        [  5.0712,  -7.6261],
        [ -9.0667,   7.5767],
        [ -9.3172,   7.4110],
        [ -6.3198,   2.9655],
        [ -9.7976,   7.5429],
        [ -8.6570,   7.1777],
        [ -8.5659,   7.1305],
        [ -5.0200,   3.1001],
        [ -4.6687,   3.0358],
        [ -8.9454,   7.5291],
        [ -5.4732,   3.9952],
        [  3.2993,  -5.2921],
        [ -7.5607,   6.0580],
        [ -7.1020,   5.6939],
        [  2.7747,  -6.3156],
        [  3.4909,  -5.0609],
        [  5.2810,  -8.8193],
        [  6.9920,  -8.3916],
        [  6.4803,  -7.8945],
        [  5.8519,  -7.8535],
        [  3.5704,  -5.0237],
        [  6.5519,  -8.1151],
        [ -9.0526,   6.9901],
        [ -8.1143,   5.7978],
        [ -8.8090,   6.6205],
        [  3.4267,  -4.8242],
        [  7.0183,  -8.9013],
        [  6.9000,  -8.3467],
        [-10.1717,   7.4967],
        [  6.8829,  -8.4853],
        [ -5.6241,   3.8349],
        [ -8.2312,   6.8361],
        [  4.9729,  -8.3086],
        [  4.2217,  -7.0964],
        [-10.2181,   7.0009],
        [ -9.1107,   7.7142],
        [ -7.9694,   6.5319],
        [ -8.2195,   6.7008]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7555, 0.2445],
        [0.2602, 0.7398]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5003, 0.4997], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1968, 0.0959],
         [0.8601, 0.4032]],

        [[0.3270, 0.1012],
         [0.0036, 0.0242]],

        [[0.8234, 0.1017],
         [0.2393, 0.9916]],

        [[0.8201, 0.0975],
         [0.5586, 0.5099]],

        [[0.9349, 0.0999],
         [0.7521, 0.8776]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919992163297293
Iteration 0: Loss = -21393.44928990001
Iteration 10: Loss = -12159.896754308229
Iteration 20: Loss = -11692.479255933209
Iteration 30: Loss = -11571.51458720208
Iteration 40: Loss = -11571.41939576364
Iteration 50: Loss = -11571.418874759209
Iteration 60: Loss = -11571.4188763039
1
Iteration 70: Loss = -11571.418858105213
Iteration 80: Loss = -11571.418860300098
1
Iteration 90: Loss = -11571.418858777557
2
Iteration 100: Loss = -11571.418858088196
Iteration 110: Loss = -11571.418858088196
1
Iteration 120: Loss = -11571.418858088196
2
Iteration 130: Loss = -11571.418858088196
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[0.4803, 0.5197],
        [0.4218, 0.5782]], dtype=torch.float64)
alpha: tensor([0.4576, 0.5424])
beta: tensor([[[0.3910, 0.0960],
         [0.3036, 0.1976]],

        [[0.6055, 0.0976],
         [0.6982, 0.7463]],

        [[0.5102, 0.1015],
         [0.5997, 0.9122]],

        [[0.7730, 0.1121],
         [0.8220, 0.3575]],

        [[0.9521, 0.1005],
         [0.1429, 0.6357]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 69
Adjusted Rand Index: 0.13133314463628645
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5232234129912827
Average Adjusted Rand Index: 0.8262666289272573
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21392.749387383068
Iteration 100: Loss = -12221.357529549226
Iteration 200: Loss = -12206.267535066881
Iteration 300: Loss = -12122.178771636392
Iteration 400: Loss = -11868.333802532923
Iteration 500: Loss = -11814.808176299674
Iteration 600: Loss = -11766.849321211621
Iteration 700: Loss = -11744.099459111187
Iteration 800: Loss = -11684.398424856565
Iteration 900: Loss = -11682.090223265217
Iteration 1000: Loss = -11681.570611030289
Iteration 1100: Loss = -11677.291742053352
Iteration 1200: Loss = -11676.460995957585
Iteration 1300: Loss = -11672.556838243509
Iteration 1400: Loss = -11672.286497499612
Iteration 1500: Loss = -11666.613115002729
Iteration 1600: Loss = -11666.52840874401
Iteration 1700: Loss = -11666.462244428034
Iteration 1800: Loss = -11666.377492638921
Iteration 1900: Loss = -11666.05159691716
Iteration 2000: Loss = -11666.010665943842
Iteration 2100: Loss = -11665.949505264909
Iteration 2200: Loss = -11663.407144366896
Iteration 2300: Loss = -11663.371996854772
Iteration 2400: Loss = -11663.306044262503
Iteration 2500: Loss = -11663.284229691992
Iteration 2600: Loss = -11663.17671452501
Iteration 2700: Loss = -11663.146775751215
Iteration 2800: Loss = -11663.133464791203
Iteration 2900: Loss = -11663.114454711333
Iteration 3000: Loss = -11655.52222082999
Iteration 3100: Loss = -11655.479093799971
Iteration 3200: Loss = -11655.170834901077
Iteration 3300: Loss = -11651.513747772937
Iteration 3400: Loss = -11651.502833406565
Iteration 3500: Loss = -11635.582023362489
Iteration 3600: Loss = -11635.531874885442
Iteration 3700: Loss = -11635.522506583757
Iteration 3800: Loss = -11635.516963554153
Iteration 3900: Loss = -11635.512155532691
Iteration 4000: Loss = -11635.507788176697
Iteration 4100: Loss = -11635.503852579606
Iteration 4200: Loss = -11635.50007229027
Iteration 4300: Loss = -11635.49641226403
Iteration 4400: Loss = -11635.492168140749
Iteration 4500: Loss = -11635.483908011052
Iteration 4600: Loss = -11635.466973867564
Iteration 4700: Loss = -11627.550507218715
Iteration 4800: Loss = -11627.560897814856
1
Iteration 4900: Loss = -11627.539771954513
Iteration 5000: Loss = -11627.538040925727
Iteration 5100: Loss = -11627.535149211008
Iteration 5200: Loss = -11627.60768254766
1
Iteration 5300: Loss = -11613.193671496749
Iteration 5400: Loss = -11613.210327614774
1
Iteration 5500: Loss = -11613.196972431548
2
Iteration 5600: Loss = -11613.185757837697
Iteration 5700: Loss = -11613.193577124332
1
Iteration 5800: Loss = -11613.182895018737
Iteration 5900: Loss = -11613.200924126346
1
Iteration 6000: Loss = -11613.18036617279
Iteration 6100: Loss = -11613.178944939495
Iteration 6200: Loss = -11613.181654879414
1
Iteration 6300: Loss = -11613.172630224342
Iteration 6400: Loss = -11611.104294407385
Iteration 6500: Loss = -11611.061943551831
Iteration 6600: Loss = -11611.050488815024
Iteration 6700: Loss = -11610.950159611013
Iteration 6800: Loss = -11610.189570221139
Iteration 6900: Loss = -11610.157337631894
Iteration 7000: Loss = -11600.941924174085
Iteration 7100: Loss = -11600.891198220455
Iteration 7200: Loss = -11600.89620057084
1
Iteration 7300: Loss = -11600.8891255733
Iteration 7400: Loss = -11600.506499894778
Iteration 7500: Loss = -11599.981286880626
Iteration 7600: Loss = -11599.974669540328
Iteration 7700: Loss = -11600.03625992581
1
Iteration 7800: Loss = -11599.968558724933
Iteration 7900: Loss = -11599.975902308064
1
Iteration 8000: Loss = -11599.972094948635
2
Iteration 8100: Loss = -11599.962479374954
Iteration 8200: Loss = -11599.96749394526
1
Iteration 8300: Loss = -11599.96182645465
Iteration 8400: Loss = -11599.961241602805
Iteration 8500: Loss = -11599.975811721277
1
Iteration 8600: Loss = -11599.962218459557
2
Iteration 8700: Loss = -11599.960629336254
Iteration 8800: Loss = -11599.976700398556
1
Iteration 8900: Loss = -11599.966019880461
2
Iteration 9000: Loss = -11599.973639840791
3
Iteration 9100: Loss = -11599.966903699917
4
Iteration 9200: Loss = -11599.959643893952
Iteration 9300: Loss = -11599.980256402441
1
Iteration 9400: Loss = -11599.958895831323
Iteration 9500: Loss = -11586.909741574704
Iteration 9600: Loss = -11587.080557860543
1
Iteration 9700: Loss = -11586.907850618836
Iteration 9800: Loss = -11586.90899807126
1
Iteration 9900: Loss = -11586.908677627089
2
Iteration 10000: Loss = -11586.907605566452
Iteration 10100: Loss = -11586.925964353291
1
Iteration 10200: Loss = -11586.909439233594
2
Iteration 10300: Loss = -11586.915871047602
3
Iteration 10400: Loss = -11586.906903985466
Iteration 10500: Loss = -11586.909387825412
1
Iteration 10600: Loss = -11586.908670929093
2
Iteration 10700: Loss = -11586.94217052272
3
Iteration 10800: Loss = -11587.019665070811
4
Iteration 10900: Loss = -11586.857128257918
Iteration 11000: Loss = -11586.851235098753
Iteration 11100: Loss = -11586.85363338684
1
Iteration 11200: Loss = -11586.851811468448
2
Iteration 11300: Loss = -11586.853037652103
3
Iteration 11400: Loss = -11586.851516483643
4
Iteration 11500: Loss = -11586.940038193832
5
Iteration 11600: Loss = -11586.851084070806
Iteration 11700: Loss = -11586.8510050684
Iteration 11800: Loss = -11586.852381570512
1
Iteration 11900: Loss = -11587.036085056072
2
Iteration 12000: Loss = -11586.855322703155
3
Iteration 12100: Loss = -11586.851207149039
4
Iteration 12200: Loss = -11586.862504242632
5
Iteration 12300: Loss = -11586.85437210033
6
Iteration 12400: Loss = -11586.852045060075
7
Iteration 12500: Loss = -11586.887366057832
8
Iteration 12600: Loss = -11586.8489521099
Iteration 12700: Loss = -11586.86181591848
1
Iteration 12800: Loss = -11586.946014841007
2
Iteration 12900: Loss = -11586.852996313211
3
Iteration 13000: Loss = -11586.84887640165
Iteration 13100: Loss = -11586.849592486766
1
Iteration 13200: Loss = -11586.89206228894
2
Iteration 13300: Loss = -11586.867056103727
3
Iteration 13400: Loss = -11586.8497495725
4
Iteration 13500: Loss = -11586.848411936768
Iteration 13600: Loss = -11586.854422530083
1
Iteration 13700: Loss = -11586.856888784987
2
Iteration 13800: Loss = -11586.85629161528
3
Iteration 13900: Loss = -11586.876678301134
4
Iteration 14000: Loss = -11586.863466497029
5
Iteration 14100: Loss = -11586.857245019928
6
Iteration 14200: Loss = -11586.849752799273
7
Iteration 14300: Loss = -11586.849127277874
8
Iteration 14400: Loss = -11586.848885463154
9
Iteration 14500: Loss = -11586.891952985987
10
Stopping early at iteration 14500 due to no improvement.
tensor([[  7.0975,  -8.5957],
        [ -7.1494,   5.1984],
        [  5.9062,  -7.4621],
        [  7.2071, -11.1194],
        [  6.8483, -10.6459],
        [  6.6825,  -9.2699],
        [-10.8643,   6.2491],
        [ -7.2428,   5.7562],
        [ -8.4713,   6.6995],
        [  6.3578,  -8.3893],
        [ -6.3679,   4.9717],
        [ -7.4271,   5.9619],
        [ -8.4120,   6.8260],
        [  7.5338,  -8.9273],
        [ -8.5081,   6.8634],
        [ -8.3890,   6.9642],
        [ -9.7911,   5.8926],
        [ -3.3270,   1.7037],
        [  6.8850,  -8.3023],
        [  6.9109,  -8.3070],
        [ -7.7000,   6.1608],
        [  7.0581,  -8.9111],
        [ -9.4121,   7.0715],
        [ -8.2316,   6.8424],
        [  6.8714,  -8.7947],
        [ -8.5834,   6.9516],
        [  4.8906,  -6.9139],
        [  2.4914,  -3.9569],
        [  7.4222,  -9.0626],
        [ -4.0320,   2.0006],
        [ -3.6449,   2.1505],
        [ -7.8766,   6.4681],
        [  7.9631,  -9.3940],
        [  6.7601,  -8.3237],
        [ -8.2228,   6.2592],
        [ -7.1741,   5.6287],
        [  6.7557,  -8.1841],
        [  7.5349,  -9.0715],
        [ -9.1167,   6.7492],
        [ -5.8073,   4.4196],
        [  7.2406,  -9.1055],
        [ -7.9265,   6.5154],
        [  0.6399,  -2.0739],
        [ -5.7303,   4.0052],
        [  6.9855,  -8.3915],
        [  0.8714,  -2.2659],
        [ -5.6411,   2.8340],
        [  6.6979,  -8.1856],
        [ -4.7384,   2.9088],
        [  6.5233, -11.1386],
        [ -8.4062,   6.9598],
        [  7.1552,  -8.6132],
        [ -8.2823,   6.8809],
        [  8.2792,  -9.8114],
        [ -8.2998,   6.9135],
        [ -4.8826,   2.8913],
        [ -6.8261,   5.4104],
        [ -7.2169,   5.1577],
        [ -9.4403,   7.2161],
        [  6.4222,  -8.1594],
        [  6.8976,  -8.2877],
        [ -7.9047,   5.8384],
        [ -6.8837,   4.5984],
        [  7.6628,  -9.0496],
        [  6.8450,  -9.5131],
        [  3.4366,  -4.9805],
        [  6.8167,  -9.6707],
        [  6.8726,  -9.3167],
        [  6.4232,  -8.2535],
        [  2.9154,  -6.3528],
        [  2.1047,  -4.5521],
        [  7.7423,  -9.7883],
        [  3.5775,  -4.9651],
        [ -5.4908,   3.9913],
        [  7.5352,  -9.0203],
        [  5.9611,  -7.3556],
        [ -4.6892,   3.2142],
        [ -8.8233,   7.4177],
        [ -7.3969,   5.5547],
        [ -8.4356,   7.0479],
        [ -7.6720,   6.2211],
        [ -7.3881,   5.7559],
        [ -5.5867,   3.8955],
        [ -8.5530,   7.1644],
        [  7.3565,  -8.7654],
        [  6.2698,  -7.8768],
        [  6.5274,  -8.2400],
        [ -4.2517,   2.7904],
        [ -7.4167,   5.9311],
        [ -8.1827,   6.6505],
        [  7.0278,  -9.1232],
        [ -8.1033,   6.5592],
        [  3.5300,  -5.0125],
        [  5.9725,  -7.5290],
        [ -6.9249,   5.3301],
        [ -8.2635,   6.8270],
        [  7.0035,  -9.0605],
        [  6.6950,  -8.7697],
        [  6.1790,  -9.0968],
        [  2.8043,  -4.8996]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4516, 0.5484],
        [0.4143, 0.5857]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4994, 0.5006], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4033, 0.0961],
         [0.3036, 0.1996]],

        [[0.6055, 0.1014],
         [0.6982, 0.7463]],

        [[0.5102, 0.1018],
         [0.5997, 0.9122]],

        [[0.7730, 0.1122],
         [0.8220, 0.3575]],

        [[0.9521, 0.1012],
         [0.1429, 0.6357]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.061611374407582936
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5466951460990551
Average Adjusted Rand Index: 0.8043214912112457
Iteration 0: Loss = -30257.085141897318
Iteration 10: Loss = -12199.706836401978
Iteration 20: Loss = -11738.63216001885
Iteration 30: Loss = -11583.504854652278
Iteration 40: Loss = -11584.869001965619
1
Iteration 50: Loss = -11587.536580106387
2
Iteration 60: Loss = -11589.874005059104
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6163, 0.3837],
        [0.5099, 0.4901]], dtype=torch.float64)
alpha: tensor([0.5579, 0.4421])
beta: tensor([[[0.1949, 0.0961],
         [0.4688, 0.3940]],

        [[0.6964, 0.0976],
         [0.0625, 0.3409]],

        [[0.0266, 0.1015],
         [0.5111, 0.3410]],

        [[0.5630, 0.1193],
         [0.6672, 0.6904]],

        [[0.3947, 0.1007],
         [0.0251, 0.5352]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.019801980198019802
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5767556147286065
Average Adjusted Rand Index: 0.803960396039604
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30257.000361563347
Iteration 100: Loss = -12222.817563689003
Iteration 200: Loss = -12220.263864420105
Iteration 300: Loss = -12209.94419998772
Iteration 400: Loss = -12182.811299969368
Iteration 500: Loss = -11989.833801006715
Iteration 600: Loss = -11886.780849017512
Iteration 700: Loss = -11876.615774995944
Iteration 800: Loss = -11867.925266795142
Iteration 900: Loss = -11858.657856780801
Iteration 1000: Loss = -11835.692453156278
Iteration 1100: Loss = -11828.963606217283
Iteration 1200: Loss = -11828.600550605543
Iteration 1300: Loss = -11826.298999022678
Iteration 1400: Loss = -11814.560491345605
Iteration 1500: Loss = -11814.396571124942
Iteration 1600: Loss = -11808.287386750884
Iteration 1700: Loss = -11800.776756686639
Iteration 1800: Loss = -11790.273594102793
Iteration 1900: Loss = -11790.186287568204
Iteration 2000: Loss = -11789.92592038671
Iteration 2100: Loss = -11768.172104456662
Iteration 2200: Loss = -11768.13447083085
Iteration 2300: Loss = -11768.085869138533
Iteration 2400: Loss = -11768.008474945478
Iteration 2500: Loss = -11767.979397759149
Iteration 2600: Loss = -11753.371445049823
Iteration 2700: Loss = -11734.711410083135
Iteration 2800: Loss = -11734.59837870407
Iteration 2900: Loss = -11734.580766715593
Iteration 3000: Loss = -11734.566786867019
Iteration 3100: Loss = -11734.55473306694
Iteration 3200: Loss = -11734.54596252034
Iteration 3300: Loss = -11734.50964735797
Iteration 3400: Loss = -11734.499898148484
Iteration 3500: Loss = -11734.493212809986
Iteration 3600: Loss = -11734.472127584251
Iteration 3700: Loss = -11733.26660771183
Iteration 3800: Loss = -11733.258567161418
Iteration 3900: Loss = -11733.25417491799
Iteration 4000: Loss = -11733.250815777868
Iteration 4100: Loss = -11733.247943807926
Iteration 4200: Loss = -11733.244920414376
Iteration 4300: Loss = -11733.234933665459
Iteration 4400: Loss = -11728.232409751783
Iteration 4500: Loss = -11728.201262818524
Iteration 4600: Loss = -11728.195332723364
Iteration 4700: Loss = -11728.193688054243
Iteration 4800: Loss = -11728.18807605158
Iteration 4900: Loss = -11728.186027650738
Iteration 5000: Loss = -11728.184365343786
Iteration 5100: Loss = -11728.18280758234
Iteration 5200: Loss = -11728.18164180966
Iteration 5300: Loss = -11728.180098574097
Iteration 5400: Loss = -11728.17851489209
Iteration 5500: Loss = -11728.17640952094
Iteration 5600: Loss = -11728.175433878556
Iteration 5700: Loss = -11728.173392626106
Iteration 5800: Loss = -11728.173982451915
1
Iteration 5900: Loss = -11728.171946634935
Iteration 6000: Loss = -11728.170941961362
Iteration 6100: Loss = -11728.170023460181
Iteration 6200: Loss = -11728.169389289096
Iteration 6300: Loss = -11728.16912711063
Iteration 6400: Loss = -11728.168446047679
Iteration 6500: Loss = -11728.16763981354
Iteration 6600: Loss = -11728.168157234784
1
Iteration 6700: Loss = -11728.166707582714
Iteration 6800: Loss = -11728.168382495656
1
Iteration 6900: Loss = -11728.165861472955
Iteration 7000: Loss = -11728.17500441176
1
Iteration 7100: Loss = -11728.162861390467
Iteration 7200: Loss = -11728.162450617709
Iteration 7300: Loss = -11728.163467347465
1
Iteration 7400: Loss = -11728.161829769675
Iteration 7500: Loss = -11728.167551401104
1
Iteration 7600: Loss = -11728.161153139583
Iteration 7700: Loss = -11728.160589752926
Iteration 7800: Loss = -11725.840577547042
Iteration 7900: Loss = -11725.902886874155
1
Iteration 8000: Loss = -11725.902175421117
2
Iteration 8100: Loss = -11725.913666779536
3
Iteration 8200: Loss = -11725.89912754643
4
Iteration 8300: Loss = -11724.827744665856
Iteration 8400: Loss = -11724.8223960198
Iteration 8500: Loss = -11724.821741967966
Iteration 8600: Loss = -11724.790183435818
Iteration 8700: Loss = -11724.448981087506
Iteration 8800: Loss = -11724.452291435939
1
Iteration 8900: Loss = -11724.455435696369
2
Iteration 9000: Loss = -11723.936844021317
Iteration 9100: Loss = -11723.454904963799
Iteration 9200: Loss = -11712.448040736588
Iteration 9300: Loss = -11712.354942145235
Iteration 9400: Loss = -11712.343423295022
Iteration 9500: Loss = -11712.343218396582
Iteration 9600: Loss = -11712.343200081323
Iteration 9700: Loss = -11711.49898699507
Iteration 9800: Loss = -11711.496503195414
Iteration 9900: Loss = -11711.492642949543
Iteration 10000: Loss = -11711.490428120667
Iteration 10100: Loss = -11711.464641018658
Iteration 10200: Loss = -11711.46928736095
1
Iteration 10300: Loss = -11711.451221929605
Iteration 10400: Loss = -11711.316492836902
Iteration 10500: Loss = -11701.771555295058
Iteration 10600: Loss = -11701.747759009459
Iteration 10700: Loss = -11701.719024148042
Iteration 10800: Loss = -11701.720242848874
1
Iteration 10900: Loss = -11701.74339021411
2
Iteration 11000: Loss = -11701.698893253435
Iteration 11100: Loss = -11701.668976197698
Iteration 11200: Loss = -11701.663244583173
Iteration 11300: Loss = -11701.660492966488
Iteration 11400: Loss = -11701.679793997775
1
Iteration 11500: Loss = -11701.65743842862
Iteration 11600: Loss = -11701.660019670222
1
Iteration 11700: Loss = -11701.656195534048
Iteration 11800: Loss = -11701.6747046448
1
Iteration 11900: Loss = -11701.629618616487
Iteration 12000: Loss = -11701.73646572199
1
Iteration 12100: Loss = -11701.628856870397
Iteration 12200: Loss = -11701.68271146909
1
Iteration 12300: Loss = -11701.62873119984
Iteration 12400: Loss = -11701.628704998679
Iteration 12500: Loss = -11692.29630120812
Iteration 12600: Loss = -11692.298022164743
1
Iteration 12700: Loss = -11692.301726513268
2
Iteration 12800: Loss = -11692.314218255284
3
Iteration 12900: Loss = -11692.296398959625
4
Iteration 13000: Loss = -11692.298193087303
5
Iteration 13100: Loss = -11692.295615916028
Iteration 13200: Loss = -11692.311597125288
1
Iteration 13300: Loss = -11692.30307501606
2
Iteration 13400: Loss = -11692.308194152187
3
Iteration 13500: Loss = -11692.297056910293
4
Iteration 13600: Loss = -11692.297810632102
5
Iteration 13700: Loss = -11692.335022252846
6
Iteration 13800: Loss = -11692.292868050361
Iteration 13900: Loss = -11692.274812944954
Iteration 14000: Loss = -11692.276216708837
1
Iteration 14100: Loss = -11692.275872338694
2
Iteration 14200: Loss = -11692.275318085945
3
Iteration 14300: Loss = -11692.282874844812
4
Iteration 14400: Loss = -11692.274356518888
Iteration 14500: Loss = -11692.28205884254
1
Iteration 14600: Loss = -11692.27435153477
Iteration 14700: Loss = -11692.278037063048
1
Iteration 14800: Loss = -11692.280143728225
2
Iteration 14900: Loss = -11692.274306871992
Iteration 15000: Loss = -11692.371220195018
1
Iteration 15100: Loss = -11692.27396219652
Iteration 15200: Loss = -11692.246132361981
Iteration 15300: Loss = -11692.239178618767
Iteration 15400: Loss = -11692.23707462626
Iteration 15500: Loss = -11692.23674464528
Iteration 15600: Loss = -11692.23641254451
Iteration 15700: Loss = -11692.261985788526
1
Iteration 15800: Loss = -11692.236126472628
Iteration 15900: Loss = -11692.255630132837
1
Iteration 16000: Loss = -11692.240568146766
2
Iteration 16100: Loss = -11692.235848187558
Iteration 16200: Loss = -11692.236681710776
1
Iteration 16300: Loss = -11692.241794430458
2
Iteration 16400: Loss = -11692.235402999891
Iteration 16500: Loss = -11692.45036221154
1
Iteration 16600: Loss = -11692.23553553602
2
Iteration 16700: Loss = -11692.238304919576
3
Iteration 16800: Loss = -11692.275396635434
4
Iteration 16900: Loss = -11692.236922154778
5
Iteration 17000: Loss = -11692.236012284522
6
Iteration 17100: Loss = -11692.240807181905
7
Iteration 17200: Loss = -11692.235474217663
8
Iteration 17300: Loss = -11692.252877426541
9
Iteration 17400: Loss = -11692.245284490215
10
Stopping early at iteration 17400 due to no improvement.
tensor([[-10.7766,   8.0645],
        [  4.3359,  -6.3963],
        [ -8.0593,   6.2930],
        [ -9.3013,   7.7368],
        [ -9.4495,   7.7527],
        [-12.0069,   7.3917],
        [  8.4366,  -9.9764],
        [  5.1255,  -6.5901],
        [  7.3762,  -8.8692],
        [ -8.2730,   6.8051],
        [  2.4152,  -7.0305],
        [  5.2107,  -7.2724],
        [  6.8949,  -9.5028],
        [ -9.5458,   7.6293],
        [  7.1441,  -9.6015],
        [  2.8668,  -4.3077],
        [  4.8252,  -9.4404],
        [  0.3259,  -2.9220],
        [-11.0554,   7.4873],
        [ -9.3543,   7.0998],
        [  6.0697,  -7.4936],
        [ -9.8135,   8.2789],
        [  6.9752,  -8.3673],
        [  6.1041,  -7.5299],
        [ -9.0324,   7.6200],
        [  7.3140,  -9.2255],
        [ -7.2685,   5.7397],
        [ -4.9048,   3.3841],
        [-10.3160,   6.9403],
        [  1.1053,  -3.4173],
        [  1.6381,  -3.0699],
        [  6.7503,  -8.1371],
        [-10.0077,   7.7907],
        [ -9.0727,   7.6863],
        [  6.8496,  -8.2361],
        [  5.2061,  -6.7457],
        [ -8.7511,   7.3487],
        [ -8.7763,   7.3168],
        [  6.2971,  -8.1473],
        [  3.7719,  -5.1669],
        [ -9.8581,   8.1930],
        [  6.7347,  -8.2545],
        [ -3.4652,   1.2886],
        [  3.3217,  -4.7083],
        [ -9.6670,   7.8369],
        [ -2.7974,   1.4013],
        [  2.7319,  -4.1246],
        [ -8.4776,   7.0694],
        [  2.0759,  -4.3900],
        [ -9.8566,   8.4675],
        [  7.4966,  -9.4969],
        [ -9.3039,   7.8175],
        [  7.0907,  -8.9830],
        [ -9.4841,   7.9199],
        [  7.2297,  -9.7101],
        [  1.8247,  -4.4102],
        [  3.7811,  -6.9266],
        [  4.3847,  -6.5647],
        [  6.2186,  -8.0792],
        [ -9.0309,   7.0854],
        [ -9.4703,   6.9321],
        [  5.9789,  -7.7729],
        [  3.5434,  -6.2012],
        [-10.9592,   8.1569],
        [ -9.4466,   7.9588],
        [ -5.7923,   4.0002],
        [-11.9426,   8.3486],
        [ -9.3459,   7.9386],
        [-10.0247,   7.6908],
        [ -5.8616,   4.4523],
        [ -4.4942,   3.0719],
        [-11.1121,   9.6514],
        [ -8.9612,   5.0133],
        [  2.8452,  -4.8715],
        [ -8.6008,   5.5931],
        [ -8.1210,   6.4444],
        [  2.2249,  -3.7852],
        [  6.0834,  -7.4705],
        [  4.7800,  -6.7573],
        [  7.4322,  -9.7175],
        [  6.1466,  -7.5723],
        [  4.9377,  -6.4589],
        [  3.1260,  -4.5951],
        [  7.2881,  -8.8789],
        [ -9.6215,   7.3985],
        [ -9.3145,   6.4594],
        [ -8.9569,   7.5275],
        [  1.7903,  -3.2306],
        [  5.3109,  -6.6986],
        [  6.9660,  -8.8744],
        [ -9.5908,   8.1794],
        [  6.7715,  -9.1092],
        [ -5.5244,   4.0368],
        [-11.1488,   8.2858],
        [  4.7331,  -6.1920],
        [  4.5049,  -5.9549],
        [-10.1264,   8.3361],
        [-10.2888,   7.3630],
        [ -8.8818,   6.8113],
        [ -4.9357,   3.3946]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6074, 0.3926],
        [0.5418, 0.4582]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4991, 0.5009], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2215, 0.0951],
         [0.4688, 0.3788]],

        [[0.6964, 0.0972],
         [0.0625, 0.3409]],

        [[0.0266, 0.1060],
         [0.5111, 0.3410]],

        [[0.5630, 0.1061],
         [0.6672, 0.6904]],

        [[0.3947, 0.0999],
         [0.0251, 0.5352]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 26
Adjusted Rand Index: 0.22262626262626262
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 28
Adjusted Rand Index: 0.18246165945733386
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.17140428693655155
Average Adjusted Rand Index: 0.6810175844167193
Iteration 0: Loss = -22848.689711005652
Iteration 10: Loss = -11970.906325837861
Iteration 20: Loss = -11435.404867053625
Iteration 30: Loss = -11435.406009601336
1
Iteration 40: Loss = -11435.406009257613
2
Iteration 50: Loss = -11435.406009257613
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7574, 0.2426],
        [0.2529, 0.7471]], dtype=torch.float64)
alpha: tensor([0.5195, 0.4805])
beta: tensor([[[0.1937, 0.0960],
         [0.9147, 0.3934]],

        [[0.7716, 0.0973],
         [0.1797, 0.5452]],

        [[0.9345, 0.1016],
         [0.6632, 0.7667]],

        [[0.5641, 0.0975],
         [0.3563, 0.3284]],

        [[0.2810, 0.0997],
         [0.8307, 0.2311]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22848.913821077338
Iteration 100: Loss = -12084.517622860143
Iteration 200: Loss = -11798.862572503984
Iteration 300: Loss = -11786.475956785074
Iteration 400: Loss = -11762.974376444765
Iteration 500: Loss = -11733.92515336996
Iteration 600: Loss = -11708.701952255478
Iteration 700: Loss = -11696.416651628953
Iteration 800: Loss = -11695.225120412533
Iteration 900: Loss = -11648.782462126355
Iteration 1000: Loss = -11640.08524336087
Iteration 1100: Loss = -11628.300543850493
Iteration 1200: Loss = -11625.975626061725
Iteration 1300: Loss = -11624.196065489641
Iteration 1400: Loss = -11624.121921600767
Iteration 1500: Loss = -11624.066350973379
Iteration 1600: Loss = -11613.326014284967
Iteration 1700: Loss = -11597.877306671684
Iteration 1800: Loss = -11591.244028174142
Iteration 1900: Loss = -11586.856273077108
Iteration 2000: Loss = -11586.739083282282
Iteration 2100: Loss = -11579.374621305471
Iteration 2200: Loss = -11571.119258960201
Iteration 2300: Loss = -11571.07707954253
Iteration 2400: Loss = -11571.058836920734
Iteration 2500: Loss = -11564.591255820586
Iteration 2600: Loss = -11563.90239464206
Iteration 2700: Loss = -11563.888845216627
Iteration 2800: Loss = -11563.87603520216
Iteration 2900: Loss = -11558.801394501996
Iteration 3000: Loss = -11542.31451840916
Iteration 3100: Loss = -11538.228542853993
Iteration 3200: Loss = -11538.218438610005
Iteration 3300: Loss = -11538.210561327513
Iteration 3400: Loss = -11538.20095113495
Iteration 3500: Loss = -11533.805569046026
Iteration 3600: Loss = -11522.06909513063
Iteration 3700: Loss = -11522.059571353824
Iteration 3800: Loss = -11522.054795910472
Iteration 3900: Loss = -11522.050976041566
Iteration 4000: Loss = -11522.047343973307
Iteration 4100: Loss = -11521.905128390325
Iteration 4200: Loss = -11513.103838436768
Iteration 4300: Loss = -11496.936759290671
Iteration 4400: Loss = -11496.931362322122
Iteration 4500: Loss = -11488.096414898837
Iteration 4600: Loss = -11488.091146753732
Iteration 4700: Loss = -11488.088607690905
Iteration 4800: Loss = -11488.082078667127
Iteration 4900: Loss = -11484.346405337217
Iteration 5000: Loss = -11484.3441385703
Iteration 5100: Loss = -11484.341859738955
Iteration 5200: Loss = -11484.337325327557
Iteration 5300: Loss = -11476.502525610942
Iteration 5400: Loss = -11476.493604262852
Iteration 5500: Loss = -11466.348325413808
Iteration 5600: Loss = -11466.307442385258
Iteration 5700: Loss = -11458.809697975736
Iteration 5800: Loss = -11444.64498191864
Iteration 5900: Loss = -11444.633102484153
Iteration 6000: Loss = -11444.63157553883
Iteration 6100: Loss = -11444.631012413607
Iteration 6200: Loss = -11444.631216669957
1
Iteration 6300: Loss = -11444.628804214075
Iteration 6400: Loss = -11444.628009989365
Iteration 6500: Loss = -11444.627086445487
Iteration 6600: Loss = -11444.603386729517
Iteration 6700: Loss = -11444.563692512404
Iteration 6800: Loss = -11444.566036780996
1
Iteration 6900: Loss = -11444.563149695323
Iteration 7000: Loss = -11444.562491663568
Iteration 7100: Loss = -11444.56239146641
Iteration 7200: Loss = -11444.56192827369
Iteration 7300: Loss = -11444.563723159697
1
Iteration 7400: Loss = -11444.570481309453
2
Iteration 7500: Loss = -11444.562115843853
3
Iteration 7600: Loss = -11444.55899337546
Iteration 7700: Loss = -11444.560693696003
1
Iteration 7800: Loss = -11444.58494948591
2
Iteration 7900: Loss = -11444.558161700828
Iteration 8000: Loss = -11444.590738552677
1
Iteration 8100: Loss = -11444.563193538195
2
Iteration 8200: Loss = -11444.653308351611
3
Iteration 8300: Loss = -11439.461416887003
Iteration 8400: Loss = -11439.466083841613
1
Iteration 8500: Loss = -11439.46479594942
2
Iteration 8600: Loss = -11439.460635331252
Iteration 8700: Loss = -11439.459355017385
Iteration 8800: Loss = -11439.45913921213
Iteration 8900: Loss = -11439.462495307223
1
Iteration 9000: Loss = -11439.463716671562
2
Iteration 9100: Loss = -11439.50998689132
3
Iteration 9200: Loss = -11439.488769339043
4
Iteration 9300: Loss = -11439.457842134416
Iteration 9400: Loss = -11439.441937171378
Iteration 9500: Loss = -11439.442085495013
1
Iteration 9600: Loss = -11439.444675328323
2
Iteration 9700: Loss = -11439.461094002836
3
Iteration 9800: Loss = -11439.444393574724
4
Iteration 9900: Loss = -11439.443586726347
5
Iteration 10000: Loss = -11439.441708009463
Iteration 10100: Loss = -11439.441521844728
Iteration 10200: Loss = -11439.442621309205
1
Iteration 10300: Loss = -11439.442317499617
2
Iteration 10400: Loss = -11439.441656386802
3
Iteration 10500: Loss = -11439.445483817894
4
Iteration 10600: Loss = -11439.456092404698
5
Iteration 10700: Loss = -11437.264066168578
Iteration 10800: Loss = -11433.856291055654
Iteration 10900: Loss = -11433.855479078373
Iteration 11000: Loss = -11433.85768164911
1
Iteration 11100: Loss = -11433.857403513663
2
Iteration 11200: Loss = -11433.897597030837
3
Iteration 11300: Loss = -11433.883813427567
4
Iteration 11400: Loss = -11433.87202154444
5
Iteration 11500: Loss = -11433.856558413963
6
Iteration 11600: Loss = -11433.858082111154
7
Iteration 11700: Loss = -11433.867438328352
8
Iteration 11800: Loss = -11433.868020458543
9
Iteration 11900: Loss = -11433.857526708449
10
Stopping early at iteration 11900 due to no improvement.
tensor([[-8.2053,  6.8139],
        [ 4.7171, -8.7176],
        [-7.4170,  6.0285],
        [-8.3782,  6.9680],
        [-8.2539,  6.8563],
        [-9.6259,  7.1820],
        [ 6.2954, -8.0895],
        [ 5.9254, -7.6568],
        [ 6.9842, -8.5370],
        [-7.7305,  6.3419],
        [ 4.4869, -5.8740],
        [ 5.7572, -7.3644],
        [ 7.4328, -9.9254],
        [-9.3550,  7.1100],
        [ 6.6330, -8.2494],
        [ 3.0047, -4.3981],
        [ 6.2904, -7.6820],
        [ 0.9121, -3.0814],
        [-8.3784,  6.9909],
        [-8.2931,  6.8805],
        [ 6.2007, -7.7697],
        [-9.1044,  7.1688],
        [ 6.4018, -7.8023],
        [ 5.8174, -7.4227],
        [-8.1886,  6.5718],
        [ 6.5683, -7.9988],
        [-6.1716,  4.5728],
        [-5.9423,  1.3943],
        [-7.8138,  6.4272],
        [ 1.4571, -5.6262],
        [ 2.7261, -4.1490],
        [ 6.1084, -8.0936],
        [-9.1965,  7.1513],
        [-9.8134,  5.7139],
        [ 7.4279, -8.8317],
        [ 6.0815, -7.6904],
        [-8.6008,  6.4096],
        [-8.3899,  6.5844],
        [ 6.1850, -8.1085],
        [ 3.7678, -5.4171],
        [-9.0435,  7.2021],
        [ 6.4727, -8.4300],
        [-3.7957, -0.1754],
        [ 4.5372, -6.2573],
        [-8.7488,  6.5587],
        [-2.8728,  1.2969],
        [ 3.0063, -4.4430],
        [-7.8482,  6.1770],
        [ 3.6681, -5.0607],
        [-9.4903,  7.5910],
        [ 6.5689, -8.4313],
        [-8.5271,  7.1176],
        [ 6.2077, -7.9471],
        [-9.3592,  7.2530],
        [ 6.5707, -8.0150],
        [ 3.7489, -5.1398],
        [ 5.6353, -7.3283],
        [ 5.7557, -7.1521],
        [ 5.9842, -7.4818],
        [-9.4674,  4.9387],
        [-8.1904,  6.7260],
        [ 5.8444, -8.2511],
        [ 5.4914, -6.9475],
        [-8.6140,  6.8754],
        [-8.7836,  7.2384],
        [-7.6347,  5.9298],
        [-8.5522,  7.0664],
        [-8.5574,  6.9384],
        [-9.4364,  6.8297],
        [-4.7585,  3.3672],
        [-4.6551,  3.1017],
        [-9.3195,  7.1079],
        [-5.5036,  4.0069],
        [ 1.9441, -6.5593],
        [-9.0865,  4.7624],
        [-7.1224,  5.5948],
        [ 3.6330, -5.4294],
        [ 3.4961, -4.9614],
        [ 5.8214, -7.3194],
        [ 6.8555, -8.2419],
        [ 5.9775, -8.1998],
        [ 5.8538, -7.2694],
        [ 3.5109, -4.9957],
        [ 6.4543, -7.8948],
        [-8.1614,  6.7629],
        [-8.4983,  6.0079],
        [-8.0026,  6.5881],
        [ 3.2694, -4.9416],
        [ 6.1624, -7.5922],
        [ 6.3198, -7.7152],
        [-8.4863,  7.0989],
        [ 6.0115, -8.6388],
        [-5.6944,  3.8142],
        [-7.9808,  6.1511],
        [ 5.5397, -6.9556],
        [ 4.5064, -6.7192],
        [-8.7921,  7.2914],
        [-8.4354,  6.7982],
        [-8.2292,  5.7237],
        [-6.5083,  5.1189]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7600, 0.2400],
        [0.2526, 0.7474]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5022, 0.4978], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1974, 0.0954],
         [0.9147, 0.4020]],

        [[0.7716, 0.0977],
         [0.1797, 0.5452]],

        [[0.9345, 0.1018],
         [0.6632, 0.7667]],

        [[0.5641, 0.0973],
         [0.3563, 0.3284]],

        [[0.2810, 0.0998],
         [0.8307, 0.2311]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11435.40992923985
new:  [0.9919999997943784, 0.5466951460990551, 0.17140428693655155, 1.0] [0.9919992163297293, 0.8043214912112457, 0.6810175844167193, 1.0] [11443.288828016564, 11586.891952985987, 11692.245284490215, 11433.857526708449]
prior:  [1.0, 0.5232234129912827, 0.5767556147286065, 1.0] [1.0, 0.8262666289272573, 0.803960396039604, 1.0] [11435.406009257613, 11571.418858088196, 11589.874005059104, 11435.406009257613]
-----------------------------------------------------------------------------------------
This iteration is 29
True Objective function: Loss = -11816.833736841018
Iteration 0: Loss = -16902.333457903063
Iteration 10: Loss = -12122.927836496357
Iteration 20: Loss = -12112.787158713136
Iteration 30: Loss = -12103.463686077986
Iteration 40: Loss = -12100.38896071111
Iteration 50: Loss = -12099.44050440038
Iteration 60: Loss = -12099.401506318616
Iteration 70: Loss = -12099.47140396066
1
Iteration 80: Loss = -12099.524923803196
2
Iteration 90: Loss = -12099.557223841148
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.4438, 0.5562],
        [0.6453, 0.3547]], dtype=torch.float64)
alpha: tensor([0.5227, 0.4773])
beta: tensor([[[0.2597, 0.1069],
         [0.5091, 0.3702]],

        [[0.3778, 0.0944],
         [0.3156, 0.4955]],

        [[0.7880, 0.0929],
         [0.2209, 0.7645]],

        [[0.2414, 0.0968],
         [0.0471, 0.3142]],

        [[0.0094, 0.1081],
         [0.6081, 0.6548]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.702393059759101
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 19
Adjusted Rand Index: 0.3781818181818182
Global Adjusted Rand Index: 0.09305035662208141
Average Adjusted Rand Index: 0.8161149755881839
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16951.878776821413
Iteration 100: Loss = -12211.202078215592
Iteration 200: Loss = -11847.188152258852
Iteration 300: Loss = -11845.670831872769
Iteration 400: Loss = -11845.409121060628
Iteration 500: Loss = -11845.278456393271
Iteration 600: Loss = -11845.1900769146
Iteration 700: Loss = -11836.970018537497
Iteration 800: Loss = -11836.649678688154
Iteration 900: Loss = -11836.621538300868
Iteration 1000: Loss = -11836.60069801536
Iteration 1100: Loss = -11836.584576941517
Iteration 1200: Loss = -11836.571786920074
Iteration 1300: Loss = -11836.561520962752
Iteration 1400: Loss = -11836.55306416001
Iteration 1500: Loss = -11836.54598148245
Iteration 1600: Loss = -11836.540062372169
Iteration 1700: Loss = -11836.535005388276
Iteration 1800: Loss = -11836.53070753152
Iteration 1900: Loss = -11836.526951687725
Iteration 2000: Loss = -11836.52369816219
Iteration 2100: Loss = -11836.520845661378
Iteration 2200: Loss = -11836.518303117027
Iteration 2300: Loss = -11836.516102899586
Iteration 2400: Loss = -11836.514121770497
Iteration 2500: Loss = -11836.512353569846
Iteration 2600: Loss = -11836.510760652993
Iteration 2700: Loss = -11836.509297175657
Iteration 2800: Loss = -11836.508007910086
Iteration 2900: Loss = -11836.50684775795
Iteration 3000: Loss = -11836.505795167692
Iteration 3100: Loss = -11836.50481654524
Iteration 3200: Loss = -11836.50394840915
Iteration 3300: Loss = -11836.503146354748
Iteration 3400: Loss = -11836.502421380383
Iteration 3500: Loss = -11836.501756386408
Iteration 3600: Loss = -11836.501120149262
Iteration 3700: Loss = -11836.50052555502
Iteration 3800: Loss = -11836.499986784032
Iteration 3900: Loss = -11836.499551495694
Iteration 4000: Loss = -11836.499058410096
Iteration 4100: Loss = -11836.526157737557
1
Iteration 4200: Loss = -11836.498225396383
Iteration 4300: Loss = -11836.497860655993
Iteration 4400: Loss = -11836.497545591461
Iteration 4500: Loss = -11836.497483469984
Iteration 4600: Loss = -11836.496929352938
Iteration 4700: Loss = -11836.496653964372
Iteration 4800: Loss = -11836.496401854081
Iteration 4900: Loss = -11836.500665018028
1
Iteration 5000: Loss = -11836.495914869745
Iteration 5100: Loss = -11836.495708020964
Iteration 5200: Loss = -11836.495486105538
Iteration 5300: Loss = -11836.495697236427
1
Iteration 5400: Loss = -11836.495165931312
Iteration 5500: Loss = -11836.49510459587
Iteration 5600: Loss = -11836.494870163968
Iteration 5700: Loss = -11836.49478301703
Iteration 5800: Loss = -11836.494619347799
Iteration 5900: Loss = -11836.494463255625
Iteration 6000: Loss = -11836.494480435596
1
Iteration 6100: Loss = -11812.724495143684
Iteration 6200: Loss = -11812.72283354096
Iteration 6300: Loss = -11812.72528554877
1
Iteration 6400: Loss = -11812.722291954895
Iteration 6500: Loss = -11812.722168790378
Iteration 6600: Loss = -11812.722329094835
1
Iteration 6700: Loss = -11812.721981262166
Iteration 6800: Loss = -11812.728254750577
1
Iteration 6900: Loss = -11812.733097975748
2
Iteration 7000: Loss = -11812.721692334651
Iteration 7100: Loss = -11812.740221857197
1
Iteration 7200: Loss = -11812.721559398471
Iteration 7300: Loss = -11812.72239018415
1
Iteration 7400: Loss = -11812.721479687938
Iteration 7500: Loss = -11812.72150633785
1
Iteration 7600: Loss = -11812.722497759758
2
Iteration 7700: Loss = -11812.721389112183
Iteration 7800: Loss = -11812.759054292263
1
Iteration 7900: Loss = -11812.721311556132
Iteration 8000: Loss = -11812.722591200974
1
Iteration 8100: Loss = -11812.723556004155
2
Iteration 8200: Loss = -11812.734687862425
3
Iteration 8300: Loss = -11812.738323604379
4
Iteration 8400: Loss = -11812.746073869659
5
Iteration 8500: Loss = -11812.723089636214
6
Iteration 8600: Loss = -11812.787107601494
7
Iteration 8700: Loss = -11812.72565673151
8
Iteration 8800: Loss = -11812.743535970154
9
Iteration 8900: Loss = -11812.720948008831
Iteration 9000: Loss = -11812.721295121382
1
Iteration 9100: Loss = -11812.736433421014
2
Iteration 9200: Loss = -11812.724757845393
3
Iteration 9300: Loss = -11812.720913130219
Iteration 9400: Loss = -11812.723343562719
1
Iteration 9500: Loss = -11812.73441682567
2
Iteration 9600: Loss = -11812.722444279118
3
Iteration 9700: Loss = -11812.765096847788
4
Iteration 9800: Loss = -11812.722278054354
5
Iteration 9900: Loss = -11812.724422283467
6
Iteration 10000: Loss = -11812.72469188284
7
Iteration 10100: Loss = -11812.725774054376
8
Iteration 10200: Loss = -11812.721111359724
9
Iteration 10300: Loss = -11812.786784968948
10
Stopping early at iteration 10300 due to no improvement.
tensor([[  3.4949,  -8.1101],
        [ -7.7737,   3.1585],
        [ -9.3306,   4.7154],
        [-10.0556,   5.4404],
        [-11.2548,   6.6395],
        [  6.8366, -11.4518],
        [  6.1153, -10.7305],
        [ -8.2052,   3.5900],
        [ -8.1720,   3.5568],
        [ -7.7301,   3.1149],
        [-10.5753,   5.9601],
        [  5.2714,  -9.8866],
        [ -9.2388,   4.6236],
        [ -9.8852,   5.2700],
        [ -9.6990,   5.0838],
        [ -6.5720,   1.9567],
        [-10.3861,   5.7708],
        [ -8.5542,   3.9390],
        [  6.2277, -10.8429],
        [  5.0131,  -9.6284],
        [  6.2159, -10.8311],
        [  5.4442, -10.0594],
        [  4.6980,  -9.3132],
        [  5.1248,  -9.7401],
        [ -8.8889,   4.2737],
        [  6.2852, -10.9005],
        [ -9.4255,   4.8102],
        [  6.2097, -10.8249],
        [  4.5787,  -9.1939],
        [ -5.4161,   0.8009],
        [  6.3445, -10.9597],
        [ -9.5088,   4.8936],
        [ -9.5524,   4.9372],
        [  1.6206,  -6.2358],
        [  5.4826, -10.0978],
        [ -7.3434,   2.7281],
        [  5.5066, -10.1218],
        [  4.8627,  -9.4779],
        [  6.7586, -11.3739],
        [ -9.5525,   4.9373],
        [  6.5955, -11.2107],
        [  5.2305,  -9.8457],
        [-10.8661,   6.2509],
        [  3.9836,  -8.5989],
        [ -9.6729,   5.0577],
        [ -7.4475,   2.8323],
        [  5.7636, -10.3788],
        [ -9.0821,   4.4669],
        [  6.4378, -11.0530],
        [ -9.6600,   5.0447],
        [ -8.9348,   4.3196],
        [  6.3346, -10.9499],
        [ -9.5157,   4.9005],
        [  6.3343, -10.9495],
        [  4.9260,  -9.5412],
        [  4.9175,  -9.5327],
        [ -9.2798,   4.6646],
        [ -9.9197,   5.3045],
        [ -8.9288,   4.3136],
        [  5.9059, -10.5211],
        [ -9.2379,   4.6226],
        [  3.9304,  -8.5456],
        [  5.6411, -10.2564],
        [  6.3865, -11.0017],
        [  4.9479,  -9.5631],
        [-10.0639,   5.4487],
        [ -8.1581,   3.5428],
        [-10.0594,   5.4441],
        [  6.6223, -11.2375],
        [  4.6051,  -9.2204],
        [  4.5682,  -9.1834],
        [ -9.4838,   4.8686],
        [  5.6894, -10.3046],
        [  2.1135,  -6.7287],
        [  5.5414, -10.1566],
        [ -7.9576,   3.3423],
        [  5.4027, -10.0179],
        [ -8.7775,   4.1622],
        [  6.5860, -11.2013],
        [  4.9671,  -9.5823],
        [  5.8862, -10.5015],
        [  0.8532,  -5.4684],
        [ -9.9352,   5.3199],
        [ -7.3512,   2.7360],
        [ -7.4604,   2.8452],
        [  0.3035,  -4.9187],
        [  6.3362, -10.9514],
        [  5.2187,  -9.8339],
        [ -9.5206,   4.9053],
        [  4.3369,  -8.9522],
        [  6.6846, -11.2998],
        [  3.7089,  -8.3241],
        [-10.0707,   5.4555],
        [  5.4388, -10.0540],
        [  6.2415, -10.8567],
        [  1.1727,  -5.7879],
        [  4.9565,  -9.5717],
        [  5.8377, -10.4529],
        [  6.1396, -10.7548],
        [  1.4650,  -6.0802]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7509, 0.2491],
        [0.2516, 0.7484]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5698, 0.4302], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4079, 0.1070],
         [0.5091, 0.2076]],

        [[0.3778, 0.0944],
         [0.3156, 0.4955]],

        [[0.7880, 0.0959],
         [0.2209, 0.7645]],

        [[0.2414, 0.0971],
         [0.0471, 0.3142]],

        [[0.0094, 0.1035],
         [0.6081, 0.6548]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -17445.95157433969
Iteration 10: Loss = -11814.153507797606
Iteration 20: Loss = -11814.153507538813
Iteration 30: Loss = -11814.153507538813
1
Iteration 40: Loss = -11814.153507538813
2
Iteration 50: Loss = -11814.153507538813
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7479, 0.2521],
        [0.2496, 0.7504]], dtype=torch.float64)
alpha: tensor([0.4602, 0.5398])
beta: tensor([[[0.2011, 0.1070],
         [0.9060, 0.4005]],

        [[0.5617, 0.0944],
         [0.9361, 0.6619]],

        [[0.7405, 0.0959],
         [0.6562, 0.9817]],

        [[0.7676, 0.0970],
         [0.8156, 0.2478]],

        [[0.8489, 0.1035],
         [0.1999, 0.4462]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17445.782361881873
Iteration 100: Loss = -12468.396329933348
Iteration 200: Loss = -12038.104693644034
Iteration 300: Loss = -12020.556223211126
Iteration 400: Loss = -12019.841919592674
Iteration 500: Loss = -12019.528437242665
Iteration 600: Loss = -12019.35327297076
Iteration 700: Loss = -12019.242562013162
Iteration 800: Loss = -12019.164931461859
Iteration 900: Loss = -12019.084902654502
Iteration 1000: Loss = -12018.767706775443
Iteration 1100: Loss = -12018.731345444756
Iteration 1200: Loss = -12018.70691940893
Iteration 1300: Loss = -12018.687674293335
Iteration 1400: Loss = -12018.672067663823
Iteration 1500: Loss = -12018.659170615521
Iteration 1600: Loss = -12018.648427499455
Iteration 1700: Loss = -12018.639303588268
Iteration 1800: Loss = -12018.631573437428
Iteration 1900: Loss = -12018.624892081221
Iteration 2000: Loss = -12018.619122315102
Iteration 2100: Loss = -12018.614003116232
Iteration 2200: Loss = -12018.609620045536
Iteration 2300: Loss = -12018.605725804528
Iteration 2400: Loss = -12018.602194630961
Iteration 2500: Loss = -12018.599064134876
Iteration 2600: Loss = -12018.596286147116
Iteration 2700: Loss = -12018.593806099754
Iteration 2800: Loss = -12018.591562683358
Iteration 2900: Loss = -12018.58953699064
Iteration 3000: Loss = -12018.58764112263
Iteration 3100: Loss = -12018.585882854768
Iteration 3200: Loss = -12018.584309501643
Iteration 3300: Loss = -12018.582814223957
Iteration 3400: Loss = -12018.58131071657
Iteration 3500: Loss = -12018.579702292029
Iteration 3600: Loss = -12018.577607003492
Iteration 3700: Loss = -12018.575029846372
Iteration 3800: Loss = -12018.573570832454
Iteration 3900: Loss = -12018.572560611858
Iteration 4000: Loss = -12018.571715890448
Iteration 4100: Loss = -12018.57095464846
Iteration 4200: Loss = -12018.570222136264
Iteration 4300: Loss = -12018.718014083213
1
Iteration 4400: Loss = -12018.568957519858
Iteration 4500: Loss = -12018.56831950943
Iteration 4600: Loss = -12018.567680263448
Iteration 4700: Loss = -12018.569654075367
1
Iteration 4800: Loss = -12018.566343786848
Iteration 4900: Loss = -12018.565870866589
Iteration 5000: Loss = -12018.579642635244
1
Iteration 5100: Loss = -12018.565075202412
Iteration 5200: Loss = -12018.564755094832
Iteration 5300: Loss = -12018.569582688984
1
Iteration 5400: Loss = -12018.564110648513
Iteration 5500: Loss = -12018.563834633294
Iteration 5600: Loss = -12018.567019733382
1
Iteration 5700: Loss = -12018.563339942102
Iteration 5800: Loss = -12018.684185637108
1
Iteration 5900: Loss = -12018.562891162559
Iteration 6000: Loss = -12018.562687761616
Iteration 6100: Loss = -12018.563984199503
1
Iteration 6200: Loss = -12018.562355392356
Iteration 6300: Loss = -12018.588678761978
1
Iteration 6400: Loss = -12018.56213292328
Iteration 6500: Loss = -12018.561902576044
Iteration 6600: Loss = -12018.564140094953
1
Iteration 6700: Loss = -12018.561564936468
Iteration 6800: Loss = -12018.605349150323
1
Iteration 6900: Loss = -12018.56628412881
2
Iteration 7000: Loss = -12018.561194684688
Iteration 7100: Loss = -12018.572182211861
1
Iteration 7200: Loss = -12018.565078672713
2
Iteration 7300: Loss = -12018.557426858315
Iteration 7400: Loss = -12018.55972171427
1
Iteration 7500: Loss = -12018.598384436853
2
Iteration 7600: Loss = -12018.556894590467
Iteration 7700: Loss = -12018.560237776985
1
Iteration 7800: Loss = -12018.55675658809
Iteration 7900: Loss = -12018.56121178054
1
Iteration 8000: Loss = -12018.556815056585
2
Iteration 8100: Loss = -12018.557086408442
3
Iteration 8200: Loss = -12018.557892055576
4
Iteration 8300: Loss = -12018.661608105558
5
Iteration 8400: Loss = -12018.556524125766
Iteration 8500: Loss = -12018.567552135772
1
Iteration 8600: Loss = -12018.559857314196
2
Iteration 8700: Loss = -12018.581424731703
3
Iteration 8800: Loss = -12018.556880210403
4
Iteration 8900: Loss = -12018.556779096214
5
Iteration 9000: Loss = -12018.556063753886
Iteration 9100: Loss = -12018.576372281821
1
Iteration 9200: Loss = -12018.556618867122
2
Iteration 9300: Loss = -12018.557771147163
3
Iteration 9400: Loss = -12018.556471329968
4
Iteration 9500: Loss = -12018.55858237305
5
Iteration 9600: Loss = -12018.560179826824
6
Iteration 9700: Loss = -12018.752032177235
7
Iteration 9800: Loss = -12018.55770901731
8
Iteration 9900: Loss = -12018.559978024872
9
Iteration 10000: Loss = -12018.610343742474
10
Stopping early at iteration 10000 due to no improvement.
tensor([[ -6.7735,   5.1976],
        [  3.5780,  -5.0267],
        [  5.5317,  -6.9184],
        [  6.2189,  -7.6141],
        [  6.3923,  -7.8188],
        [ -9.1395,   7.3806],
        [ -9.6657,   7.2342],
        [  4.6522,  -7.4559],
        [  3.8336,  -5.7375],
        [  4.9670,  -6.3546],
        [  6.7335, -10.5223],
        [ -7.8193,   5.9365],
        [  2.5775,  -4.6731],
        [  6.3665,  -7.9912],
        [  6.0046,  -7.8809],
        [  3.0606,  -5.4416],
        [  6.0084, -10.1297],
        [  3.9804,  -6.5417],
        [ -9.0453,   6.8315],
        [ -8.8697,   5.9780],
        [ -8.7501,   6.7655],
        [ -8.5917,   7.0926],
        [ -8.6275,   7.1052],
        [ -7.8597,   6.4348],
        [  4.9829,  -6.4728],
        [ -9.2459,   7.5939],
        [  4.8166,  -8.1097],
        [ -9.0454,   7.4383],
        [ -7.8716,   5.3403],
        [  1.9350,  -3.7283],
        [ -8.5336,   6.4077],
        [  6.7048,  -9.5324],
        [  6.2533,  -8.3651],
        [ -5.7042,   4.2943],
        [ -7.8225,   6.3693],
        [  4.2580,  -6.1487],
        [ -8.3261,   6.5782],
        [ -8.5714,   6.7761],
        [ -8.0980,   6.6929],
        [  6.4836,  -7.8775],
        [ -8.9440,   7.4859],
        [ -9.7218,   5.8592],
        [  6.5928,  -8.4555],
        [ -8.7656,   4.1504],
        [  6.3084,  -8.0952],
        [  4.0735,  -6.1414],
        [ -9.4951,   7.2395],
        [  5.5519,  -6.9648],
        [ -9.0750,   7.6858],
        [  3.0840,  -5.7172],
        [  4.9094,  -6.4521],
        [ -9.3468,   5.7622],
        [  6.0684,  -8.3160],
        [-10.1707,   6.2338],
        [ -8.4285,   5.8056],
        [ -7.6982,   6.1797],
        [  5.3664,  -6.8182],
        [  6.2369,  -7.6271],
        [  6.2251,  -7.6193],
        [ -9.9014,   7.5606],
        [  3.9592,  -5.3645],
        [ -6.9524,   5.5612],
        [ -8.3068,   5.7741],
        [-10.3167,   7.6669],
        [ -8.2893,   6.2021],
        [  5.0541,  -8.1895],
        [  3.4102,  -6.1363],
        [  6.5949,  -8.4225],
        [ -9.1484,   7.3319],
        [ -9.1986,   6.4977],
        [ -7.9194,   6.3135],
        [  5.0508,  -7.6596],
        [ -8.6906,   7.2334],
        [ -6.2233,   4.5600],
        [ -8.3118,   6.9231],
        [  4.2713,  -5.6635],
        [ -8.4414,   6.9155],
        [  4.7405,  -6.5471],
        [ -8.6181,   7.2129],
        [ -8.5217,   6.9555],
        [ -8.0994,   6.6566],
        [ -4.8461,   3.2738],
        [  6.1921,  -7.6643],
        [  4.3895,  -6.0390],
        [  2.6950,  -6.4481],
        [ -3.1087,   1.6780],
        [ -8.8069,   7.1375],
        [ -8.4446,   6.3549],
        [  2.0565,  -4.2452],
        [ -8.0182,   6.1858],
        [ -8.9611,   7.4321],
        [ -6.5943,   5.1399],
        [  6.0119,  -7.8456],
        [ -8.3608,   6.8656],
        [ -9.3022,   7.1484],
        [ -4.0555,   2.4264],
        [ -7.8581,   5.7687],
        [ -9.1505,   6.2456],
        [ -7.9979,   6.5826],
        [ -4.5381,   3.1091]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5633, 0.4367],
        [0.4911, 0.5089]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4301, 0.5699], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2348, 0.1069],
         [0.9060, 0.4004]],

        [[0.5617, 0.0944],
         [0.9361, 0.6619]],

        [[0.7405, 0.0949],
         [0.6562, 0.9817]],

        [[0.7676, 0.1004],
         [0.8156, 0.2478]],

        [[0.8489, 0.1034],
         [0.1999, 0.4462]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 18
Adjusted Rand Index: 0.40428365888647305
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4504892035835673
Average Adjusted Rand Index: 0.8808567317772946
Iteration 0: Loss = -21633.34375868622
Iteration 10: Loss = -11814.153509596665
Iteration 20: Loss = -11814.1535095966
Iteration 30: Loss = -11814.1535095966
1
Iteration 40: Loss = -11814.1535095966
2
Iteration 50: Loss = -11814.1535095966
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7504, 0.2496],
        [0.2521, 0.7479]], dtype=torch.float64)
alpha: tensor([0.5398, 0.4602])
beta: tensor([[[0.4005, 0.1070],
         [0.4578, 0.2011]],

        [[0.6564, 0.0944],
         [0.3333, 0.4939]],

        [[0.3942, 0.0959],
         [0.0315, 0.6321]],

        [[0.3199, 0.0970],
         [0.8845, 0.3439]],

        [[0.9980, 0.1035],
         [0.9777, 0.4287]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21633.43581041108
Iteration 100: Loss = -12897.79286315762
Iteration 200: Loss = -12512.678149576168
Iteration 300: Loss = -12396.608632518986
Iteration 400: Loss = -12145.476432298883
Iteration 500: Loss = -12000.848921132805
Iteration 600: Loss = -11973.851322891302
Iteration 700: Loss = -11883.395026196149
Iteration 800: Loss = -11844.984747158289
Iteration 900: Loss = -11844.7132868167
Iteration 1000: Loss = -11844.529313695297
Iteration 1100: Loss = -11826.894930089935
Iteration 1200: Loss = -11826.80686770718
Iteration 1300: Loss = -11826.74324641423
Iteration 1400: Loss = -11826.694025227189
Iteration 1500: Loss = -11826.65518631717
Iteration 1600: Loss = -11826.622988588884
Iteration 1700: Loss = -11826.596659180019
Iteration 1800: Loss = -11826.585474534048
Iteration 1900: Loss = -11826.555838304866
Iteration 2000: Loss = -11826.539773847157
Iteration 2100: Loss = -11826.617006940722
1
Iteration 2200: Loss = -11826.513780337787
Iteration 2300: Loss = -11826.503206207617
Iteration 2400: Loss = -11826.494541267804
Iteration 2500: Loss = -11826.485550793801
Iteration 2600: Loss = -11826.478156782614
Iteration 2700: Loss = -11826.471595159437
Iteration 2800: Loss = -11826.466283444388
Iteration 2900: Loss = -11826.460335132051
Iteration 3000: Loss = -11826.455452972328
Iteration 3100: Loss = -11826.462290146614
1
Iteration 3200: Loss = -11826.447153193034
Iteration 3300: Loss = -11826.44351636305
Iteration 3400: Loss = -11826.44017549728
Iteration 3500: Loss = -11826.437250913214
Iteration 3600: Loss = -11826.438811539665
1
Iteration 3700: Loss = -11826.43183386172
Iteration 3800: Loss = -11826.429963608465
Iteration 3900: Loss = -11826.427254501075
Iteration 4000: Loss = -11826.42592508878
Iteration 4100: Loss = -11826.423380384107
Iteration 4200: Loss = -11826.425565231431
1
Iteration 4300: Loss = -11826.420051789868
Iteration 4400: Loss = -11826.418613143043
Iteration 4500: Loss = -11826.417130200129
Iteration 4600: Loss = -11826.416191428762
Iteration 4700: Loss = -11826.418133399868
1
Iteration 4800: Loss = -11826.41355203644
Iteration 4900: Loss = -11826.444181357503
1
Iteration 5000: Loss = -11826.411470858979
Iteration 5100: Loss = -11826.411362018329
Iteration 5200: Loss = -11826.409660991912
Iteration 5300: Loss = -11826.40928918532
Iteration 5400: Loss = -11826.417602078547
1
Iteration 5500: Loss = -11826.407412689452
Iteration 5600: Loss = -11826.730287327002
1
Iteration 5700: Loss = -11826.406080182755
Iteration 5800: Loss = -11826.40576947027
Iteration 5900: Loss = -11826.404915055153
Iteration 6000: Loss = -11826.406902504137
1
Iteration 6100: Loss = -11826.40389473612
Iteration 6200: Loss = -11826.404024504045
1
Iteration 6300: Loss = -11826.417084129902
2
Iteration 6400: Loss = -11826.402602733902
Iteration 6500: Loss = -11826.402165384587
Iteration 6600: Loss = -11826.435815299586
1
Iteration 6700: Loss = -11826.401425232862
Iteration 6800: Loss = -11826.401088324099
Iteration 6900: Loss = -11826.40570712792
1
Iteration 7000: Loss = -11817.051177279263
Iteration 7100: Loss = -11812.766960168268
Iteration 7200: Loss = -11812.789488795499
1
Iteration 7300: Loss = -11812.762384991744
Iteration 7400: Loss = -11812.762114544545
Iteration 7500: Loss = -11812.831761935271
1
Iteration 7600: Loss = -11812.761707332966
Iteration 7700: Loss = -11812.76138580665
Iteration 7800: Loss = -11812.777218652333
1
Iteration 7900: Loss = -11812.76115464918
Iteration 8000: Loss = -11812.76086994021
Iteration 8100: Loss = -11812.772371486619
1
Iteration 8200: Loss = -11812.761789186081
2
Iteration 8300: Loss = -11812.762430661829
3
Iteration 8400: Loss = -11812.842775250752
4
Iteration 8500: Loss = -11812.761477132331
5
Iteration 8600: Loss = -11812.763074285498
6
Iteration 8700: Loss = -11812.763574960383
7
Iteration 8800: Loss = -11812.914097925659
8
Iteration 8900: Loss = -11812.760091317761
Iteration 9000: Loss = -11812.761146504652
1
Iteration 9100: Loss = -11812.759497044055
Iteration 9200: Loss = -11812.759481900195
Iteration 9300: Loss = -11812.76036159244
1
Iteration 9400: Loss = -11812.760032111179
2
Iteration 9500: Loss = -11812.76466578935
3
Iteration 9600: Loss = -11812.759183445572
Iteration 9700: Loss = -11812.759119251952
Iteration 9800: Loss = -11812.761643915024
1
Iteration 9900: Loss = -11812.758919109721
Iteration 10000: Loss = -11812.767305969614
1
Iteration 10100: Loss = -11812.759705697432
2
Iteration 10200: Loss = -11812.75892149382
3
Iteration 10300: Loss = -11812.775845882454
4
Iteration 10400: Loss = -11812.758642667442
Iteration 10500: Loss = -11812.766930912985
1
Iteration 10600: Loss = -11812.75855359783
Iteration 10700: Loss = -11812.758541551928
Iteration 10800: Loss = -11812.760461824046
1
Iteration 10900: Loss = -11812.758452096588
Iteration 11000: Loss = -11812.827378505051
1
Iteration 11100: Loss = -11812.767218580368
2
Iteration 11200: Loss = -11812.770138339449
3
Iteration 11300: Loss = -11812.763301197785
4
Iteration 11400: Loss = -11812.768197053016
5
Iteration 11500: Loss = -11812.763869166278
6
Iteration 11600: Loss = -11812.760522384548
7
Iteration 11700: Loss = -11812.78204017635
8
Iteration 11800: Loss = -11812.758210895618
Iteration 11900: Loss = -11812.758642575833
1
Iteration 12000: Loss = -11812.770787510133
2
Iteration 12100: Loss = -11812.758159462534
Iteration 12200: Loss = -11812.76077843083
1
Iteration 12300: Loss = -11812.758468338918
2
Iteration 12400: Loss = -11812.807845552703
3
Iteration 12500: Loss = -11812.758305676576
4
Iteration 12600: Loss = -11812.806318093155
5
Iteration 12700: Loss = -11812.7588687264
6
Iteration 12800: Loss = -11812.763261856062
7
Iteration 12900: Loss = -11812.764947803034
8
Iteration 13000: Loss = -11812.766382857515
9
Iteration 13100: Loss = -11812.764139967838
10
Stopping early at iteration 13100 due to no improvement.
tensor([[  5.0804,  -6.5779],
        [ -9.7532,   7.3487],
        [ -9.1284,   7.5692],
        [-11.9156,   7.3004],
        [-10.1008,   8.5632],
        [  7.6476,  -9.4274],
        [  7.4967,  -8.8854],
        [ -6.8926,   4.9282],
        [-10.7600,   7.4471],
        [ -6.2720,   4.5794],
        [ -9.5914,   8.0387],
        [  6.7056,  -8.0990],
        [ -9.6602,   6.5052],
        [ -9.2558,   7.8403],
        [ -9.3643,   7.9617],
        [ -5.2061,   3.3235],
        [ -8.1811,   6.7899],
        [ -9.2394,   7.6733],
        [  7.9747,  -9.7321],
        [  7.4168,  -8.9033],
        [  7.2752,  -8.8130],
        [  8.1379, -11.5667],
        [  7.9311, -12.5463],
        [  7.2836, -10.5750],
        [ -9.2585,   7.8719],
        [  7.6885,  -9.1178],
        [ -9.3667,   7.1351],
        [  7.5245,  -8.9108],
        [  5.9889,  -7.3835],
        [ -4.4985,   1.7138],
        [  6.8853, -10.3517],
        [ -9.1767,   7.4996],
        [ -9.3895,   7.3826],
        [  7.0352,  -9.4426],
        [  6.1714,  -9.8391],
        [ -6.0990,   3.9700],
        [  6.8162,  -9.4382],
        [  6.8907,  -8.5940],
        [  6.3886, -11.0038],
        [ -9.1024,   7.1301],
        [  7.8073,  -9.2006],
        [  7.5010,  -8.9879],
        [ -9.0324,   7.5305],
        [  5.5470,  -7.0685],
        [ -9.7561,   8.0712],
        [ -5.8926,   4.3903],
        [  7.1215,  -8.6007],
        [ -9.1923,   7.1176],
        [  7.4616,  -8.9472],
        [ -8.8637,   6.8734],
        [ -9.7095,   8.2001],
        [  7.8091,  -9.2375],
        [ -7.9894,   6.2353],
        [  7.0328,  -8.5168],
        [  6.0989,  -8.2984],
        [  7.1820,  -9.6856],
        [ -9.8871,   8.4863],
        [ -9.3935,   7.2023],
        [ -7.9216,   6.3773],
        [  7.5114,  -8.9662],
        [ -9.0131,   7.2188],
        [  5.6363,  -7.0266],
        [  6.9745,  -8.3639],
        [  7.5846,  -8.9709],
        [  6.1565,  -9.1798],
        [ -9.2556,   7.8061],
        [ -8.5486,   7.1214],
        [ -9.1552,   7.4228],
        [  7.1342,  -8.6051],
        [  8.2775,  -9.8513],
        [  6.6284,  -8.7094],
        [ -9.5865,   7.6632],
        [  6.8188,  -8.2912],
        [  7.2713,  -8.6665],
        [  7.1857,  -8.5755],
        [ -8.6145,   7.2245],
        [  7.4334,  -8.8259],
        [ -9.4570,   8.0144],
        [  7.4765,  -8.8629],
        [  7.2225,  -8.7686],
        [  6.6108, -10.9308],
        [  7.5590,  -9.5522],
        [ -9.2549,   7.6129],
        [ -5.8608,   4.2554],
        [ -8.6109,   7.1296],
        [  1.7579,  -3.4673],
        [  7.9102, -10.1809],
        [  6.2552,  -8.5691],
        [ -4.8624,   3.1317],
        [  4.9358,  -8.8898],
        [  7.3103,  -9.0377],
        [  5.3245,  -6.7531],
        [ -9.8993,   8.0575],
        [  7.0052,  -8.3949],
        [  7.3255,  -9.5461],
        [  2.6213,  -4.3455],
        [  6.3605,  -7.7488],
        [  7.9408,  -9.4328],
        [  6.4052,  -8.9085],
        [  2.9674,  -4.5830]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7514, 0.2486],
        [0.2516, 0.7484]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5700, 0.4300], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4078, 0.1069],
         [0.4578, 0.2059]],

        [[0.6564, 0.0944],
         [0.3333, 0.4939]],

        [[0.3942, 0.0959],
         [0.0315, 0.6321]],

        [[0.3199, 0.0967],
         [0.8845, 0.3439]],

        [[0.9980, 0.1031],
         [0.9777, 0.4287]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -23435.412831261383
Iteration 10: Loss = -12420.788312625971
Iteration 20: Loss = -11814.153507538704
Iteration 30: Loss = -11814.153507538813
1
Iteration 40: Loss = -11814.153507538813
2
Iteration 50: Loss = -11814.153507538813
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7479, 0.2521],
        [0.2496, 0.7504]], dtype=torch.float64)
alpha: tensor([0.4602, 0.5398])
beta: tensor([[[0.2011, 0.1070],
         [0.7611, 0.4005]],

        [[0.3984, 0.0944],
         [0.5784, 0.7617]],

        [[0.6318, 0.0959],
         [0.0446, 0.9901]],

        [[0.4067, 0.0970],
         [0.1324, 0.8114]],

        [[0.2235, 0.1035],
         [0.3894, 0.4817]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23378.867686539925
Iteration 100: Loss = -12755.34533194183
Iteration 200: Loss = -12580.81622820049
Iteration 300: Loss = -12145.070808644894
Iteration 400: Loss = -12006.08139893209
Iteration 500: Loss = -11951.915193272147
Iteration 600: Loss = -11930.752574200365
Iteration 700: Loss = -11917.662677597116
Iteration 800: Loss = -11907.025018688983
Iteration 900: Loss = -11891.925661886376
Iteration 1000: Loss = -11891.722765970124
Iteration 1100: Loss = -11891.577680644186
Iteration 1200: Loss = -11891.458377689827
Iteration 1300: Loss = -11888.529714885339
Iteration 1400: Loss = -11888.456517212435
Iteration 1500: Loss = -11888.366253983586
Iteration 1600: Loss = -11888.321865923439
Iteration 1700: Loss = -11888.283449403869
Iteration 1800: Loss = -11842.418255640061
Iteration 1900: Loss = -11842.264347019012
Iteration 2000: Loss = -11842.23485946533
Iteration 2100: Loss = -11842.210382548841
Iteration 2200: Loss = -11842.189653459898
Iteration 2300: Loss = -11842.171849461296
Iteration 2400: Loss = -11842.156303971076
Iteration 2500: Loss = -11842.142773294647
Iteration 2600: Loss = -11842.130827730878
Iteration 2700: Loss = -11842.12021519367
Iteration 2800: Loss = -11842.11077769837
Iteration 2900: Loss = -11842.102305123628
Iteration 3000: Loss = -11842.094704415718
Iteration 3100: Loss = -11842.087782706589
Iteration 3200: Loss = -11842.081569351718
Iteration 3300: Loss = -11842.075904965073
Iteration 3400: Loss = -11842.074355648288
Iteration 3500: Loss = -11842.065996547497
Iteration 3600: Loss = -11842.06164108774
Iteration 3700: Loss = -11842.057637456099
Iteration 3800: Loss = -11842.053897850152
Iteration 3900: Loss = -11842.050285715886
Iteration 4000: Loss = -11842.046473871289
Iteration 4100: Loss = -11842.036357524037
Iteration 4200: Loss = -11833.07587810049
Iteration 4300: Loss = -11833.097051572799
1
Iteration 4400: Loss = -11833.070917605613
Iteration 4500: Loss = -11833.068699219319
Iteration 4600: Loss = -11833.066665220957
Iteration 4700: Loss = -11833.064781741987
Iteration 4800: Loss = -11833.063051921145
Iteration 4900: Loss = -11833.061389313403
Iteration 5000: Loss = -11833.059857788683
Iteration 5100: Loss = -11833.058434777227
Iteration 5200: Loss = -11833.05718993321
Iteration 5300: Loss = -11833.055904635012
Iteration 5400: Loss = -11833.054757989377
Iteration 5500: Loss = -11833.053632395593
Iteration 5600: Loss = -11833.05260531777
Iteration 5700: Loss = -11833.057600488597
1
Iteration 5800: Loss = -11833.050772908378
Iteration 5900: Loss = -11833.049916765214
Iteration 6000: Loss = -11833.05350512107
1
Iteration 6100: Loss = -11833.048537020784
Iteration 6200: Loss = -11833.047715840912
Iteration 6300: Loss = -11833.047820546211
1
Iteration 6400: Loss = -11833.046409311737
Iteration 6500: Loss = -11833.045843878574
Iteration 6600: Loss = -11833.047278260814
1
Iteration 6700: Loss = -11833.044742348671
Iteration 6800: Loss = -11833.080073892723
1
Iteration 6900: Loss = -11833.0444233562
Iteration 7000: Loss = -11833.102060955154
1
Iteration 7100: Loss = -11833.043212726663
Iteration 7200: Loss = -11833.068127666196
1
Iteration 7300: Loss = -11833.04216216308
Iteration 7400: Loss = -11833.045123860631
1
Iteration 7500: Loss = -11833.041503400887
Iteration 7600: Loss = -11833.041763668725
1
Iteration 7700: Loss = -11833.040919353542
Iteration 7800: Loss = -11833.040638797873
Iteration 7900: Loss = -11833.040845228817
1
Iteration 8000: Loss = -11833.04080175855
2
Iteration 8100: Loss = -11833.039976481528
Iteration 8200: Loss = -11833.042771823044
1
Iteration 8300: Loss = -11833.039765338548
Iteration 8400: Loss = -11833.146693032695
1
Iteration 8500: Loss = -11833.038539909667
Iteration 8600: Loss = -11833.04599253723
1
Iteration 8700: Loss = -11833.037838716566
Iteration 8800: Loss = -11833.037259351217
Iteration 8900: Loss = -11833.056825101612
1
Iteration 9000: Loss = -11833.037967705988
2
Iteration 9100: Loss = -11833.080583182546
3
Iteration 9200: Loss = -11833.03664217229
Iteration 9300: Loss = -11819.570505806136
Iteration 9400: Loss = -11819.530326174945
Iteration 9500: Loss = -11819.576097422721
1
Iteration 9600: Loss = -11819.530414770923
2
Iteration 9700: Loss = -11819.538663255567
3
Iteration 9800: Loss = -11819.529877639758
Iteration 9900: Loss = -11819.543366288399
1
Iteration 10000: Loss = -11819.52965808516
Iteration 10100: Loss = -11819.619358644399
1
Iteration 10200: Loss = -11819.538631951737
2
Iteration 10300: Loss = -11819.531558046025
3
Iteration 10400: Loss = -11819.603760835258
4
Iteration 10500: Loss = -11819.529294114433
Iteration 10600: Loss = -11819.53478661489
1
Iteration 10700: Loss = -11819.532734862523
2
Iteration 10800: Loss = -11819.529203194319
Iteration 10900: Loss = -11819.543910931186
1
Iteration 11000: Loss = -11819.530467530973
2
Iteration 11100: Loss = -11819.529074158329
Iteration 11200: Loss = -11819.536428112951
1
Iteration 11300: Loss = -11819.537743280409
2
Iteration 11400: Loss = -11819.537930867367
3
Iteration 11500: Loss = -11819.529088669356
4
Iteration 11600: Loss = -11819.540683499352
5
Iteration 11700: Loss = -11819.53477955444
6
Iteration 11800: Loss = -11819.528574150896
Iteration 11900: Loss = -11819.531968369396
1
Iteration 12000: Loss = -11819.57567238936
2
Iteration 12100: Loss = -11819.54022751008
3
Iteration 12200: Loss = -11819.542642845874
4
Iteration 12300: Loss = -11819.564478953278
5
Iteration 12400: Loss = -11819.54849905593
6
Iteration 12500: Loss = -11819.532881436297
7
Iteration 12600: Loss = -11819.547442091603
8
Iteration 12700: Loss = -11819.535955623733
9
Iteration 12800: Loss = -11819.53528086737
10
Stopping early at iteration 12800 due to no improvement.
tensor([[ -8.9320,   6.9484],
        [  4.9795,  -7.9301],
        [  6.0565,  -7.4510],
        [  6.6377,  -8.1369],
        [  6.0300,  -8.4674],
        [-10.8904,   8.8320],
        [-10.4836,   5.8683],
        [  6.5569, -10.3242],
        [  5.6106,  -7.9436],
        [  6.7906,  -8.5516],
        [  6.7195,  -8.2132],
        [ -8.4039,   6.8794],
        [  3.4464,  -4.8463],
        [  6.4911,  -7.9992],
        [  6.1189,  -7.5596],
        [  3.2863,  -4.8060],
        [  7.9483, -10.3189],
        [  6.0220,  -7.6099],
        [ -9.7336,   8.0549],
        [ -8.1731,   6.6874],
        [ -8.8736,   7.4758],
        [ -8.9016,   7.4683],
        [ -9.5314,   5.5041],
        [ -8.1075,   6.3395],
        [  5.3030,  -7.1120],
        [ -9.8102,   8.1246],
        [  6.1175,  -7.7295],
        [ -9.1581,   7.7363],
        [ -8.9194,   7.5162],
        [  7.6402,  -9.0268],
        [ -8.8379,   7.2366],
        [  7.2521,  -9.0931],
        [  7.7846, -10.9356],
        [ -4.8145,   3.1105],
        [ -9.0125,   7.6234],
        [  5.2738,  -6.7503],
        [ -8.9375,   7.3981],
        [ -9.7924,   8.3966],
        [ -9.1284,   7.6995],
        [  6.4859,  -8.7365],
        [ -8.5315,   7.1433],
        [ -9.6401,   8.2166],
        [  6.3877,  -7.7742],
        [ -9.0697,   7.5924],
        [  6.6656,  -8.2554],
        [  7.3136,  -8.8772],
        [ -9.2702,   7.8053],
        [  5.5517,  -6.9700],
        [ -9.1660,   7.4936],
        [  4.4130,  -5.8549],
        [  5.3623,  -7.3168],
        [ -9.0826,   7.4733],
        [  6.5306,  -8.2349],
        [ -8.9409,   7.5533],
        [ -9.1289,   7.2121],
        [ -8.2301,   3.9206],
        [  5.8370,  -7.2529],
        [  6.1230,  -9.4878],
        [  6.7964,  -8.9520],
        [ -9.3436,   7.5131],
        [  4.0678,  -5.4552],
        [ -7.3372,   5.1380],
        [ -8.4511,   6.9066],
        [ -9.9510,   8.1206],
        [-12.0350,   7.5823],
        [  6.0992,  -7.6731],
        [  4.8895,  -6.4824],
        [  6.0748,  -7.8275],
        [ -9.3341,   7.4695],
        [ -7.2573,   5.7969],
        [ -9.0420,   7.3169],
        [  5.7487,  -8.0644],
        [ -9.4759,   7.8749],
        [ -5.1685,   3.7396],
        [ -8.1863,   6.7244],
        [  5.3074,  -7.0925],
        [ -9.5895,   7.5165],
        [  5.3785,  -7.5592],
        [ -9.7339,   7.9563],
        [ -9.3539,   7.5179],
        [-10.4486,   7.3166],
        [ -2.8526,   1.1703],
        [  6.0844,  -7.6631],
        [  5.0765,  -6.9979],
        [  4.1671,  -5.6206],
        [-11.2402,   8.2453],
        [ -8.4205,   7.0341],
        [ -8.6529,   6.7971],
        [  2.9092,  -4.6881],
        [ -9.2002,   6.7093],
        [ -9.5811,   7.8450],
        [ -5.5995,   4.0493],
        [  7.0414,  -8.4577],
        [ -8.8736,   7.4712],
        [ -8.8482,   7.4578],
        [  7.3171,  -9.5960],
        [ -9.5114,   7.7981],
        [ -8.4624,   7.0044],
        [ -7.8417,   6.4361],
        [ -9.6554,   6.9936]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7441, 0.2559],
        [0.2497, 0.7503]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4368, 0.5632], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2054, 0.1101],
         [0.7611, 0.4089]],

        [[0.3984, 0.0944],
         [0.5784, 0.7617]],

        [[0.6318, 0.0952],
         [0.0446, 0.9901]],

        [[0.4067, 0.0972],
         [0.1324, 0.8114]],

        [[0.2235, 0.1034],
         [0.3894, 0.4817]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919998665493138
Average Adjusted Rand Index: 0.9919996552039955
Iteration 0: Loss = -28965.237680537273
Iteration 10: Loss = -11814.153870953674
Iteration 20: Loss = -11814.1535095966
Iteration 30: Loss = -11814.1535095966
1
Iteration 40: Loss = -11814.1535095966
2
Iteration 50: Loss = -11814.1535095966
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7504, 0.2496],
        [0.2521, 0.7479]], dtype=torch.float64)
alpha: tensor([0.5398, 0.4602])
beta: tensor([[[0.4005, 0.1070],
         [0.3039, 0.2011]],

        [[0.6098, 0.0944],
         [0.3695, 0.2172]],

        [[0.7882, 0.0959],
         [0.6394, 0.8975]],

        [[0.3082, 0.0970],
         [0.8563, 0.4758]],

        [[0.9557, 0.1035],
         [0.3133, 0.6448]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28964.99881740621
Iteration 100: Loss = -12802.247763496542
Iteration 200: Loss = -12394.999306100035
Iteration 300: Loss = -12295.697068102698
Iteration 400: Loss = -12289.528927939178
Iteration 500: Loss = -12286.75262685253
Iteration 600: Loss = -12285.062946289874
Iteration 700: Loss = -12283.72438343263
Iteration 800: Loss = -12282.870310736322
Iteration 900: Loss = -12282.35304541166
Iteration 1000: Loss = -12281.962152039647
Iteration 1100: Loss = -12281.689910979578
Iteration 1200: Loss = -12280.724015285465
Iteration 1300: Loss = -12279.310308542712
Iteration 1400: Loss = -12278.092912850712
Iteration 1500: Loss = -12277.89019317634
Iteration 1600: Loss = -12277.02974729685
Iteration 1700: Loss = -12276.861704103621
Iteration 1800: Loss = -12276.665950764269
Iteration 1900: Loss = -12271.422638298178
Iteration 2000: Loss = -12255.685650716383
Iteration 2100: Loss = -12243.595262768247
Iteration 2200: Loss = -12233.819451090727
Iteration 2300: Loss = -12233.695395318467
Iteration 2400: Loss = -12233.56456621007
Iteration 2500: Loss = -12228.302834676155
Iteration 2600: Loss = -12228.237591958134
Iteration 2700: Loss = -12227.080762921318
Iteration 2800: Loss = -12224.557131329964
Iteration 2900: Loss = -12216.467722301946
Iteration 3000: Loss = -12216.439438001913
Iteration 3100: Loss = -12216.387906530188
Iteration 3200: Loss = -12204.067596702138
Iteration 3300: Loss = -12198.49146843913
Iteration 3400: Loss = -12198.470601519426
Iteration 3500: Loss = -12198.431640294346
Iteration 3600: Loss = -12186.604615703045
Iteration 3700: Loss = -12185.10566266616
Iteration 3800: Loss = -12167.117037130447
Iteration 3900: Loss = -12167.096002238259
Iteration 4000: Loss = -12164.110827319178
Iteration 4100: Loss = -12164.114440624304
1
Iteration 4200: Loss = -12163.953315755558
Iteration 4300: Loss = -12141.683483314328
Iteration 4400: Loss = -12140.25489655969
Iteration 4500: Loss = -12138.398925581492
Iteration 4600: Loss = -12136.222964793182
Iteration 4700: Loss = -12135.404938658754
Iteration 4800: Loss = -12133.583456660284
Iteration 4900: Loss = -12133.670419446114
1
Iteration 5000: Loss = -12129.053268644644
Iteration 5100: Loss = -12127.679270774735
Iteration 5200: Loss = -12126.715975835923
Iteration 5300: Loss = -12123.621423281173
Iteration 5400: Loss = -12123.454977548672
Iteration 5500: Loss = -12121.037388842602
Iteration 5600: Loss = -12118.156187049495
Iteration 5700: Loss = -12117.195886660431
Iteration 5800: Loss = -12101.353212147464
Iteration 5900: Loss = -12100.999654295927
Iteration 6000: Loss = -12086.369077117348
Iteration 6100: Loss = -12086.357678511626
Iteration 6200: Loss = -12086.344099875838
Iteration 6300: Loss = -12063.246121237888
Iteration 6400: Loss = -12056.858534943063
Iteration 6500: Loss = -12056.830106614958
Iteration 6600: Loss = -12049.371337396777
Iteration 6700: Loss = -12045.190510908458
Iteration 6800: Loss = -12045.130253964331
Iteration 6900: Loss = -12018.33837112899
Iteration 7000: Loss = -11999.741602897733
Iteration 7100: Loss = -11990.552820478924
Iteration 7200: Loss = -11972.8711382271
Iteration 7300: Loss = -11969.280304596354
Iteration 7400: Loss = -11957.735107792472
Iteration 7500: Loss = -11957.745796542446
1
Iteration 7600: Loss = -11957.72267438802
Iteration 7700: Loss = -11941.70209240359
Iteration 7800: Loss = -11929.985474902242
Iteration 7900: Loss = -11913.512426182635
Iteration 8000: Loss = -11899.025625855933
Iteration 8100: Loss = -11885.597521074706
Iteration 8200: Loss = -11885.64333004272
1
Iteration 8300: Loss = -11872.42201932783
Iteration 8400: Loss = -11872.419430289538
Iteration 8500: Loss = -11864.892543421103
Iteration 8600: Loss = -11864.886728324771
Iteration 8700: Loss = -11864.88589072274
Iteration 8800: Loss = -11864.886784705881
1
Iteration 8900: Loss = -11864.883408891948
Iteration 9000: Loss = -11846.666060001902
Iteration 9100: Loss = -11846.6260805253
Iteration 9200: Loss = -11846.623969369974
Iteration 9300: Loss = -11835.687813014742
Iteration 9400: Loss = -11835.675863019998
Iteration 9500: Loss = -11822.096270122382
Iteration 9600: Loss = -11822.095317285737
Iteration 9700: Loss = -11822.100638335192
1
Iteration 9800: Loss = -11822.094788139368
Iteration 9900: Loss = -11822.095646882339
1
Iteration 10000: Loss = -11822.097246690475
2
Iteration 10100: Loss = -11822.123299018942
3
Iteration 10200: Loss = -11822.093774495006
Iteration 10300: Loss = -11822.094144037874
1
Iteration 10400: Loss = -11812.726052223159
Iteration 10500: Loss = -11812.724890967933
Iteration 10600: Loss = -11812.73410881837
1
Iteration 10700: Loss = -11812.726178123861
2
Iteration 10800: Loss = -11812.724531555647
Iteration 10900: Loss = -11812.727634620498
1
Iteration 11000: Loss = -11812.763788944807
2
Iteration 11100: Loss = -11812.732665855974
3
Iteration 11200: Loss = -11812.737064992161
4
Iteration 11300: Loss = -11812.724078071948
Iteration 11400: Loss = -11812.723968533983
Iteration 11500: Loss = -11812.726804278143
1
Iteration 11600: Loss = -11812.726541442109
2
Iteration 11700: Loss = -11812.728735608798
3
Iteration 11800: Loss = -11812.723454791232
Iteration 11900: Loss = -11812.762166054807
1
Iteration 12000: Loss = -11812.723420441258
Iteration 12100: Loss = -11812.724922139534
1
Iteration 12200: Loss = -11812.757414782925
2
Iteration 12300: Loss = -11812.723129871192
Iteration 12400: Loss = -11812.736930045627
1
Iteration 12500: Loss = -11812.723023357697
Iteration 12600: Loss = -11812.729675747354
1
Iteration 12700: Loss = -11812.723392133163
2
Iteration 12800: Loss = -11812.724127480513
3
Iteration 12900: Loss = -11812.727074288898
4
Iteration 13000: Loss = -11812.72314702469
5
Iteration 13100: Loss = -11812.736373624832
6
Iteration 13200: Loss = -11812.730917863799
7
Iteration 13300: Loss = -11812.730214121819
8
Iteration 13400: Loss = -11812.72616819559
9
Iteration 13500: Loss = -11812.731046796753
10
Stopping early at iteration 13500 due to no improvement.
tensor([[  6.4373,  -7.8328],
        [ -6.2594,   4.6728],
        [ -7.8875,   5.2505],
        [ -7.7729,   6.3812],
        [ -7.9233,   6.4040],
        [  7.3039,  -8.6979],
        [  6.9907,  -8.4108],
        [ -6.6064,   5.1517],
        [ -6.7075,   4.9263],
        [ -6.2535,   4.5921],
        [ -9.0733,   6.7373],
        [  6.2833,  -7.7312],
        [ -5.1234,   3.6384],
        [ -7.8773,   6.3815],
        [ -7.6227,   6.1694],
        [ -8.3003,   6.6403],
        [ -7.7945,   6.3405],
        [ -6.9974,   5.4771],
        [  6.8493,  -8.3251],
        [  6.4391,  -7.8526],
        [  6.9217,  -8.4121],
        [  6.8620,  -8.3263],
        [  6.7206,  -8.1459],
        [  6.2984,  -8.0669],
        [ -7.1976,   5.3271],
        [  6.9931,  -8.7942],
        [ -8.5450,   4.5879],
        [  7.0048,  -8.4397],
        [  5.7930,  -7.2302],
        [ -8.4555,   6.3225],
        [  7.4912,  -9.0662],
        [ -8.8848,   5.1788],
        [ -8.1568,   6.7470],
        [  3.1466,  -4.7043],
        [  6.1436,  -7.7936],
        [ -5.8507,   4.2305],
        [  6.5406,  -8.1443],
        [  5.8994,  -8.6274],
        [  5.5462,  -9.7653],
        [ -8.0839,   6.5951],
        [  5.9668,  -8.9635],
        [  7.1492,  -8.5356],
        [ -7.8689,   6.4258],
        [  5.0288,  -7.1877],
        [ -8.1476,   6.7583],
        [ -5.9174,   4.3689],
        [  6.6942,  -8.0818],
        [ -7.2690,   5.8811],
        [  7.1109,  -8.5134],
        [ -6.7753,   3.9731],
        [ -8.9306,   4.3154],
        [  7.1344, -10.4930],
        [ -9.5171,   5.8982],
        [  6.5674,  -8.9639],
        [  5.9892,  -7.3971],
        [  6.3045,  -7.8370],
        [ -7.3210,   5.5654],
        [ -7.7867,   6.0055],
        [ -7.7748,   5.9991],
        [  7.1105,  -8.6451],
        [ -6.0229,   4.0718],
        [  4.3221,  -8.2553],
        [  6.0081,  -7.6753],
        [  6.9282,  -8.3888],
        [  5.9367,  -7.5328],
        [ -7.6273,   6.1278],
        [ -7.0155,   4.5689],
        [ -7.8057,   6.4161],
        [  6.9427,  -8.3303],
        [  6.3785,  -7.7959],
        [  6.2889,  -7.8583],
        [ -8.7059,   5.7296],
        [  6.6410,  -8.0736],
        [  3.6830,  -5.1597],
        [  6.9172,  -8.3051],
        [ -6.3722,   4.9376],
        [  7.3623,  -8.7487],
        [ -7.1556,   5.5933],
        [  7.1569,  -8.6408],
        [  7.2077,  -8.7661],
        [  6.0769,  -7.9436],
        [  1.9047,  -4.4196],
        [ -8.9588,   4.7976],
        [ -6.6036,   3.4915],
        [ -6.1265,   4.1828],
        [  1.8984,  -3.3251],
        [  7.7593,  -9.5977],
        [  5.9406,  -7.5468],
        [ -4.7462,   3.2514],
        [  5.9148,  -7.3875],
        [  6.6648,  -8.1627],
        [  5.1654,  -6.7978],
        [ -8.4262,   6.8766],
        [  5.9423,  -8.0464],
        [  7.3012,  -8.9165],
        [  2.7099,  -4.2514],
        [  5.6384,  -7.6501],
        [  6.9204,  -8.8434],
        [  6.9006,  -8.3378],
        [  2.8314,  -4.7112]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7509, 0.2491],
        [0.2513, 0.7487]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5700, 0.4300], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4085, 0.1069],
         [0.3039, 0.2056]],

        [[0.6098, 0.0944],
         [0.3695, 0.2172]],

        [[0.7882, 0.0959],
         [0.6394, 0.8975]],

        [[0.3082, 0.0968],
         [0.8563, 0.4758]],

        [[0.9557, 0.1032],
         [0.3133, 0.6448]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11816.833736841018
new:  [0.4504892035835673, 1.0, 0.9919998665493138, 1.0] [0.8808567317772946, 1.0, 0.9919996552039955, 1.0] [12018.610343742474, 11812.764139967838, 11819.53528086737, 11812.731046796753]
prior:  [1.0, 1.0, 1.0, 1.0] [1.0, 1.0, 1.0, 1.0] [11814.153507538813, 11814.1535095966, 11814.153507538813, 11814.1535095966]
-----------------------------------------------------------------------------------------
This iteration is 30
True Objective function: Loss = -11688.924413486695
Iteration 0: Loss = -15836.071528073599
Iteration 10: Loss = -11684.568559314788
Iteration 20: Loss = -11684.56856960396
1
Iteration 30: Loss = -11684.56856960396
2
Iteration 40: Loss = -11684.56856960396
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7441, 0.2559],
        [0.2838, 0.7162]], dtype=torch.float64)
alpha: tensor([0.5418, 0.4582])
beta: tensor([[[0.3951, 0.0923],
         [0.5676, 0.1924]],

        [[0.5904, 0.1001],
         [0.2514, 0.1918]],

        [[0.8839, 0.0943],
         [0.6769, 0.9785]],

        [[0.9666, 0.0968],
         [0.8757, 0.9302]],

        [[0.6989, 0.1042],
         [0.3634, 0.4837]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15909.325764411251
Iteration 100: Loss = -12111.311331586787
Iteration 200: Loss = -11731.454441549466
Iteration 300: Loss = -11684.376948519752
Iteration 400: Loss = -11683.84494029564
Iteration 500: Loss = -11683.607707130659
Iteration 600: Loss = -11683.475512775773
Iteration 700: Loss = -11683.39366698416
Iteration 800: Loss = -11683.3367611555
Iteration 900: Loss = -11683.29582214847
Iteration 1000: Loss = -11683.264875113677
Iteration 1100: Loss = -11683.241353947205
Iteration 1200: Loss = -11683.222658658085
Iteration 1300: Loss = -11683.207334070747
Iteration 1400: Loss = -11683.194775419262
Iteration 1500: Loss = -11683.184513447202
Iteration 1600: Loss = -11683.175808803318
Iteration 1700: Loss = -11683.168318704047
Iteration 1800: Loss = -11683.161967301468
Iteration 1900: Loss = -11683.156414285879
Iteration 2000: Loss = -11683.151686322013
Iteration 2100: Loss = -11683.147476718757
Iteration 2200: Loss = -11683.143721361936
Iteration 2300: Loss = -11683.140479473148
Iteration 2400: Loss = -11683.137464360148
Iteration 2500: Loss = -11683.134843669048
Iteration 2600: Loss = -11683.13583560234
1
Iteration 2700: Loss = -11683.130264565094
Iteration 2800: Loss = -11683.130208940804
Iteration 2900: Loss = -11683.127323227196
Iteration 3000: Loss = -11683.125066334234
Iteration 3100: Loss = -11683.13188919097
1
Iteration 3200: Loss = -11683.122342486531
Iteration 3300: Loss = -11683.126066959207
1
Iteration 3400: Loss = -11683.120019760137
Iteration 3500: Loss = -11683.119004850989
Iteration 3600: Loss = -11683.120179547608
1
Iteration 3700: Loss = -11683.117214840988
Iteration 3800: Loss = -11683.146742163965
1
Iteration 3900: Loss = -11683.116271731591
Iteration 4000: Loss = -11683.114992539884
Iteration 4100: Loss = -11683.138737598681
1
Iteration 4200: Loss = -11683.113771250226
Iteration 4300: Loss = -11683.113230466477
Iteration 4400: Loss = -11683.12071906322
1
Iteration 4500: Loss = -11683.112231394245
Iteration 4600: Loss = -11683.112131107337
Iteration 4700: Loss = -11683.115140533113
1
Iteration 4800: Loss = -11683.110969411318
Iteration 4900: Loss = -11683.11081075453
Iteration 5000: Loss = -11683.110298009227
Iteration 5100: Loss = -11683.110741352908
1
Iteration 5200: Loss = -11683.110256169324
Iteration 5300: Loss = -11683.112838629178
1
Iteration 5400: Loss = -11683.115376640246
2
Iteration 5500: Loss = -11683.109347983165
Iteration 5600: Loss = -11683.111191054644
1
Iteration 5700: Loss = -11683.10884774186
Iteration 5800: Loss = -11683.114169240052
1
Iteration 5900: Loss = -11683.10834611365
Iteration 6000: Loss = -11683.119752271825
1
Iteration 6100: Loss = -11683.10872437622
2
Iteration 6200: Loss = -11683.107623525717
Iteration 6300: Loss = -11683.107968841612
1
Iteration 6400: Loss = -11683.110619427616
2
Iteration 6500: Loss = -11683.107966894106
3
Iteration 6600: Loss = -11683.107094543344
Iteration 6700: Loss = -11683.107447547138
1
Iteration 6800: Loss = -11683.157126346185
2
Iteration 6900: Loss = -11683.106800224949
Iteration 7000: Loss = -11683.106546029014
Iteration 7100: Loss = -11683.117696088762
1
Iteration 7200: Loss = -11683.106824816343
2
Iteration 7300: Loss = -11683.111738958025
3
Iteration 7400: Loss = -11683.10814079784
4
Iteration 7500: Loss = -11683.118842697852
5
Iteration 7600: Loss = -11683.111070000825
6
Iteration 7700: Loss = -11683.106157797038
Iteration 7800: Loss = -11683.163091204995
1
Iteration 7900: Loss = -11683.106023041588
Iteration 8000: Loss = -11683.112636390695
1
Iteration 8100: Loss = -11683.107993738175
2
Iteration 8200: Loss = -11683.106266334224
3
Iteration 8300: Loss = -11683.12521926939
4
Iteration 8400: Loss = -11683.105576614304
Iteration 8500: Loss = -11683.111167234038
1
Iteration 8600: Loss = -11683.132528957565
2
Iteration 8700: Loss = -11683.107000976468
3
Iteration 8800: Loss = -11683.107528754867
4
Iteration 8900: Loss = -11683.107289408497
5
Iteration 9000: Loss = -11683.136317939245
6
Iteration 9100: Loss = -11683.109683601857
7
Iteration 9200: Loss = -11683.137786908981
8
Iteration 9300: Loss = -11683.108092979612
9
Iteration 9400: Loss = -11683.180645236329
10
Stopping early at iteration 9400 due to no improvement.
tensor([[  5.4598, -10.0750],
        [ -9.5213,   4.9061],
        [  5.1180,  -9.7332],
        [ -9.5881,   4.9728],
        [  3.4186,  -8.0339],
        [ -9.1214,   4.5062],
        [ -9.5043,   4.8891],
        [  4.7186,  -9.3339],
        [  4.9506,  -9.5658],
        [ -9.5000,   4.8848],
        [  4.9160,  -9.5313],
        [ -9.8433,   5.2280],
        [ -6.1074,   1.4922],
        [  5.5645, -10.1798],
        [ -4.8566,   0.2414],
        [  5.2203,  -9.8355],
        [  4.8104,  -9.4257],
        [  4.4838,  -9.0991],
        [ -9.2165,   4.6013],
        [  4.7336,  -9.3488],
        [  5.3974, -10.0127],
        [  4.5685,  -9.1837],
        [  5.2472,  -9.8624],
        [  4.9569,  -9.5722],
        [-10.0099,   5.3947],
        [  5.6762, -10.2914],
        [ -7.1308,   2.5155],
        [ -8.4341,   3.8189],
        [  5.1660,  -9.7812],
        [ -9.6413,   5.0261],
        [-10.4014,   5.7861],
        [  4.8873,  -9.5025],
        [  4.7302,  -9.3454],
        [  3.9695,  -8.5848],
        [  5.0776,  -9.6928],
        [ -9.5906,   4.9754],
        [  4.7561,  -9.3714],
        [-10.2654,   5.6502],
        [  5.1263,  -9.7415],
        [  5.2242,  -9.8394],
        [ -9.4592,   4.8440],
        [  3.3787,  -7.9940],
        [  5.5857, -10.2009],
        [ -9.7173,   5.1021],
        [ -9.8702,   5.2550],
        [ -8.5106,   3.8954],
        [ -0.0921,  -4.5231],
        [-10.1040,   5.4887],
        [ -6.5995,   1.9843],
        [ -9.9808,   5.3655],
        [  5.5406, -10.1559],
        [ -9.8986,   5.2834],
        [ -7.9746,   3.3594],
        [  4.5365,  -9.1517],
        [  5.3656,  -9.9808],
        [  4.9771,  -9.5923],
        [  2.2548,  -6.8700],
        [  3.9985,  -8.6138],
        [ -9.9429,   5.3276],
        [  5.3140,  -9.9292],
        [  5.9034, -10.5187],
        [ -9.6360,   5.0207],
        [  4.1815,  -8.7967],
        [ -9.6482,   5.0330],
        [ -7.9224,   3.3071],
        [ -7.6173,   3.0021],
        [  5.2284,  -9.8437],
        [  4.1525,  -8.7677],
        [ -4.7220,   0.1068],
        [  4.0913,  -8.7065],
        [  4.9957,  -9.6109],
        [  4.6213,  -9.2366],
        [ -9.7539,   5.1386],
        [  5.5866, -10.2019],
        [ -8.0509,   3.4357],
        [ -8.9386,   4.3234],
        [  4.9230,  -9.5382],
        [ -9.1462,   4.5310],
        [ -9.7680,   5.1528],
        [  4.9462,  -9.5615],
        [ -9.2903,   4.6751],
        [ -8.5511,   3.9359],
        [  5.0605,  -9.6758],
        [  4.9037,  -9.5189],
        [  5.1850,  -9.8002],
        [  4.7734,  -9.3887],
        [  4.6049,  -9.2201],
        [  5.5105, -10.1257],
        [  5.3799,  -9.9951],
        [ -7.1056,   2.4903],
        [-10.1841,   5.5689],
        [  5.4729, -10.0882],
        [  4.7938,  -9.4091],
        [  5.4187, -10.0339],
        [  3.4934,  -8.1087],
        [  5.0327,  -9.6479],
        [ -9.4979,   4.8827],
        [ -9.7645,   5.1493],
        [ -9.2162,   4.6010],
        [  4.9493,  -9.5646]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7468, 0.2532],
        [0.2846, 0.7154]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5699, 0.4301], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4006, 0.0925],
         [0.5676, 0.1974]],

        [[0.5904, 0.1004],
         [0.2514, 0.1918]],

        [[0.8839, 0.0943],
         [0.6769, 0.9785]],

        [[0.9666, 0.0966],
         [0.8757, 0.9302]],

        [[0.6989, 0.1034],
         [0.3634, 0.4837]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
Iteration 0: Loss = -22504.577488536295
Iteration 10: Loss = -11684.71320492603
Iteration 20: Loss = -11684.568569603958
Iteration 30: Loss = -11684.56856960396
1
Iteration 40: Loss = -11684.56856960396
2
Iteration 50: Loss = -11684.56856960396
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7441, 0.2559],
        [0.2838, 0.7162]], dtype=torch.float64)
alpha: tensor([0.5418, 0.4582])
beta: tensor([[[0.3951, 0.0923],
         [0.3896, 0.1924]],

        [[0.1744, 0.1001],
         [0.7394, 0.3562]],

        [[0.3826, 0.0943],
         [0.3013, 0.7514]],

        [[0.4109, 0.0968],
         [0.0703, 0.2793]],

        [[0.4281, 0.1042],
         [0.7567, 0.1837]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22503.877237036464
Iteration 100: Loss = -11926.418181242887
Iteration 200: Loss = -11688.818371847783
Iteration 300: Loss = -11684.890138416778
Iteration 400: Loss = -11684.21858918204
Iteration 500: Loss = -11683.881817908705
Iteration 600: Loss = -11683.682709913388
Iteration 700: Loss = -11683.553573808631
Iteration 800: Loss = -11683.464523891695
Iteration 900: Loss = -11683.40012660661
Iteration 1000: Loss = -11683.35179046593
Iteration 1100: Loss = -11683.314487307065
Iteration 1200: Loss = -11683.284921146957
Iteration 1300: Loss = -11683.26119212302
Iteration 1400: Loss = -11683.24176946629
Iteration 1500: Loss = -11683.225657625126
Iteration 1600: Loss = -11683.212144611794
Iteration 1700: Loss = -11683.200700082172
Iteration 1800: Loss = -11683.190910999323
Iteration 1900: Loss = -11683.182466358261
Iteration 2000: Loss = -11683.175132001075
Iteration 2100: Loss = -11683.16869606691
Iteration 2200: Loss = -11683.163069930784
Iteration 2300: Loss = -11683.158040388655
Iteration 2400: Loss = -11683.1535944909
Iteration 2500: Loss = -11683.149647605462
Iteration 2600: Loss = -11683.146113973224
Iteration 2700: Loss = -11683.142913867609
Iteration 2800: Loss = -11683.14769094104
1
Iteration 2900: Loss = -11683.137397180792
Iteration 3000: Loss = -11683.135034509085
Iteration 3100: Loss = -11683.132836443083
Iteration 3200: Loss = -11683.1309042967
Iteration 3300: Loss = -11683.129090256107
Iteration 3400: Loss = -11683.127437135992
Iteration 3500: Loss = -11683.125905156503
Iteration 3600: Loss = -11683.124531031011
Iteration 3700: Loss = -11683.124213030917
Iteration 3800: Loss = -11683.121991016553
Iteration 3900: Loss = -11683.120916681859
Iteration 4000: Loss = -11683.119922588441
Iteration 4100: Loss = -11683.141766756076
1
Iteration 4200: Loss = -11683.118115673955
Iteration 4300: Loss = -11683.117269495942
Iteration 4400: Loss = -11683.11650919103
Iteration 4500: Loss = -11683.11576326531
Iteration 4600: Loss = -11683.115740408792
Iteration 4700: Loss = -11683.114492636265
Iteration 4800: Loss = -11683.113910513981
Iteration 4900: Loss = -11683.122134786414
1
Iteration 5000: Loss = -11683.119108119196
2
Iteration 5100: Loss = -11683.112409965306
Iteration 5200: Loss = -11683.136038607068
1
Iteration 5300: Loss = -11683.128527402318
2
Iteration 5400: Loss = -11683.123190702052
3
Iteration 5500: Loss = -11683.112946323405
4
Iteration 5600: Loss = -11683.110457525841
Iteration 5700: Loss = -11683.137379871212
1
Iteration 5800: Loss = -11683.109831156808
Iteration 5900: Loss = -11683.12169099445
1
Iteration 6000: Loss = -11683.114561013854
2
Iteration 6100: Loss = -11683.10983452522
3
Iteration 6200: Loss = -11683.11165086313
4
Iteration 6300: Loss = -11683.11121024688
5
Iteration 6400: Loss = -11683.117026637208
6
Iteration 6500: Loss = -11683.109100410436
Iteration 6600: Loss = -11683.114672072887
1
Iteration 6700: Loss = -11683.108880151292
Iteration 6800: Loss = -11683.109281159896
1
Iteration 6900: Loss = -11683.116632020125
2
Iteration 7000: Loss = -11683.111142710932
3
Iteration 7100: Loss = -11683.114024002532
4
Iteration 7200: Loss = -11683.113720659445
5
Iteration 7300: Loss = -11683.110283205246
6
Iteration 7400: Loss = -11683.110188451137
7
Iteration 7500: Loss = -11683.109178256143
8
Iteration 7600: Loss = -11683.109431228266
9
Iteration 7700: Loss = -11683.107044175063
Iteration 7800: Loss = -11683.108641782132
1
Iteration 7900: Loss = -11683.106634237385
Iteration 8000: Loss = -11683.116774638478
1
Iteration 8100: Loss = -11683.111256656699
2
Iteration 8200: Loss = -11683.106341559926
Iteration 8300: Loss = -11683.106172725293
Iteration 8400: Loss = -11683.106690802164
1
Iteration 8500: Loss = -11683.118831324806
2
Iteration 8600: Loss = -11683.110821867753
3
Iteration 8700: Loss = -11683.109408374119
4
Iteration 8800: Loss = -11683.112629227351
5
Iteration 8900: Loss = -11683.106042575462
Iteration 9000: Loss = -11683.111823418772
1
Iteration 9100: Loss = -11683.119359291712
2
Iteration 9200: Loss = -11683.107172489126
3
Iteration 9300: Loss = -11683.105738706163
Iteration 9400: Loss = -11683.108325081688
1
Iteration 9500: Loss = -11683.141173329694
2
Iteration 9600: Loss = -11683.105562741684
Iteration 9700: Loss = -11683.105861701051
1
Iteration 9800: Loss = -11683.105648039205
2
Iteration 9900: Loss = -11683.110105029344
3
Iteration 10000: Loss = -11683.115917319443
4
Iteration 10100: Loss = -11683.142883497834
5
Iteration 10200: Loss = -11683.10532525552
Iteration 10300: Loss = -11683.109198169936
1
Iteration 10400: Loss = -11683.106247152562
2
Iteration 10500: Loss = -11683.262376428076
3
Iteration 10600: Loss = -11683.107225833564
4
Iteration 10700: Loss = -11683.106922552863
5
Iteration 10800: Loss = -11683.113358279295
6
Iteration 10900: Loss = -11683.105875096613
7
Iteration 11000: Loss = -11683.117810901365
8
Iteration 11100: Loss = -11683.106364027297
9
Iteration 11200: Loss = -11683.10820809075
10
Stopping early at iteration 11200 due to no improvement.
tensor([[-10.4077,   7.7129],
        [  6.6333,  -8.4116],
        [ -9.1870,   7.7824],
        [  6.9820,  -8.8381],
        [ -7.6438,   6.0521],
        [  6.0581,  -7.9035],
        [  6.3028,  -8.4264],
        [ -7.8174,   6.4303],
        [-10.8934,   6.6282],
        [  6.4819,  -8.5119],
        [ -8.4802,   7.0720],
        [  7.1446,  -8.5471],
        [  2.6593,  -4.9401],
        [ -9.0402,   7.4778],
        [  0.7339,  -4.3606],
        [ -8.5962,   6.6243],
        [ -7.8161,   6.4184],
        [ -8.9872,   6.9582],
        [  6.4198,  -8.2947],
        [-10.9575,   6.6775],
        [ -8.9382,   7.4032],
        [ -8.4448,   6.3644],
        [ -9.8087,   6.0734],
        [ -8.9741,   7.1174],
        [  7.4885,  -8.9161],
        [ -9.2307,   7.4141],
        [  3.6568,  -5.9663],
        [  5.8773,  -7.3507],
        [ -8.4569,   6.9365],
        [  6.7001,  -8.1156],
        [  7.8370,  -9.5430],
        [ -9.1402,   7.1203],
        [ -7.6994,   6.2911],
        [ -7.3443,   5.5851],
        [ -8.8645,   7.3847],
        [  6.5868,  -8.0694],
        [ -8.4354,   6.5495],
        [  6.6952,  -9.6169],
        [ -9.1352,   7.5237],
        [ -8.2538,   6.8657],
        [  6.3608,  -7.8064],
        [ -6.8372,   4.8308],
        [ -8.5627,   7.1580],
        [  6.9025,  -8.2997],
        [  6.6508, -10.7064],
        [  5.5409,  -7.5789],
        [ -3.5770,   0.8557],
        [  6.3204,  -9.6383],
        [  3.4925,  -5.0935],
        [  6.6013,  -8.8527],
        [ -8.8039,   7.3861],
        [  6.8159,  -8.3666],
        [  4.9857,  -6.3853],
        [ -8.5979,   6.6826],
        [ -8.8709,   7.4375],
        [ -8.3455,   6.7628],
        [ -5.4178,   3.7120],
        [ -7.0963,   5.5995],
        [  7.0428,  -8.4330],
        [ -8.3495,   6.9479],
        [ -8.8130,   7.3995],
        [  6.7234,  -8.3266],
        [ -7.8325,   5.7643],
        [  6.4799,  -8.1166],
        [  4.9012,  -6.3444],
        [  4.6094,  -6.0174],
        [-10.1972,   7.7296],
        [ -8.3520,   5.8575],
        [  0.7846,  -4.0405],
        [ -8.9657,   4.6651],
        [ -8.6848,   6.7257],
        [ -7.7589,   6.3592],
        [  6.7500,  -8.6805],
        [ -9.8117,   7.5229],
        [  7.3864,  -8.7897],
        [  5.8960,  -7.8886],
        [ -8.7388,   7.2464],
        [  6.3777,  -7.7742],
        [  6.3714,  -8.3405],
        [ -9.8238,   7.4175],
        [  6.6684,  -8.4536],
        [  7.0902,  -8.6152],
        [ -9.0100,   6.6361],
        [ -9.0651,   6.0955],
        [ -8.7776,   6.5349],
        [ -9.2341,   7.0949],
        [ -8.1450,   6.1599],
        [ -9.9009,   7.1784],
        [ -8.5225,   6.9669],
        [  3.5191,  -6.0793],
        [  6.9105,  -9.5818],
        [ -8.8104,   7.1705],
        [ -8.3910,   6.8858],
        [ -9.1688,   7.1196],
        [ -6.6242,   5.2321],
        [ -8.4673,   7.0809],
        [  6.3981,  -8.3288],
        [  6.6842,  -8.9877],
        [  6.2279,  -7.7072],
        [ -8.0987,   6.7050]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7161, 0.2839],
        [0.2554, 0.7446]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4302, 0.5698], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1967, 0.0923],
         [0.3896, 0.4021]],

        [[0.1744, 0.1003],
         [0.7394, 0.3562]],

        [[0.3826, 0.0943],
         [0.3013, 0.7514]],

        [[0.4109, 0.0968],
         [0.0703, 0.2793]],

        [[0.4281, 0.1040],
         [0.7567, 0.1837]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
Iteration 0: Loss = -18585.2930910546
Iteration 10: Loss = -11684.568573020819
Iteration 20: Loss = -11684.56857208016
Iteration 30: Loss = -11684.56857208016
1
Iteration 40: Loss = -11684.56857208016
2
Iteration 50: Loss = -11684.56857208016
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7162, 0.2838],
        [0.2559, 0.7441]], dtype=torch.float64)
alpha: tensor([0.4582, 0.5418])
beta: tensor([[[0.1924, 0.0923],
         [0.6583, 0.3951]],

        [[0.0951, 0.1001],
         [0.2864, 0.5216]],

        [[0.0161, 0.0943],
         [0.5109, 0.5833]],

        [[0.0192, 0.0968],
         [0.1995, 0.5856]],

        [[0.2066, 0.1042],
         [0.2278, 0.7815]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18585.035978283526
Iteration 100: Loss = -11911.284044534132
Iteration 200: Loss = -11700.764634059029
Iteration 300: Loss = -11685.082723710157
Iteration 400: Loss = -11684.282837132174
Iteration 500: Loss = -11683.913197315656
Iteration 600: Loss = -11683.70171399619
Iteration 700: Loss = -11683.566839313458
Iteration 800: Loss = -11683.474503251595
Iteration 900: Loss = -11683.408086691272
Iteration 1000: Loss = -11683.35846652083
Iteration 1100: Loss = -11683.320222059818
Iteration 1200: Loss = -11683.290162291942
Iteration 1300: Loss = -11683.265954331964
Iteration 1400: Loss = -11683.24616592121
Iteration 1500: Loss = -11683.22980059723
Iteration 1600: Loss = -11683.216031156713
Iteration 1700: Loss = -11683.204386900346
Iteration 1800: Loss = -11683.19446059856
Iteration 1900: Loss = -11683.186050869372
Iteration 2000: Loss = -11683.178447346058
Iteration 2100: Loss = -11683.171995143028
Iteration 2200: Loss = -11683.166532637511
Iteration 2300: Loss = -11683.161231018468
Iteration 2400: Loss = -11683.156767128297
Iteration 2500: Loss = -11683.152754790586
Iteration 2600: Loss = -11683.149149586689
Iteration 2700: Loss = -11683.14601397807
Iteration 2800: Loss = -11683.143073997993
Iteration 2900: Loss = -11683.140661081045
Iteration 3000: Loss = -11683.138041834623
Iteration 3100: Loss = -11683.135858934564
Iteration 3200: Loss = -11683.134547274514
Iteration 3300: Loss = -11683.132088983211
Iteration 3400: Loss = -11683.13042859138
Iteration 3500: Loss = -11683.135007400728
1
Iteration 3600: Loss = -11683.127542900342
Iteration 3700: Loss = -11683.126166948019
Iteration 3800: Loss = -11683.124957133883
Iteration 3900: Loss = -11683.124231555117
Iteration 4000: Loss = -11683.122785328576
Iteration 4100: Loss = -11683.121844607816
Iteration 4200: Loss = -11683.130973658725
1
Iteration 4300: Loss = -11683.1200930613
Iteration 4400: Loss = -11683.119318797328
Iteration 4500: Loss = -11683.118633972415
Iteration 4600: Loss = -11683.118001711066
Iteration 4700: Loss = -11683.117362996334
Iteration 4800: Loss = -11683.11682180349
Iteration 4900: Loss = -11683.116426188782
Iteration 5000: Loss = -11683.115753490882
Iteration 5100: Loss = -11683.115313312177
Iteration 5200: Loss = -11683.14384402506
1
Iteration 5300: Loss = -11683.114524884659
Iteration 5400: Loss = -11683.114100589233
Iteration 5500: Loss = -11683.113720645
Iteration 5600: Loss = -11683.119034444022
1
Iteration 5700: Loss = -11683.11308617319
Iteration 5800: Loss = -11683.112822820647
Iteration 5900: Loss = -11683.11250246279
Iteration 6000: Loss = -11683.112498255396
Iteration 6100: Loss = -11683.111959723568
Iteration 6200: Loss = -11683.118031783812
1
Iteration 6300: Loss = -11683.115159534158
2
Iteration 6400: Loss = -11683.112670717748
3
Iteration 6500: Loss = -11683.11328360962
4
Iteration 6600: Loss = -11683.111243738824
Iteration 6700: Loss = -11683.111867507721
1
Iteration 6800: Loss = -11683.11297105662
2
Iteration 6900: Loss = -11683.110513526948
Iteration 7000: Loss = -11683.119501689218
1
Iteration 7100: Loss = -11683.110386378963
Iteration 7200: Loss = -11683.158643165349
1
Iteration 7300: Loss = -11683.109937786867
Iteration 7400: Loss = -11683.128798121976
1
Iteration 7500: Loss = -11683.10975610613
Iteration 7600: Loss = -11683.11793398969
1
Iteration 7700: Loss = -11683.110428216378
2
Iteration 7800: Loss = -11683.117118419714
3
Iteration 7900: Loss = -11683.124048851985
4
Iteration 8000: Loss = -11683.125208742658
5
Iteration 8100: Loss = -11683.11164255819
6
Iteration 8200: Loss = -11683.110180546573
7
Iteration 8300: Loss = -11683.122058458419
8
Iteration 8400: Loss = -11683.2311171986
9
Iteration 8500: Loss = -11683.111146657398
10
Stopping early at iteration 8500 due to no improvement.
tensor([[-8.8977,  6.8977],
        [ 5.4007, -8.3194],
        [-8.2358,  6.4254],
        [ 6.1985, -7.5948],
        [-6.3911,  4.9827],
        [ 5.5742, -7.6955],
        [ 5.5200, -7.4312],
        [-7.8929,  5.4225],
        [-7.9303,  6.0391],
        [ 5.7381, -7.1315],
        [-9.8687,  7.4647],
        [ 6.5641, -8.7020],
        [ 2.6776, -4.9223],
        [-8.3292,  6.3957],
        [ 1.4427, -3.6503],
        [-8.6885,  7.2955],
        [-9.1657,  7.0654],
        [-8.1101,  6.3212],
        [ 5.8694, -8.2632],
        [-8.5102,  6.6026],
        [-8.1040,  6.7019],
        [-7.5491,  5.9131],
        [-8.1181,  6.7294],
        [-8.6417,  6.8043],
        [ 5.9118, -7.3343],
        [-8.0802,  6.6231],
        [ 3.9305, -5.6927],
        [ 5.2225, -6.6273],
        [-8.0876,  6.6664],
        [ 6.0120, -7.4778],
        [ 6.2591, -8.2036],
        [-8.2166,  6.8302],
        [-7.9430,  6.2058],
        [-8.2661,  6.7754],
        [-8.2547,  6.8667],
        [ 6.0862, -8.7014],
        [-7.6775,  6.2093],
        [ 6.6509, -8.3861],
        [-8.2869,  6.5883],
        [-9.5221,  4.9069],
        [ 5.8057, -7.2488],
        [-6.8863,  4.5537],
        [-9.4748,  5.9034],
        [ 6.1893, -7.9884],
        [ 6.1903, -8.6800],
        [ 5.2568, -6.6681],
        [-3.9827,  0.4518],
        [ 6.5418, -7.9290],
        [ 3.5906, -4.9953],
        [ 6.5778, -8.4268],
        [-8.0236,  6.6058],
        [ 5.9461, -7.3324],
        [ 4.8160, -6.5799],
        [-8.7235,  7.2372],
        [-9.7995,  5.8219],
        [-8.4649,  6.4552],
        [-5.2943,  3.8389],
        [-8.1872,  6.5537],
        [ 6.4442, -9.5787],
        [-8.4394,  6.6075],
        [-8.3429,  6.7538],
        [ 6.0755, -7.5430],
        [-7.4920,  5.9863],
        [ 5.7864, -7.6064],
        [ 4.6289, -6.6451],
        [ 4.1010, -6.4808],
        [-9.7087,  6.8046],
        [-8.2357,  6.1429],
        [ 1.7055, -3.1182],
        [-7.1681,  5.3598],
        [-7.5565,  6.0300],
        [-7.6605,  6.2661],
        [ 5.6832, -7.8672],
        [-9.7340,  7.0087],
        [ 4.9143, -6.3978],
        [ 5.4349, -7.1815],
        [-9.4951,  5.0240],
        [ 6.0325, -8.0869],
        [ 6.1673, -7.5960],
        [-9.1575,  6.2072],
        [ 6.0079, -7.4042],
        [ 5.2085, -6.7162],
        [-8.2361,  6.7160],
        [-8.9089,  6.2450],
        [-8.2205,  6.8214],
        [-7.8690,  6.4460],
        [-7.5460,  6.1301],
        [-8.2776,  6.7567],
        [-8.1622,  6.3079],
        [ 3.7354, -5.8666],
        [ 6.3229, -7.7172],
        [-9.3403,  5.8775],
        [-8.9846,  7.5895],
        [-9.7027,  7.5351],
        [-6.6704,  4.9387],
        [-8.2648,  6.7898],
        [ 5.9421, -7.6608],
        [ 6.3779, -8.0405],
        [ 4.4230, -8.9269],
        [-9.1873,  5.8614]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7158, 0.2842],
        [0.2557, 0.7443]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4300, 0.5700], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1967, 0.0924],
         [0.6583, 0.4025]],

        [[0.0951, 0.1000],
         [0.2864, 0.5216]],

        [[0.0161, 0.0943],
         [0.5109, 0.5833]],

        [[0.0192, 0.0966],
         [0.1995, 0.5856]],

        [[0.2066, 0.1041],
         [0.2278, 0.7815]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
Iteration 0: Loss = -16920.785387768286
Iteration 10: Loss = -11684.56856898835
Iteration 20: Loss = -11684.56857208016
1
Iteration 30: Loss = -11684.56857208016
2
Iteration 40: Loss = -11684.56857208016
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7162, 0.2838],
        [0.2559, 0.7441]], dtype=torch.float64)
alpha: tensor([0.4582, 0.5418])
beta: tensor([[[0.1924, 0.0923],
         [0.2964, 0.3951]],

        [[0.2252, 0.1001],
         [0.3578, 0.2978]],

        [[0.7590, 0.0943],
         [0.3771, 0.7921]],

        [[0.1685, 0.0968],
         [0.0863, 0.2452]],

        [[0.0966, 0.1042],
         [0.1229, 0.2924]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16920.38474138171
Iteration 100: Loss = -12201.809196475442
Iteration 200: Loss = -11831.967464478024
Iteration 300: Loss = -11697.113862341641
Iteration 400: Loss = -11696.16526778361
Iteration 500: Loss = -11695.759346031595
Iteration 600: Loss = -11695.531068784063
Iteration 700: Loss = -11695.386151585413
Iteration 800: Loss = -11695.287716543835
Iteration 900: Loss = -11695.217122884294
Iteration 1000: Loss = -11695.164460825421
Iteration 1100: Loss = -11695.124144281861
Iteration 1200: Loss = -11695.09248875092
Iteration 1300: Loss = -11695.067153179425
Iteration 1400: Loss = -11695.046418589403
Iteration 1500: Loss = -11695.029314603706
Iteration 1600: Loss = -11695.01490044631
Iteration 1700: Loss = -11695.002713504366
Iteration 1800: Loss = -11694.992286416431
Iteration 1900: Loss = -11694.983303709487
Iteration 2000: Loss = -11694.97546098836
Iteration 2100: Loss = -11694.968610595508
Iteration 2200: Loss = -11694.962619508138
Iteration 2300: Loss = -11694.957304774643
Iteration 2400: Loss = -11694.952492164302
Iteration 2500: Loss = -11694.94819110215
Iteration 2600: Loss = -11694.944205408354
Iteration 2700: Loss = -11694.940060055998
Iteration 2800: Loss = -11694.721300529014
Iteration 2900: Loss = -11688.629603008143
Iteration 3000: Loss = -11688.627022870449
Iteration 3100: Loss = -11688.624711396325
Iteration 3200: Loss = -11688.622677448337
Iteration 3300: Loss = -11688.620630827449
Iteration 3400: Loss = -11688.628540465741
1
Iteration 3500: Loss = -11688.647627148057
2
Iteration 3600: Loss = -11688.619264074283
Iteration 3700: Loss = -11688.633475077953
1
Iteration 3800: Loss = -11688.613171174626
Iteration 3900: Loss = -11688.62749460396
1
Iteration 4000: Loss = -11688.610890018552
Iteration 4100: Loss = -11688.631843448102
1
Iteration 4200: Loss = -11688.637086976989
2
Iteration 4300: Loss = -11688.628770214656
3
Iteration 4400: Loss = -11688.609537318389
Iteration 4500: Loss = -11688.606600299525
Iteration 4600: Loss = -11688.606151812448
Iteration 4700: Loss = -11688.605256425606
Iteration 4800: Loss = -11688.606154046041
1
Iteration 4900: Loss = -11688.603977145562
Iteration 5000: Loss = -11688.63726336084
1
Iteration 5100: Loss = -11688.602948577201
Iteration 5200: Loss = -11688.602512965552
Iteration 5300: Loss = -11688.602112296028
Iteration 5400: Loss = -11688.601616377053
Iteration 5500: Loss = -11688.602706537864
1
Iteration 5600: Loss = -11688.601280769613
Iteration 5700: Loss = -11688.603405970753
1
Iteration 5800: Loss = -11688.600344520779
Iteration 5900: Loss = -11688.60035286394
1
Iteration 6000: Loss = -11688.608489938555
2
Iteration 6100: Loss = -11688.599445634834
Iteration 6200: Loss = -11688.728201077114
1
Iteration 6300: Loss = -11688.598886410595
Iteration 6400: Loss = -11688.614633579853
1
Iteration 6500: Loss = -11688.674164943373
2
Iteration 6600: Loss = -11688.598239723622
Iteration 6700: Loss = -11688.598305553574
1
Iteration 6800: Loss = -11688.619468170295
2
Iteration 6900: Loss = -11688.617352090592
3
Iteration 7000: Loss = -11688.602250145363
4
Iteration 7100: Loss = -11688.605120185504
5
Iteration 7200: Loss = -11688.666077339258
6
Iteration 7300: Loss = -11688.597543196574
Iteration 7400: Loss = -11688.597595658188
1
Iteration 7500: Loss = -11688.597707312852
2
Iteration 7600: Loss = -11688.596987151555
Iteration 7700: Loss = -11688.64612047618
1
Iteration 7800: Loss = -11688.604521583304
2
Iteration 7900: Loss = -11688.596917597126
Iteration 8000: Loss = -11688.616546456427
1
Iteration 8100: Loss = -11688.613447649246
2
Iteration 8200: Loss = -11688.606673948003
3
Iteration 8300: Loss = -11688.606400944947
4
Iteration 8400: Loss = -11688.596514816378
Iteration 8500: Loss = -11688.60148430899
1
Iteration 8600: Loss = -11688.598825211882
2
Iteration 8700: Loss = -11683.13899387285
Iteration 8800: Loss = -11683.141234487493
1
Iteration 8900: Loss = -11683.13639649117
Iteration 9000: Loss = -11683.15052915141
1
Iteration 9100: Loss = -11683.137585971248
2
Iteration 9200: Loss = -11683.135878978526
Iteration 9300: Loss = -11683.278604311243
1
Iteration 9400: Loss = -11683.145553121036
2
Iteration 9500: Loss = -11683.14723504826
3
Iteration 9600: Loss = -11683.166750181876
4
Iteration 9700: Loss = -11683.137055183917
5
Iteration 9800: Loss = -11683.139113051437
6
Iteration 9900: Loss = -11683.141158113005
7
Iteration 10000: Loss = -11683.146599019998
8
Iteration 10100: Loss = -11683.135296148874
Iteration 10200: Loss = -11683.144868966017
1
Iteration 10300: Loss = -11683.14476306496
2
Iteration 10400: Loss = -11683.13590715384
3
Iteration 10500: Loss = -11683.135768627433
4
Iteration 10600: Loss = -11683.140247692341
5
Iteration 10700: Loss = -11683.145398560482
6
Iteration 10800: Loss = -11683.14708148822
7
Iteration 10900: Loss = -11683.146953005227
8
Iteration 11000: Loss = -11683.192999853294
9
Iteration 11100: Loss = -11683.142315599296
10
Stopping early at iteration 11100 due to no improvement.
tensor([[ -9.6451,   6.5988],
        [  6.6445,  -8.0518],
        [ -8.8062,   7.2440],
        [  7.1761,  -8.5687],
        [ -7.2845,   4.6138],
        [  5.4940,  -8.4205],
        [  6.9071,  -8.3111],
        [ -8.8661,   6.3798],
        [ -8.6656,   6.7231],
        [  6.8090,  -8.3280],
        [ -8.8387,   7.4417],
        [  7.0599,  -8.6498],
        [  2.6516,  -4.9500],
        [ -8.7682,   6.9759],
        [  1.7424,  -3.3584],
        [-10.1139,   7.2794],
        [ -9.3634,   6.6627],
        [ -8.4348,   6.8646],
        [  5.4880, -10.1032],
        [ -8.7090,   6.6022],
        [ -9.4345,   7.4272],
        [ -8.2499,   6.2221],
        [ -8.8247,   7.2502],
        [-10.9239,   6.4986],
        [  7.0146,  -9.0483],
        [ -8.8253,   7.3402],
        [  3.6449,  -5.9804],
        [  4.5632,  -7.7138],
        [ -8.4841,   6.9411],
        [  7.1028,  -8.4906],
        [  6.6468,  -9.9654],
        [ -8.3588,   6.7196],
        [ -7.7465,   6.2847],
        [ -7.2035,   5.6241],
        [ -9.0196,   7.5907],
        [  6.8326,  -8.7608],
        [-10.1821,   6.5101],
        [  6.7926,  -8.6126],
        [ -8.2848,   6.8836],
        [ -8.2254,   6.8027],
        [  6.7397,  -8.4437],
        [ -6.6736,   4.8247],
        [ -8.5421,   6.9904],
        [  7.4138,  -8.9597],
        [  7.3691,  -8.8324],
        [  5.2322,  -7.9123],
        [ -3.0814,   1.3477],
        [  7.3135,  -8.8274],
        [  3.1946,  -5.3916],
        [  5.9622, -10.5774],
        [ -8.9857,   7.3008],
        [  6.3363,  -9.2826],
        [  4.7266,  -6.6184],
        [ -7.8697,   6.4823],
        [ -8.5329,   7.1215],
        [ -8.4038,   6.7335],
        [ -5.2684,   3.8579],
        [ -7.0701,   5.6837],
        [  6.7732,  -9.2610],
        [ -8.9016,   7.1684],
        [ -9.0943,   6.7837],
        [  6.9997,  -8.3997],
        [ -8.9027,   5.6147],
        [  5.5669,  -9.6215],
        [  4.8260,  -6.4388],
        [  4.3503,  -6.2717],
        [ -8.8179,   7.3838],
        [ -7.8114,   6.4080],
        [  1.5826,  -3.2458],
        [ -7.5533,   6.0013],
        [ -8.4882,   7.0996],
        [ -9.0637,   5.4396],
        [  6.7469,  -8.6806],
        [ -8.7842,   6.8481],
        [  4.7836,  -6.7148],
        [  6.0667,  -7.4531],
        [ -8.5663,   7.1794],
        [  7.0940,  -8.7324],
        [  7.1307,  -8.5179],
        [ -8.5909,   7.0683],
        [  6.2574,  -8.1664],
        [  6.8095,  -8.1998],
        [ -8.6227,   7.0185],
        [ -9.0769,   6.9351],
        [ -8.5104,   6.9371],
        [ -8.6832,   7.2490],
        [ -8.3912,   6.6922],
        [ -9.2890,   7.6562],
        [ -9.5143,   6.2163],
        [  4.0621,  -5.5508],
        [  7.3211,  -9.0804],
        [-10.5199,   6.0561],
        [ -8.4319,   6.7546],
        [ -8.8306,   7.0362],
        [ -6.5780,   5.1855],
        [ -8.7706,   6.4061],
        [  7.1034,  -8.5113],
        [  6.9727,  -8.9652],
        [  5.3785,  -9.3785],
        [ -9.9994,   6.9990]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7162, 0.2838],
        [0.2564, 0.7436]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4300, 0.5700], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1974, 0.0922],
         [0.2964, 0.4019]],

        [[0.2252, 0.1001],
         [0.3578, 0.2978]],

        [[0.7590, 0.0942],
         [0.3771, 0.7921]],

        [[0.1685, 0.0967],
         [0.0863, 0.2452]],

        [[0.0966, 0.1037],
         [0.1229, 0.2924]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
Iteration 0: Loss = -27842.28696267616
Iteration 10: Loss = -11689.622403927207
Iteration 20: Loss = -11684.56857208965
Iteration 30: Loss = -11684.56857208016
Iteration 40: Loss = -11684.56857208016
1
Iteration 50: Loss = -11684.56857208016
2
Iteration 60: Loss = -11684.56857208016
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7162, 0.2838],
        [0.2559, 0.7441]], dtype=torch.float64)
alpha: tensor([0.4582, 0.5418])
beta: tensor([[[0.1924, 0.0923],
         [0.2848, 0.3951]],

        [[0.1537, 0.1001],
         [0.8935, 0.3930]],

        [[0.6319, 0.0943],
         [0.6231, 0.8427]],

        [[0.5878, 0.0968],
         [0.3638, 0.8603]],

        [[0.7340, 0.1042],
         [0.6050, 0.0678]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27841.90936387037
Iteration 100: Loss = -12695.25077843758
Iteration 200: Loss = -12523.667883615688
Iteration 300: Loss = -12213.728722329668
Iteration 400: Loss = -12106.797315591228
Iteration 500: Loss = -12070.940365137794
Iteration 600: Loss = -12069.599124195101
Iteration 700: Loss = -12068.880928435885
Iteration 800: Loss = -12068.427546754416
Iteration 900: Loss = -12068.112439229933
Iteration 1000: Loss = -12067.80519402743
Iteration 1100: Loss = -12061.15116783212
Iteration 1200: Loss = -12061.003157422923
Iteration 1300: Loss = -12050.165755213322
Iteration 1400: Loss = -12037.763044760364
Iteration 1500: Loss = -12037.653466014022
Iteration 1600: Loss = -12037.558503370607
Iteration 1700: Loss = -12037.506852158964
Iteration 1800: Loss = -12037.463479464155
Iteration 1900: Loss = -12037.426496319706
Iteration 2000: Loss = -12037.39461267127
Iteration 2100: Loss = -12037.366453897259
Iteration 2200: Loss = -12017.751037534614
Iteration 2300: Loss = -12017.723396134741
Iteration 2400: Loss = -12017.70317068319
Iteration 2500: Loss = -12017.685500379856
Iteration 2600: Loss = -12017.669831391187
Iteration 2700: Loss = -12017.655856529635
Iteration 2800: Loss = -12017.643231344844
Iteration 2900: Loss = -12017.631856781796
Iteration 3000: Loss = -12017.621139851639
Iteration 3100: Loss = -12017.598929648972
Iteration 3200: Loss = -12013.743112270107
Iteration 3300: Loss = -12013.733307777577
Iteration 3400: Loss = -12013.725816697644
Iteration 3500: Loss = -12013.719098246642
Iteration 3600: Loss = -12013.713146214239
Iteration 3700: Loss = -12013.707673513518
Iteration 3800: Loss = -12013.702700546046
Iteration 3900: Loss = -12013.69809013752
Iteration 4000: Loss = -12013.69382089339
Iteration 4100: Loss = -12013.696630231272
1
Iteration 4200: Loss = -12013.686232517695
Iteration 4300: Loss = -12013.682887298548
Iteration 4400: Loss = -12013.679878169458
Iteration 4500: Loss = -12013.67686679465
Iteration 4600: Loss = -12013.674145978977
Iteration 4700: Loss = -12013.671606857599
Iteration 4800: Loss = -12013.67052369205
Iteration 4900: Loss = -12013.667012248367
Iteration 5000: Loss = -12013.664905789068
Iteration 5100: Loss = -12013.665700328875
1
Iteration 5200: Loss = -12013.661070317052
Iteration 5300: Loss = -12013.659153415121
Iteration 5400: Loss = -12013.656679250653
Iteration 5500: Loss = -12011.214330335244
Iteration 5600: Loss = -12006.987008779974
Iteration 5700: Loss = -12006.985888009405
Iteration 5800: Loss = -12006.990337571253
1
Iteration 5900: Loss = -12006.98304949001
Iteration 6000: Loss = -12006.979437180156
Iteration 6100: Loss = -12000.591909489014
Iteration 6200: Loss = -11997.457364817968
Iteration 6300: Loss = -11988.945753636346
Iteration 6400: Loss = -11985.631297188236
Iteration 6500: Loss = -11985.614919654703
Iteration 6600: Loss = -11981.924775857462
Iteration 6700: Loss = -11977.645319237852
Iteration 6800: Loss = -11974.435739248129
Iteration 6900: Loss = -11972.91352198587
Iteration 7000: Loss = -11968.571264672468
Iteration 7100: Loss = -11962.359420417217
Iteration 7200: Loss = -11958.565944329152
Iteration 7300: Loss = -11949.177688517158
Iteration 7400: Loss = -11942.559422200464
Iteration 7500: Loss = -11926.281290347537
Iteration 7600: Loss = -11923.58719106557
Iteration 7700: Loss = -11919.392033359652
Iteration 7800: Loss = -11905.009598032762
Iteration 7900: Loss = -11904.5437705737
Iteration 8000: Loss = -11898.56142021567
Iteration 8100: Loss = -11898.472846957526
Iteration 8200: Loss = -11883.231637232639
Iteration 8300: Loss = -11879.796671226197
Iteration 8400: Loss = -11879.83469867019
1
Iteration 8500: Loss = -11871.263102053472
Iteration 8600: Loss = -11871.252926095285
Iteration 8700: Loss = -11862.391650624902
Iteration 8800: Loss = -11862.3302666108
Iteration 8900: Loss = -11843.626168414405
Iteration 9000: Loss = -11843.073919018443
Iteration 9100: Loss = -11841.364947966878
Iteration 9200: Loss = -11837.046542203927
Iteration 9300: Loss = -11837.039699854711
Iteration 9400: Loss = -11837.04297154983
1
Iteration 9500: Loss = -11820.160140743532
Iteration 9600: Loss = -11804.398669820073
Iteration 9700: Loss = -11765.34306410021
Iteration 9800: Loss = -11765.332636857735
Iteration 9900: Loss = -11752.46276548702
Iteration 10000: Loss = -11752.523397088982
1
Iteration 10100: Loss = -11752.4964421086
2
Iteration 10200: Loss = -11727.13212325291
Iteration 10300: Loss = -11727.13801771309
1
Iteration 10400: Loss = -11727.149914002399
2
Iteration 10500: Loss = -11727.221817234986
3
Iteration 10600: Loss = -11705.236035981743
Iteration 10700: Loss = -11705.24178149188
1
Iteration 10800: Loss = -11705.238735566292
2
Iteration 10900: Loss = -11705.241287899515
3
Iteration 11000: Loss = -11705.255357383805
4
Iteration 11100: Loss = -11705.321549719769
5
Iteration 11200: Loss = -11705.263657802854
6
Iteration 11300: Loss = -11705.252533867764
7
Iteration 11400: Loss = -11692.867438872741
Iteration 11500: Loss = -11692.870456338904
1
Iteration 11600: Loss = -11683.266061070464
Iteration 11700: Loss = -11683.251358547974
Iteration 11800: Loss = -11683.254591306566
1
Iteration 11900: Loss = -11683.310844663149
2
Iteration 12000: Loss = -11683.250874125644
Iteration 12100: Loss = -11683.273193626299
1
Iteration 12200: Loss = -11683.251543286018
2
Iteration 12300: Loss = -11683.255822362278
3
Iteration 12400: Loss = -11683.258663117154
4
Iteration 12500: Loss = -11683.26503033267
5
Iteration 12600: Loss = -11683.251969407247
6
Iteration 12700: Loss = -11683.250596017464
Iteration 12800: Loss = -11683.253021238696
1
Iteration 12900: Loss = -11683.252726183826
2
Iteration 13000: Loss = -11683.250156942962
Iteration 13100: Loss = -11683.408479931168
1
Iteration 13200: Loss = -11683.250052925443
Iteration 13300: Loss = -11683.279630585184
1
Iteration 13400: Loss = -11683.28134960508
2
Iteration 13500: Loss = -11683.29121743919
3
Iteration 13600: Loss = -11683.251018517141
4
Iteration 13700: Loss = -11683.2522709749
5
Iteration 13800: Loss = -11683.332837109816
6
Iteration 13900: Loss = -11683.252856345862
7
Iteration 14000: Loss = -11683.250381641777
8
Iteration 14100: Loss = -11683.252284653583
9
Iteration 14200: Loss = -11683.276757115846
10
Stopping early at iteration 14200 due to no improvement.
tensor([[ -9.6767,   7.6709],
        [  6.9222,  -9.2907],
        [ -8.4890,   6.7993],
        [  7.1411,  -9.0171],
        [ -6.8736,   4.6698],
        [  8.0417,  -9.4695],
        [  6.3271,  -7.7149],
        [-12.0586,   7.6575],
        [ -9.2220,   7.0065],
        [  6.4473,  -8.8430],
        [ -9.4497,   7.9445],
        [  6.7825, -10.2395],
        [  2.4181,  -5.1983],
        [-10.1293,   8.7164],
        [  1.6811,  -3.4225],
        [ -9.5664,   7.7171],
        [ -9.6734,   8.1770],
        [ -8.9108,   7.2189],
        [  6.8117,  -8.3477],
        [ -8.3543,   6.6665],
        [ -9.6396,   8.2504],
        [ -8.5244,   6.8213],
        [ -9.7125,   8.2720],
        [ -9.8532,   7.4766],
        [  7.0427,  -8.4384],
        [-11.3938,   8.3410],
        [  6.2271, -10.8423],
        [  5.1889,  -7.1082],
        [ -9.1590,   7.6850],
        [  7.4147,  -8.8010],
        [  7.3263,  -8.8122],
        [ -9.3103,   7.6522],
        [ -8.1838,   6.7919],
        [ -8.7301,   4.1148],
        [ -9.4282,   7.9481],
        [  7.3847,  -8.8017],
        [-10.4993,   8.0079],
        [  8.5331, -10.0870],
        [ -9.8147,   8.3458],
        [-11.0639,   8.4453],
        [  6.6843,  -8.0914],
        [ -7.1067,   4.3762],
        [ -9.9355,   7.4163],
        [  6.7954,  -8.2951],
        [  6.7575,  -8.8113],
        [  5.3810,  -7.8015],
        [ -2.9277,   1.5046],
        [  7.4568,  -8.8492],
        [  3.6029,  -4.9946],
        [  7.3926,  -9.9173],
        [ -9.1510,   7.6872],
        [  7.1582,  -8.6510],
        [  4.0761,  -7.3024],
        [ -9.6262,   8.1540],
        [-10.6223,   7.7580],
        [ -9.7406,   7.4038],
        [ -5.3189,   3.8016],
        [ -7.2072,   5.5840],
        [  6.9810, -11.5962],
        [ -9.5407,   8.1344],
        [ -9.9931,   8.6063],
        [  6.1455, -10.7607],
        [ -7.7532,   6.3541],
        [  6.9700,  -9.1555],
        [  4.8734,  -6.4175],
        [  4.6190,  -6.0150],
        [-10.0826,   8.0258],
        [ -9.4611,   8.0505],
        [  1.5852,  -3.2533],
        [ -7.6026,   5.6903],
        [ -8.3899,   6.9526],
        [ -8.3267,   6.8477],
        [  7.0618,  -9.0556],
        [ -9.6724,   7.5971],
        [  4.7011,  -6.8050],
        [  5.8945,  -7.4877],
        [-11.1711,   6.5559],
        [  6.8526,  -8.3474],
        [  6.6945,  -8.2481],
        [ -9.1929,   7.8020],
        [  6.2412,  -8.5132],
        [  5.5617,  -6.9544],
        [-10.0040,   7.8466],
        [ -9.6677,   7.9268],
        [-10.7776,   8.6442],
        [ -9.1225,   7.5685],
        [ -8.3907,   6.8139],
        [ -9.6384,   8.2093],
        [ -9.1464,   7.7269],
        [  3.9178,  -5.6915],
        [  7.5162, -12.1314],
        [ -9.9046,   7.0995],
        [-11.3453,   6.7301],
        [ -9.8875,   8.0452],
        [ -6.5975,   5.2086],
        [ -9.3711,   7.2561],
        [  6.4057, -10.1823],
        [  6.4527,  -9.1750],
        [  6.6379,  -8.1284],
        [ -8.3318,   6.9200]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7176, 0.2824],
        [0.2553, 0.7447]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4311, 0.5689], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1969, 0.0926],
         [0.2848, 0.4038]],

        [[0.1537, 0.1002],
         [0.8935, 0.3930]],

        [[0.6319, 0.0943],
         [0.6231, 0.8427]],

        [[0.5878, 0.0970],
         [0.3638, 0.8603]],

        [[0.7340, 0.1039],
         [0.6050, 0.0678]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919995611635631
11688.924413486695
new:  [0.9919998213107827, 0.9919998213107827, 0.9919998213107827, 0.9919998213107827] [0.9919995611635631, 0.9919995611635631, 0.9919995611635631, 0.9919995611635631] [11683.10820809075, 11683.111146657398, 11683.142315599296, 11683.276757115846]
prior:  [0.9919998213107827, 0.9919998213107827, 0.9919998213107827, 0.9919998213107827] [0.9919995611635631, 0.9919995611635631, 0.9919995611635631, 0.9919995611635631] [11684.56856960396, 11684.56857208016, 11684.56857208016, 11684.56857208016]
-----------------------------------------------------------------------------------------
This iteration is 31
True Objective function: Loss = -11488.429501537514
Iteration 0: Loss = -23711.91544066238
Iteration 10: Loss = -11486.938413043968
Iteration 20: Loss = -11486.94139943072
1
Iteration 30: Loss = -11486.941398985584
2
Iteration 40: Loss = -11486.941398985584
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7595, 0.2405],
        [0.2449, 0.7551]], dtype=torch.float64)
alpha: tensor([0.5038, 0.4962])
beta: tensor([[[0.1957, 0.0948],
         [0.5851, 0.3954]],

        [[0.1034, 0.1014],
         [0.8675, 0.2747]],

        [[0.5637, 0.1036],
         [0.1049, 0.5290]],

        [[0.8843, 0.1007],
         [0.5374, 0.4054]],

        [[0.0187, 0.0905],
         [0.5059, 0.7489]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919993417272899
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23752.484529483278
Iteration 100: Loss = -12303.07188104264
Iteration 200: Loss = -12182.724058545236
Iteration 300: Loss = -12169.366610351972
Iteration 400: Loss = -12129.502401847985
Iteration 500: Loss = -12034.180172536824
Iteration 600: Loss = -11943.021215179953
Iteration 700: Loss = -11912.691069454599
Iteration 800: Loss = -11903.629699111452
Iteration 900: Loss = -11902.888321497612
Iteration 1000: Loss = -11902.55105578534
Iteration 1100: Loss = -11902.221404783224
Iteration 1200: Loss = -11900.947093951723
Iteration 1300: Loss = -11900.049228984699
Iteration 1400: Loss = -11895.094793107155
Iteration 1500: Loss = -11887.830098000999
Iteration 1600: Loss = -11887.635207474683
Iteration 1700: Loss = -11878.284544486429
Iteration 1800: Loss = -11869.43531832041
Iteration 1900: Loss = -11868.959855541274
Iteration 2000: Loss = -11868.92119588866
Iteration 2100: Loss = -11868.894164990066
Iteration 2200: Loss = -11868.87224792546
Iteration 2300: Loss = -11868.85243407817
Iteration 2400: Loss = -11868.83102061805
Iteration 2500: Loss = -11868.7912977986
Iteration 2600: Loss = -11865.431642836176
Iteration 2700: Loss = -11859.518254777428
Iteration 2800: Loss = -11859.337233073515
Iteration 2900: Loss = -11859.321601586016
Iteration 3000: Loss = -11859.308146706044
Iteration 3100: Loss = -11859.292154930656
Iteration 3200: Loss = -11859.256552605191
Iteration 3300: Loss = -11842.475568584514
Iteration 3400: Loss = -11842.267090789677
Iteration 3500: Loss = -11835.589401750452
Iteration 3600: Loss = -11832.378879557198
Iteration 3700: Loss = -11826.253732960886
Iteration 3800: Loss = -11818.163173644623
Iteration 3900: Loss = -11815.13969126226
Iteration 4000: Loss = -11815.126637026777
Iteration 4100: Loss = -11815.10803860035
Iteration 4200: Loss = -11811.117800945192
Iteration 4300: Loss = -11804.513298500297
Iteration 4400: Loss = -11784.237210902253
Iteration 4500: Loss = -11763.127373358544
Iteration 4600: Loss = -11750.30632940859
Iteration 4700: Loss = -11750.28797409819
Iteration 4800: Loss = -11750.152161433209
Iteration 4900: Loss = -11738.93165695084
Iteration 5000: Loss = -11738.908622758652
Iteration 5100: Loss = -11738.904986412625
Iteration 5200: Loss = -11738.899495632937
Iteration 5300: Loss = -11738.883990551687
Iteration 5400: Loss = -11723.300449358609
Iteration 5500: Loss = -11722.324691181198
Iteration 5600: Loss = -11722.32206702054
Iteration 5700: Loss = -11722.32030454983
Iteration 5800: Loss = -11722.318718440849
Iteration 5900: Loss = -11722.317387054778
Iteration 6000: Loss = -11722.327603768874
1
Iteration 6100: Loss = -11722.315242556066
Iteration 6200: Loss = -11722.31418878635
Iteration 6300: Loss = -11722.320569509566
1
Iteration 6400: Loss = -11722.318320052538
2
Iteration 6500: Loss = -11722.316857099873
3
Iteration 6600: Loss = -11722.342880350674
4
Iteration 6700: Loss = -11722.356575465541
5
Iteration 6800: Loss = -11722.310140305897
Iteration 6900: Loss = -11722.320775296643
1
Iteration 7000: Loss = -11722.310588912798
2
Iteration 7100: Loss = -11722.308629972626
Iteration 7200: Loss = -11722.308219643946
Iteration 7300: Loss = -11722.307772276108
Iteration 7400: Loss = -11722.308426943438
1
Iteration 7500: Loss = -11722.317719460098
2
Iteration 7600: Loss = -11722.309704862893
3
Iteration 7700: Loss = -11722.308155358125
4
Iteration 7800: Loss = -11722.304725094615
Iteration 7900: Loss = -11721.96679093636
Iteration 8000: Loss = -11719.301061448783
Iteration 8100: Loss = -11715.041987746907
Iteration 8200: Loss = -11715.029892439954
Iteration 8300: Loss = -11713.632404355207
Iteration 8400: Loss = -11713.468515538683
Iteration 8500: Loss = -11708.188792808507
Iteration 8600: Loss = -11703.4246363198
Iteration 8700: Loss = -11695.736728054982
Iteration 8800: Loss = -11685.6691992577
Iteration 8900: Loss = -11676.066959686796
Iteration 9000: Loss = -11674.622249929653
Iteration 9100: Loss = -11669.134126390996
Iteration 9200: Loss = -11639.472147572953
Iteration 9300: Loss = -11592.88574926895
Iteration 9400: Loss = -11553.643130130875
Iteration 9500: Loss = -11547.360580526944
Iteration 9600: Loss = -11540.559418032562
Iteration 9700: Loss = -11524.282630020256
Iteration 9800: Loss = -11524.28920053136
1
Iteration 9900: Loss = -11524.287270417879
2
Iteration 10000: Loss = -11524.280221033032
Iteration 10100: Loss = -11509.158263191313
Iteration 10200: Loss = -11490.172127574862
Iteration 10300: Loss = -11485.019656220535
Iteration 10400: Loss = -11485.019399805386
Iteration 10500: Loss = -11485.073865708528
1
Iteration 10600: Loss = -11485.016881603146
Iteration 10700: Loss = -11485.017187999165
1
Iteration 10800: Loss = -11485.04716675861
2
Iteration 10900: Loss = -11485.016272156161
Iteration 11000: Loss = -11485.016109892791
Iteration 11100: Loss = -11485.018984979626
1
Iteration 11200: Loss = -11485.145058991917
2
Iteration 11300: Loss = -11485.014566841455
Iteration 11400: Loss = -11485.037390080448
1
Iteration 11500: Loss = -11485.014607231278
2
Iteration 11600: Loss = -11485.048176284092
3
Iteration 11700: Loss = -11485.019231913806
4
Iteration 11800: Loss = -11485.063219218935
5
Iteration 11900: Loss = -11485.132469225866
6
Iteration 12000: Loss = -11485.025185070454
7
Iteration 12100: Loss = -11485.01597266061
8
Iteration 12200: Loss = -11485.014191635626
Iteration 12300: Loss = -11485.01517646973
1
Iteration 12400: Loss = -11485.01205266446
Iteration 12500: Loss = -11485.015007566766
1
Iteration 12600: Loss = -11485.022256367714
2
Iteration 12700: Loss = -11485.058810459006
3
Iteration 12800: Loss = -11485.036505307144
4
Iteration 12900: Loss = -11485.010194190356
Iteration 13000: Loss = -11485.01001919428
Iteration 13100: Loss = -11485.010279643253
1
Iteration 13200: Loss = -11485.016867726816
2
Iteration 13300: Loss = -11485.020318977846
3
Iteration 13400: Loss = -11485.01586267717
4
Iteration 13500: Loss = -11485.031235710894
5
Iteration 13600: Loss = -11485.027155401416
6
Iteration 13700: Loss = -11485.029658275627
7
Iteration 13800: Loss = -11485.012262268458
8
Iteration 13900: Loss = -11485.017365497373
9
Iteration 14000: Loss = -11485.009859960945
Iteration 14100: Loss = -11485.00998029827
1
Iteration 14200: Loss = -11485.021507818657
2
Iteration 14300: Loss = -11485.035878542514
3
Iteration 14400: Loss = -11485.009650248125
Iteration 14500: Loss = -11485.01028562697
1
Iteration 14600: Loss = -11485.060713615972
2
Iteration 14700: Loss = -11485.02907175536
3
Iteration 14800: Loss = -11485.017422369247
4
Iteration 14900: Loss = -11485.012563168382
5
Iteration 15000: Loss = -11485.010211192508
6
Iteration 15100: Loss = -11485.020505589853
7
Iteration 15200: Loss = -11485.019382684348
8
Iteration 15300: Loss = -11485.011261667943
9
Iteration 15400: Loss = -11485.022376850055
10
Stopping early at iteration 15400 due to no improvement.
tensor([[ -7.4594,   2.8442],
        [-11.0285,   6.4133],
        [  6.1785, -10.7938],
        [  6.8620, -11.4772],
        [  5.7998, -10.4150],
        [  6.2735, -10.8887],
        [  5.4157, -10.0309],
        [  6.3616, -10.9769],
        [  6.8461, -11.4613],
        [ -6.0919,   1.4766],
        [-10.9808,   6.3656],
        [ -9.3265,   4.7113],
        [  4.9366,  -9.5519],
        [ -8.3961,   3.7809],
        [-11.5218,   6.9066],
        [  5.8759, -10.4912],
        [-10.8543,   6.2391],
        [-11.1749,   6.5597],
        [ -5.0835,   0.4683],
        [-10.7310,   6.1158],
        [  7.7350, -12.3502],
        [-10.3175,   5.7023],
        [  5.6835, -10.2988],
        [  5.6048, -10.2200],
        [ -8.4671,   3.8519],
        [  5.3301,  -9.9453],
        [-11.0247,   6.4095],
        [  4.8071,  -9.4223],
        [  5.2298,  -9.8450],
        [ -9.4745,   4.8593],
        [ -9.8283,   5.2130],
        [  5.4560, -10.0712],
        [-11.6248,   7.0096],
        [  4.8569,  -9.4721],
        [ -8.1054,   3.4902],
        [ -8.9655,   4.3503],
        [ -9.3940,   4.7788],
        [  3.1082,  -7.7235],
        [ -7.9639,   3.3486],
        [ -5.7304,   1.1152],
        [ -8.4621,   3.8469],
        [-11.3023,   6.6871],
        [-11.4535,   6.8383],
        [  3.0540,  -7.6693],
        [  6.5325, -11.1477],
        [  5.7997, -10.4149],
        [-11.8424,   7.2272],
        [  4.0870,  -8.7023],
        [  5.8902, -10.5054],
        [  5.8040, -10.4192],
        [  5.9837, -10.5989],
        [ -8.7573,   4.1421],
        [  6.4613, -11.0765],
        [ -9.0465,   4.4313],
        [  5.1435,  -9.7587],
        [ -9.8297,   5.2145],
        [-10.1221,   5.5068],
        [ -7.4605,   2.8452],
        [ -8.8990,   4.2838],
        [  5.2577,  -9.8730],
        [-11.1619,   6.5467],
        [-10.7519,   6.1367],
        [  4.3702,  -8.9854],
        [  4.7794,  -9.3947],
        [  5.2245,  -9.8398],
        [-10.8683,   6.2531],
        [ -9.5455,   4.9302],
        [-10.6447,   6.0295],
        [  7.0068, -11.6220],
        [ -6.6653,   2.0501],
        [  6.8613, -11.4765],
        [-11.2033,   6.5881],
        [  1.6684,  -6.2836],
        [  4.9167,  -9.5319],
        [  6.7582, -11.3734],
        [  5.3772,  -9.9924],
        [  5.4682, -10.0835],
        [-11.1677,   6.5525],
        [  5.8804, -10.4956],
        [ -6.8145,   2.1992],
        [  1.7409,  -6.3561],
        [  6.7938, -11.4090],
        [  5.5425, -10.1577],
        [  5.0507,  -9.6660],
        [ -6.5950,   1.9798],
        [  6.1959, -10.8111],
        [  5.8298, -10.4450],
        [  6.7047, -11.3199],
        [  4.3876,  -9.0028],
        [  3.8306,  -8.4458],
        [ -7.6022,   2.9870],
        [-11.5870,   6.9718],
        [-10.9862,   6.3710],
        [-10.4690,   5.8538],
        [  3.9032,  -8.5184],
        [-11.2211,   6.6059],
        [ -9.4036,   4.7884],
        [  5.2915,  -9.9067],
        [ -8.3961,   3.7809],
        [  5.7755, -10.3907]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7569, 0.2431],
        [0.2377, 0.7623]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5131, 0.4869], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4048, 0.0948],
         [0.5851, 0.1987]],

        [[0.1034, 0.1022],
         [0.8675, 0.2747]],

        [[0.5637, 0.1047],
         [0.1049, 0.5290]],

        [[0.8843, 0.1006],
         [0.5374, 0.4054]],

        [[0.0187, 0.0903],
         [0.5059, 0.7489]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -26385.198540380086
Iteration 10: Loss = -12237.713085423744
Iteration 20: Loss = -11486.941238399817
Iteration 30: Loss = -11486.941386891316
1
Iteration 40: Loss = -11486.941386891318
2
Iteration 50: Loss = -11486.941386891318
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7595, 0.2405],
        [0.2449, 0.7551]], dtype=torch.float64)
alpha: tensor([0.5038, 0.4962])
beta: tensor([[[0.1957, 0.0948],
         [0.6194, 0.3954]],

        [[0.9475, 0.1014],
         [0.4393, 0.0149]],

        [[0.5409, 0.1036],
         [0.7729, 0.5111]],

        [[0.7005, 0.1007],
         [0.9677, 0.8537]],

        [[0.7170, 0.0905],
         [0.3521, 0.7463]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26385.23343979322
Iteration 100: Loss = -12365.865819864981
Iteration 200: Loss = -12323.609551481206
Iteration 300: Loss = -12305.398155470753
Iteration 400: Loss = -12162.646333657229
Iteration 500: Loss = -11880.279320024698
Iteration 600: Loss = -11761.335814913928
Iteration 700: Loss = -11713.80562350346
Iteration 800: Loss = -11703.140818628073
Iteration 900: Loss = -11702.32924471406
Iteration 1000: Loss = -11701.97093341387
Iteration 1100: Loss = -11701.719448527003
Iteration 1200: Loss = -11701.511116675134
Iteration 1300: Loss = -11701.334974253992
Iteration 1400: Loss = -11701.180386741122
Iteration 1500: Loss = -11700.907592549134
Iteration 1600: Loss = -11700.74895499533
Iteration 1700: Loss = -11700.691657235519
Iteration 1800: Loss = -11700.643107744934
Iteration 1900: Loss = -11700.600966448003
Iteration 2000: Loss = -11700.561226366943
Iteration 2100: Loss = -11700.515111381297
Iteration 2200: Loss = -11700.476183917532
Iteration 2300: Loss = -11700.452590853642
Iteration 2400: Loss = -11700.431797509724
Iteration 2500: Loss = -11700.410916272464
Iteration 2600: Loss = -11692.281364008806
Iteration 2700: Loss = -11692.258380445479
Iteration 2800: Loss = -11692.24438804568
Iteration 2900: Loss = -11692.231961266505
Iteration 3000: Loss = -11692.220475623117
Iteration 3100: Loss = -11692.209679642427
Iteration 3200: Loss = -11692.19996538006
Iteration 3300: Loss = -11692.19188126489
Iteration 3400: Loss = -11692.185091073132
Iteration 3500: Loss = -11692.179168905883
Iteration 3600: Loss = -11692.173731707164
Iteration 3700: Loss = -11692.168744197379
Iteration 3800: Loss = -11692.164156104569
Iteration 3900: Loss = -11692.159941311309
Iteration 4000: Loss = -11692.156024793812
Iteration 4100: Loss = -11692.152458811073
Iteration 4200: Loss = -11692.149058953946
Iteration 4300: Loss = -11692.145974491928
Iteration 4400: Loss = -11692.14308729417
Iteration 4500: Loss = -11692.140419544063
Iteration 4600: Loss = -11692.137863883367
Iteration 4700: Loss = -11692.135569779917
Iteration 4800: Loss = -11692.133418586354
Iteration 4900: Loss = -11692.131368430248
Iteration 5000: Loss = -11692.129475208945
Iteration 5100: Loss = -11692.127715359233
Iteration 5200: Loss = -11692.126073987773
Iteration 5300: Loss = -11692.124483490377
Iteration 5400: Loss = -11692.12303378404
Iteration 5500: Loss = -11692.12168557441
Iteration 5600: Loss = -11692.120380721852
Iteration 5700: Loss = -11692.119237383431
Iteration 5800: Loss = -11692.11807770031
Iteration 5900: Loss = -11692.117109099308
Iteration 6000: Loss = -11692.115988413447
Iteration 6100: Loss = -11692.115032382491
Iteration 6200: Loss = -11692.12050338983
1
Iteration 6300: Loss = -11692.118031524627
2
Iteration 6400: Loss = -11692.112470713839
Iteration 6500: Loss = -11692.116357420779
1
Iteration 6600: Loss = -11692.111040905778
Iteration 6700: Loss = -11692.111118808203
1
Iteration 6800: Loss = -11692.109684081983
Iteration 6900: Loss = -11692.116440506212
1
Iteration 7000: Loss = -11692.10855229388
Iteration 7100: Loss = -11692.108098648017
Iteration 7200: Loss = -11692.121309887762
1
Iteration 7300: Loss = -11692.110039213016
2
Iteration 7400: Loss = -11692.110171188411
3
Iteration 7500: Loss = -11692.11103725862
4
Iteration 7600: Loss = -11692.105704638056
Iteration 7700: Loss = -11692.105469817463
Iteration 7800: Loss = -11692.132086315065
1
Iteration 7900: Loss = -11692.105050915176
Iteration 8000: Loss = -11692.105545684413
1
Iteration 8100: Loss = -11692.138005949544
2
Iteration 8200: Loss = -11692.123111107085
3
Iteration 8300: Loss = -11692.104003375869
Iteration 8400: Loss = -11692.101619444398
Iteration 8500: Loss = -11692.106028347003
1
Iteration 8600: Loss = -11692.101134255347
Iteration 8700: Loss = -11692.102999732557
1
Iteration 8800: Loss = -11692.109512207147
2
Iteration 8900: Loss = -11692.111060427771
3
Iteration 9000: Loss = -11692.099768984088
Iteration 9100: Loss = -11692.106483769538
1
Iteration 9200: Loss = -11692.101954926744
2
Iteration 9300: Loss = -11692.160130301403
3
Iteration 9400: Loss = -11692.100125569425
4
Iteration 9500: Loss = -11692.10076814812
5
Iteration 9600: Loss = -11692.098004963413
Iteration 9700: Loss = -11692.100027993354
1
Iteration 9800: Loss = -11692.178990738694
2
Iteration 9900: Loss = -11692.097374681458
Iteration 10000: Loss = -11692.099231284352
1
Iteration 10100: Loss = -11692.098935570462
2
Iteration 10200: Loss = -11692.09769717963
3
Iteration 10300: Loss = -11692.111131836358
4
Iteration 10400: Loss = -11692.100623332704
5
Iteration 10500: Loss = -11692.105673062018
6
Iteration 10600: Loss = -11692.099807671955
7
Iteration 10700: Loss = -11692.10822742512
8
Iteration 10800: Loss = -11692.098449864929
9
Iteration 10900: Loss = -11692.100202341622
10
Stopping early at iteration 10900 due to no improvement.
tensor([[  2.8365,  -4.2462],
        [  1.6436,  -4.3089],
        [  6.0435,  -7.4949],
        [  5.8633,  -7.3325],
        [  6.5789,  -7.9676],
        [  6.5362, -10.6247],
        [  4.9688,  -6.6342],
        [  5.8979,  -7.2860],
        [  5.7626,  -7.4308],
        [  4.2961,  -6.5049],
        [ -5.6739,   3.6623],
        [  2.6101,  -5.3700],
        [  5.0082,  -7.3653],
        [  4.1203,  -5.6139],
        [  1.5652,  -3.2281],
        [  6.2709,  -8.3067],
        [  1.8438,  -3.5703],
        [ -0.2024,  -2.0211],
        [  0.9866,  -2.4684],
        [  2.3241,  -4.7663],
        [  5.9192,  -7.4517],
        [  0.0807,  -1.9909],
        [  6.0170,  -7.4086],
        [  5.4385,  -8.1098],
        [  1.8736,  -4.5910],
        [  5.9827,  -8.5383],
        [  1.2531,  -3.3333],
        [  6.0082,  -7.5924],
        [  6.3836,  -7.7807],
        [  0.6206,  -2.3344],
        [  2.4111,  -4.1454],
        [  5.7070,  -7.3911],
        [  1.4291,  -3.0643],
        [  4.4862,  -6.1870],
        [  0.6509,  -2.2654],
        [  1.4031,  -3.4785],
        [  2.0201,  -3.8176],
        [  3.9386,  -6.8516],
        [  2.5853,  -4.1231],
        [  0.8341,  -3.2959],
        [  2.0338,  -4.3970],
        [ -0.4508,  -1.1764],
        [  2.6630,  -4.1119],
        [  3.0736,  -4.5345],
        [  5.1310,  -7.5804],
        [  5.4054,  -7.8590],
        [  1.3388,  -2.7593],
        [  4.7817,  -6.4228],
        [  5.8791,  -7.4194],
        [  6.2628,  -7.6492],
        [  5.8564,  -7.4069],
        [ -2.2021,   0.5437],
        [  5.4661,  -6.8752],
        [  1.8791,  -4.2932],
        [  4.7562,  -9.3714],
        [  0.6102,  -2.0792],
        [  5.6686,  -7.0686],
        [  3.9031,  -5.4935],
        [  2.7891,  -4.2480],
        [  5.8338,  -9.5663],
        [  2.9398,  -4.4176],
        [  1.3903,  -2.8451],
        [  6.2652,  -7.6791],
        [  4.9806,  -6.6424],
        [  5.4612,  -8.2680],
        [  3.8454,  -5.5830],
        [ -1.1744,  -0.3521],
        [  0.7001,  -2.2195],
        [  5.6939,  -7.0909],
        [  0.9163,  -3.1186],
        [  5.5537,  -7.1259],
        [  0.6852,  -2.0847],
        [  2.4271,  -5.1697],
        [  5.7156,  -7.1279],
        [  5.5929,  -8.0834],
        [  4.7917,  -7.8579],
        [  5.5109,  -6.9328],
        [ -2.0407,   0.6429],
        [  5.7060,  -7.4750],
        [  2.5375,  -3.9254],
        [  2.8682,  -4.8855],
        [  5.9869,  -7.5102],
        [  6.1400,  -7.5392],
        [  5.6462,  -7.0347],
        [  3.5767,  -5.3901],
        [  5.7861,  -7.1746],
        [  4.6364,  -9.2517],
        [  6.1336,  -7.5933],
        [  6.4553,  -7.8852],
        [  3.9395,  -8.5205],
        [  1.5494,  -2.9541],
        [  1.9309,  -3.3199],
        [  1.0202,  -3.2947],
        [  7.1653,  -8.8730],
        [  5.2882,  -6.7251],
        [  1.5407,  -5.2143],
        [ -0.9036,  -0.6681],
        [  5.9001,  -7.9724],
        [  3.1161,  -5.2944],
        [  5.1902,  -7.7550]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6440, 0.3560],
        [0.2920, 0.7080]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9483, 0.0517], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2043, 0.0851],
         [0.6194, 0.4067]],

        [[0.9475, 0.1021],
         [0.4393, 0.0149]],

        [[0.5409, 0.1040],
         [0.7729, 0.5111]],

        [[0.7005, 0.1063],
         [0.9677, 0.8537]],

        [[0.7170, 0.0905],
         [0.3521, 0.7463]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.012184899471542247
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5890323425815494
Average Adjusted Rand Index: 0.7865985960559245
Iteration 0: Loss = -16850.846214376215
Iteration 10: Loss = -11486.94146575306
Iteration 20: Loss = -11486.941398986224
Iteration 30: Loss = -11486.941398985584
Iteration 40: Loss = -11486.941398985584
1
Iteration 50: Loss = -11486.941398985584
2
Iteration 60: Loss = -11486.941398985584
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7595, 0.2405],
        [0.2449, 0.7551]], dtype=torch.float64)
alpha: tensor([0.5038, 0.4962])
beta: tensor([[[0.1957, 0.0948],
         [0.6232, 0.3954]],

        [[0.3498, 0.1014],
         [0.3424, 0.2783]],

        [[0.5797, 0.1036],
         [0.9835, 0.3037]],

        [[0.7084, 0.1007],
         [0.8160, 0.4553]],

        [[0.8010, 0.0905],
         [0.3769, 0.3015]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16850.634943210815
Iteration 100: Loss = -12072.744355661871
Iteration 200: Loss = -11984.659599344866
Iteration 300: Loss = -11823.42167097706
Iteration 400: Loss = -11506.565010463917
Iteration 500: Loss = -11500.01979750669
Iteration 600: Loss = -11498.57697433091
Iteration 700: Loss = -11498.34248655083
Iteration 800: Loss = -11493.008929183918
Iteration 900: Loss = -11492.106025430483
Iteration 1000: Loss = -11492.038076725983
Iteration 1100: Loss = -11491.98897757608
Iteration 1200: Loss = -11491.951671579272
Iteration 1300: Loss = -11491.922495674542
Iteration 1400: Loss = -11491.899040082166
Iteration 1500: Loss = -11491.87995053414
Iteration 1600: Loss = -11491.864073703802
Iteration 1700: Loss = -11491.850813093193
Iteration 1800: Loss = -11491.839556315936
Iteration 1900: Loss = -11491.829892122716
Iteration 2000: Loss = -11491.821534076364
Iteration 2100: Loss = -11491.81428089831
Iteration 2200: Loss = -11491.807902769231
Iteration 2300: Loss = -11491.802266867942
Iteration 2400: Loss = -11491.797320112548
Iteration 2500: Loss = -11491.792897495538
Iteration 2600: Loss = -11491.78896431069
Iteration 2700: Loss = -11491.785433823463
Iteration 2800: Loss = -11491.782206032474
Iteration 2900: Loss = -11491.779335733141
Iteration 3000: Loss = -11491.776703831454
Iteration 3100: Loss = -11491.77429212157
Iteration 3200: Loss = -11491.772155491097
Iteration 3300: Loss = -11491.770129172482
Iteration 3400: Loss = -11491.768343347614
Iteration 3500: Loss = -11491.916279424158
1
Iteration 3600: Loss = -11491.769469415893
2
Iteration 3700: Loss = -11491.76366482366
Iteration 3800: Loss = -11491.81094000352
1
Iteration 3900: Loss = -11491.764255474522
2
Iteration 4000: Loss = -11491.761157827612
Iteration 4100: Loss = -11491.758872755538
Iteration 4200: Loss = -11491.766557850435
1
Iteration 4300: Loss = -11491.757254451708
Iteration 4400: Loss = -11491.756026315523
Iteration 4500: Loss = -11491.768968261402
1
Iteration 4600: Loss = -11491.757928490906
2
Iteration 4700: Loss = -11491.755686717876
Iteration 4800: Loss = -11491.756748659074
1
Iteration 4900: Loss = -11485.015124494066
Iteration 5000: Loss = -11485.011403245351
Iteration 5100: Loss = -11485.012621280503
1
Iteration 5200: Loss = -11485.010509817215
Iteration 5300: Loss = -11485.019312208875
1
Iteration 5400: Loss = -11485.011631302785
2
Iteration 5500: Loss = -11485.01291436299
3
Iteration 5600: Loss = -11485.014540217757
4
Iteration 5700: Loss = -11485.013398717003
5
Iteration 5800: Loss = -11485.008691076846
Iteration 5900: Loss = -11485.00855386971
Iteration 6000: Loss = -11485.007918188468
Iteration 6100: Loss = -11485.009843968659
1
Iteration 6200: Loss = -11485.042333380454
2
Iteration 6300: Loss = -11485.020235899494
3
Iteration 6400: Loss = -11485.026968985554
4
Iteration 6500: Loss = -11485.037072018564
5
Iteration 6600: Loss = -11485.016021809522
6
Iteration 6700: Loss = -11485.073583130295
7
Iteration 6800: Loss = -11485.028100223173
8
Iteration 6900: Loss = -11485.010647113251
9
Iteration 7000: Loss = -11485.008175618528
10
Stopping early at iteration 7000 due to no improvement.
tensor([[ 4.6255, -6.7854],
        [ 5.7760, -8.5343],
        [-8.8681,  5.8197],
        [-8.1040,  6.6470],
        [-8.1111,  6.3721],
        [-7.6811,  6.2665],
        [-8.1764,  5.3019],
        [-9.0583,  5.8138],
        [-8.9860,  6.4434],
        [ 4.7766, -7.4861],
        [ 5.2835, -7.6911],
        [ 5.3063, -7.8546],
        [-7.2421,  5.6955],
        [ 6.1394, -7.6440],
        [ 6.2608, -8.8356],
        [-9.0753,  6.7720],
        [ 6.3861, -7.7818],
        [ 5.4740, -7.2663],
        [ 2.0288, -3.5223],
        [ 6.2726, -7.9086],
        [-7.8493,  6.0019],
        [ 6.4386, -7.9704],
        [-7.4387,  6.0513],
        [-7.8204,  6.4339],
        [ 6.8452, -8.3696],
        [-8.1458,  6.2639],
        [ 6.7506, -8.1986],
        [-7.4810,  5.1292],
        [-7.8602,  6.3802],
        [ 6.5793, -9.0686],
        [ 5.8516, -7.3509],
        [-8.4392,  7.0453],
        [ 6.5900, -8.3580],
        [-8.0892,  5.5351],
        [ 5.7240, -7.7410],
        [ 6.1701, -8.0817],
        [ 5.5387, -7.1440],
        [-6.0507,  4.4631],
        [ 5.6773, -7.5204],
        [ 6.5027, -7.9103],
        [ 6.3832, -7.9250],
        [ 6.1247, -7.5162],
        [ 6.3336, -9.4147],
        [-5.8854,  4.4231],
        [-8.8890,  5.8959],
        [-8.0360,  6.6112],
        [ 6.9557, -8.8246],
        [-6.9150,  5.5235],
        [-9.2959,  6.8793],
        [-8.9598,  6.2890],
        [-7.0789,  5.5752],
        [ 5.0866, -7.2542],
        [-9.8217,  6.5592],
        [ 5.4176, -6.8120],
        [-7.4854,  5.9849],
        [ 5.5763, -6.9670],
        [ 6.6269, -8.6384],
        [ 5.4771, -8.7108],
        [ 5.9829, -7.3953],
        [-7.7631,  5.7569],
        [ 5.1702, -7.2165],
        [ 5.9506, -7.5638],
        [-6.6035,  5.0335],
        [-7.1066,  5.7075],
        [-8.3543,  4.8994],
        [ 6.3633, -7.9665],
        [ 6.3904, -8.0179],
        [ 6.6679, -8.0939],
        [-7.8541,  6.4024],
        [ 6.1737, -7.9889],
        [-7.4331,  6.0422],
        [ 6.8379, -8.2284],
        [-4.7012,  3.2715],
        [-7.6388,  5.9564],
        [-7.2079,  5.8119],
        [-7.3812,  5.8590],
        [-7.3195,  4.6394],
        [ 6.3521, -8.4144],
        [-8.2080,  6.8204],
        [ 3.8306, -5.2340],
        [-5.0155,  3.1073],
        [-8.8825,  7.4030],
        [-7.5756,  6.1870],
        [-7.3387,  5.8176],
        [ 6.7707, -8.2162],
        [-7.8454,  4.6150],
        [-6.8267,  5.2718],
        [-7.0160,  5.5201],
        [-7.3973,  5.8788],
        [-7.0246,  5.5772],
        [ 5.6838, -7.9760],
        [ 7.0086, -8.4751],
        [ 5.6109, -8.4882],
        [ 5.7212, -7.4023],
        [-7.0812,  5.5285],
        [ 6.4372, -8.3310],
        [ 6.2217, -7.6085],
        [-7.9282,  6.1073],
        [ 5.8481, -7.7156],
        [-7.8297,  6.4415]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7622, 0.2378],
        [0.2447, 0.7553]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4900, 0.5100], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1991, 0.0948],
         [0.6232, 0.4048]],

        [[0.3498, 0.1019],
         [0.3424, 0.2783]],

        [[0.5797, 0.1048],
         [0.9835, 0.3037]],

        [[0.7084, 0.1005],
         [0.8160, 0.4553]],

        [[0.8010, 0.0904],
         [0.3769, 0.3015]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -33033.92203963822
Iteration 10: Loss = -12216.14926021397
Iteration 20: Loss = -11668.357272040737
Iteration 30: Loss = -11671.81505834938
1
Iteration 40: Loss = -11684.255782456828
2
Iteration 50: Loss = -11691.012530191962
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.5675, 0.4325],
        [0.3391, 0.6609]], dtype=torch.float64)
alpha: tensor([0.4530, 0.5470])
beta: tensor([[[0.3873, 0.0948],
         [0.0291, 0.2065]],

        [[0.6640, 0.1014],
         [0.2319, 0.7543]],

        [[0.9847, 0.1035],
         [0.8502, 0.1991]],

        [[0.2859, 0.1250],
         [0.7202, 0.6317]],

        [[0.6513, 0.0903],
         [0.9033, 0.5910]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 67
Adjusted Rand Index: 0.10666666666666667
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5349014282809299
Average Adjusted Rand Index: 0.8213333333333332
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33034.245680907894
Iteration 100: Loss = -12386.240134688764
Iteration 200: Loss = -12327.38621271782
Iteration 300: Loss = -12254.967350498819
Iteration 400: Loss = -12181.486309624392
Iteration 500: Loss = -12177.95756892334
Iteration 600: Loss = -12174.41554940234
Iteration 700: Loss = -12173.815296741246
Iteration 800: Loss = -12173.446814644065
Iteration 900: Loss = -12171.911245445874
Iteration 1000: Loss = -12171.594168295496
Iteration 1100: Loss = -12171.470057333812
Iteration 1200: Loss = -12171.380048966901
Iteration 1300: Loss = -12171.310572340943
Iteration 1400: Loss = -12171.249913027927
Iteration 1500: Loss = -12170.906952245527
Iteration 1600: Loss = -12166.552987568546
Iteration 1700: Loss = -12166.470702868643
Iteration 1800: Loss = -12164.580170002644
Iteration 1900: Loss = -12164.545195630863
Iteration 2000: Loss = -12164.44578849662
Iteration 2100: Loss = -12164.434313563295
Iteration 2200: Loss = -12164.401020957306
Iteration 2300: Loss = -12163.866821791482
Iteration 2400: Loss = -12162.60655572101
Iteration 2500: Loss = -12162.11237147635
Iteration 2600: Loss = -12161.70951645302
Iteration 2700: Loss = -12161.460409659903
Iteration 2800: Loss = -12161.438991649879
Iteration 2900: Loss = -12160.985350419067
Iteration 3000: Loss = -12160.895270012883
Iteration 3100: Loss = -12160.20053944516
Iteration 3200: Loss = -12159.57378202918
Iteration 3300: Loss = -12158.330714595386
Iteration 3400: Loss = -12154.58146895492
Iteration 3500: Loss = -12147.38696341518
Iteration 3600: Loss = -12140.572709499585
Iteration 3700: Loss = -12113.486755871272
Iteration 3800: Loss = -12081.741416518376
Iteration 3900: Loss = -12066.981048328282
Iteration 4000: Loss = -12044.18958261811
Iteration 4100: Loss = -12032.01902127329
Iteration 4200: Loss = -12005.519716709343
Iteration 4300: Loss = -11989.655825008089
Iteration 4400: Loss = -11989.623993749126
Iteration 4500: Loss = -11989.59672442458
Iteration 4600: Loss = -11989.553939500047
Iteration 4700: Loss = -11982.747963231077
Iteration 4800: Loss = -11982.680547891165
Iteration 4900: Loss = -11982.634805734608
Iteration 5000: Loss = -11982.612071390377
Iteration 5100: Loss = -11982.605515264622
Iteration 5200: Loss = -11982.598445944244
Iteration 5300: Loss = -11982.582457538569
Iteration 5400: Loss = -11982.579061233742
Iteration 5500: Loss = -11982.576676133343
Iteration 5600: Loss = -11982.575108582934
Iteration 5700: Loss = -11982.573894957583
Iteration 5800: Loss = -11982.572800554643
Iteration 5900: Loss = -11982.571804331974
Iteration 6000: Loss = -11982.5719063246
1
Iteration 6100: Loss = -11982.569878607195
Iteration 6200: Loss = -11982.568648701672
Iteration 6300: Loss = -11982.560929405196
Iteration 6400: Loss = -11982.527735276211
Iteration 6500: Loss = -11982.523106114397
Iteration 6600: Loss = -11982.544468423846
1
Iteration 6700: Loss = -11982.52194706792
Iteration 6800: Loss = -11982.521394594143
Iteration 6900: Loss = -11982.520962447406
Iteration 7000: Loss = -11982.52050159257
Iteration 7100: Loss = -11982.52495990075
1
Iteration 7200: Loss = -11982.51976255131
Iteration 7300: Loss = -11982.51926948028
Iteration 7400: Loss = -11982.517009904159
Iteration 7500: Loss = -11982.530070545425
1
Iteration 7600: Loss = -11982.514597886726
Iteration 7700: Loss = -11982.51425335412
Iteration 7800: Loss = -11982.51385827626
Iteration 7900: Loss = -11982.51340058819
Iteration 8000: Loss = -11982.511655021088
Iteration 8100: Loss = -11982.510447316563
Iteration 8200: Loss = -11982.510576420582
1
Iteration 8300: Loss = -11982.510195592467
Iteration 8400: Loss = -11982.509599858682
Iteration 8500: Loss = -11982.5090316473
Iteration 8600: Loss = -11982.51179414231
1
Iteration 8700: Loss = -11982.509031491778
Iteration 8800: Loss = -11982.508934424472
Iteration 8900: Loss = -11982.50762954558
Iteration 9000: Loss = -11982.50691322775
Iteration 9100: Loss = -11982.507492634397
1
Iteration 9200: Loss = -11982.510588771669
2
Iteration 9300: Loss = -11982.504546133716
Iteration 9400: Loss = -11982.504247584322
Iteration 9500: Loss = -11982.504021997362
Iteration 9600: Loss = -11982.50361669308
Iteration 9700: Loss = -11982.502101227223
Iteration 9800: Loss = -11982.749703647392
1
Iteration 9900: Loss = -11982.501456222746
Iteration 10000: Loss = -11982.501365598462
Iteration 10100: Loss = -11982.524739307488
1
Iteration 10200: Loss = -11982.501246699134
Iteration 10300: Loss = -11982.501141909766
Iteration 10400: Loss = -11982.501056990825
Iteration 10500: Loss = -11982.501079320306
1
Iteration 10600: Loss = -11982.500798751891
Iteration 10700: Loss = -11982.5020168338
1
Iteration 10800: Loss = -11982.51234373661
2
Iteration 10900: Loss = -11982.722170681536
3
Iteration 11000: Loss = -11982.49957352426
Iteration 11100: Loss = -11982.50007878172
1
Iteration 11200: Loss = -11982.499505170073
Iteration 11300: Loss = -11982.501946205684
1
Iteration 11400: Loss = -11982.500033807435
2
Iteration 11500: Loss = -11982.499769507118
3
Iteration 11600: Loss = -11982.607893202985
4
Iteration 11700: Loss = -11982.499359803202
Iteration 11800: Loss = -11982.501280951768
1
Iteration 11900: Loss = -11982.5269980477
2
Iteration 12000: Loss = -11982.49925392335
Iteration 12100: Loss = -11982.49930407708
1
Iteration 12200: Loss = -11982.50121171121
2
Iteration 12300: Loss = -11982.71742815489
3
Iteration 12400: Loss = -11982.499048676396
Iteration 12500: Loss = -11982.488240688064
Iteration 12600: Loss = -11982.486187816514
Iteration 12700: Loss = -11982.487242938518
1
Iteration 12800: Loss = -11982.486168331188
Iteration 12900: Loss = -11982.51222599466
1
Iteration 13000: Loss = -11982.486137069058
Iteration 13100: Loss = -11982.487931703838
1
Iteration 13200: Loss = -11982.487131422939
2
Iteration 13300: Loss = -11982.486277594126
3
Iteration 13400: Loss = -11982.487864319683
4
Iteration 13500: Loss = -11982.464115618362
Iteration 13600: Loss = -11982.460337034683
Iteration 13700: Loss = -11982.462324015853
1
Iteration 13800: Loss = -11982.460262772578
Iteration 13900: Loss = -11982.461529619197
1
Iteration 14000: Loss = -11982.459865166535
Iteration 14100: Loss = -11982.459896109825
1
Iteration 14200: Loss = -11982.459778923007
Iteration 14300: Loss = -11982.460297253834
1
Iteration 14400: Loss = -11982.459790984703
2
Iteration 14500: Loss = -11982.477326898315
3
Iteration 14600: Loss = -11982.45978269869
4
Iteration 14700: Loss = -11982.46133463434
5
Iteration 14800: Loss = -11982.487694642377
6
Iteration 14900: Loss = -11982.459958376368
7
Iteration 15000: Loss = -11982.463150207952
8
Iteration 15100: Loss = -11982.497905452507
9
Iteration 15200: Loss = -11982.461499301131
10
Stopping early at iteration 15200 due to no improvement.
tensor([[ -4.5649,   2.8534],
        [ -4.7120,   3.1732],
        [ -2.8244,   1.3417],
        [ -2.9691,   0.5409],
        [ -2.5262,   1.1388],
        [ -8.2687,   5.3193],
        [ -3.9150,   2.0982],
        [ -4.3895,   2.2935],
        [ -2.3612,   0.3830],
        [ -4.2769,   2.8775],
        [ -7.7496,   4.1392],
        [ -4.8379,   3.3262],
        [ -3.7612,   2.0354],
        [ -5.6350,   1.9549],
        [ -4.6329,   3.2465],
        [ -2.6431,   0.9703],
        [ -5.8506,   1.7102],
        [ -6.0440,   4.3915],
        [ -6.8189,   5.4228],
        [ -5.2121,   2.3025],
        [ -1.5378,   0.0992],
        [ -8.7079,   7.3209],
        [ -3.9227,   0.6009],
        [  8.0517, -11.0932],
        [ -5.1771,   3.4331],
        [ -2.5730,   0.9794],
        [ -5.6489,   3.9253],
        [ -2.9728,   1.2544],
        [ -2.2625,   0.8241],
        [ -5.8206,   4.2655],
        [ -5.7998,   2.8138],
        [ -4.0709,   0.4851],
        [ -5.2101,   3.3130],
        [ -3.9297,   2.5432],
        [ -5.7391,   4.3361],
        [ -5.6764,   3.7453],
        [ -5.2501,   3.6790],
        [ -3.9565,   2.5492],
        [ -5.7524,   2.1559],
        [ -5.3650,   3.2734],
        [ -6.0559,   2.8009],
        [ -6.0542,   3.6512],
        [ -4.2852,   1.0953],
        [ -4.4853,   3.0813],
        [ -3.2190,   1.3669],
        [ -2.0279,   0.5287],
        [ -4.8304,   3.4158],
        [ -4.0021,   2.3814],
        [ -2.8395,   0.7512],
        [ -2.3031,   0.5484],
        [ -2.5227,   1.1146],
        [ -5.4189,   3.9806],
        [ -5.1086,   0.5721],
        [ -4.1712,   2.5982],
        [ -3.9468,   1.8104],
        [ -5.1414,   2.9795],
        [ -4.2949,   1.8521],
        [ -4.7831,   2.6213],
        [ -4.8537,   3.4125],
        [ -4.0621,   1.0397],
        [ -4.3579,   2.6826],
        [ -5.2027,   3.6321],
        [-10.3306,   7.4833],
        [ -3.8211,   1.4824],
        [ -3.0019,   1.5306],
        [ -4.5771,   2.9902],
        [ -6.3801,   4.8509],
        [ -7.8420,   6.0251],
        [ -2.8981,   0.9418],
        [ -9.3953,   7.0124],
        [ -3.0979,   1.5984],
        [ -5.5047,   3.8388],
        [ -5.0136,   2.6073],
        [ -2.1139,  -0.1277],
        [ -7.8962,   4.2196],
        [ -3.7199,   2.3329],
        [ -3.5442,   1.5150],
        [ -5.7651,   4.3259],
        [ -2.6546,   1.2396],
        [ -4.8411,   2.7140],
        [ -4.1696,   2.2759],
        [ -3.4771,   2.0049],
        [ -2.8405,   1.3629],
        [ -3.0217,   1.6297],
        [ -4.8684,   3.0505],
        [ -3.7140,   1.5772],
        [ -3.3759,   1.1846],
        [ -2.8189,   0.3909],
        [-10.0169,   7.5032],
        [ -3.7841,   2.3728],
        [ -5.4227,   3.9223],
        [ -5.0095,   3.5999],
        [ -5.8464,   3.5181],
        [-10.2301,   7.3192],
        [ -3.7720,   2.0053],
        [ -4.6109,   2.9342],
        [ -5.3847,   3.5843],
        [ -2.8612,   0.7355],
        [ -5.7497,   2.0585],
        [ -2.9159,  -0.5545]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5014, 0.4986],
        [0.4055, 0.5945]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0206, 0.9794], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3346, 0.3081],
         [0.0291, 0.2361]],

        [[0.6640, 0.1006],
         [0.2319, 0.7543]],

        [[0.9847, 0.1028],
         [0.8502, 0.1991]],

        [[0.2859, 0.1023],
         [0.7202, 0.6317]],

        [[0.6513, 0.0878],
         [0.9033, 0.5910]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 82
Adjusted Rand Index: 0.4036363636363636
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 90
Adjusted Rand Index: 0.6365119243670657
Global Adjusted Rand Index: 0.008987430858263446
Average Adjusted Rand Index: 0.5918714180276121
Iteration 0: Loss = -15555.581461580747
Iteration 10: Loss = -11489.979773769142
Iteration 20: Loss = -11486.941389099544
Iteration 30: Loss = -11486.941401165668
1
Iteration 40: Loss = -11486.941401165668
2
Iteration 50: Loss = -11486.941401165668
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7551, 0.2449],
        [0.2405, 0.7595]], dtype=torch.float64)
alpha: tensor([0.4962, 0.5038])
beta: tensor([[[0.3954, 0.0948],
         [0.8465, 0.1957]],

        [[0.1163, 0.1014],
         [0.4889, 0.4705]],

        [[0.1398, 0.1036],
         [0.0016, 0.2839]],

        [[0.9312, 0.1007],
         [0.3751, 0.3424]],

        [[0.7054, 0.0905],
         [0.4148, 0.7595]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15243.880113005205
Iteration 100: Loss = -11575.22564291827
Iteration 200: Loss = -11487.343919406692
Iteration 300: Loss = -11485.937272412537
Iteration 400: Loss = -11485.59439405595
Iteration 500: Loss = -11485.41837645766
Iteration 600: Loss = -11485.313321032236
Iteration 700: Loss = -11485.24472884615
Iteration 800: Loss = -11485.197108202783
Iteration 900: Loss = -11485.162490559265
Iteration 1000: Loss = -11485.136496542002
Iteration 1100: Loss = -11485.116365915117
Iteration 1200: Loss = -11485.100469085854
Iteration 1300: Loss = -11485.087683409023
Iteration 1400: Loss = -11485.077265083466
Iteration 1500: Loss = -11485.068442144126
Iteration 1600: Loss = -11485.061188614518
Iteration 1700: Loss = -11485.054990773893
Iteration 1800: Loss = -11485.04965859238
Iteration 1900: Loss = -11485.04514054845
Iteration 2000: Loss = -11485.041150667725
Iteration 2100: Loss = -11485.037669231224
Iteration 2200: Loss = -11485.034600785413
Iteration 2300: Loss = -11485.031934973194
Iteration 2400: Loss = -11485.02954193937
Iteration 2500: Loss = -11485.02739599512
Iteration 2600: Loss = -11485.025477278139
Iteration 2700: Loss = -11485.023789924962
Iteration 2800: Loss = -11485.143929830656
1
Iteration 2900: Loss = -11485.020802012223
Iteration 3000: Loss = -11485.019509286501
Iteration 3100: Loss = -11485.018369512354
Iteration 3200: Loss = -11485.019908420167
1
Iteration 3300: Loss = -11485.016367652379
Iteration 3400: Loss = -11485.015474478629
Iteration 3500: Loss = -11485.055631919678
1
Iteration 3600: Loss = -11485.013931332096
Iteration 3700: Loss = -11485.013222583942
Iteration 3800: Loss = -11485.013878005688
1
Iteration 3900: Loss = -11485.01219380918
Iteration 4000: Loss = -11485.011433576587
Iteration 4100: Loss = -11485.010952554472
Iteration 4200: Loss = -11485.01648485364
1
Iteration 4300: Loss = -11485.172921598436
2
Iteration 4400: Loss = -11485.009596693424
Iteration 4500: Loss = -11485.011532512388
1
Iteration 4600: Loss = -11485.008870842506
Iteration 4700: Loss = -11485.01657510835
1
Iteration 4800: Loss = -11485.013538417474
2
Iteration 4900: Loss = -11485.016001410168
3
Iteration 5000: Loss = -11485.026423522808
4
Iteration 5100: Loss = -11485.026935363796
5
Iteration 5200: Loss = -11485.03038586024
6
Iteration 5300: Loss = -11485.006833992578
Iteration 5400: Loss = -11485.008127544608
1
Iteration 5500: Loss = -11485.008052494513
2
Iteration 5600: Loss = -11485.008037300682
3
Iteration 5700: Loss = -11485.020539995314
4
Iteration 5800: Loss = -11485.005926056469
Iteration 5900: Loss = -11485.009292366809
1
Iteration 6000: Loss = -11485.007437883518
2
Iteration 6100: Loss = -11485.01794273382
3
Iteration 6200: Loss = -11485.008560155671
4
Iteration 6300: Loss = -11485.006174652111
5
Iteration 6400: Loss = -11485.013545744727
6
Iteration 6500: Loss = -11485.005025466498
Iteration 6600: Loss = -11485.021760552647
1
Iteration 6700: Loss = -11485.010045066325
2
Iteration 6800: Loss = -11485.004912871358
Iteration 6900: Loss = -11485.006100864473
1
Iteration 7000: Loss = -11485.005423993704
2
Iteration 7100: Loss = -11485.097193436373
3
Iteration 7200: Loss = -11485.007275267566
4
Iteration 7300: Loss = -11485.032630742522
5
Iteration 7400: Loss = -11485.010750641446
6
Iteration 7500: Loss = -11485.009694215072
7
Iteration 7600: Loss = -11485.020230765957
8
Iteration 7700: Loss = -11485.020690756519
9
Iteration 7800: Loss = -11485.031129858338
10
Stopping early at iteration 7800 due to no improvement.
tensor([[  4.4989,  -5.8868],
        [  5.8096,  -8.1506],
        [ -7.4671,   6.0417],
        [ -8.7559,   7.1336],
        [ -8.2171,   6.7166],
        [ -7.7963,   6.2447],
        [ -8.6173,   7.0758],
        [ -7.8736,   6.0644],
        [ -8.0740,   6.6683],
        [  3.0901,  -4.4780],
        [  6.1725,  -7.5610],
        [  5.7325,  -7.1208],
        [ -8.5402,   6.7981],
        [  4.9208,  -7.3018],
        [  6.7711,  -8.5330],
        [-10.1296,   6.2347],
        [  5.4014,  -9.2510],
        [  5.8219,  -8.6486],
        [  1.8844,  -3.6665],
        [  6.8371,  -8.2414],
        [ -8.7520,   6.5841],
        [  5.3141,  -9.1662],
        [ -7.5956,   6.0723],
        [ -8.9380,   7.4421],
        [  5.2761,  -7.4222],
        [ -9.1273,   7.4873],
        [  6.2901,  -8.4104],
        [ -9.2939,   6.7641],
        [ -8.0893,   6.5745],
        [  5.2932,  -7.0819],
        [  6.0905,  -7.4794],
        [ -9.5994,   6.6619],
        [  6.6352,  -8.0780],
        [ -8.8350,   7.4344],
        [  4.9904,  -6.6246],
        [  5.4767,  -7.3445],
        [  5.8557,  -7.2420],
        [ -6.9935,   3.9201],
        [  4.8390,  -6.5528],
        [  2.6716,  -4.1741],
        [  5.4155,  -7.1248],
        [  6.5134,  -7.9663],
        [  6.0725,  -7.7005],
        [ -7.5966,   6.1796],
        [ -8.2482,   6.4679],
        [ -8.8443,   7.0726],
        [  6.7702,  -8.1617],
        [ -9.0996,   7.0636],
        [ -8.3941,   6.8970],
        [ -8.8219,   6.9328],
        [ -8.8308,   5.5769],
        [  5.6464,  -7.0592],
        [ -7.9375,   6.3257],
        [  5.5232,  -7.2534],
        [ -8.0278,   6.3572],
        [  5.5750,  -7.0832],
        [  5.6593,  -7.7382],
        [  3.6513,  -6.7601],
        [  5.5643,  -6.9668],
        [ -7.6831,   6.2767],
        [  6.4118,  -7.8567],
        [  6.7816,  -8.2288],
        [ -8.1887,   6.2838],
        [ -8.6034,   5.3162],
        [ -9.6987,   6.9728],
        [  6.4153,  -7.9561],
        [  6.3425,  -7.7385],
        [  6.2640,  -7.7522],
        [ -9.3901,   5.6833],
        [  3.5964,  -5.1232],
        [ -9.0433,   7.5362],
        [  5.3038,  -7.9912],
        [ -5.3333,   2.6268],
        [ -8.9705,   7.2257],
        [ -8.5050,   6.9175],
        [ -7.9726,   6.5506],
        [ -9.1438,   7.6993],
        [  5.8729,  -7.7712],
        [ -8.2951,   6.8636],
        [  3.7993,  -5.2239],
        [ -8.0326,   6.5763],
        [ -8.0374,   6.6492],
        [ -8.3382,   6.9400],
        [ -8.5361,   6.6007],
        [  3.5520,  -5.0269],
        [ -7.9309,   6.4702],
        [ -7.6061,   5.9194],
        [ -9.3401,   7.2505],
        [ -7.2775,   5.7724],
        [ -7.5658,   4.6993],
        [  4.2941,  -6.3361],
        [  6.5650,  -8.0997],
        [  6.6183,  -8.0750],
        [  5.8103,  -8.0032],
        [ -6.7202,   5.3014],
        [  6.3549,  -8.1059],
        [  5.4425,  -7.1341],
        [ -8.4270,   6.8806],
        [  5.3006,  -6.6895],
        [ -9.5793,   6.2221]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7649, 0.2351],
        [0.2442, 0.7558]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4899, 0.5101], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.0948],
         [0.8465, 0.4066]],

        [[0.1163, 0.1021],
         [0.4889, 0.4705]],

        [[0.1398, 0.1048],
         [0.0016, 0.2839]],

        [[0.9312, 0.1006],
         [0.3751, 0.3424]],

        [[0.7054, 0.0905],
         [0.4148, 0.7595]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11488.429501537514
new:  [0.5890323425815494, 1.0, 0.008987430858263446, 1.0] [0.7865985960559245, 1.0, 0.5918714180276121, 1.0] [11692.100202341622, 11485.008175618528, 11982.461499301131, 11485.031129858338]
prior:  [0.9919999711388391, 0.9919999711388391, 0.5349014282809299, 0.9919999711388391] [0.9919993417272899, 0.9919993417272899, 0.8213333333333332, 0.9919993417272899] [11486.941386891318, 11486.941398985584, 11691.012530191962, 11486.941401165668]
-----------------------------------------------------------------------------------------
This iteration is 32
True Objective function: Loss = -11456.330076877
Iteration 0: Loss = -18037.456228783572
Iteration 10: Loss = -11638.24365035373
Iteration 20: Loss = -11659.279200422003
1
Iteration 30: Loss = -11663.1199054933
2
Iteration 40: Loss = -11664.77644391611
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.6497, 0.3503],
        [0.3157, 0.6843]], dtype=torch.float64)
alpha: tensor([0.4308, 0.5692])
beta: tensor([[[0.3820, 0.1298],
         [0.4530, 0.2064]],

        [[0.9271, 0.1046],
         [0.8301, 0.3567]],

        [[0.6009, 0.0938],
         [0.4469, 0.5405]],

        [[0.7957, 0.0976],
         [0.2939, 0.5066]],

        [[0.9271, 0.1113],
         [0.2187, 0.5899]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 78
Adjusted Rand Index: 0.3069711157192286
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.46674811845564196
Average Adjusted Rand Index: 0.8533942231438457
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17990.185688059435
Iteration 100: Loss = -11457.186981560895
Iteration 200: Loss = -11449.156843818293
Iteration 300: Loss = -11448.524396489998
Iteration 400: Loss = -11448.25095482527
Iteration 500: Loss = -11448.100108388608
Iteration 600: Loss = -11448.00760146275
Iteration 700: Loss = -11447.94595980135
Iteration 800: Loss = -11447.90279983505
Iteration 900: Loss = -11447.8712360183
Iteration 1000: Loss = -11447.847464761137
Iteration 1100: Loss = -11447.829059651285
Iteration 1200: Loss = -11447.814453844214
Iteration 1300: Loss = -11447.802673287495
Iteration 1400: Loss = -11447.792965749342
Iteration 1500: Loss = -11447.784922345269
Iteration 1600: Loss = -11447.778228876985
Iteration 1700: Loss = -11447.772535105281
Iteration 1800: Loss = -11447.767654139503
Iteration 1900: Loss = -11447.763440502182
Iteration 2000: Loss = -11447.759736822232
Iteration 2100: Loss = -11447.756588808072
Iteration 2200: Loss = -11447.753811392147
Iteration 2300: Loss = -11447.751273265094
Iteration 2400: Loss = -11447.749097257172
Iteration 2500: Loss = -11447.747146864102
Iteration 2600: Loss = -11447.745384801403
Iteration 2700: Loss = -11447.743842832959
Iteration 2800: Loss = -11447.742433662865
Iteration 2900: Loss = -11447.741124190781
Iteration 3000: Loss = -11447.739939609035
Iteration 3100: Loss = -11447.73892439363
Iteration 3200: Loss = -11447.737989897249
Iteration 3300: Loss = -11447.737084962342
Iteration 3400: Loss = -11447.736278456792
Iteration 3500: Loss = -11447.737178032614
1
Iteration 3600: Loss = -11447.734857728996
Iteration 3700: Loss = -11447.734259222592
Iteration 3800: Loss = -11447.733693714126
Iteration 3900: Loss = -11447.733167298571
Iteration 4000: Loss = -11447.732658683542
Iteration 4100: Loss = -11447.732209260505
Iteration 4200: Loss = -11447.732012861037
Iteration 4300: Loss = -11447.731398078453
Iteration 4400: Loss = -11447.731033052602
Iteration 4500: Loss = -11447.733860097738
1
Iteration 4600: Loss = -11447.730371969243
Iteration 4700: Loss = -11447.730069678135
Iteration 4800: Loss = -11447.72979048233
Iteration 4900: Loss = -11447.729525305738
Iteration 5000: Loss = -11447.729211011916
Iteration 5100: Loss = -11447.728927464032
Iteration 5200: Loss = -11447.728867957581
Iteration 5300: Loss = -11447.728516640527
Iteration 5400: Loss = -11447.728321019928
Iteration 5500: Loss = -11447.763482451917
1
Iteration 5600: Loss = -11447.728002076385
Iteration 5700: Loss = -11447.727837423601
Iteration 5800: Loss = -11447.727702826944
Iteration 5900: Loss = -11447.72761735073
Iteration 6000: Loss = -11447.727389992397
Iteration 6100: Loss = -11447.727551530403
1
Iteration 6200: Loss = -11447.727284299006
Iteration 6300: Loss = -11447.727923550729
1
Iteration 6400: Loss = -11447.72727270551
Iteration 6500: Loss = -11447.799533908126
1
Iteration 6600: Loss = -11447.726764271205
Iteration 6700: Loss = -11447.742095750767
1
Iteration 6800: Loss = -11447.726596288736
Iteration 6900: Loss = -11447.727006386936
1
Iteration 7000: Loss = -11447.72905219674
2
Iteration 7100: Loss = -11447.73133698817
3
Iteration 7200: Loss = -11447.726343743727
Iteration 7300: Loss = -11447.732312949436
1
Iteration 7400: Loss = -11447.750447208624
2
Iteration 7500: Loss = -11447.736476854365
3
Iteration 7600: Loss = -11447.782695082813
4
Iteration 7700: Loss = -11447.743123952612
5
Iteration 7800: Loss = -11447.736209275314
6
Iteration 7900: Loss = -11447.729767519822
7
Iteration 8000: Loss = -11447.72596551439
Iteration 8100: Loss = -11447.725972281458
1
Iteration 8200: Loss = -11447.72741914755
2
Iteration 8300: Loss = -11447.733068977188
3
Iteration 8400: Loss = -11447.759054231054
4
Iteration 8500: Loss = -11447.729904266527
5
Iteration 8600: Loss = -11447.726841938244
6
Iteration 8700: Loss = -11447.732892671529
7
Iteration 8800: Loss = -11447.725880085942
Iteration 8900: Loss = -11447.731068623749
1
Iteration 9000: Loss = -11447.730163198825
2
Iteration 9100: Loss = -11447.73506528658
3
Iteration 9200: Loss = -11447.725986253436
4
Iteration 9300: Loss = -11447.733176029838
5
Iteration 9400: Loss = -11447.728262937911
6
Iteration 9500: Loss = -11447.730813471278
7
Iteration 9600: Loss = -11447.731687948088
8
Iteration 9700: Loss = -11447.811080276806
9
Iteration 9800: Loss = -11447.72862772409
10
Stopping early at iteration 9800 due to no improvement.
tensor([[-10.2481,   5.6329],
        [  5.7142, -10.3294],
        [-10.2114,   5.5962],
        [  3.6696,  -8.2848],
        [-10.4229,   5.8077],
        [  5.2923,  -9.9075],
        [  6.0644, -10.6796],
        [  6.2804, -10.8956],
        [  5.2978,  -9.9130],
        [ -7.5583,   2.9430],
        [  2.4093,  -7.0245],
        [ -7.2642,   2.6490],
        [  4.9001,  -9.5153],
        [ -9.1618,   4.5466],
        [ -8.0126,   3.3974],
        [-10.1792,   5.5640],
        [-10.4280,   5.8128],
        [ -9.2742,   4.6590],
        [  6.2832, -10.8985],
        [ -8.2834,   3.6681],
        [ -8.4443,   3.8291],
        [  0.8518,  -5.4670],
        [  1.3585,  -5.9738],
        [  0.3094,  -4.9246],
        [  6.1904, -10.8057],
        [-10.5248,   5.9096],
        [ -6.7575,   2.1423],
        [ -9.1548,   4.5396],
        [  5.9017, -10.5169],
        [-10.7644,   6.1491],
        [  5.1522,  -9.7674],
        [  3.8838,  -8.4990],
        [  0.9517,  -5.5669],
        [  5.6843, -10.2995],
        [  5.8321, -10.4473],
        [-10.1536,   5.5384],
        [  4.7247,  -9.3399],
        [-10.2232,   5.6080],
        [  3.3842,  -7.9994],
        [-10.3319,   5.7167],
        [ -9.4581,   4.8428],
        [ -8.7680,   4.1528],
        [ -0.2107,  -4.4045],
        [-10.1130,   5.4978],
        [  4.2299,  -8.8451],
        [  4.3461,  -8.9613],
        [  4.5703,  -9.1855],
        [  5.7043, -10.3195],
        [ -9.9462,   5.3310],
        [  1.9764,  -6.5917],
        [  5.0697,  -9.6849],
        [-10.6198,   6.0045],
        [  5.4133, -10.0286],
        [-10.3814,   5.7662],
        [  2.2707,  -6.8859],
        [-10.6118,   5.9965],
        [ -6.8288,   2.2136],
        [  5.4561, -10.0714],
        [  4.3983,  -9.0135],
        [-10.9415,   6.3263],
        [  3.4565,  -8.0718],
        [ -8.2468,   3.6316],
        [ -7.7020,   3.0868],
        [-10.2832,   5.6680],
        [-10.5647,   5.9494],
        [ -9.1974,   4.5822],
        [  4.2176,  -8.8328],
        [  4.4968,  -9.1120],
        [-10.2407,   5.6255],
        [  5.8784, -10.4936],
        [  4.8345,  -9.4497],
        [  5.3906, -10.0059],
        [-10.4050,   5.7898],
        [ -9.5595,   4.9443],
        [  2.8547,  -7.4699],
        [  5.7283, -10.3435],
        [  4.0095,  -8.6247],
        [  4.6449,  -9.2601],
        [  4.5606,  -9.1758],
        [-10.6652,   6.0500],
        [  6.6416, -11.2568],
        [ -9.7720,   5.1568],
        [  4.8878,  -9.5030],
        [-10.4584,   5.8431],
        [  4.3329,  -8.9481],
        [-10.2800,   5.6648],
        [  3.0170,  -7.6322],
        [ -6.8930,   2.2778],
        [  5.2418,  -9.8570],
        [ -9.5888,   4.9736],
        [ -7.7101,   3.0949],
        [-10.2770,   5.6618],
        [  5.7906, -10.4058],
        [ -8.6191,   4.0039],
        [  4.7086,  -9.3239],
        [  4.0388,  -8.6540],
        [ -2.0171,  -2.5981],
        [  5.1783,  -9.7935],
        [-10.9159,   6.3007],
        [  4.8813,  -9.4965]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7998, 0.2002],
        [0.2434, 0.7566]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5358, 0.4642], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.1034],
         [0.4530, 0.4089]],

        [[0.9271, 0.1047],
         [0.8301, 0.3567]],

        [[0.6009, 0.0946],
         [0.4469, 0.5405]],

        [[0.7957, 0.0976],
         [0.2939, 0.5066]],

        [[0.9271, 0.1114],
         [0.2187, 0.5899]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9840316589316259
Average Adjusted Rand Index: 0.984
Iteration 0: Loss = -17610.29003834114
Iteration 10: Loss = -11449.392349946254
Iteration 20: Loss = -11449.392429205978
1
Iteration 30: Loss = -11449.392429187961
2
Iteration 40: Loss = -11449.392429187961
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7538, 0.2462],
        [0.2024, 0.7976]], dtype=torch.float64)
alpha: tensor([0.4587, 0.5413])
beta: tensor([[[0.3995, 0.1028],
         [0.7208, 0.1928]],

        [[0.6394, 0.1047],
         [0.0849, 0.3365]],

        [[0.4618, 0.0942],
         [0.3785, 0.5348]],

        [[0.1009, 0.0977],
         [0.2778, 0.5248]],

        [[0.1611, 0.1114],
         [0.2983, 0.3673]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9840317537427083
Average Adjusted Rand Index: 0.9839998119331363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17610.356394866976
Iteration 100: Loss = -11469.913605955204
Iteration 200: Loss = -11451.730659469702
Iteration 300: Loss = -11449.82851940217
Iteration 400: Loss = -11449.061520500762
Iteration 500: Loss = -11448.66180326814
Iteration 600: Loss = -11448.42305328722
Iteration 700: Loss = -11448.267538480337
Iteration 800: Loss = -11448.15973099011
Iteration 900: Loss = -11448.081668996694
Iteration 1000: Loss = -11448.023021592051
Iteration 1100: Loss = -11447.977796912566
Iteration 1200: Loss = -11447.942052520566
Iteration 1300: Loss = -11447.913288990465
Iteration 1400: Loss = -11447.889793756856
Iteration 1500: Loss = -11447.870305740076
Iteration 1600: Loss = -11447.854015112363
Iteration 1700: Loss = -11447.840149853759
Iteration 1800: Loss = -11447.828302506827
Iteration 1900: Loss = -11447.818086503348
Iteration 2000: Loss = -11447.809214934612
Iteration 2100: Loss = -11447.801451689955
Iteration 2200: Loss = -11447.794631078426
Iteration 2300: Loss = -11447.788638957858
Iteration 2400: Loss = -11447.783273039651
Iteration 2500: Loss = -11447.778509470414
Iteration 2600: Loss = -11447.774224655712
Iteration 2700: Loss = -11447.770434128988
Iteration 2800: Loss = -11447.76692488382
Iteration 2900: Loss = -11447.763804066375
Iteration 3000: Loss = -11447.76096114381
Iteration 3100: Loss = -11447.758395304765
Iteration 3200: Loss = -11447.756015195424
Iteration 3300: Loss = -11447.753888549312
Iteration 3400: Loss = -11447.751896486818
Iteration 3500: Loss = -11447.750081493426
Iteration 3600: Loss = -11447.748436032429
Iteration 3700: Loss = -11447.746902417706
Iteration 3800: Loss = -11447.745470247033
Iteration 3900: Loss = -11447.744180153039
Iteration 4000: Loss = -11447.743001386623
Iteration 4100: Loss = -11447.741849201066
Iteration 4200: Loss = -11447.741895777655
1
Iteration 4300: Loss = -11447.739827151958
Iteration 4400: Loss = -11447.73892590203
Iteration 4500: Loss = -11447.745422291284
1
Iteration 4600: Loss = -11447.73731021595
Iteration 4700: Loss = -11447.736579173206
Iteration 4800: Loss = -11447.7359312892
Iteration 4900: Loss = -11447.735716734454
Iteration 5000: Loss = -11447.734676664599
Iteration 5100: Loss = -11447.734093864045
Iteration 5200: Loss = -11447.733632334468
Iteration 5300: Loss = -11447.734148079022
1
Iteration 5400: Loss = -11447.732692685457
Iteration 5500: Loss = -11447.73225550877
Iteration 5600: Loss = -11447.731859180956
Iteration 5700: Loss = -11447.731518711576
Iteration 5800: Loss = -11447.73112538761
Iteration 5900: Loss = -11447.730752775
Iteration 6000: Loss = -11447.730461913718
Iteration 6100: Loss = -11447.734637682628
1
Iteration 6200: Loss = -11447.73039334326
Iteration 6300: Loss = -11447.731369809137
1
Iteration 6400: Loss = -11447.731827044565
2
Iteration 6500: Loss = -11447.73049310525
3
Iteration 6600: Loss = -11447.732489997168
4
Iteration 6700: Loss = -11447.759892401666
5
Iteration 6800: Loss = -11447.728706060068
Iteration 6900: Loss = -11447.72889513162
1
Iteration 7000: Loss = -11447.732191514688
2
Iteration 7100: Loss = -11447.729650462134
3
Iteration 7200: Loss = -11447.729171439563
4
Iteration 7300: Loss = -11447.73861908522
5
Iteration 7400: Loss = -11447.737564881798
6
Iteration 7500: Loss = -11447.730291935874
7
Iteration 7600: Loss = -11447.743035390378
8
Iteration 7700: Loss = -11447.741890365152
9
Iteration 7800: Loss = -11447.7413252995
10
Stopping early at iteration 7800 due to no improvement.
tensor([[ 5.8656, -7.9344],
        [-8.5760,  5.2058],
        [ 5.5097, -8.4078],
        [-6.5471,  5.1591],
        [ 6.5900, -7.9900],
        [-7.4502,  5.4108],
        [-7.6391,  6.2479],
        [-8.3129,  5.7691],
        [-7.1617,  5.3934],
        [ 4.5377, -5.9773],
        [-6.3970,  3.2020],
        [ 4.2881, -5.7216],
        [-8.7943,  5.1785],
        [ 5.1911, -6.9100],
        [ 5.0037, -6.4372],
        [ 6.2128, -7.6038],
        [ 5.3804, -8.0684],
        [ 5.1270, -7.0176],
        [-7.0593,  5.5863],
        [ 5.6489, -8.0320],
        [ 5.9979, -7.5033],
        [-5.0255,  1.3071],
        [-4.4324,  2.9033],
        [-3.3722,  1.8617],
        [-7.6436,  6.0763],
        [ 5.5061, -7.9757],
        [ 2.1385, -6.7537],
        [ 5.1715, -6.7201],
        [-7.6873,  5.9490],
        [ 6.5185, -8.6744],
        [-6.9541,  5.3400],
        [-6.5204,  5.1078],
        [-3.9690,  2.5477],
        [-8.4836,  6.0447],
        [-7.5437,  5.5273],
        [ 6.1726, -7.6030],
        [-6.8009,  5.0724],
        [ 6.3072, -7.7114],
        [-7.0702,  4.3298],
        [ 5.3604, -7.9586],
        [ 5.7516, -7.1424],
        [ 5.8697, -7.8142],
        [-2.8122,  1.3806],
        [ 5.9043, -7.6105],
        [-6.7732,  5.3759],
        [-6.8726,  5.1759],
        [-6.7989,  5.2358],
        [-7.1681,  5.7672],
        [ 5.6429, -7.7581],
        [-4.9851,  3.5980],
        [-7.7310,  5.9985],
        [ 6.1130, -7.5600],
        [-7.6112,  6.1284],
        [ 6.1348, -7.6860],
        [-5.7201,  3.4385],
        [ 5.9284, -7.5203],
        [ 3.7493, -5.3278],
        [-7.5661,  5.9190],
        [-7.5615,  5.5530],
        [ 5.9261, -7.4041],
        [-6.4438,  4.7601],
        [ 5.0702, -6.8003],
        [ 4.6617, -6.4019],
        [ 5.1172, -8.7003],
        [ 5.0768, -8.4616],
        [ 5.5479, -6.9945],
        [-7.8477,  5.1264],
        [-7.4066,  5.9330],
        [ 6.2344, -7.6918],
        [-7.2710,  5.8187],
        [-6.5915,  5.2050],
        [-6.9044,  5.5057],
        [ 6.5598, -8.7180],
        [ 5.7396, -7.3997],
        [-5.8567,  4.4696],
        [-7.0384,  5.6269],
        [-7.5448,  5.0878],
        [-6.9810,  5.5943],
        [-6.9205,  5.5119],
        [ 6.3971, -9.0340],
        [-8.9181,  5.5499],
        [ 5.5582, -8.5051],
        [-8.2578,  5.6068],
        [ 6.2164, -9.4004],
        [-7.3742,  5.1764],
        [ 5.6954, -7.2956],
        [-6.4853,  4.2042],
        [ 3.8503, -5.3315],
        [-9.3835,  5.0039],
        [ 5.0664, -8.2903],
        [ 4.6017, -6.2169],
        [ 6.4293, -7.8160],
        [-8.3962,  5.4706],
        [ 4.0063, -8.6215],
        [-6.7852,  5.3883],
        [-6.4331,  4.8988],
        [-1.8158, -1.2392],
        [-9.7298,  5.1146],
        [ 6.0479, -7.6255],
        [-8.0147,  5.2152]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7572, 0.2428],
        [0.1970, 0.8030]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4638, 0.5362], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4093, 0.1035],
         [0.7208, 0.1961]],

        [[0.6394, 0.1057],
         [0.0849, 0.3365]],

        [[0.4618, 0.0945],
         [0.3785, 0.5348]],

        [[0.1009, 0.0976],
         [0.2778, 0.5248]],

        [[0.1611, 0.1114],
         [0.2983, 0.3673]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9840316589316259
Average Adjusted Rand Index: 0.984
Iteration 0: Loss = -13875.550095894521
Iteration 10: Loss = -11449.392277267842
Iteration 20: Loss = -11449.392429202146
1
Iteration 30: Loss = -11449.392429187961
2
Iteration 40: Loss = -11449.392429187961
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7538, 0.2462],
        [0.2024, 0.7976]], dtype=torch.float64)
alpha: tensor([0.4587, 0.5413])
beta: tensor([[[0.3995, 0.1028],
         [0.7945, 0.1928]],

        [[0.7517, 0.1047],
         [0.5746, 0.6738]],

        [[0.6634, 0.0942],
         [0.1701, 0.3287]],

        [[0.2092, 0.0977],
         [0.7663, 0.2451]],

        [[0.9168, 0.1114],
         [0.3624, 0.4276]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9840317537427083
Average Adjusted Rand Index: 0.9839998119331363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -13875.374404484646
Iteration 100: Loss = -11545.892846326611
Iteration 200: Loss = -11449.20631765822
Iteration 300: Loss = -11448.520407094116
Iteration 400: Loss = -11448.260574420303
Iteration 500: Loss = -11448.1247369157
Iteration 600: Loss = -11448.042660388543
Iteration 700: Loss = -11447.988750427787
Iteration 800: Loss = -11447.951239142314
Iteration 900: Loss = -11447.92391583873
Iteration 1000: Loss = -11447.903290055487
Iteration 1100: Loss = -11447.887330068837
Iteration 1200: Loss = -11447.87453370846
Iteration 1300: Loss = -11447.863690386443
Iteration 1400: Loss = -11447.853952507989
Iteration 1500: Loss = -11447.846477371088
Iteration 1600: Loss = -11447.840540801835
Iteration 1700: Loss = -11447.858565999677
1
Iteration 1800: Loss = -11447.831304219695
Iteration 1900: Loss = -11447.827504687544
Iteration 2000: Loss = -11447.84933241473
1
Iteration 2100: Loss = -11447.755369078217
Iteration 2200: Loss = -11447.752773499611
Iteration 2300: Loss = -11447.762222765881
1
Iteration 2400: Loss = -11447.748521752561
Iteration 2500: Loss = -11447.750411266386
1
Iteration 2600: Loss = -11447.745158267266
Iteration 2700: Loss = -11447.920053562284
1
Iteration 2800: Loss = -11447.742223496054
Iteration 2900: Loss = -11447.74084830381
Iteration 3000: Loss = -11447.7399298979
Iteration 3100: Loss = -11447.738564519546
Iteration 3200: Loss = -11447.737694430169
Iteration 3300: Loss = -11447.736965295411
Iteration 3400: Loss = -11447.736183173136
Iteration 3500: Loss = -11447.7355557134
Iteration 3600: Loss = -11447.735036963197
Iteration 3700: Loss = -11447.734448119345
Iteration 3800: Loss = -11447.734349234448
Iteration 3900: Loss = -11447.733417133606
Iteration 4000: Loss = -11447.73357500917
1
Iteration 4100: Loss = -11447.732641058785
Iteration 4200: Loss = -11447.73232550471
Iteration 4300: Loss = -11447.734559285378
1
Iteration 4400: Loss = -11447.731605888843
Iteration 4500: Loss = -11447.731576013684
Iteration 4600: Loss = -11447.73118513705
Iteration 4700: Loss = -11447.730735538611
Iteration 4800: Loss = -11447.749370101716
1
Iteration 4900: Loss = -11447.731802595006
2
Iteration 5000: Loss = -11447.742294569947
3
Iteration 5100: Loss = -11447.73130847178
4
Iteration 5200: Loss = -11447.732126030009
5
Iteration 5300: Loss = -11447.730771209679
6
Iteration 5400: Loss = -11447.730429655649
Iteration 5500: Loss = -11447.729713482426
Iteration 5600: Loss = -11447.753066074803
1
Iteration 5700: Loss = -11447.762624889045
2
Iteration 5800: Loss = -11447.754395419428
3
Iteration 5900: Loss = -11447.731098461194
4
Iteration 6000: Loss = -11447.843195929185
5
Iteration 6100: Loss = -11447.728710562447
Iteration 6200: Loss = -11447.728452433557
Iteration 6300: Loss = -11447.729337807395
1
Iteration 6400: Loss = -11447.728236024092
Iteration 6500: Loss = -11447.72890090351
1
Iteration 6600: Loss = -11447.728056212352
Iteration 6700: Loss = -11447.72856853531
1
Iteration 6800: Loss = -11447.730549059483
2
Iteration 6900: Loss = -11447.734888812725
3
Iteration 7000: Loss = -11447.834056066074
4
Iteration 7100: Loss = -11447.749753593936
5
Iteration 7200: Loss = -11447.727695945165
Iteration 7300: Loss = -11447.742295444026
1
Iteration 7400: Loss = -11447.730351544753
2
Iteration 7500: Loss = -11447.735652507301
3
Iteration 7600: Loss = -11447.741649248803
4
Iteration 7700: Loss = -11447.747974935173
5
Iteration 7800: Loss = -11447.732910127323
6
Iteration 7900: Loss = -11447.735291088808
7
Iteration 8000: Loss = -11447.72793176998
8
Iteration 8100: Loss = -11447.72733244954
Iteration 8200: Loss = -11447.727696403002
1
Iteration 8300: Loss = -11447.72873993767
2
Iteration 8400: Loss = -11447.762295119925
3
Iteration 8500: Loss = -11447.747952634865
4
Iteration 8600: Loss = -11447.727675175818
5
Iteration 8700: Loss = -11447.731672079013
6
Iteration 8800: Loss = -11447.727495611918
7
Iteration 8900: Loss = -11447.727496401063
8
Iteration 9000: Loss = -11447.738677603456
9
Iteration 9100: Loss = -11447.729329537742
10
Stopping early at iteration 9100 due to no improvement.
tensor([[-9.8310,  5.2629],
        [ 5.8463, -8.4934],
        [-7.9452,  6.4557],
        [ 4.3872, -7.7294],
        [-8.3520,  6.5963],
        [ 6.5995, -8.0094],
        [ 6.6197, -8.3417],
        [ 6.5134, -9.5225],
        [ 5.8630, -7.5089],
        [-6.0029,  4.5330],
        [ 3.9532, -5.4873],
        [-6.2892,  3.6361],
        [ 6.1870, -7.9711],
        [-8.3144,  6.3931],
        [-6.4743,  4.9835],
        [-8.3070,  6.9204],
        [-8.8228,  7.3327],
        [-6.9866,  5.5757],
        [ 6.9420, -8.4326],
        [-7.6477,  5.8901],
        [-8.3827,  5.7026],
        [ 2.4439, -3.8751],
        [ 2.8808, -4.4535],
        [ 1.7330, -3.5001],
        [ 6.1407, -9.7471],
        [-8.3657,  6.4630],
        [-5.4335,  3.4751],
        [-7.6401,  5.8172],
        [ 6.8114, -9.1788],
        [-9.6607,  7.5997],
        [ 6.4326, -7.9130],
        [ 5.4725, -6.9377],
        [ 4.4778, -7.0416],
        [ 6.3403, -7.7894],
        [ 6.5205, -7.9530],
        [-8.9034,  5.9342],
        [ 6.1301, -7.5450],
        [-8.2762,  6.8717],
        [ 4.9998, -6.4369],
        [-8.2266,  6.5764],
        [-9.8339,  6.6567],
        [-8.2353,  6.2590],
        [ 1.2273, -2.9647],
        [-8.5489,  6.8532],
        [ 7.9201, -9.6146],
        [ 5.8996, -7.3111],
        [ 5.8193, -7.8081],
        [ 6.7832, -8.3347],
        [-8.7281,  7.3418],
        [ 3.5632, -5.0063],
        [ 6.6955, -8.6474],
        [-8.7997,  7.0398],
        [ 6.1432, -7.7273],
        [-8.7997,  7.3992],
        [ 6.9463, -9.1157],
        [-8.8724,  7.4793],
        [-6.1211,  2.9315],
        [ 6.4990, -8.0873],
        [ 6.2497, -7.6683],
        [-8.8805,  7.3907],
        [ 5.0225, -6.5095],
        [-7.9207,  4.0428],
        [-7.5002,  3.5629],
        [-7.8379,  6.3992],
        [-8.4184,  6.7227],
        [-8.8975,  4.2823],
        [ 5.7921, -7.1849],
        [ 6.7989, -8.3523],
        [-9.6158,  5.9876],
        [ 6.5681, -7.9544],
        [ 6.1077, -7.8849],
        [ 6.6970, -8.6458],
        [-8.8818,  7.4402],
        [-7.9678,  6.2219],
        [ 4.4194, -5.9113],
        [ 6.0563, -7.9792],
        [ 4.7907, -8.3780],
        [ 6.0638, -7.4571],
        [ 5.8430, -7.7051],
        [-8.7826,  7.1717],
        [ 6.5404, -9.4127],
        [-8.0926,  6.6795],
        [ 7.9309, -9.6040],
        [-8.2298,  6.8291],
        [ 5.8648, -7.2777],
        [-9.0455,  6.1425],
        [ 4.6748, -6.0753],
        [-5.3384,  3.8381],
        [ 5.7252, -8.1153],
        [-8.5601,  6.7823],
        [-8.0857,  6.6919],
        [-8.8288,  7.4107],
        [ 6.6834, -8.0910],
        [-6.8689,  5.2148],
        [ 6.2306, -7.7993],
        [ 5.5823, -7.0075],
        [-0.4968, -1.0723],
        [ 6.9927, -8.3897],
        [-9.0806,  7.6264],
        [ 7.6258, -9.2769]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8001, 0.1999],
        [0.2423, 0.7577]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5366, 0.4634], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1959, 0.1035],
         [0.7945, 0.4093]],

        [[0.7517, 0.1048],
         [0.5746, 0.6738]],

        [[0.6634, 0.0946],
         [0.1701, 0.3287]],

        [[0.2092, 0.0976],
         [0.7663, 0.2451]],

        [[0.9168, 0.1115],
         [0.3624, 0.4276]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9840316589316259
Average Adjusted Rand Index: 0.984
Iteration 0: Loss = -23493.343445228093
Iteration 10: Loss = -12190.108275833805
Iteration 20: Loss = -12128.826503703423
Iteration 30: Loss = -11646.486408428176
Iteration 40: Loss = -11626.75393029051
Iteration 50: Loss = -11627.733048226524
1
Iteration 60: Loss = -11629.68958277339
2
Iteration 70: Loss = -11634.793260172386
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.5077, 0.4923],
        [0.3452, 0.6548]], dtype=torch.float64)
alpha: tensor([0.4243, 0.5757])
beta: tensor([[[0.3906, 0.1026],
         [0.1826, 0.2002]],

        [[0.1054, 0.1047],
         [0.6720, 0.8075]],

        [[0.6378, 0.0939],
         [0.2805, 0.4865]],

        [[0.7127, 0.1094],
         [0.1562, 0.6106]],

        [[0.0755, 0.1120],
         [0.6825, 0.7000]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.0012621439840571286
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.570634988106087
Average Adjusted Rand Index: 0.7759092552676845
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23492.79615038613
Iteration 100: Loss = -12192.190758962824
Iteration 200: Loss = -12175.51329433468
Iteration 300: Loss = -12158.48592535213
Iteration 400: Loss = -12117.221045072074
Iteration 500: Loss = -11902.559055772974
Iteration 600: Loss = -11639.633626052808
Iteration 700: Loss = -11578.494768391074
Iteration 800: Loss = -11560.541450554208
Iteration 900: Loss = -11559.553645396638
Iteration 1000: Loss = -11550.794731228896
Iteration 1100: Loss = -11542.210578107975
Iteration 1200: Loss = -11534.017411332788
Iteration 1300: Loss = -11527.455695269631
Iteration 1400: Loss = -11526.926142611976
Iteration 1500: Loss = -11526.7889983299
Iteration 1600: Loss = -11526.080277378003
Iteration 1700: Loss = -11519.445415193648
Iteration 1800: Loss = -11519.3799646671
Iteration 1900: Loss = -11519.326172282426
Iteration 2000: Loss = -11519.280497265518
Iteration 2100: Loss = -11519.24006055561
Iteration 2200: Loss = -11519.200255862468
Iteration 2300: Loss = -11519.130296517233
Iteration 2400: Loss = -11515.71340861
Iteration 2500: Loss = -11515.682081703622
Iteration 2600: Loss = -11515.659943082399
Iteration 2700: Loss = -11515.648000012186
Iteration 2800: Loss = -11515.62684524125
Iteration 2900: Loss = -11515.614233843799
Iteration 3000: Loss = -11515.601088700236
Iteration 3100: Loss = -11515.592740294518
Iteration 3200: Loss = -11515.586964625425
Iteration 3300: Loss = -11515.572156835811
Iteration 3400: Loss = -11515.563965313853
Iteration 3500: Loss = -11515.560196673528
Iteration 3600: Loss = -11515.55020566004
Iteration 3700: Loss = -11515.550546671506
1
Iteration 3800: Loss = -11515.538663079813
Iteration 3900: Loss = -11515.53354135007
Iteration 4000: Loss = -11515.529359718877
Iteration 4100: Loss = -11515.525569854819
Iteration 4200: Loss = -11515.527784214819
1
Iteration 4300: Loss = -11515.516415549031
Iteration 4400: Loss = -11515.513092299134
Iteration 4500: Loss = -11515.511640064717
Iteration 4600: Loss = -11515.506838770974
Iteration 4700: Loss = -11515.504077369982
Iteration 4800: Loss = -11515.501494260654
Iteration 4900: Loss = -11515.499122139963
Iteration 5000: Loss = -11515.499259440905
1
Iteration 5100: Loss = -11515.509296267035
2
Iteration 5200: Loss = -11515.491360141466
Iteration 5300: Loss = -11507.064336962241
Iteration 5400: Loss = -11507.02203361384
Iteration 5500: Loss = -11507.013791934933
Iteration 5600: Loss = -11507.002803381709
Iteration 5700: Loss = -11507.00792205027
1
Iteration 5800: Loss = -11506.999324458387
Iteration 5900: Loss = -11507.000934647142
1
Iteration 6000: Loss = -11506.994555949981
Iteration 6100: Loss = -11495.809399542888
Iteration 6200: Loss = -11495.800289513258
Iteration 6300: Loss = -11495.791763549758
Iteration 6400: Loss = -11483.568760456737
Iteration 6500: Loss = -11483.561476913479
Iteration 6600: Loss = -11483.560954942159
Iteration 6700: Loss = -11483.600236134587
1
Iteration 6800: Loss = -11483.55747746669
Iteration 6900: Loss = -11483.561872262671
1
Iteration 7000: Loss = -11483.555760455236
Iteration 7100: Loss = -11483.55735532285
1
Iteration 7200: Loss = -11483.554315762385
Iteration 7300: Loss = -11483.555569370357
1
Iteration 7400: Loss = -11483.553302471902
Iteration 7500: Loss = -11483.553280460357
Iteration 7600: Loss = -11483.552751947766
Iteration 7700: Loss = -11476.186616351893
Iteration 7800: Loss = -11463.201083556733
Iteration 7900: Loss = -11463.15080238632
Iteration 8000: Loss = -11463.155270812997
1
Iteration 8100: Loss = -11463.151200292625
2
Iteration 8200: Loss = -11463.145348337795
Iteration 8300: Loss = -11463.15118693823
1
Iteration 8400: Loss = -11463.162610967544
2
Iteration 8500: Loss = -11463.146493614235
3
Iteration 8600: Loss = -11463.146414981926
4
Iteration 8700: Loss = -11463.146725960149
5
Iteration 8800: Loss = -11463.189489579428
6
Iteration 8900: Loss = -11463.144025238527
Iteration 9000: Loss = -11463.16341631467
1
Iteration 9100: Loss = -11463.161824680445
2
Iteration 9200: Loss = -11463.140371264742
Iteration 9300: Loss = -11463.140374452129
1
Iteration 9400: Loss = -11463.140521330773
2
Iteration 9500: Loss = -11463.14078807109
3
Iteration 9600: Loss = -11463.144103595369
4
Iteration 9700: Loss = -11463.146240277621
5
Iteration 9800: Loss = -11463.140950999175
6
Iteration 9900: Loss = -11463.225823341341
7
Iteration 10000: Loss = -11463.154508367494
8
Iteration 10100: Loss = -11463.14256098715
9
Iteration 10200: Loss = -11463.157449012822
10
Stopping early at iteration 10200 due to no improvement.
tensor([[  6.3942,  -7.9568],
        [ -7.2978,   5.8930],
        [  5.3170,  -6.7894],
        [ -7.2146,   3.2632],
        [  6.4148, -10.8485],
        [ -7.5890,   5.9889],
        [ -7.4491,   5.4864],
        [ -7.9544,   6.0333],
        [ -8.2551,   4.8986],
        [  3.4158,  -4.8021],
        [ -5.7159,   2.9031],
        [  4.3825,  -5.7813],
        [ -6.5864,   5.1919],
        [  5.4078,  -8.0364],
        [  3.9262,  -5.4257],
        [  5.6266,  -7.1409],
        [  7.0398,  -8.4440],
        [  5.3824,  -6.8961],
        [ -7.0990,   5.6004],
        [  5.6493,  -7.4666],
        [  5.0345,  -7.4725],
        [ -3.4591,   1.9280],
        [ -5.7425,   3.1595],
        [ -4.1220,   2.6033],
        [ -7.2156,   5.7321],
        [  6.3218,  -8.0433],
        [  2.9896,  -6.3039],
        [  6.1608,  -8.0449],
        [ -7.6482,   5.6212],
        [  6.7172,  -8.1038],
        [ -7.4362,   5.8171],
        [ -6.2979,   4.6025],
        [ -5.0181,   3.3251],
        [ -7.0124,   5.4217],
        [ -6.8572,   5.3476],
        [  5.0553,  -6.8665],
        [ -6.9214,   5.2322],
        [  5.6341,  -7.0845],
        [ -5.8176,   4.4295],
        [  5.5983,  -7.6851],
        [  5.2627,  -7.4037],
        [  5.1401,  -8.5878],
        [ -4.8701,   0.8504],
        [  5.1444,  -6.5576],
        [ -6.3975,   5.0086],
        [ -6.2002,   4.7784],
        [ -6.8065,   4.9518],
        [ -8.2557,   5.0087],
        [  4.7309,  -7.0843],
        [ -4.5822,   2.8250],
        [ -6.7628,   5.3579],
        [  6.2383,  -8.3410],
        [ -7.1630,   5.6153],
        [  6.2124,  -7.6964],
        [ -8.6269,   6.1236],
        [  6.2682,  -7.6708],
        [ -8.7296,   6.8001],
        [ -6.7444,   5.3426],
        [ -6.8317,   5.4281],
        [  6.7816,  -8.2524],
        [ -5.7923,   4.3896],
        [  3.8427,  -5.8395],
        [  3.6453,  -5.3800],
        [  5.8000,  -8.7219],
        [  5.9433,  -8.0335],
        [  5.2828,  -6.7442],
        [ -7.0485,   4.6310],
        [ -8.0329,   6.1959],
        [  5.7394,  -7.3017],
        [ -6.8270,   5.4407],
        [ -7.9969,   5.0399],
        [ -6.9812,   5.5513],
        [  6.7729,  -8.9938],
        [  5.2707,  -9.0274],
        [ -6.4965,   4.8175],
        [ -8.4621,   6.6685],
        [ -6.8747,   4.9449],
        [ -9.1798,   5.8624],
        [ -6.6489,   5.2505],
        [  6.5013,  -8.0067],
        [ -7.7918,   5.1903],
        [  6.2296,  -7.6245],
        [ -6.7927,   5.3879],
        [  5.6318,  -7.0442],
        [ -6.8812,   5.3362],
        [  7.3196,  -8.7110],
        [ -7.2493,   4.6023],
        [ -8.2328,   6.2298],
        [ -7.0931,   4.8878],
        [  5.2330,  -6.8906],
        [  5.5573,  -9.2147],
        [  7.3618,  -8.7668],
        [ -6.8800,   5.4222],
        [  4.6446,  -6.0576],
        [ -8.0784,   5.1474],
        [ -6.2689,   4.7560],
        [ -1.0637,  -0.9523],
        [ -9.0888,   5.9028],
        [  6.1158,  -8.8937],
        [ -8.2518,   6.1932]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7534, 0.2466],
        [0.2088, 0.7912]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4452, 0.5548], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4099, 0.1099],
         [0.1826, 0.1950]],

        [[0.1054, 0.1047],
         [0.6720, 0.8075]],

        [[0.6378, 0.0943],
         [0.2805, 0.4865]],

        [[0.7127, 0.0976],
         [0.1562, 0.6106]],

        [[0.0755, 0.1112],
         [0.6825, 0.7000]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9681913707809395
Average Adjusted Rand Index: 0.9681618721313594
Iteration 0: Loss = -16915.3612708753
Iteration 10: Loss = -11449.392053139365
Iteration 20: Loss = -11449.392426896356
1
Iteration 30: Loss = -11449.392429187961
2
Iteration 40: Loss = -11449.392429187961
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7538, 0.2462],
        [0.2024, 0.7976]], dtype=torch.float64)
alpha: tensor([0.4587, 0.5413])
beta: tensor([[[0.3995, 0.1028],
         [0.9658, 0.1928]],

        [[0.5506, 0.1047],
         [0.7842, 0.2101]],

        [[0.3512, 0.0942],
         [0.5186, 0.7198]],

        [[0.9998, 0.0977],
         [0.8260, 0.6005]],

        [[0.7036, 0.1114],
         [0.8780, 0.6260]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9840317537427083
Average Adjusted Rand Index: 0.9839998119331363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16915.300979476717
Iteration 100: Loss = -12123.514585020252
Iteration 200: Loss = -11516.263508904463
Iteration 300: Loss = -11460.202461372599
Iteration 400: Loss = -11448.90183422064
Iteration 500: Loss = -11448.504029484147
Iteration 600: Loss = -11448.288431242801
Iteration 700: Loss = -11448.15512654631
Iteration 800: Loss = -11448.065662757037
Iteration 900: Loss = -11448.002099772526
Iteration 1000: Loss = -11447.955008499492
Iteration 1100: Loss = -11447.919083634493
Iteration 1200: Loss = -11447.890899709937
Iteration 1300: Loss = -11447.868396601913
Iteration 1400: Loss = -11447.850105384263
Iteration 1500: Loss = -11447.835009029704
Iteration 1600: Loss = -11447.822409612325
Iteration 1700: Loss = -11447.811831423394
Iteration 1800: Loss = -11447.802786121381
Iteration 1900: Loss = -11447.794972847536
Iteration 2000: Loss = -11447.788269230872
Iteration 2100: Loss = -11447.782447431397
Iteration 2200: Loss = -11447.777233397686
Iteration 2300: Loss = -11447.772690813494
Iteration 2400: Loss = -11447.76868330081
Iteration 2500: Loss = -11447.765074633882
Iteration 2600: Loss = -11447.761906928632
Iteration 2700: Loss = -11447.759031396929
Iteration 2800: Loss = -11447.756432863856
Iteration 2900: Loss = -11447.75409056173
Iteration 3000: Loss = -11447.752013890573
Iteration 3100: Loss = -11447.75006428029
Iteration 3200: Loss = -11447.748352191624
Iteration 3300: Loss = -11447.74674214282
Iteration 3400: Loss = -11447.745280818233
Iteration 3500: Loss = -11447.743927133331
Iteration 3600: Loss = -11447.74272289196
Iteration 3700: Loss = -11447.74161221828
Iteration 3800: Loss = -11447.740538654376
Iteration 3900: Loss = -11447.739560626804
Iteration 4000: Loss = -11447.738698422692
Iteration 4100: Loss = -11447.737871312414
Iteration 4200: Loss = -11447.737111631095
Iteration 4300: Loss = -11447.736449199716
Iteration 4400: Loss = -11447.735783636725
Iteration 4500: Loss = -11447.7352175357
Iteration 4600: Loss = -11447.734620521383
Iteration 4700: Loss = -11447.734089948872
Iteration 4800: Loss = -11447.733594697498
Iteration 4900: Loss = -11447.733137990317
Iteration 5000: Loss = -11447.732684719187
Iteration 5100: Loss = -11447.732369481248
Iteration 5200: Loss = -11447.732002540892
Iteration 5300: Loss = -11447.731575481366
Iteration 5400: Loss = -11447.731234922776
Iteration 5500: Loss = -11447.730967743173
Iteration 5600: Loss = -11447.730677184963
Iteration 5700: Loss = -11447.732836672956
1
Iteration 5800: Loss = -11447.730150604308
Iteration 5900: Loss = -11447.729958527514
Iteration 6000: Loss = -11447.72997450983
1
Iteration 6100: Loss = -11447.731534761106
2
Iteration 6200: Loss = -11447.730971119738
3
Iteration 6300: Loss = -11447.7699593572
4
Iteration 6400: Loss = -11447.729096400764
Iteration 6500: Loss = -11447.779531579909
1
Iteration 6600: Loss = -11447.730852596316
2
Iteration 6700: Loss = -11447.72874349424
Iteration 6800: Loss = -11447.728420876216
Iteration 6900: Loss = -11447.732103900938
1
Iteration 7000: Loss = -11447.729599324013
2
Iteration 7100: Loss = -11447.737131330268
3
Iteration 7200: Loss = -11447.727923806682
Iteration 7300: Loss = -11447.728064913916
1
Iteration 7400: Loss = -11447.734849652645
2
Iteration 7500: Loss = -11447.728003252416
3
Iteration 7600: Loss = -11447.727973997167
4
Iteration 7700: Loss = -11447.729750350514
5
Iteration 7800: Loss = -11447.72965895892
6
Iteration 7900: Loss = -11447.729974716302
7
Iteration 8000: Loss = -11447.727512650688
Iteration 8100: Loss = -11447.74905057388
1
Iteration 8200: Loss = -11447.736438649434
2
Iteration 8300: Loss = -11447.73864929253
3
Iteration 8400: Loss = -11447.729431320307
4
Iteration 8500: Loss = -11447.730668132466
5
Iteration 8600: Loss = -11447.7522091849
6
Iteration 8700: Loss = -11447.726766673997
Iteration 8800: Loss = -11447.728958533218
1
Iteration 8900: Loss = -11447.729834568783
2
Iteration 9000: Loss = -11447.726902841878
3
Iteration 9100: Loss = -11447.72906776968
4
Iteration 9200: Loss = -11447.73867214411
5
Iteration 9300: Loss = -11447.728740031422
6
Iteration 9400: Loss = -11447.728438719616
7
Iteration 9500: Loss = -11447.73176022872
8
Iteration 9600: Loss = -11447.842679183621
9
Iteration 9700: Loss = -11447.727077518852
10
Stopping early at iteration 9700 due to no improvement.
tensor([[  6.9070,  -9.0925],
        [ -7.6672,   6.1139],
        [  6.2695,  -7.7016],
        [ -6.8106,   5.1426],
        [  6.7345,  -8.7275],
        [ -8.9665,   7.2175],
        [ -8.1764,   6.4526],
        [-10.8444,   6.2292],
        [ -7.7755,   5.9088],
        [  7.3129,  -9.6143],
        [ -5.5030,   3.9557],
        [  6.3682, -10.9834],
        [ -8.6930,   4.8801],
        [  6.3787,  -8.1431],
        [  5.0228,  -6.5263],
        [  7.1832,  -8.7947],
        [  6.0979, -10.7131],
        [  5.8597,  -7.9801],
        [ -7.9525,   6.5650],
        [  5.5929,  -7.0540],
        [  5.7351,  -8.2238],
        [ -3.8735,   2.4451],
        [ -4.3608,   2.9744],
        [ -3.3379,   1.8957],
        [ -8.5933,   6.8583],
        [  7.0232,  -9.1614],
        [  3.7441,  -5.1688],
        [  5.7246,  -7.3636],
        [ -8.5723,   7.1772],
        [  7.4146,  -8.8894],
        [ -7.7065,   6.1799],
        [ -7.1470,   5.2378],
        [ -3.9541,   2.5625],
        [ -7.7550,   5.8143],
        [ -8.1447,   6.4961],
        [  6.3898,  -8.4171],
        [ -7.4433,   5.5533],
        [  6.4408,  -9.7489],
        [ -6.4585,   4.9049],
        [  6.6861,  -8.5727],
        [  6.1013,  -7.9421],
        [  6.3954,  -7.8115],
        [ -2.9070,   1.2857],
        [  7.4070,  -8.9476],
        [ -8.6705,   6.5717],
        [ -7.2966,   5.5900],
        [ -8.4118,   5.6266],
        [ -8.8830,   6.1442],
        [  7.0425,  -8.6421],
        [ -5.1891,   3.3779],
        [ -8.2311,   6.6267],
        [  7.6088,  -8.9982],
        [ -8.2510,   6.0132],
        [  6.3494,  -7.9282],
        [ -6.0194,   3.1380],
        [  7.3182,  -8.8447],
        [  3.8358,  -5.2229],
        [ -7.6871,   6.2998],
        [ -7.1779,   5.7254],
        [  7.1969,  -9.2118],
        [ -6.4616,   5.0694],
        [  5.4875,  -7.2180],
        [  4.7916,  -6.2690],
        [  6.5472,  -9.2371],
        [  6.7547,  -9.2980],
        [  7.5642,  -9.1392],
        [ -7.1213,   5.4119],
        [ -8.1843,   6.6707],
        [  6.8379,  -8.6287],
        [ -9.2640,   6.3399],
        [ -7.2799,   5.8814],
        [ -7.3666,   5.9653],
        [  7.2282,  -8.6149],
        [  7.3622,  -9.1516],
        [ -5.8603,   4.4661],
        [ -8.4633,   6.2802],
        [ -7.3616,   5.9236],
        [ -8.4266,   6.8079],
        [ -7.7882,   6.0416],
        [  7.2512,  -9.8735],
        [ -8.8720,   6.8770],
        [  5.8394,  -9.3133],
        [ -8.1444,   6.3892],
        [  6.5159,  -7.9367],
        [ -7.5784,   5.4933],
        [  7.6779,  -9.1723],
        [ -6.1357,   4.5604],
        [  3.8676,  -5.3208],
        [ -7.9469,   5.9355],
        [  6.6683, -10.2769],
        [  6.4152,  -8.2887],
        [  6.6864,  -8.0878],
        [ -8.9768,   6.2373],
        [  6.9002,  -8.3757],
        [ -8.3543,   6.9322],
        [ -6.8776,   5.4640],
        [ -0.9857,  -0.4068],
        [ -8.4906,   6.7866],
        [  7.4300,  -8.8668],
        [ -8.4273,   6.8940]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7570, 0.2430],
        [0.1998, 0.8002]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4640, 0.5360], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4092, 0.1034],
         [0.9658, 0.1963]],

        [[0.5506, 0.1048],
         [0.7842, 0.2101]],

        [[0.3512, 0.0945],
         [0.5186, 0.7198]],

        [[0.9998, 0.0976],
         [0.8260, 0.6005]],

        [[0.7036, 0.1114],
         [0.8780, 0.6260]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9840316589316259
Average Adjusted Rand Index: 0.984
11456.330076877
new:  [0.9840316589316259, 0.9840316589316259, 0.9681913707809395, 0.9840316589316259] [0.984, 0.984, 0.9681618721313594, 0.984] [11447.7413252995, 11447.729329537742, 11463.157449012822, 11447.727077518852]
prior:  [0.9840317537427083, 0.9840317537427083, 0.570634988106087, 0.9840317537427083] [0.9839998119331363, 0.9839998119331363, 0.7759092552676845, 0.9839998119331363] [11449.392429187961, 11449.392429187961, 11634.793260172386, 11449.392429187961]
-----------------------------------------------------------------------------------------
This iteration is 33
True Objective function: Loss = -11746.934486969172
Iteration 0: Loss = -20448.815973983732
Iteration 10: Loss = -11739.891017012378
Iteration 20: Loss = -11739.890980827457
Iteration 30: Loss = -11739.890980827453
Iteration 40: Loss = -11739.890980827453
1
Iteration 50: Loss = -11739.890980827453
2
Iteration 60: Loss = -11739.890980827453
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7488, 0.2512],
        [0.2094, 0.7906]], dtype=torch.float64)
alpha: tensor([0.4807, 0.5193])
beta: tensor([[[0.2064, 0.1005],
         [0.1374, 0.3931]],

        [[0.6151, 0.1034],
         [0.2663, 0.6723]],

        [[0.8374, 0.1064],
         [0.7317, 0.9967]],

        [[0.9878, 0.0861],
         [0.4770, 0.7506]],

        [[0.1762, 0.1091],
         [0.3621, 0.5011]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20603.32389795644
Iteration 100: Loss = -12235.377876074352
Iteration 200: Loss = -12001.48745600378
Iteration 300: Loss = -11802.902244766372
Iteration 400: Loss = -11747.322801215741
Iteration 500: Loss = -11738.827328659687
Iteration 600: Loss = -11738.545448761008
Iteration 700: Loss = -11738.428698561002
Iteration 800: Loss = -11738.354093209551
Iteration 900: Loss = -11738.301340735852
Iteration 1000: Loss = -11738.261301486855
Iteration 1100: Loss = -11738.228564615181
Iteration 1200: Loss = -11738.206307363887
Iteration 1300: Loss = -11738.189771229805
Iteration 1400: Loss = -11738.176312083786
Iteration 1500: Loss = -11738.16524417529
Iteration 1600: Loss = -11738.15599552281
Iteration 1700: Loss = -11738.148198009005
Iteration 1800: Loss = -11738.141510624257
Iteration 1900: Loss = -11738.135775989354
Iteration 2000: Loss = -11738.130798664663
Iteration 2100: Loss = -11738.126414957514
Iteration 2200: Loss = -11738.122610460456
Iteration 2300: Loss = -11738.119207938007
Iteration 2400: Loss = -11738.116270144157
Iteration 2500: Loss = -11738.11360289039
Iteration 2600: Loss = -11738.111211937434
Iteration 2700: Loss = -11738.109071097657
Iteration 2800: Loss = -11738.107146466264
Iteration 2900: Loss = -11738.105359876432
Iteration 3000: Loss = -11738.10381799293
Iteration 3100: Loss = -11738.102430627394
Iteration 3200: Loss = -11738.101044726649
Iteration 3300: Loss = -11738.099825921674
Iteration 3400: Loss = -11738.098758256121
Iteration 3500: Loss = -11738.097757770654
Iteration 3600: Loss = -11738.096824973149
Iteration 3700: Loss = -11738.09599834469
Iteration 3800: Loss = -11738.09518375158
Iteration 3900: Loss = -11738.094485371035
Iteration 4000: Loss = -11738.093809162547
Iteration 4100: Loss = -11738.09317383137
Iteration 4200: Loss = -11738.092637354554
Iteration 4300: Loss = -11738.092068414388
Iteration 4400: Loss = -11738.091545515366
Iteration 4500: Loss = -11738.09109469632
Iteration 4600: Loss = -11738.090647090155
Iteration 4700: Loss = -11738.090263518143
Iteration 4800: Loss = -11738.089912607467
Iteration 4900: Loss = -11738.089521701691
Iteration 5000: Loss = -11738.089204544467
Iteration 5100: Loss = -11738.088884219404
Iteration 5200: Loss = -11738.088624251473
Iteration 5300: Loss = -11738.088308254635
Iteration 5400: Loss = -11738.088043486874
Iteration 5500: Loss = -11738.087833329413
Iteration 5600: Loss = -11738.087627364845
Iteration 5700: Loss = -11738.087389563549
Iteration 5800: Loss = -11738.08721759597
Iteration 5900: Loss = -11738.087021954754
Iteration 6000: Loss = -11738.08684455972
Iteration 6100: Loss = -11738.08667637754
Iteration 6200: Loss = -11738.086518655264
Iteration 6300: Loss = -11738.086389937678
Iteration 6400: Loss = -11738.086244613962
Iteration 6500: Loss = -11738.133065308364
1
Iteration 6600: Loss = -11738.088029228671
2
Iteration 6700: Loss = -11738.092312639506
3
Iteration 6800: Loss = -11738.096780515756
4
Iteration 6900: Loss = -11738.086333737123
5
Iteration 7000: Loss = -11738.176563758609
6
Iteration 7100: Loss = -11738.085500188567
Iteration 7200: Loss = -11738.104177625819
1
Iteration 7300: Loss = -11738.085280962223
Iteration 7400: Loss = -11738.086177634814
1
Iteration 7500: Loss = -11738.085177748148
Iteration 7600: Loss = -11738.124963793065
1
Iteration 7700: Loss = -11738.085039541134
Iteration 7800: Loss = -11738.084956499066
Iteration 7900: Loss = -11738.085049591806
1
Iteration 8000: Loss = -11738.084955170745
Iteration 8100: Loss = -11738.158421982098
1
Iteration 8200: Loss = -11738.084775327343
Iteration 8300: Loss = -11738.100201852872
1
Iteration 8400: Loss = -11738.085312947602
2
Iteration 8500: Loss = -11738.092938944714
3
Iteration 8600: Loss = -11738.16579243805
4
Iteration 8700: Loss = -11738.086131019034
5
Iteration 8800: Loss = -11738.18463150263
6
Iteration 8900: Loss = -11738.08453658352
Iteration 9000: Loss = -11738.084543284154
1
Iteration 9100: Loss = -11738.106685620804
2
Iteration 9200: Loss = -11738.084414952838
Iteration 9300: Loss = -11738.085497018381
1
Iteration 9400: Loss = -11738.086348053414
2
Iteration 9500: Loss = -11738.198224423893
3
Iteration 9600: Loss = -11738.093120986901
4
Iteration 9700: Loss = -11738.087729367036
5
Iteration 9800: Loss = -11738.096727431615
6
Iteration 9900: Loss = -11738.244368402431
7
Iteration 10000: Loss = -11738.087344999567
8
Iteration 10100: Loss = -11738.084970017677
9
Iteration 10200: Loss = -11738.085962707992
10
Stopping early at iteration 10200 due to no improvement.
tensor([[ -8.2474,   3.6321],
        [ -0.4880,  -4.1273],
        [-10.0269,   5.4117],
        [ -9.6841,   5.0689],
        [  5.5042, -10.1194],
        [  5.5196, -10.1348],
        [  1.6208,  -6.2360],
        [  5.6748, -10.2900],
        [ -9.7618,   5.1465],
        [  3.8998,  -8.5150],
        [  4.8783,  -9.4935],
        [ -9.5015,   4.8862],
        [ -5.6509,   1.0357],
        [  4.5422,  -9.1574],
        [ -8.9441,   4.3289],
        [ -9.7922,   5.1770],
        [ -7.0916,   2.4764],
        [  5.0169,  -9.6321],
        [-10.0830,   5.4678],
        [-10.0042,   5.3890],
        [  5.3227,  -9.9379],
        [ -9.6166,   5.0014],
        [ -9.1661,   4.5509],
        [ -9.5356,   4.9203],
        [ -9.1937,   4.5785],
        [  5.3532,  -9.9684],
        [ -5.8583,   1.2430],
        [  5.2329,  -9.8482],
        [ -9.9393,   5.3240],
        [-10.3832,   5.7680],
        [  3.9584,  -8.5736],
        [  5.4249, -10.0401],
        [ -9.6333,   5.0181],
        [  4.8106,  -9.4258],
        [  2.2363,  -6.8516],
        [  5.6106, -10.2259],
        [  4.7103,  -9.3256],
        [  0.1909,  -4.8062],
        [ -9.3885,   4.7733],
        [  3.8315,  -8.4467],
        [  2.5591,  -7.1743],
        [  4.3337,  -8.9490],
        [  4.3088,  -8.9240],
        [  5.4854, -10.1006],
        [  5.6884, -10.3036],
        [  4.8089,  -9.4241],
        [ -9.4094,   4.7942],
        [ -6.9657,   2.3505],
        [ -7.0051,   2.3899],
        [ -9.2769,   4.6617],
        [ -5.7864,   1.1711],
        [  4.2855,  -8.9007],
        [ -6.6690,   2.0538],
        [ -9.7095,   5.0942],
        [  3.4751,  -8.0903],
        [ -5.8355,   1.2203],
        [-10.1448,   5.5295],
        [ -9.7763,   5.1611],
        [  5.0742,  -9.6895],
        [  2.4719,  -7.0871],
        [  0.7444,  -5.3596],
        [  5.7086, -10.3238],
        [ -0.0365,  -4.5788],
        [ -4.4194,  -0.1959],
        [  4.7171,  -9.3323],
        [ -9.4632,   4.8480],
        [ -0.9024,  -3.7128],
        [  4.2413,  -8.8565],
        [  5.1686,  -9.7838],
        [  4.5684,  -9.1836],
        [ -9.3950,   4.7798],
        [ -9.9107,   5.2955],
        [ -5.1843,   0.5691],
        [  1.3040,  -5.9192],
        [  4.8076,  -9.4228],
        [-10.0479,   5.4327],
        [ -8.3244,   3.7092],
        [-10.1071,   5.4919],
        [ -8.4620,   3.8467],
        [  3.1913,  -7.8065],
        [  4.1857,  -8.8009],
        [  5.1625,  -9.7777],
        [  4.9838,  -9.5990],
        [  2.4208,  -7.0360],
        [  4.7027,  -9.3179],
        [ -8.1395,   3.5242],
        [  3.3852,  -8.0005],
        [  4.6071,  -9.2223],
        [ -9.8649,   5.2496],
        [ -1.8832,  -2.7320],
        [ -7.0994,   2.4842],
        [ -9.5694,   4.9541],
        [  5.0593,  -9.6745],
        [ -9.3421,   4.7269],
        [ -9.9055,   5.2902],
        [  5.1235,  -9.7387],
        [ -9.2085,   4.5933],
        [ -9.6551,   5.0399],
        [ -9.6620,   5.0468],
        [  5.3179,  -9.9331]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7496, 0.2504],
        [0.2081, 0.7919]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5164, 0.4836], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2111, 0.1005],
         [0.1374, 0.4003]],

        [[0.6151, 0.1032],
         [0.2663, 0.6723]],

        [[0.8374, 0.1066],
         [0.7317, 0.9967]],

        [[0.9878, 0.0861],
         [0.4770, 0.7506]],

        [[0.1762, 0.1091],
         [0.3621, 0.5011]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -19792.836420033826
Iteration 10: Loss = -11740.31928894334
Iteration 20: Loss = -11739.890982370896
Iteration 30: Loss = -11739.89098263347
1
Iteration 40: Loss = -11739.89098263347
2
Iteration 50: Loss = -11739.89098263347
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7906, 0.2094],
        [0.2512, 0.7488]], dtype=torch.float64)
alpha: tensor([0.5193, 0.4807])
beta: tensor([[[0.3931, 0.1005],
         [0.8384, 0.2064]],

        [[0.6500, 0.1034],
         [0.9141, 0.0395]],

        [[0.6283, 0.1064],
         [0.0410, 0.2710]],

        [[0.4224, 0.0861],
         [0.7101, 0.6142]],

        [[0.7121, 0.1091],
         [0.9650, 0.0830]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19792.436629426098
Iteration 100: Loss = -12262.27537600984
Iteration 200: Loss = -11866.706260442159
Iteration 300: Loss = -11747.103973168567
Iteration 400: Loss = -11739.528330239442
Iteration 500: Loss = -11738.792170334498
Iteration 600: Loss = -11738.578964841343
Iteration 700: Loss = -11738.456966050117
Iteration 800: Loss = -11738.377537147162
Iteration 900: Loss = -11738.321828450726
Iteration 1000: Loss = -11738.280965657326
Iteration 1100: Loss = -11738.249917154279
Iteration 1200: Loss = -11738.225735125247
Iteration 1300: Loss = -11738.20638243017
Iteration 1400: Loss = -11738.190749355816
Iteration 1500: Loss = -11738.177828261467
Iteration 1600: Loss = -11738.167061781904
Iteration 1700: Loss = -11738.157974416199
Iteration 1800: Loss = -11738.15027364462
Iteration 1900: Loss = -11738.143606435056
Iteration 2000: Loss = -11738.137842851964
Iteration 2100: Loss = -11738.132839286063
Iteration 2200: Loss = -11738.12846190176
Iteration 2300: Loss = -11738.12453612308
Iteration 2400: Loss = -11738.12110963779
Iteration 2500: Loss = -11738.118057192389
Iteration 2600: Loss = -11738.115333877351
Iteration 2700: Loss = -11738.11290609732
Iteration 2800: Loss = -11738.110656968633
Iteration 2900: Loss = -11738.108641720286
Iteration 3000: Loss = -11738.106829040935
Iteration 3100: Loss = -11738.10522217974
Iteration 3200: Loss = -11738.103738228516
Iteration 3300: Loss = -11738.102359665801
Iteration 3400: Loss = -11738.101049564397
Iteration 3500: Loss = -11738.099923126947
Iteration 3600: Loss = -11738.098909735218
Iteration 3700: Loss = -11738.097907141999
Iteration 3800: Loss = -11738.097053261408
Iteration 3900: Loss = -11738.096218793515
Iteration 4000: Loss = -11738.095431295942
Iteration 4100: Loss = -11738.094711167701
Iteration 4200: Loss = -11738.094087902235
Iteration 4300: Loss = -11738.093416534115
Iteration 4400: Loss = -11738.092881164426
Iteration 4500: Loss = -11738.092339411927
Iteration 4600: Loss = -11738.091876556447
Iteration 4700: Loss = -11738.091395136795
Iteration 4800: Loss = -11738.09094863816
Iteration 4900: Loss = -11738.09055977352
Iteration 5000: Loss = -11738.09019533858
Iteration 5100: Loss = -11738.089830618172
Iteration 5200: Loss = -11738.089475132314
Iteration 5300: Loss = -11738.089229067707
Iteration 5400: Loss = -11738.088941311875
Iteration 5500: Loss = -11738.088622416833
Iteration 5600: Loss = -11738.088476651585
Iteration 5700: Loss = -11738.088174001901
Iteration 5800: Loss = -11738.087936447964
Iteration 5900: Loss = -11738.087715756898
Iteration 6000: Loss = -11738.087522036896
Iteration 6100: Loss = -11738.087390615323
Iteration 6200: Loss = -11738.087213519011
Iteration 6300: Loss = -11738.087007618102
Iteration 6400: Loss = -11738.087099488335
1
Iteration 6500: Loss = -11738.086715688456
Iteration 6600: Loss = -11738.086665437671
Iteration 6700: Loss = -11738.086491579777
Iteration 6800: Loss = -11738.08655103026
1
Iteration 6900: Loss = -11738.086229013332
Iteration 7000: Loss = -11738.086412139097
1
Iteration 7100: Loss = -11738.08603435134
Iteration 7200: Loss = -11738.108342269488
1
Iteration 7300: Loss = -11738.085834645863
Iteration 7400: Loss = -11738.087301066567
1
Iteration 7500: Loss = -11738.085657653011
Iteration 7600: Loss = -11738.085582082133
Iteration 7700: Loss = -11738.091479266188
1
Iteration 7800: Loss = -11738.088059935068
2
Iteration 7900: Loss = -11738.085626065204
3
Iteration 8000: Loss = -11738.085464605074
Iteration 8100: Loss = -11738.086534306869
1
Iteration 8200: Loss = -11738.086923467807
2
Iteration 8300: Loss = -11738.128770447152
3
Iteration 8400: Loss = -11738.093390869259
4
Iteration 8500: Loss = -11738.09592295216
5
Iteration 8600: Loss = -11738.153799887372
6
Iteration 8700: Loss = -11738.086940216392
7
Iteration 8800: Loss = -11738.084962075858
Iteration 8900: Loss = -11738.095096686315
1
Iteration 9000: Loss = -11738.09161575356
2
Iteration 9100: Loss = -11738.109271655425
3
Iteration 9200: Loss = -11738.086158705028
4
Iteration 9300: Loss = -11738.08715027416
5
Iteration 9400: Loss = -11738.085761847595
6
Iteration 9500: Loss = -11738.093703035225
7
Iteration 9600: Loss = -11738.08589801206
8
Iteration 9700: Loss = -11738.08470215488
Iteration 9800: Loss = -11738.09509641296
1
Iteration 9900: Loss = -11738.106004408723
2
Iteration 10000: Loss = -11738.175273331011
3
Iteration 10100: Loss = -11738.088950244695
4
Iteration 10200: Loss = -11738.09066390392
5
Iteration 10300: Loss = -11738.084745662956
6
Iteration 10400: Loss = -11738.085004054461
7
Iteration 10500: Loss = -11738.090790348355
8
Iteration 10600: Loss = -11738.094067727745
9
Iteration 10700: Loss = -11738.085371075274
10
Stopping early at iteration 10700 due to no improvement.
tensor([[  4.8420,  -7.1307],
        [ -2.6200,   1.0196],
        [  7.2572,  -8.7279],
        [  6.8391,  -8.4830],
        [ -8.0495,   6.6447],
        [ -8.0787,   6.6920],
        [ -5.1176,   2.7392],
        [ -8.0966,   6.6679],
        [  7.0993, -10.3396],
        [ -6.8559,   5.3695],
        [ -8.7488,   6.1528],
        [  6.2616,  -7.8470],
        [  2.5755,  -4.1099],
        [ -7.3758,   5.8459],
        [  5.6692,  -8.0766],
        [  5.7632,  -7.1498],
        [  3.9563,  -5.6110],
        [ -7.5315,   6.1451],
        [  7.0633,  -8.4939],
        [  6.6539,  -8.0409],
        [ -7.7838,   6.0676],
        [  6.1515,  -9.1582],
        [  6.7628,  -8.9681],
        [  7.0161,  -9.4316],
        [  6.1303,  -7.8939],
        [ -8.4348,   6.8455],
        [  2.8546,  -4.2461],
        [ -7.9300,   6.5115],
        [  7.5733,  -9.2738],
        [  7.2163,  -8.6139],
        [ -6.8899,   5.3206],
        [ -7.9281,   6.2778],
        [  7.2380,  -9.6762],
        [ -8.0321,   6.0953],
        [ -5.2955,   3.7923],
        [ -9.1110,   7.0466],
        [ -7.5591,   5.5910],
        [ -3.2063,   1.7897],
        [  6.0968,  -8.1462],
        [ -6.8482,   5.3447],
        [ -7.1743,   2.5591],
        [ -7.1642,   5.7579],
        [ -7.1668,   5.7592],
        [ -8.2893,   6.7511],
        [ -8.2025,   6.1426],
        [ -7.4876,   5.6650],
        [  6.6599,  -8.9620],
        [  3.3579,  -5.9599],
        [  3.9006,  -5.4922],
        [  6.2247,  -7.6539],
        [  2.1105,  -4.8465],
        [ -7.0435,   5.4475],
        [  3.6270,  -5.0950],
        [  6.6684,  -8.1061],
        [ -6.5401,   5.0284],
        [  1.8206,  -5.2355],
        [  6.2830,  -7.7495],
        [  6.7832,  -8.2179],
        [ -8.2045,   6.3890],
        [ -5.6342,   3.9251],
        [ -3.7506,   2.3536],
        [ -9.8902,   5.7182],
        [ -3.1392,   1.4030],
        [  1.3642,  -2.8598],
        [ -8.0032,   5.7862],
        [  6.3926,  -8.6086],
        [ -2.2707,   0.5387],
        [ -8.0760,   4.6579],
        [ -7.7241,   6.1101],
        [ -7.2372,   5.8104],
        [  6.0206,  -8.4619],
        [  6.7147,  -8.1065],
        [  0.5690,  -5.1842],
        [ -4.4433,   2.7766],
        [ -8.8733,   5.2989],
        [  6.2436,  -8.9233],
        [  5.1499,  -6.8718],
        [  6.7187,  -8.1065],
        [  5.0879,  -7.7289],
        [ -6.5005,   4.3766],
        [ -7.4647,   5.0189],
        [ -8.8233,   6.3532],
        [ -8.5064,   6.3150],
        [ -5.5659,   3.8974],
        [ -7.8116,   6.4069],
        [  5.1283,  -6.5775],
        [ -6.9137,   4.4724],
        [ -7.5195,   5.2866],
        [  6.9339,  -8.4258],
        [ -1.2524,  -0.4024],
        [  3.7550,  -5.8298],
        [  6.7022,  -8.3155],
        [ -9.3945,   4.7793],
        [  6.7677,  -8.3445],
        [  5.4007,  -8.0105],
        [ -8.4222,   6.3873],
        [  5.7909,  -7.9216],
        [  6.9384,  -8.4432],
        [  6.5715,  -8.0036],
        [ -8.9122,   6.7582]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7920, 0.2080],
        [0.2491, 0.7509]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4837, 0.5163], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4005, 0.1003],
         [0.8384, 0.2111]],

        [[0.6500, 0.1032],
         [0.9141, 0.0395]],

        [[0.6283, 0.1065],
         [0.0410, 0.2710]],

        [[0.4224, 0.0861],
         [0.7101, 0.6142]],

        [[0.7121, 0.1091],
         [0.9650, 0.0830]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -17751.020016303875
Iteration 10: Loss = -11739.895387864846
Iteration 20: Loss = -11739.890980827682
Iteration 30: Loss = -11739.890980827453
Iteration 40: Loss = -11739.890980827453
1
Iteration 50: Loss = -11739.890980827453
2
Iteration 60: Loss = -11739.890980827453
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7488, 0.2512],
        [0.2094, 0.7906]], dtype=torch.float64)
alpha: tensor([0.4807, 0.5193])
beta: tensor([[[0.2064, 0.1005],
         [0.5675, 0.3931]],

        [[0.2817, 0.1034],
         [0.5414, 0.4299]],

        [[0.8572, 0.1064],
         [0.7997, 0.0865]],

        [[0.3155, 0.0861],
         [0.1496, 0.8382]],

        [[0.9476, 0.1091],
         [0.6402, 0.7897]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17750.820072808117
Iteration 100: Loss = -12464.335260494094
Iteration 200: Loss = -12178.326577510157
Iteration 300: Loss = -12080.673307621119
Iteration 400: Loss = -11946.188933798172
Iteration 500: Loss = -11891.77393028414
Iteration 600: Loss = -11891.250687126249
Iteration 700: Loss = -11890.993622408287
Iteration 800: Loss = -11890.810163290333
Iteration 900: Loss = -11890.71132667329
Iteration 1000: Loss = -11890.638918100965
Iteration 1100: Loss = -11887.39050326784
Iteration 1200: Loss = -11887.198929358232
Iteration 1300: Loss = -11887.156848869643
Iteration 1400: Loss = -11887.129874390483
Iteration 1500: Loss = -11887.107273947373
Iteration 1600: Loss = -11887.086110464199
Iteration 1700: Loss = -11887.067674821155
Iteration 1800: Loss = -11887.054690062676
Iteration 1900: Loss = -11887.044518204571
Iteration 2000: Loss = -11887.067477450008
1
Iteration 2100: Loss = -11887.028068424534
Iteration 2200: Loss = -11887.021310007973
Iteration 2300: Loss = -11887.0155053589
Iteration 2400: Loss = -11887.010113518605
Iteration 2500: Loss = -11887.005463971902
Iteration 2600: Loss = -11887.001206263123
Iteration 2700: Loss = -11886.997451178624
Iteration 2800: Loss = -11886.994054914601
Iteration 2900: Loss = -11886.990966763155
Iteration 3000: Loss = -11886.990104048231
Iteration 3100: Loss = -11886.98558882467
Iteration 3200: Loss = -11886.98316118814
Iteration 3300: Loss = -11886.981561678742
Iteration 3400: Loss = -11886.978877962327
Iteration 3500: Loss = -11886.97691300969
Iteration 3600: Loss = -11886.97472634255
Iteration 3700: Loss = -11886.974107711365
Iteration 3800: Loss = -11886.856868579102
Iteration 3900: Loss = -11886.859454926807
1
Iteration 4000: Loss = -11886.853732778742
Iteration 4100: Loss = -11886.85261059873
Iteration 4200: Loss = -11886.851496582394
Iteration 4300: Loss = -11886.870057386435
1
Iteration 4400: Loss = -11886.84936038025
Iteration 4500: Loss = -11886.85178518751
1
Iteration 4600: Loss = -11886.847447195998
Iteration 4700: Loss = -11886.847082066124
Iteration 4800: Loss = -11886.845954304925
Iteration 4900: Loss = -11886.845386045285
Iteration 5000: Loss = -11886.844840511783
Iteration 5100: Loss = -11886.854852202596
1
Iteration 5200: Loss = -11886.843595039009
Iteration 5300: Loss = -11886.843045242382
Iteration 5400: Loss = -11886.846715018137
1
Iteration 5500: Loss = -11886.842118752334
Iteration 5600: Loss = -11886.842352948064
1
Iteration 5700: Loss = -11886.843634133718
2
Iteration 5800: Loss = -11886.841980141606
Iteration 5900: Loss = -11886.89670203672
1
Iteration 6000: Loss = -11886.845723314882
2
Iteration 6100: Loss = -11886.83997773055
Iteration 6200: Loss = -11886.839686281883
Iteration 6300: Loss = -11886.842108170955
1
Iteration 6400: Loss = -11886.85097202487
2
Iteration 6500: Loss = -11886.8428801327
3
Iteration 6600: Loss = -11886.83877383262
Iteration 6700: Loss = -11886.843537234941
1
Iteration 6800: Loss = -11886.842281186784
2
Iteration 6900: Loss = -11886.855709364108
3
Iteration 7000: Loss = -11886.837981555127
Iteration 7100: Loss = -11886.841616189531
1
Iteration 7200: Loss = -11886.837594696457
Iteration 7300: Loss = -11886.857355368516
1
Iteration 7400: Loss = -11886.838013738587
2
Iteration 7500: Loss = -11886.838423814377
3
Iteration 7600: Loss = -11886.837576648559
Iteration 7700: Loss = -11886.844857649683
1
Iteration 7800: Loss = -11886.836792273827
Iteration 7900: Loss = -11886.838433997475
1
Iteration 8000: Loss = -11886.846517004746
2
Iteration 8100: Loss = -11886.861299059516
3
Iteration 8200: Loss = -11886.852891611448
4
Iteration 8300: Loss = -11886.838789568612
5
Iteration 8400: Loss = -11886.841580803679
6
Iteration 8500: Loss = -11886.836367783184
Iteration 8600: Loss = -11886.843607050085
1
Iteration 8700: Loss = -11886.836023449116
Iteration 8800: Loss = -11886.83643461157
1
Iteration 8900: Loss = -11886.835883100333
Iteration 9000: Loss = -11886.840455390942
1
Iteration 9100: Loss = -11886.83574549769
Iteration 9200: Loss = -11886.837058369858
1
Iteration 9300: Loss = -11886.838453886372
2
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 34%|      | 34/100 [20:23:17<34:00:30, 1855.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
