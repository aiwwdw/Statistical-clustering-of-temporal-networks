nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [51:27<84:54:38, 3087.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|         | 2/100 [1:28:30<70:12:33, 2579.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|         | 3/100 [2:11:06<69:12:11, 2568.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|         | 4/100 [2:47:00<64:07:33, 2404.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|         | 5/100 [3:26:04<62:53:11, 2383.06s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|         | 6/100 [4:13:10<66:09:17, 2533.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|         | 7/100 [5:02:45<69:10:32, 2677.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|         | 8/100 [5:37:15<63:29:19, 2484.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|         | 9/100 [6:26:26<66:29:10, 2630.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|         | 10/100 [7:08:09<64:46:35, 2591.06s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-----------------------------------------------------------------------------------------
This iteration is 0
True Objective function: Loss = -9930.763321721859
Iteration 0: Loss = -28724.6378060566
Iteration 10: Loss = -9784.942109696502
Iteration 20: Loss = -9784.813386787973
Iteration 30: Loss = -9784.796696466596
Iteration 40: Loss = -9784.794138184854
Iteration 50: Loss = -9784.796537951672
1
Iteration 60: Loss = -9784.799492936163
2
Iteration 70: Loss = -9784.8018267329
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.0985, 0.9015],
        [0.0518, 0.9482]], dtype=torch.float64)
alpha: tensor([0.0549, 0.9451])
beta: tensor([[[0.1912, 0.1746],
         [0.4206, 0.1285]],

        [[0.4871, 0.1123],
         [0.6365, 0.8072]],

        [[0.2022, 0.1867],
         [0.9068, 0.8086]],

        [[0.8918, 0.2126],
         [0.3151, 0.0207]],

        [[0.4326, 0.1858],
         [0.3913, 0.2526]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.012378759859606748
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: 0.0032003860414685976
Average Adjusted Rand Index: 0.0021007570582299772
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28716.849802948305
Iteration 100: Loss = -9794.98941213513
Iteration 200: Loss = -9792.512883000645
Iteration 300: Loss = -9791.392161043532
Iteration 400: Loss = -9790.792641829397
Iteration 500: Loss = -9790.434357455659
Iteration 600: Loss = -9790.203186575704
Iteration 700: Loss = -9790.042500774693
Iteration 800: Loss = -9789.924563101094
Iteration 900: Loss = -9789.834715762143
Iteration 1000: Loss = -9789.763497507693
Iteration 1100: Loss = -9789.704121828145
Iteration 1200: Loss = -9789.653173003479
Iteration 1300: Loss = -9789.604748553014
Iteration 1400: Loss = -9789.555937705783
Iteration 1500: Loss = -9789.487759666237
Iteration 1600: Loss = -9788.956855586754
Iteration 1700: Loss = -9787.80292854792
Iteration 1800: Loss = -9787.583923734926
Iteration 1900: Loss = -9787.417259576328
Iteration 2000: Loss = -9787.26346133742
Iteration 2100: Loss = -9787.092501750483
Iteration 2200: Loss = -9786.910400507057
Iteration 2300: Loss = -9786.746715363546
Iteration 2400: Loss = -9786.527005462694
Iteration 2500: Loss = -9782.579460216204
Iteration 2600: Loss = -9779.918689239228
Iteration 2700: Loss = -9779.69224485944
Iteration 2800: Loss = -9779.573094102658
Iteration 2900: Loss = -9779.480442243113
Iteration 3000: Loss = -9779.404413680793
Iteration 3100: Loss = -9779.340473324979
Iteration 3200: Loss = -9779.286553223075
Iteration 3300: Loss = -9779.240397019252
Iteration 3400: Loss = -9779.200497891285
Iteration 3500: Loss = -9779.165749434536
Iteration 3600: Loss = -9779.135375480253
Iteration 3700: Loss = -9779.108516707804
Iteration 3800: Loss = -9779.084666548419
Iteration 3900: Loss = -9779.063543789669
Iteration 4000: Loss = -9779.04460355751
Iteration 4100: Loss = -9779.027586172071
Iteration 4200: Loss = -9779.012321413817
Iteration 4300: Loss = -9778.99854109038
Iteration 4400: Loss = -9778.98610295087
Iteration 4500: Loss = -9778.974751851503
Iteration 4600: Loss = -9778.964467989283
Iteration 4700: Loss = -9778.955023572425
Iteration 4800: Loss = -9778.946384247693
Iteration 4900: Loss = -9778.938521076012
Iteration 5000: Loss = -9778.931249664023
Iteration 5100: Loss = -9778.924602622987
Iteration 5200: Loss = -9778.918380394267
Iteration 5300: Loss = -9778.912648329862
Iteration 5400: Loss = -9778.907385562012
Iteration 5500: Loss = -9778.902468930279
Iteration 5600: Loss = -9778.897909155576
Iteration 5700: Loss = -9778.89366288019
Iteration 5800: Loss = -9778.88975711961
Iteration 5900: Loss = -9778.886076613868
Iteration 6000: Loss = -9778.882690206294
Iteration 6100: Loss = -9778.879531517967
Iteration 6200: Loss = -9778.8765549866
Iteration 6300: Loss = -9778.873764057033
Iteration 6400: Loss = -9778.87115980261
Iteration 6500: Loss = -9778.868751243292
Iteration 6600: Loss = -9778.866459331557
Iteration 6700: Loss = -9778.864325249782
Iteration 6800: Loss = -9778.862349960567
Iteration 6900: Loss = -9778.860446446171
Iteration 7000: Loss = -9778.935938472
1
Iteration 7100: Loss = -9778.857022854192
Iteration 7200: Loss = -9778.855440396903
Iteration 7300: Loss = -9778.85397396297
Iteration 7400: Loss = -9778.852742143796
Iteration 7500: Loss = -9778.85126377039
Iteration 7600: Loss = -9778.850054369517
Iteration 7700: Loss = -9778.862135037747
1
Iteration 7800: Loss = -9778.847785737444
Iteration 7900: Loss = -9778.84672628087
Iteration 8000: Loss = -9778.845783634282
Iteration 8100: Loss = -9778.844908093144
Iteration 8200: Loss = -9778.843959048583
Iteration 8300: Loss = -9778.843118223986
Iteration 8400: Loss = -9778.843637721957
1
Iteration 8500: Loss = -9778.841620062376
Iteration 8600: Loss = -9778.840894544783
Iteration 8700: Loss = -9778.841797125413
1
Iteration 8800: Loss = -9778.839600884234
Iteration 8900: Loss = -9778.839029163008
Iteration 9000: Loss = -9778.83844346029
Iteration 9100: Loss = -9778.876322061098
1
Iteration 9200: Loss = -9778.837408807944
Iteration 9300: Loss = -9778.836943098948
Iteration 9400: Loss = -9778.83650446343
Iteration 9500: Loss = -9778.8367115187
1
Iteration 9600: Loss = -9778.835703678466
Iteration 9700: Loss = -9778.835288125187
Iteration 9800: Loss = -9778.83491632083
Iteration 9900: Loss = -9778.834543966752
Iteration 10000: Loss = -9778.864331292483
1
Iteration 10100: Loss = -9778.833949102192
Iteration 10200: Loss = -9778.83364887218
Iteration 10300: Loss = -9778.833352516485
Iteration 10400: Loss = -9778.860531351631
1
Iteration 10500: Loss = -9778.832861642546
Iteration 10600: Loss = -9778.83264653865
Iteration 10700: Loss = -9778.833246444794
1
Iteration 10800: Loss = -9778.83223917849
Iteration 10900: Loss = -9778.83196402268
Iteration 11000: Loss = -9778.831792875593
Iteration 11100: Loss = -9778.831747669099
Iteration 11200: Loss = -9778.831431548315
Iteration 11300: Loss = -9778.831291452108
Iteration 11400: Loss = -9778.831369374935
1
Iteration 11500: Loss = -9778.83097167023
Iteration 11600: Loss = -9778.830801488795
Iteration 11700: Loss = -9778.839661742459
1
Iteration 11800: Loss = -9778.830546701945
Iteration 11900: Loss = -9778.830408881513
Iteration 12000: Loss = -9778.832772912436
1
Iteration 12100: Loss = -9778.830230812076
Iteration 12200: Loss = -9778.830128275962
Iteration 12300: Loss = -9778.829991500405
Iteration 12400: Loss = -9778.829933294943
Iteration 12500: Loss = -9778.829814625216
Iteration 12600: Loss = -9778.829750732171
Iteration 12700: Loss = -9778.829656272677
Iteration 12800: Loss = -9778.829564602886
Iteration 12900: Loss = -9778.829559292963
Iteration 13000: Loss = -9778.829466003626
Iteration 13100: Loss = -9778.829392304777
Iteration 13200: Loss = -9778.829659850113
1
Iteration 13300: Loss = -9778.829259454425
Iteration 13400: Loss = -9778.82918609987
Iteration 13500: Loss = -9778.829807720655
1
Iteration 13600: Loss = -9778.829076943644
Iteration 13700: Loss = -9778.829048917487
Iteration 13800: Loss = -9778.830639456957
1
Iteration 13900: Loss = -9778.829008801044
Iteration 14000: Loss = -9778.828918058913
Iteration 14100: Loss = -9778.828877399008
Iteration 14200: Loss = -9778.829537203397
1
Iteration 14300: Loss = -9778.828825626084
Iteration 14400: Loss = -9778.828761952984
Iteration 14500: Loss = -9778.82873529067
Iteration 14600: Loss = -9778.828891279669
1
Iteration 14700: Loss = -9778.828684734133
Iteration 14800: Loss = -9778.828662099188
Iteration 14900: Loss = -9778.828625283892
Iteration 15000: Loss = -9778.95346634595
1
Iteration 15100: Loss = -9778.828609009593
Iteration 15200: Loss = -9778.828617734502
1
Iteration 15300: Loss = -9778.828536877396
Iteration 15400: Loss = -9778.947888780656
1
Iteration 15500: Loss = -9778.828506813972
Iteration 15600: Loss = -9778.828488998315
Iteration 15700: Loss = -9778.829123347592
1
Iteration 15800: Loss = -9778.828466536257
Iteration 15900: Loss = -9778.832527291794
1
Iteration 16000: Loss = -9778.82842961444
Iteration 16100: Loss = -9778.847016772726
1
Iteration 16200: Loss = -9778.828396917464
Iteration 16300: Loss = -9778.831724015401
1
Iteration 16400: Loss = -9778.828378863085
Iteration 16500: Loss = -9778.828654251021
1
Iteration 16600: Loss = -9778.82841479342
2
Iteration 16700: Loss = -9778.828329403814
Iteration 16800: Loss = -9778.828336141627
1
Iteration 16900: Loss = -9778.828298021883
Iteration 17000: Loss = -9778.833588760719
1
Iteration 17100: Loss = -9778.828292873648
Iteration 17200: Loss = -9778.828275891463
Iteration 17300: Loss = -9778.828665558312
1
Iteration 17400: Loss = -9778.828382794356
2
Iteration 17500: Loss = -9778.828287481567
3
Iteration 17600: Loss = -9778.829680516845
4
Iteration 17700: Loss = -9778.82850692882
5
Iteration 17800: Loss = -9778.828217059241
Iteration 17900: Loss = -9778.828657080147
1
Iteration 18000: Loss = -9778.828221883306
2
Iteration 18100: Loss = -9778.828624216105
3
Iteration 18200: Loss = -9778.828201552498
Iteration 18300: Loss = -9778.82820425466
1
Iteration 18400: Loss = -9778.82829024092
2
Iteration 18500: Loss = -9778.828182781233
Iteration 18600: Loss = -9778.828176567447
Iteration 18700: Loss = -9778.828259355163
1
Iteration 18800: Loss = -9778.828180616925
2
Iteration 18900: Loss = -9778.849579983094
3
Iteration 19000: Loss = -9778.82834903983
4
Iteration 19100: Loss = -9778.828511126063
5
Iteration 19200: Loss = -9778.828174887445
Iteration 19300: Loss = -9778.828269378568
1
Iteration 19400: Loss = -9778.828442007565
2
Iteration 19500: Loss = -9778.828208548384
3
Iteration 19600: Loss = -9778.832342014888
4
Iteration 19700: Loss = -9778.828168314076
Iteration 19800: Loss = -9778.829471455296
1
Iteration 19900: Loss = -9778.82842229916
2
tensor([[-10.1668,   5.5516],
        [-13.3746,   8.7593],
        [-10.4849,   5.8696],
        [-11.3852,   6.7700],
        [-13.0043,   8.3890],
        [-12.0204,   7.4052],
        [-11.2813,   6.6661],
        [-10.8680,   6.2528],
        [-12.9989,   8.3837],
        [-11.3988,   6.7835],
        [-10.6197,   6.0045],
        [-12.0903,   7.4750],
        [-12.9727,   8.3575],
        [-12.0552,   7.4400],
        [ -1.1089,  -3.5063],
        [-11.9653,   7.3501],
        [-12.9873,   8.3721],
        [-11.5259,   6.9107],
        [-11.3758,   6.7606],
        [-10.9959,   6.3806],
        [-11.3728,   6.7575],
        [-11.0670,   6.4518],
        [ -4.5687,  -0.0465],
        [-13.2664,   8.6511],
        [-10.8635,   6.2483],
        [-11.3292,   6.7140],
        [-11.5138,   6.8986],
        [-12.5352,   7.9200],
        [-12.9564,   8.3411],
        [-10.6278,   6.0126],
        [-10.6642,   6.0490],
        [-11.3628,   6.7476],
        [-10.7605,   6.1453],
        [-12.8055,   8.1903],
        [-10.6123,   5.9971],
        [  3.5523,  -8.1675],
        [ -6.2889,   1.6737],
        [-11.5123,   6.8971],
        [-11.2983,   6.6831],
        [-11.2879,   6.6726],
        [-12.5065,   7.8913],
        [-13.0208,   8.4056],
        [ -6.5345,   1.9193],
        [-12.5587,   7.9434],
        [-10.5959,   5.9807],
        [-10.5450,   5.9297],
        [-11.8890,   7.2738],
        [-10.9571,   6.3419],
        [-12.4930,   7.8777],
        [-12.8409,   8.2257],
        [-10.5090,   5.8938],
        [-12.2106,   7.5954],
        [-11.5819,   6.9667],
        [-10.7297,   6.1145],
        [-11.2974,   6.6822],
        [-10.6895,   6.0743],
        [ -9.7404,   5.1252],
        [ -8.8449,   4.2297],
        [-12.6402,   8.0249],
        [-10.7631,   6.1479],
        [-11.0172,   6.4019],
        [-12.4881,   7.8729],
        [-12.9752,   8.3600],
        [-10.9483,   6.3331],
        [-10.0515,   5.4362],
        [-13.3158,   8.7006],
        [-11.3981,   6.7829],
        [-10.9420,   6.3268],
        [-12.6686,   8.0534],
        [-12.3991,   7.7839],
        [-10.0985,   5.4833],
        [-10.7210,   6.1058],
        [-10.7343,   6.1191],
        [-11.5589,   6.9437],
        [-11.3366,   6.7214],
        [-12.4423,   7.8271],
        [-12.4199,   7.8047],
        [-13.1529,   8.5376],
        [-11.3936,   6.7784],
        [-10.9031,   6.2879],
        [-12.4642,   7.8490],
        [-10.6906,   6.0754],
        [-11.1365,   6.5213],
        [-10.7800,   6.1648],
        [-11.9438,   7.3285],
        [-10.0881,   5.4729],
        [-12.4226,   7.8074],
        [-11.3512,   6.7359],
        [-10.0047,   5.3894],
        [-11.4780,   6.8628],
        [-11.8529,   7.2377],
        [-11.4276,   6.8124],
        [-10.6483,   6.0331],
        [-10.1650,   5.5498],
        [-13.0069,   8.3917],
        [-10.6070,   5.9918],
        [-13.0196,   8.4044],
        [-11.2985,   6.6833],
        [-10.8434,   6.2282],
        [-10.6196,   6.0044]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.6850e-07],
        [1.4257e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0194, 0.9806], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[7.0585e-06, 2.2028e-01],
         [4.2064e-01, 1.3279e-01]],

        [[4.8706e-01, 7.2230e-02],
         [6.3650e-01, 8.0715e-01]],

        [[2.0220e-01, 2.0494e-01],
         [9.0678e-01, 8.0855e-01]],

        [[8.9183e-01, 2.2091e-01],
         [3.1508e-01, 2.0748e-02]],

        [[4.3261e-01, 1.7875e-01],
         [3.9126e-01, 2.5260e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: -0.013574768645372375
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.0028941180812326324
Average Adjusted Rand Index: -0.0047312454790009355
Iteration 0: Loss = -36688.67850819989
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6773,    nan]],

        [[0.7844,    nan],
         [0.2557, 0.7564]],

        [[0.2710,    nan],
         [0.5932, 0.9157]],

        [[0.8179,    nan],
         [0.8881, 0.9769]],

        [[0.8642,    nan],
         [0.2699, 0.5486]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36686.929023233766
Iteration 100: Loss = -9817.660014881112
Iteration 200: Loss = -9802.04861489211
Iteration 300: Loss = -9795.203580655483
Iteration 400: Loss = -9793.397042675759
Iteration 500: Loss = -9792.324117132353
Iteration 600: Loss = -9791.633636817256
Iteration 700: Loss = -9791.164589266818
Iteration 800: Loss = -9790.833945653812
Iteration 900: Loss = -9790.594389637956
Iteration 1000: Loss = -9790.416268534775
Iteration 1100: Loss = -9790.281179396728
Iteration 1200: Loss = -9790.176997574552
Iteration 1300: Loss = -9790.094120529948
Iteration 1400: Loss = -9790.026667891942
Iteration 1500: Loss = -9789.970589861316
Iteration 1600: Loss = -9789.923089982736
Iteration 1700: Loss = -9789.882308009799
Iteration 1800: Loss = -9789.847040320557
Iteration 1900: Loss = -9789.816218919723
Iteration 2000: Loss = -9789.789055207146
Iteration 2100: Loss = -9789.76480680643
Iteration 2200: Loss = -9789.742960589523
Iteration 2300: Loss = -9789.72302558754
Iteration 2400: Loss = -9789.70464559532
Iteration 2500: Loss = -9789.687526227195
Iteration 2600: Loss = -9789.67126844095
Iteration 2700: Loss = -9789.655516292025
Iteration 2800: Loss = -9789.63994362917
Iteration 2900: Loss = -9789.62388042754
Iteration 3000: Loss = -9789.606068132236
Iteration 3100: Loss = -9789.583200739946
Iteration 3200: Loss = -9789.542276735348
Iteration 3300: Loss = -9789.437390054058
Iteration 3400: Loss = -9789.21674909008
Iteration 3500: Loss = -9788.606830848272
Iteration 3600: Loss = -9788.324813670428
Iteration 3700: Loss = -9788.118438697902
Iteration 3800: Loss = -9787.980224846862
Iteration 3900: Loss = -9787.194088236673
Iteration 4000: Loss = -9786.857068369407
Iteration 4100: Loss = -9786.728948655404
Iteration 4200: Loss = -9786.637597453775
Iteration 4300: Loss = -9786.583496671119
Iteration 4400: Loss = -9786.502330603895
Iteration 4500: Loss = -9786.446360996579
Iteration 4600: Loss = -9786.399820018269
Iteration 4700: Loss = -9786.361213813665
Iteration 4800: Loss = -9786.325902783996
Iteration 4900: Loss = -9786.295227400615
Iteration 5000: Loss = -9786.479057797354
1
Iteration 5100: Loss = -9786.241510892702
Iteration 5200: Loss = -9786.216895962578
Iteration 5300: Loss = -9786.191847583537
Iteration 5400: Loss = -9786.166003279184
Iteration 5500: Loss = -9786.13985612727
Iteration 5600: Loss = -9786.116196973318
Iteration 5700: Loss = -9786.09307554476
Iteration 5800: Loss = -9786.06891576241
Iteration 5900: Loss = -9786.044230166279
Iteration 6000: Loss = -9786.018277668665
Iteration 6100: Loss = -9786.038570324055
1
Iteration 6200: Loss = -9785.961195445432
Iteration 6300: Loss = -9785.92937989199
Iteration 6400: Loss = -9785.894982085985
Iteration 6500: Loss = -9785.859070782684
Iteration 6600: Loss = -9785.8174927491
Iteration 6700: Loss = -9785.77204925156
Iteration 6800: Loss = -9785.700611802868
Iteration 6900: Loss = -9785.602510871046
Iteration 7000: Loss = -9785.33171746667
Iteration 7100: Loss = -9785.2449785945
Iteration 7200: Loss = -9785.192022713585
Iteration 7300: Loss = -9785.245785759706
1
Iteration 7400: Loss = -9785.095969393813
Iteration 7500: Loss = -9785.06469517542
Iteration 7600: Loss = -9785.03889473583
Iteration 7700: Loss = -9785.018293784033
Iteration 7800: Loss = -9784.993920894914
Iteration 7900: Loss = -9784.963049223274
Iteration 8000: Loss = -9784.934598565838
Iteration 8100: Loss = -9784.9213888693
Iteration 8200: Loss = -9784.910591915623
Iteration 8300: Loss = -9784.901748312079
Iteration 8400: Loss = -9784.894655866216
Iteration 8500: Loss = -9784.879555732265
Iteration 8600: Loss = -9784.87127391443
Iteration 8700: Loss = -9784.870188998813
Iteration 8800: Loss = -9784.866332375666
Iteration 8900: Loss = -9784.864966109892
Iteration 9000: Loss = -9784.872858234363
1
Iteration 9100: Loss = -9784.862380581659
Iteration 9200: Loss = -9784.860673148734
Iteration 9300: Loss = -9785.154587717096
1
Iteration 9400: Loss = -9784.858250418258
Iteration 9500: Loss = -9784.857439446409
Iteration 9600: Loss = -9784.856874965724
Iteration 9700: Loss = -9784.856206430175
Iteration 9800: Loss = -9784.854774710815
Iteration 9900: Loss = -9784.85340880381
Iteration 10000: Loss = -9784.856066484252
1
Iteration 10100: Loss = -9784.852943328417
Iteration 10200: Loss = -9784.85279431138
Iteration 10300: Loss = -9784.96968070269
1
Iteration 10400: Loss = -9784.852428693905
Iteration 10500: Loss = -9784.852254921892
Iteration 10600: Loss = -9784.95608327742
1
Iteration 10700: Loss = -9784.851952229057
Iteration 10800: Loss = -9784.85535192363
1
Iteration 10900: Loss = -9784.851569833912
Iteration 11000: Loss = -9785.09936841243
1
Iteration 11100: Loss = -9784.849762209193
Iteration 11200: Loss = -9784.8515547836
1
Iteration 11300: Loss = -9784.849579557607
Iteration 11400: Loss = -9784.849897835466
1
Iteration 11500: Loss = -9784.92677346291
2
Iteration 11600: Loss = -9784.84942776382
Iteration 11700: Loss = -9784.857636307364
1
Iteration 11800: Loss = -9784.849292777386
Iteration 11900: Loss = -9784.849249967034
Iteration 12000: Loss = -9784.84919356868
Iteration 12100: Loss = -9784.84908729598
Iteration 12200: Loss = -9784.849044631133
Iteration 12300: Loss = -9784.849031665526
Iteration 12400: Loss = -9784.84897944614
Iteration 12500: Loss = -9784.848971368407
Iteration 12600: Loss = -9784.852635435964
1
Iteration 12700: Loss = -9784.848935537
Iteration 12800: Loss = -9784.848899870076
Iteration 12900: Loss = -9784.848911244979
1
Iteration 13000: Loss = -9784.849136978231
2
Iteration 13100: Loss = -9784.861377097583
3
Iteration 13200: Loss = -9784.848842692476
Iteration 13300: Loss = -9784.849733737412
1
Iteration 13400: Loss = -9784.8489834602
2
Iteration 13500: Loss = -9784.848920509114
3
Iteration 13600: Loss = -9784.851104904108
4
Iteration 13700: Loss = -9784.848803109699
Iteration 13800: Loss = -9784.850773217966
1
Iteration 13900: Loss = -9784.855827294718
2
Iteration 14000: Loss = -9784.849943004509
3
Iteration 14100: Loss = -9784.8488881723
4
Iteration 14200: Loss = -9784.850351315814
5
Iteration 14300: Loss = -9784.861613777686
6
Iteration 14400: Loss = -9784.848654112438
Iteration 14500: Loss = -9784.848742495575
1
Iteration 14600: Loss = -9784.854705200374
2
Iteration 14700: Loss = -9784.914601482476
3
Iteration 14800: Loss = -9784.848628540956
Iteration 14900: Loss = -9784.848653341714
1
Iteration 15000: Loss = -9784.849882091616
2
Iteration 15100: Loss = -9784.848621333742
Iteration 15200: Loss = -9784.849243275847
1
Iteration 15300: Loss = -9784.84988369793
2
Iteration 15400: Loss = -9784.848744102937
3
Iteration 15500: Loss = -9784.848701677254
4
Iteration 15600: Loss = -9784.851045738866
5
Iteration 15700: Loss = -9784.851579301892
6
Iteration 15800: Loss = -9784.87442612459
7
Iteration 15900: Loss = -9784.848551117346
Iteration 16000: Loss = -9784.855313424116
1
Iteration 16100: Loss = -9784.850923779573
2
Iteration 16200: Loss = -9784.853108779793
3
Iteration 16300: Loss = -9784.8798604617
4
Iteration 16400: Loss = -9784.861358507012
5
Iteration 16500: Loss = -9784.850695472986
6
Iteration 16600: Loss = -9784.850724409585
7
Iteration 16700: Loss = -9784.913817826828
8
Iteration 16800: Loss = -9784.858775558316
9
Iteration 16900: Loss = -9784.84863286151
10
Stopping early at iteration 16900 due to no improvement.
tensor([[ 5.4432, -8.5354],
        [ 5.5157, -7.6630],
        [ 5.4057, -7.6929],
        [ 5.7614, -7.1763],
        [ 5.5518, -7.3892],
        [ 5.7870, -7.1787],
        [ 4.8813, -8.1591],
        [ 5.9963, -7.4629],
        [ 5.5770, -7.2462],
        [ 5.3804, -7.8703],
        [ 5.7744, -7.1787],
        [ 5.6063, -7.0113],
        [ 5.7086, -7.0956],
        [ 5.5591, -7.2417],
        [ 6.0456, -7.7296],
        [ 5.3015, -7.8819],
        [ 5.0400, -6.5086],
        [ 5.5686, -7.2108],
        [ 5.3927, -7.1151],
        [ 5.5169, -7.4638],
        [ 5.5824, -7.0062],
        [ 4.6479, -7.5388],
        [ 5.1055, -7.8248],
        [ 5.6450, -7.3115],
        [ 5.5192, -7.0055],
        [ 5.6804, -7.0714],
        [ 5.1378, -7.2034],
        [ 5.2471, -6.7608],
        [ 5.5854, -7.1827],
        [ 5.6731, -7.9342],
        [ 5.3416, -7.4824],
        [ 5.4052, -6.9956],
        [ 5.3824, -7.7215],
        [ 5.2980, -6.7667],
        [ 5.2576, -7.6012],
        [ 6.7270, -8.1142],
        [ 5.2606, -6.6475],
        [ 4.6118, -7.2262],
        [ 4.7636, -6.2558],
        [ 5.5616, -6.9571],
        [ 5.2561, -6.8211],
        [ 5.6636, -7.2226],
        [ 4.4891, -6.0226],
        [ 5.6172, -7.0843],
        [ 5.6225, -7.1726],
        [ 5.6266, -7.1605],
        [ 5.7235, -7.1916],
        [ 4.7704, -6.3416],
        [ 4.5845, -9.0579],
        [ 5.8026, -7.5136],
        [ 5.4308, -7.5632],
        [ 5.3941, -6.8057],
        [ 5.6979, -7.1777],
        [ 5.1981, -7.8280],
        [ 5.3997, -7.2649],
        [ 5.6067, -7.1667],
        [ 5.5236, -7.3876],
        [ 5.5422, -6.9851],
        [ 5.6226, -7.0586],
        [ 5.3769, -7.4017],
        [ 5.8779, -7.3608],
        [ 5.3490, -6.8618],
        [ 5.2746, -6.8854],
        [ 5.4935, -7.1868],
        [ 4.0856, -8.7009],
        [ 5.6899, -7.2425],
        [ 4.9486, -7.4646],
        [ 5.3258, -7.0024],
        [ 5.8755, -7.5697],
        [ 3.9681, -8.5833],
        [ 5.9674, -7.4070],
        [ 4.9822, -7.8521],
        [ 5.6600, -7.1282],
        [ 4.9148, -7.9830],
        [ 4.9441, -7.9753],
        [ 5.2597, -6.7439],
        [ 5.9321, -7.3345],
        [ 4.8513, -7.7393],
        [ 4.8260, -7.8229],
        [ 5.5551, -6.9827],
        [ 5.5991, -7.0277],
        [ 5.5808, -7.0136],
        [ 6.0625, -7.4585],
        [ 4.6140, -8.1147],
        [ 5.5679, -7.0866],
        [ 5.1888, -6.8460],
        [ 5.4472, -7.8364],
        [ 4.7958, -7.9564],
        [ 5.7850, -7.7114],
        [ 5.0817, -7.7932],
        [ 5.6330, -7.1271],
        [ 5.4710, -7.6401],
        [ 5.2361, -7.8235],
        [ 5.4441, -8.3239],
        [ 5.4841, -6.8789],
        [ 5.8306, -7.2454],
        [ 4.7677, -7.8656],
        [ 5.8159, -7.4556],
        [ 4.2559, -5.9324],
        [ 4.6142, -8.4500]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9770, 0.0230],
        [0.4977, 0.5023]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 3.8560e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1323, 0.1179],
         [0.6773, 0.1551]],

        [[0.7844, 0.0819],
         [0.2557, 0.7564]],

        [[0.2710, 0.1965],
         [0.5932, 0.9157]],

        [[0.8179, 0.2242],
         [0.8881, 0.9769]],

        [[0.8642, 0.1899],
         [0.2699, 0.5486]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: -0.013574768645372375
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: -0.004226893610597426
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: 0.00014555556650875184
Average Adjusted Rand Index: -0.002833281052803375
Iteration 0: Loss = -24897.127460303633
Iteration 10: Loss = -9789.763926907202
Iteration 20: Loss = -9789.763926907232
1
Iteration 30: Loss = -9789.763926962658
2
Iteration 40: Loss = -9789.76395091884
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[3.7620e-01, 6.2380e-01],
        [4.0305e-09, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([5.6216e-09, 1.0000e+00])
beta: tensor([[[0.1218, 0.2352],
         [0.8327, 0.1335]],

        [[0.8121, 0.0847],
         [0.8197, 0.6584]],

        [[0.5128, 0.2167],
         [0.1609, 0.7171]],

        [[0.4522, 0.2372],
         [0.7246, 0.8062]],

        [[0.9711, 0.2438],
         [0.6424, 0.4218]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24854.067160350256
Iteration 100: Loss = -9793.383253437365
Iteration 200: Loss = -9791.471681627761
Iteration 300: Loss = -9790.696718007533
Iteration 400: Loss = -9790.260278398251
Iteration 500: Loss = -9789.945614175065
Iteration 600: Loss = -9789.64434352866
Iteration 700: Loss = -9789.206337834232
Iteration 800: Loss = -9788.583964823747
Iteration 900: Loss = -9787.994259814994
Iteration 1000: Loss = -9787.327294415285
Iteration 1100: Loss = -9786.635253616556
Iteration 1200: Loss = -9786.174555752248
Iteration 1300: Loss = -9783.675401985494
Iteration 1400: Loss = -9783.013297322746
Iteration 1500: Loss = -9782.548336033738
Iteration 1600: Loss = -9782.232501859891
Iteration 1700: Loss = -9781.950160516195
Iteration 1800: Loss = -9781.678630524148
Iteration 1900: Loss = -9781.415594995307
Iteration 2000: Loss = -9781.149990152755
Iteration 2100: Loss = -9780.932019950324
Iteration 2200: Loss = -9780.752699419834
Iteration 2300: Loss = -9780.59902042457
Iteration 2400: Loss = -9780.465410006384
Iteration 2500: Loss = -9780.345673965645
Iteration 2600: Loss = -9780.235333263587
Iteration 2700: Loss = -9780.130898257923
Iteration 2800: Loss = -9780.02812326797
Iteration 2900: Loss = -9779.934646730288
Iteration 3000: Loss = -9779.851549835385
Iteration 3100: Loss = -9779.774772475892
Iteration 3200: Loss = -9779.70376490467
Iteration 3300: Loss = -9779.638455781942
Iteration 3400: Loss = -9779.578591376467
Iteration 3500: Loss = -9779.52365169295
Iteration 3600: Loss = -9779.47319860045
Iteration 3700: Loss = -9779.426863168626
Iteration 3800: Loss = -9779.384322916314
Iteration 3900: Loss = -9779.345398950562
Iteration 4000: Loss = -9779.309616864286
Iteration 4100: Loss = -9779.276798853427
Iteration 4200: Loss = -9779.246632530405
Iteration 4300: Loss = -9779.218910125315
Iteration 4400: Loss = -9779.19334180028
Iteration 4500: Loss = -9779.169759071192
Iteration 4600: Loss = -9779.14798465122
Iteration 4700: Loss = -9779.127879337031
Iteration 4800: Loss = -9779.109260093484
Iteration 4900: Loss = -9779.092018451349
Iteration 5000: Loss = -9779.076047320203
Iteration 5100: Loss = -9779.061198399379
Iteration 5200: Loss = -9779.04738915339
Iteration 5300: Loss = -9779.034574640089
Iteration 5400: Loss = -9779.022613594485
Iteration 5500: Loss = -9779.01152756609
Iteration 5600: Loss = -9779.001118761387
Iteration 5700: Loss = -9778.991461737
Iteration 5800: Loss = -9778.982401811048
Iteration 5900: Loss = -9778.973958278146
Iteration 6000: Loss = -9778.966063303778
Iteration 6100: Loss = -9778.958694946894
Iteration 6200: Loss = -9778.95177288508
Iteration 6300: Loss = -9778.945285079226
Iteration 6400: Loss = -9778.939205438408
Iteration 6500: Loss = -9778.933516738458
Iteration 6600: Loss = -9778.92814032069
Iteration 6700: Loss = -9778.92310405104
Iteration 6800: Loss = -9778.918402918034
Iteration 6900: Loss = -9778.913927196489
Iteration 7000: Loss = -9778.909748039574
Iteration 7100: Loss = -9778.905801161633
Iteration 7200: Loss = -9778.902085662536
Iteration 7300: Loss = -9778.912292564959
1
Iteration 7400: Loss = -9778.895322598422
Iteration 7500: Loss = -9778.892189002212
Iteration 7600: Loss = -9778.889280840916
Iteration 7700: Loss = -9778.88652245501
Iteration 7800: Loss = -9778.88390265646
Iteration 7900: Loss = -9778.881455495159
Iteration 8000: Loss = -9778.879103483458
Iteration 8100: Loss = -9778.87710063049
Iteration 8200: Loss = -9778.874849308171
Iteration 8300: Loss = -9778.872928458306
Iteration 8400: Loss = -9778.871050701446
Iteration 8500: Loss = -9778.869374271071
Iteration 8600: Loss = -9778.867615708607
Iteration 8700: Loss = -9778.866071633767
Iteration 8800: Loss = -9778.868597981831
1
Iteration 8900: Loss = -9778.86318284778
Iteration 9000: Loss = -9778.861851483092
Iteration 9100: Loss = -9778.86059673046
Iteration 9200: Loss = -9778.86252111822
1
Iteration 9300: Loss = -9778.858259784603
Iteration 9400: Loss = -9778.857191358617
Iteration 9500: Loss = -9778.856199408709
Iteration 9600: Loss = -9778.856655320467
1
Iteration 9700: Loss = -9778.854335447637
Iteration 9800: Loss = -9778.853474291192
Iteration 9900: Loss = -9778.852647192025
Iteration 10000: Loss = -9778.853359973378
1
Iteration 10100: Loss = -9778.851148811827
Iteration 10200: Loss = -9778.850433801967
Iteration 10300: Loss = -9778.850105775839
Iteration 10400: Loss = -9778.84917948245
Iteration 10500: Loss = -9778.848571754714
Iteration 10600: Loss = -9778.848007109142
Iteration 10700: Loss = -9778.84769083951
Iteration 10800: Loss = -9778.846980596707
Iteration 10900: Loss = -9778.846479288062
Iteration 11000: Loss = -9778.849486752453
1
Iteration 11100: Loss = -9778.84563053485
Iteration 11200: Loss = -9778.845202199449
Iteration 11300: Loss = -9778.844792828966
Iteration 11400: Loss = -9778.964119030763
1
Iteration 11500: Loss = -9778.844062790435
Iteration 11600: Loss = -9778.843751828379
Iteration 11700: Loss = -9778.848244628689
1
Iteration 11800: Loss = -9778.843157834
Iteration 11900: Loss = -9778.889244658016
1
Iteration 12000: Loss = -9778.842588950987
Iteration 12100: Loss = -9778.842322290473
Iteration 12200: Loss = -9778.84235177028
1
Iteration 12300: Loss = -9778.841987962538
Iteration 12400: Loss = -9778.887532712632
1
Iteration 12500: Loss = -9778.841416534877
Iteration 12600: Loss = -9778.841226147833
Iteration 12700: Loss = -9778.84528016163
1
Iteration 12800: Loss = -9778.840831081705
Iteration 12900: Loss = -9778.840661094364
Iteration 13000: Loss = -9778.845221482847
1
Iteration 13100: Loss = -9778.840359504216
Iteration 13200: Loss = -9778.840221959292
Iteration 13300: Loss = -9778.96143967352
1
Iteration 13400: Loss = -9778.839917624053
Iteration 13500: Loss = -9778.83981463543
Iteration 13600: Loss = -9778.839804974215
Iteration 13700: Loss = -9778.85438130466
1
Iteration 13800: Loss = -9778.839542747137
Iteration 13900: Loss = -9778.844480470356
1
Iteration 14000: Loss = -9778.83929119786
Iteration 14100: Loss = -9778.854765186526
1
Iteration 14200: Loss = -9778.839129743204
Iteration 14300: Loss = -9779.039140901123
1
Iteration 14400: Loss = -9778.839916591656
2
Iteration 14500: Loss = -9778.845970197033
3
Iteration 14600: Loss = -9778.838806184496
Iteration 14700: Loss = -9778.839328050435
1
Iteration 14800: Loss = -9778.838729703333
Iteration 14900: Loss = -9778.838995470878
1
Iteration 15000: Loss = -9778.838683721866
Iteration 15100: Loss = -9778.839865769414
1
Iteration 15200: Loss = -9778.838466559442
Iteration 15300: Loss = -9778.842080652645
1
Iteration 15400: Loss = -9778.838352614308
Iteration 15500: Loss = -9778.843432958072
1
Iteration 15600: Loss = -9778.838272032594
Iteration 15700: Loss = -9778.858607757451
1
Iteration 15800: Loss = -9778.838206642293
Iteration 15900: Loss = -9778.874155829859
1
Iteration 16000: Loss = -9778.838121767287
Iteration 16100: Loss = -9778.838786682029
1
Iteration 16200: Loss = -9778.838095974517
Iteration 16300: Loss = -9778.83812115702
1
Iteration 16400: Loss = -9778.838144223979
2
Iteration 16500: Loss = -9778.964789931082
3
Iteration 16600: Loss = -9778.837971734834
Iteration 16700: Loss = -9778.93870206951
1
Iteration 16800: Loss = -9778.837945857817
Iteration 16900: Loss = -9778.88961358064
1
Iteration 17000: Loss = -9778.83793766898
Iteration 17100: Loss = -9778.84333375404
1
Iteration 17200: Loss = -9778.837918059686
Iteration 17300: Loss = -9778.838379886434
1
Iteration 17400: Loss = -9778.838279075255
2
Iteration 17500: Loss = -9778.897701572947
3
Iteration 17600: Loss = -9778.838298107468
4
Iteration 17700: Loss = -9778.849271053059
5
Iteration 17800: Loss = -9778.837963283811
6
Iteration 17900: Loss = -9778.83776196072
Iteration 18000: Loss = -9778.838269079146
1
Iteration 18100: Loss = -9778.837725431053
Iteration 18200: Loss = -9778.838517738179
1
Iteration 18300: Loss = -9778.837929591795
2
Iteration 18400: Loss = -9778.837765164386
3
Iteration 18500: Loss = -9778.837705643176
Iteration 18600: Loss = -9778.83770447309
Iteration 18700: Loss = -9778.838022053913
1
Iteration 18800: Loss = -9778.837747363463
2
Iteration 18900: Loss = -9779.022028964097
3
Iteration 19000: Loss = -9778.837690853654
Iteration 19100: Loss = -9778.83768897114
Iteration 19200: Loss = -9778.83764958864
Iteration 19300: Loss = -9778.837744169336
1
Iteration 19400: Loss = -9778.837686857823
2
Iteration 19500: Loss = -9779.10883144096
3
Iteration 19600: Loss = -9778.838017245407
4
Iteration 19700: Loss = -9778.837843915722
5
Iteration 19800: Loss = -9778.838076444292
6
Iteration 19900: Loss = -9778.83832966772
7
tensor([[ -8.4262,   6.8992],
        [-10.7479,   8.9863],
        [ -9.7504,   5.9767],
        [ -9.2057,   7.2220],
        [-10.2702,   8.0684],
        [-10.2299,   7.7116],
        [ -9.9514,   8.0606],
        [ -8.8802,   6.9709],
        [-10.2000,   8.4782],
        [ -9.0192,   7.6041],
        [ -9.3980,   7.8238],
        [-10.0440,   7.5127],
        [-10.0183,   8.6199],
        [ -9.7970,   8.0790],
        [  0.2859,  -2.1017],
        [-10.4146,   6.9155],
        [ -9.6368,   8.2488],
        [ -9.8258,   7.0473],
        [ -9.9374,   8.2642],
        [ -9.2446,   7.2025],
        [-10.4162,   7.9659],
        [ -8.5208,   7.0559],
        [ -9.0153,   7.6277],
        [-10.2798,   8.7284],
        [ -9.3764,   7.6275],
        [-10.5987,   8.0802],
        [ -9.1935,   7.8072],
        [ -9.6922,   7.7171],
        [-11.8665,   7.3848],
        [ -8.9845,   6.7586],
        [ -9.8296,   7.2035],
        [ -9.9961,   8.1626],
        [ -9.2468,   7.8274],
        [ -9.9879,   8.5421],
        [ -8.9821,   7.5953],
        [  4.6435,  -7.0629],
        [ -8.8437,   7.4574],
        [ -9.1595,   7.4326],
        [ -9.2601,   7.8092],
        [-10.4859,   8.7790],
        [ -9.4945,   8.0996],
        [-11.0065,   8.7390],
        [-11.1118,   7.1336],
        [ -9.7938,   7.8641],
        [ -3.9835,   2.5906],
        [-10.2276,   7.2232],
        [ -9.4802,   7.7261],
        [ -9.3335,   7.7586],
        [ -9.5447,   7.9277],
        [ -9.4709,   8.0098],
        [ -8.9785,   7.4617],
        [ -9.9271,   7.6548],
        [-11.3283,   7.9119],
        [ -8.9954,   7.5521],
        [-10.4863,   8.0940],
        [ -9.5571,   7.5692],
        [ -7.7532,   6.3034],
        [ -9.7320,   7.8164],
        [-10.5173,   8.3657],
        [ -9.7893,   8.1513],
        [ -8.7228,   7.2789],
        [ -9.9137,   7.6740],
        [ -9.8067,   8.2258],
        [ -9.3143,   7.7938],
        [ -8.4347,   7.0437],
        [-10.9229,   8.7203],
        [ -9.9812,   8.5304],
        [-10.6624,   8.6820],
        [-11.7290,   8.7734],
        [-10.9425,   8.0530],
        [ -9.3828,   6.3311],
        [-11.4143,   7.3738],
        [ -9.4224,   7.8300],
        [ -9.4193,   7.7232],
        [ -9.9361,   8.2210],
        [ -9.6362,   8.1429],
        [ -9.3911,   7.9522],
        [ -9.8112,   8.4209],
        [-10.9068,   8.7314],
        [ -9.5519,   8.1614],
        [ -9.7623,   7.6605],
        [ -9.9066,   7.7673],
        [ -9.8126,   8.4082],
        [ -9.6737,   7.3491],
        [ -9.6715,   7.6223],
        [ -6.6442,   5.1812],
        [ -9.8337,   8.4177],
        [ -9.8481,   8.4269],
        [-10.1044,   8.3250],
        [-11.1144,   6.4992],
        [ -9.3696,   7.8005],
        [ -9.7816,   8.2811],
        [ -9.3089,   7.7206],
        [-10.2114,   5.5961],
        [ -9.8825,   8.4913],
        [-11.1313,   6.5161],
        [ -9.9624,   8.2831],
        [-10.1846,   8.7536],
        [ -9.0718,   7.6854],
        [ -9.0581,   7.5807]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 6.3787e-07],
        [2.4094e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0190, 0.9810], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.5056e-05, 2.2159e-01],
         [8.3266e-01, 1.3338e-01]],

        [[8.1212e-01, 7.2331e-02],
         [8.1968e-01, 6.5842e-01]],

        [[5.1284e-01, 2.0619e-01],
         [1.6094e-01, 7.1707e-01]],

        [[4.5219e-01, 2.2154e-01],
         [7.2465e-01, 8.0618e-01]],

        [[9.7114e-01, 1.7904e-01],
         [6.4244e-01, 4.2178e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: -0.013574768645372375
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.0028941180812326324
Average Adjusted Rand Index: -0.0047312454790009355
Iteration 0: Loss = -14399.296204792234
Iteration 10: Loss = -9784.916462719262
Iteration 20: Loss = -9784.827412370696
Iteration 30: Loss = -9784.814836803751
Iteration 40: Loss = -9784.810233796687
Iteration 50: Loss = -9784.808064468265
Iteration 60: Loss = -9784.80685511271
Iteration 70: Loss = -9784.806277122765
Iteration 80: Loss = -9784.805953072291
Iteration 90: Loss = -9784.805743911136
Iteration 100: Loss = -9784.805643466909
Iteration 110: Loss = -9784.805584618458
Iteration 120: Loss = -9784.805563067364
Iteration 130: Loss = -9784.805536833312
Iteration 140: Loss = -9784.8055283738
Iteration 150: Loss = -9784.805550275843
1
Iteration 160: Loss = -9784.805490151672
Iteration 170: Loss = -9784.805484379667
Iteration 180: Loss = -9784.805514353873
1
Iteration 190: Loss = -9784.80551137973
2
Iteration 200: Loss = -9784.805518665906
3
Stopping early at iteration 199 due to no improvement.
pi: tensor([[0.0996, 0.9004],
        [0.0505, 0.9495]], dtype=torch.float64)
alpha: tensor([0.0536, 0.9464])
beta: tensor([[[0.1907, 0.1752],
         [0.6301, 0.1286]],

        [[0.7324, 0.1116],
         [0.3741, 0.7975]],

        [[0.9009, 0.1871],
         [0.3122, 0.4137]],

        [[0.3902, 0.2134],
         [0.3387, 0.7013]],

        [[0.2664, 0.1867],
         [0.8373, 0.2917]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.012378759859606748
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: 0.0032003860414685976
Average Adjusted Rand Index: 0.0021007570582299772
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14399.064587074121
Iteration 100: Loss = -9795.821668812388
Iteration 200: Loss = -9790.082876396504
Iteration 300: Loss = -9786.520217420633
Iteration 400: Loss = -9783.151695609507
Iteration 500: Loss = -9781.649821797997
Iteration 600: Loss = -9781.238469437922
Iteration 700: Loss = -9781.00007688849
Iteration 800: Loss = -9780.873984867101
Iteration 900: Loss = -9780.790289116885
Iteration 1000: Loss = -9780.73873489827
Iteration 1100: Loss = -9780.70279746
Iteration 1200: Loss = -9780.676070819194
Iteration 1300: Loss = -9780.655340056393
Iteration 1400: Loss = -9780.638777183065
Iteration 1500: Loss = -9780.6250729544
Iteration 1600: Loss = -9780.613402342988
Iteration 1700: Loss = -9780.603822702979
Iteration 1800: Loss = -9780.596432391567
Iteration 1900: Loss = -9780.59037073794
Iteration 2000: Loss = -9780.585258083682
Iteration 2100: Loss = -9780.580907807403
Iteration 2200: Loss = -9780.57725787713
Iteration 2300: Loss = -9780.574201422698
Iteration 2400: Loss = -9780.571640161856
Iteration 2500: Loss = -9780.56939632531
Iteration 2600: Loss = -9780.567400222015
Iteration 2700: Loss = -9780.56552766004
Iteration 2800: Loss = -9780.563679853934
Iteration 2900: Loss = -9780.561752285646
Iteration 3000: Loss = -9780.5597118015
Iteration 3100: Loss = -9780.557651914596
Iteration 3200: Loss = -9780.555982574597
Iteration 3300: Loss = -9780.554964167248
Iteration 3400: Loss = -9780.554250624402
Iteration 3500: Loss = -9780.553680678295
Iteration 3600: Loss = -9780.553223194023
Iteration 3700: Loss = -9780.554950209611
1
Iteration 3800: Loss = -9780.552426311251
Iteration 3900: Loss = -9780.552150452282
Iteration 4000: Loss = -9780.555314742449
1
Iteration 4100: Loss = -9780.55110527379
Iteration 4200: Loss = -9780.550842803368
Iteration 4300: Loss = -9780.550470389295
Iteration 4400: Loss = -9780.61059826768
1
Iteration 4500: Loss = -9780.550094210024
Iteration 4600: Loss = -9780.54987158069
Iteration 4700: Loss = -9780.556473697072
1
Iteration 4800: Loss = -9780.548983524477
Iteration 4900: Loss = -9780.548625544987
Iteration 5000: Loss = -9780.59568404152
1
Iteration 5100: Loss = -9780.548403213712
Iteration 5200: Loss = -9780.548284416898
Iteration 5300: Loss = -9780.548117681123
Iteration 5400: Loss = -9780.553198512873
1
Iteration 5500: Loss = -9780.547742517496
Iteration 5600: Loss = -9780.547423404794
Iteration 5700: Loss = -9780.54681339648
Iteration 5800: Loss = -9780.546380983622
Iteration 5900: Loss = -9780.546266283385
Iteration 6000: Loss = -9780.54617816083
Iteration 6100: Loss = -9780.54609936537
Iteration 6200: Loss = -9780.5460213508
Iteration 6300: Loss = -9780.545956987395
Iteration 6400: Loss = -9780.545816741915
Iteration 6500: Loss = -9780.545686550384
Iteration 6600: Loss = -9780.545564471127
Iteration 6700: Loss = -9780.545464639856
Iteration 6800: Loss = -9780.54536781976
Iteration 6900: Loss = -9780.545319301478
Iteration 7000: Loss = -9780.54523659996
Iteration 7100: Loss = -9780.545167491342
Iteration 7200: Loss = -9780.545404145318
1
Iteration 7300: Loss = -9780.544974827957
Iteration 7400: Loss = -9780.544883744713
Iteration 7500: Loss = -9780.593438175087
1
Iteration 7600: Loss = -9780.5447885578
Iteration 7700: Loss = -9780.54473605331
Iteration 7800: Loss = -9780.544698552902
Iteration 7900: Loss = -9780.545299226815
1
Iteration 8000: Loss = -9780.544565149323
Iteration 8100: Loss = -9780.544535159066
Iteration 8200: Loss = -9780.944861725327
1
Iteration 8300: Loss = -9780.5444141707
Iteration 8400: Loss = -9780.544369309604
Iteration 8500: Loss = -9780.544346910292
Iteration 8600: Loss = -9780.54699960962
1
Iteration 8700: Loss = -9780.544238656565
Iteration 8800: Loss = -9780.578388723594
1
Iteration 8900: Loss = -9780.544161630234
Iteration 9000: Loss = -9780.64742220064
1
Iteration 9100: Loss = -9780.543804706353
Iteration 9200: Loss = -9780.558510217852
1
Iteration 9300: Loss = -9780.543563764439
Iteration 9400: Loss = -9780.543504083545
Iteration 9500: Loss = -9780.546637947546
1
Iteration 9600: Loss = -9780.54346147283
Iteration 9700: Loss = -9780.546401259344
1
Iteration 9800: Loss = -9780.543472488513
2
Iteration 9900: Loss = -9780.543465382616
3
Iteration 10000: Loss = -9780.543814306106
4
Iteration 10100: Loss = -9780.551967111225
5
Iteration 10200: Loss = -9780.543511876813
6
Iteration 10300: Loss = -9780.543390670286
Iteration 10400: Loss = -9780.543442520591
1
Iteration 10500: Loss = -9780.543410598275
2
Iteration 10600: Loss = -9780.543398341972
3
Iteration 10700: Loss = -9780.543560147626
4
Iteration 10800: Loss = -9780.544001834785
5
Iteration 10900: Loss = -9780.543409447917
6
Iteration 11000: Loss = -9780.543968011061
7
Iteration 11100: Loss = -9780.54369414101
8
Iteration 11200: Loss = -9780.54329755703
Iteration 11300: Loss = -9780.54367227315
1
Iteration 11400: Loss = -9780.543405650917
2
Iteration 11500: Loss = -9780.543312814256
3
Iteration 11600: Loss = -9780.545125873421
4
Iteration 11700: Loss = -9780.543327421718
5
Iteration 11800: Loss = -9780.543287973398
Iteration 11900: Loss = -9780.54358959046
1
Iteration 12000: Loss = -9780.54337129443
2
Iteration 12100: Loss = -9780.54330470668
3
Iteration 12200: Loss = -9780.568829629914
4
Iteration 12300: Loss = -9780.543262641491
Iteration 12400: Loss = -9780.547037057375
1
Iteration 12500: Loss = -9780.543701636396
2
Iteration 12600: Loss = -9780.543283266865
3
Iteration 12700: Loss = -9780.547166007878
4
Iteration 12800: Loss = -9780.543251277115
Iteration 12900: Loss = -9780.559208952389
1
Iteration 13000: Loss = -9780.543238487518
Iteration 13100: Loss = -9780.550823468031
1
Iteration 13200: Loss = -9780.543242753412
2
Iteration 13300: Loss = -9780.564338141601
3
Iteration 13400: Loss = -9780.543221658543
Iteration 13500: Loss = -9780.546201046047
1
Iteration 13600: Loss = -9780.543322688756
2
Iteration 13700: Loss = -9780.543241145573
3
Iteration 13800: Loss = -9780.544028189466
4
Iteration 13900: Loss = -9780.543245503206
5
Iteration 14000: Loss = -9780.547473537572
6
Iteration 14100: Loss = -9780.557140074408
7
Iteration 14200: Loss = -9780.543298964121
8
Iteration 14300: Loss = -9780.566333554729
9
Iteration 14400: Loss = -9780.543271468046
10
Stopping early at iteration 14400 due to no improvement.
tensor([[-2.1925,  0.7336],
        [-5.4237,  1.1632],
        [-3.1753,  1.1191],
        [-3.5742,  1.5899],
        [-4.3486,  2.8227],
        [-4.0128,  2.6137],
        [-5.0313,  2.8767],
        [-2.6510,  1.1979],
        [-4.2906,  2.9037],
        [-3.2154,  1.2575],
        [-3.6404,  2.1822],
        [-5.3865,  2.9282],
        [-5.0245,  3.6382],
        [-4.4901,  2.7507],
        [-2.4173, -2.1979],
        [-4.9814,  1.6772],
        [-4.8666,  2.4970],
        [-3.4309,  2.0037],
        [-6.6072,  3.7643],
        [-3.2021,  1.2791],
        [-5.2105,  2.7438],
        [-5.1156,  3.6004],
        [-3.1995,  1.0497],
        [-5.7814,  1.4750],
        [-4.8850,  3.4974],
        [-5.6748,  3.3314],
        [-3.8768,  1.4651],
        [-5.7079,  4.2467],
        [-3.8158,  1.5567],
        [-2.4381,  0.8835],
        [-4.8469,  1.4699],
        [-4.8950,  3.5087],
        [-5.3330,  1.4291],
        [-3.6224,  1.9919],
        [-3.6060,  1.8854],
        [ 0.7191, -4.3691],
        [-4.4788,  2.1232],
        [-3.4781,  2.0747],
        [-2.8981,  0.2752],
        [-4.2890,  2.8618],
        [-5.9879,  4.5879],
        [-5.0219,  2.3524],
        [-5.8068,  2.3228],
        [-2.7675,  1.2671],
        [-4.3782,  1.9678],
        [-3.3182,  1.6836],
        [-4.6028,  2.0625],
        [-5.1937,  3.2490],
        [-3.0386,  1.2429],
        [-3.6211,  2.1147],
        [-3.0655,  1.3652],
        [-4.8199,  3.0342],
        [-3.6765,  2.0303],
        [-3.9318,  2.1696],
        [-6.0150,  2.6433],
        [-3.4513,  2.0273],
        [-3.2873,  1.6682],
        [-4.8115,  3.4106],
        [-4.7116,  3.1796],
        [-5.0023,  2.6362],
        [-2.8004,  1.3929],
        [-5.4519,  3.9309],
        [-5.6195,  2.7835],
        [-4.9472,  3.4124],
        [-2.2930,  0.9040],
        [-3.7640,  2.2642],
        [-6.2382,  4.8263],
        [-5.4204,  3.9318],
        [-2.6152,  0.2132],
        [-5.2850,  3.4348],
        [-2.5055,  0.6546],
        [-3.7657,  2.2657],
        [-4.1285,  2.5662],
        [-3.3724,  1.9836],
        [-5.0423,  2.1793],
        [-6.0604,  2.8296],
        [-2.0587,  0.3485],
        [-5.4264,  3.7082],
        [-5.9321,  4.1918],
        [-6.5295,  1.9143],
        [-4.8240,  3.3617],
        [-4.2732,  2.3568],
        [-3.2997,  1.7347],
        [-5.1671,  2.5098],
        [-4.1955,  2.7395],
        [-2.4598,  0.8567],
        [-3.7596,  2.2404],
        [-6.0368,  3.4920],
        [-3.1296,  1.5029],
        [-4.6668,  1.3786],
        [-3.7162,  2.3202],
        [-4.3940,  2.9034],
        [-4.0848,  2.5739],
        [-2.7394, -1.8758],
        [-6.2039,  4.0962],
        [-4.3047,  2.0152],
        [-5.2977,  3.6906],
        [-4.9895,  1.8584],
        [-3.5985,  1.6843],
        [-3.7790,  2.2169]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6846, 0.3154],
        [0.0097, 0.9903]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0238, 0.9762], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0989, 0.2146],
         [0.6301, 0.1332]],

        [[0.7324, 0.0787],
         [0.3741, 0.7975]],

        [[0.9009, 0.1959],
         [0.3122, 0.4137]],

        [[0.3902, 0.2384],
         [0.3387, 0.7013]],

        [[0.2664, 0.0793],
         [0.8373, 0.2917]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: -0.013574768645372375
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.012378759859606748
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
Global Adjusted Rand Index: -0.0020059309140797467
Average Adjusted Rand Index: -0.0014017132127863664
Iteration 0: Loss = -22335.10719909541
Iteration 10: Loss = -9789.763917541495
Iteration 20: Loss = -9789.576749030608
Iteration 30: Loss = -9784.994446754023
Iteration 40: Loss = -9784.86827477533
Iteration 50: Loss = -9784.838544814662
Iteration 60: Loss = -9784.821461335692
Iteration 70: Loss = -9784.813480068435
Iteration 80: Loss = -9784.809691980226
Iteration 90: Loss = -9784.807772937618
Iteration 100: Loss = -9784.806734414791
Iteration 110: Loss = -9784.80620097449
Iteration 120: Loss = -9784.805905901894
Iteration 130: Loss = -9784.805730971862
Iteration 140: Loss = -9784.805644429
Iteration 150: Loss = -9784.805573832042
Iteration 160: Loss = -9784.805544344801
Iteration 170: Loss = -9784.805532854467
Iteration 180: Loss = -9784.8055456459
1
Iteration 190: Loss = -9784.805520568532
Iteration 200: Loss = -9784.805507687492
Iteration 210: Loss = -9784.805525070698
1
Iteration 220: Loss = -9784.805528220284
2
Iteration 230: Loss = -9784.805524273226
3
Stopping early at iteration 229 due to no improvement.
pi: tensor([[0.0996, 0.9004],
        [0.0505, 0.9495]], dtype=torch.float64)
alpha: tensor([0.0536, 0.9464])
beta: tensor([[[0.1907, 0.1752],
         [0.4861, 0.1286]],

        [[0.3033, 0.1116],
         [0.9071, 0.6513]],

        [[0.8504, 0.1871],
         [0.9527, 0.2728]],

        [[0.2560, 0.2134],
         [0.1843, 0.2103]],

        [[0.6694, 0.1867],
         [0.1201, 0.3135]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.012378759859606748
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: 0.0032003860414685976
Average Adjusted Rand Index: 0.0021007570582299772
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22334.461944444254
Iteration 100: Loss = -9810.084193207687
Iteration 200: Loss = -9793.941495968653
Iteration 300: Loss = -9791.559465080065
Iteration 400: Loss = -9790.353070973866
Iteration 500: Loss = -9789.393376070255
Iteration 600: Loss = -9787.899078535833
Iteration 700: Loss = -9786.666635197884
Iteration 800: Loss = -9786.119131744565
Iteration 900: Loss = -9785.845600239289
Iteration 1000: Loss = -9785.649592422304
Iteration 1100: Loss = -9785.490984638196
Iteration 1200: Loss = -9785.365270759516
Iteration 1300: Loss = -9785.269726350147
Iteration 1400: Loss = -9785.192201451615
Iteration 1500: Loss = -9785.128452645504
Iteration 1600: Loss = -9784.946942979544
Iteration 1700: Loss = -9782.712716696282
Iteration 1800: Loss = -9782.262352341068
Iteration 1900: Loss = -9782.015690918508
Iteration 2000: Loss = -9781.78288465574
Iteration 2100: Loss = -9781.518554227385
Iteration 2200: Loss = -9781.257703854439
Iteration 2300: Loss = -9781.047489699376
Iteration 2400: Loss = -9780.896874274958
Iteration 2500: Loss = -9780.791481318522
Iteration 2600: Loss = -9780.71590678852
Iteration 2700: Loss = -9780.659592724138
Iteration 2800: Loss = -9780.616116452244
Iteration 2900: Loss = -9780.58153590331
Iteration 3000: Loss = -9780.553374598787
Iteration 3100: Loss = -9780.52988555757
Iteration 3200: Loss = -9780.509939061494
Iteration 3300: Loss = -9780.492730268144
Iteration 3400: Loss = -9780.477710476023
Iteration 3500: Loss = -9780.464388827955
Iteration 3600: Loss = -9780.452420409993
Iteration 3700: Loss = -9780.441394035346
Iteration 3800: Loss = -9780.430980787689
Iteration 3900: Loss = -9780.42097738339
Iteration 4000: Loss = -9780.411045977857
Iteration 4100: Loss = -9780.399227943
Iteration 4200: Loss = -9780.372211965492
Iteration 4300: Loss = -9779.609271793803
Iteration 4400: Loss = -9779.438733072757
Iteration 4500: Loss = -9779.334680127387
Iteration 4600: Loss = -9779.257727838703
Iteration 4700: Loss = -9779.212348922088
Iteration 4800: Loss = -9779.153147485365
Iteration 4900: Loss = -9779.11626728065
Iteration 5000: Loss = -9779.086085086143
Iteration 5100: Loss = -9779.060767930678
Iteration 5200: Loss = -9779.039352632377
Iteration 5300: Loss = -9779.020952694285
Iteration 5400: Loss = -9779.052292936849
1
Iteration 5500: Loss = -9778.990879269022
Iteration 5600: Loss = -9778.978371448164
Iteration 5700: Loss = -9778.967126508256
Iteration 5800: Loss = -9778.95719599063
Iteration 5900: Loss = -9778.947421176135
Iteration 6000: Loss = -9778.937568851232
Iteration 6100: Loss = -9778.944181863591
1
Iteration 6200: Loss = -9778.915281018926
Iteration 6300: Loss = -9778.9083702494
Iteration 6400: Loss = -9778.903149278633
Iteration 6500: Loss = -9778.898457597172
Iteration 6600: Loss = -9778.89412105032
Iteration 6700: Loss = -9778.89011708937
Iteration 6800: Loss = -9778.906105639124
1
Iteration 6900: Loss = -9778.882935330348
Iteration 7000: Loss = -9778.879731468569
Iteration 7100: Loss = -9778.87672545081
Iteration 7200: Loss = -9778.874032474743
Iteration 7300: Loss = -9778.871295681009
Iteration 7400: Loss = -9778.868829308092
Iteration 7500: Loss = -9778.879627487264
1
Iteration 7600: Loss = -9778.86434772937
Iteration 7700: Loss = -9778.862310906581
Iteration 7800: Loss = -9778.860449344702
Iteration 7900: Loss = -9778.858777560008
Iteration 8000: Loss = -9778.856916793859
Iteration 8100: Loss = -9778.855326268198
Iteration 8200: Loss = -9778.919301509506
1
Iteration 8300: Loss = -9778.852451414788
Iteration 8400: Loss = -9778.851113360108
Iteration 8500: Loss = -9778.849816341513
Iteration 8600: Loss = -9778.84992690211
1
Iteration 8700: Loss = -9778.8475411664
Iteration 8800: Loss = -9778.846451962245
Iteration 8900: Loss = -9778.8454601981
Iteration 9000: Loss = -9778.845254022106
Iteration 9100: Loss = -9778.843605481936
Iteration 9200: Loss = -9778.842758873003
Iteration 9300: Loss = -9778.888996190559
1
Iteration 9400: Loss = -9778.841216725106
Iteration 9500: Loss = -9778.840514897873
Iteration 9600: Loss = -9778.839826970861
Iteration 9700: Loss = -9778.839475736284
Iteration 9800: Loss = -9778.838547645353
Iteration 9900: Loss = -9778.83795284438
Iteration 10000: Loss = -9778.837425156293
Iteration 10100: Loss = -9778.837084307548
Iteration 10200: Loss = -9778.836458045054
Iteration 10300: Loss = -9778.83596023348
Iteration 10400: Loss = -9778.843368144917
1
Iteration 10500: Loss = -9778.835118923818
Iteration 10600: Loss = -9778.834687459763
Iteration 10700: Loss = -9778.834425012437
Iteration 10800: Loss = -9778.833996963403
Iteration 10900: Loss = -9778.83363706069
Iteration 11000: Loss = -9778.833376025199
Iteration 11100: Loss = -9778.832980347855
Iteration 11200: Loss = -9778.832676744278
Iteration 11300: Loss = -9778.832404542805
Iteration 11400: Loss = -9778.832128649488
Iteration 11500: Loss = -9778.831860489896
Iteration 11600: Loss = -9778.83466667853
1
Iteration 11700: Loss = -9778.83139421908
Iteration 11800: Loss = -9778.831177405205
Iteration 11900: Loss = -9778.830958167442
Iteration 12000: Loss = -9778.832496940502
1
Iteration 12100: Loss = -9778.830602617025
Iteration 12200: Loss = -9778.83037344712
Iteration 12300: Loss = -9778.830207878707
Iteration 12400: Loss = -9779.096199581962
1
Iteration 12500: Loss = -9778.829935733118
Iteration 12600: Loss = -9778.831574906773
1
Iteration 12700: Loss = -9778.829604783363
Iteration 12800: Loss = -9778.830033355522
1
Iteration 12900: Loss = -9778.829905179435
2
Iteration 13000: Loss = -9778.829315085937
Iteration 13100: Loss = -9778.83253413134
1
Iteration 13200: Loss = -9778.828801298585
Iteration 13300: Loss = -9778.82889203076
1
Iteration 13400: Loss = -9778.829185064977
2
Iteration 13500: Loss = -9778.828524255778
Iteration 13600: Loss = -9778.981094173037
1
Iteration 13700: Loss = -9778.82835838089
Iteration 13800: Loss = -9778.831593188383
1
Iteration 13900: Loss = -9778.828179865364
Iteration 14000: Loss = -9778.829379071116
1
Iteration 14100: Loss = -9778.834257577464
2
Iteration 14200: Loss = -9778.828012906559
Iteration 14300: Loss = -9778.85207528411
1
Iteration 14400: Loss = -9778.828217486474
2
Iteration 14500: Loss = -9778.864212695533
3
Iteration 14600: Loss = -9778.827805625278
Iteration 14700: Loss = -9778.827751894201
Iteration 14800: Loss = -9778.828939351737
1
Iteration 14900: Loss = -9778.827709471241
Iteration 15000: Loss = -9778.829892723246
1
Iteration 15100: Loss = -9778.827604652193
Iteration 15200: Loss = -9778.828968776957
1
Iteration 15300: Loss = -9778.8275560184
Iteration 15400: Loss = -9778.829139691637
1
Iteration 15500: Loss = -9778.82828872732
2
Iteration 15600: Loss = -9778.852190175838
3
Iteration 15700: Loss = -9778.82766077444
4
Iteration 15800: Loss = -9778.882204350648
5
Iteration 15900: Loss = -9778.82744523807
Iteration 16000: Loss = -9778.82778203058
1
Iteration 16100: Loss = -9778.827336019243
Iteration 16200: Loss = -9778.829694695327
1
Iteration 16300: Loss = -9778.827307428952
Iteration 16400: Loss = -9778.832660955313
1
Iteration 16500: Loss = -9778.82763326521
2
Iteration 16600: Loss = -9778.827334549209
3
Iteration 16700: Loss = -9778.834225813785
4
Iteration 16800: Loss = -9778.82722863607
Iteration 16900: Loss = -9778.827321025938
1
Iteration 17000: Loss = -9778.82774673594
2
Iteration 17100: Loss = -9778.850135398132
3
Iteration 17200: Loss = -9778.827232741345
4
Iteration 17300: Loss = -9778.827586636286
5
Iteration 17400: Loss = -9778.827765866015
6
Iteration 17500: Loss = -9778.827186249475
Iteration 17600: Loss = -9778.836752289619
1
Iteration 17700: Loss = -9778.82728291757
2
Iteration 17800: Loss = -9778.82713558509
Iteration 17900: Loss = -9778.949613790697
1
Iteration 18000: Loss = -9778.828284408346
2
Iteration 18100: Loss = -9778.827531678766
3
Iteration 18200: Loss = -9778.829903987218
4
Iteration 18300: Loss = -9778.827093910488
Iteration 18400: Loss = -9778.82709373609
Iteration 18500: Loss = -9778.954134694055
1
Iteration 18600: Loss = -9778.827058119228
Iteration 18700: Loss = -9779.114510525413
1
Iteration 18800: Loss = -9778.827668697353
2
Iteration 18900: Loss = -9778.82702844839
Iteration 19000: Loss = -9778.828216302205
1
Iteration 19100: Loss = -9778.827017536754
Iteration 19200: Loss = -9778.82702036109
1
Iteration 19300: Loss = -9778.827305675895
2
Iteration 19400: Loss = -9778.827029936932
3
Iteration 19500: Loss = -9778.827130727
4
Iteration 19600: Loss = -9778.83229649736
5
Iteration 19700: Loss = -9778.827036092775
6
Iteration 19800: Loss = -9778.828877392358
7
Iteration 19900: Loss = -9778.827023729033
8
tensor([[ -8.3915,   7.0045],
        [-10.1586,   8.7720],
        [ -8.4793,   7.0119],
        [ -9.5645,   7.6299],
        [-10.0805,   8.6180],
        [ -9.9065,   8.0071],
        [-10.2130,   7.5076],
        [ -8.8277,   6.8758],
        [ -9.8997,   7.8326],
        [ -9.2886,   6.8389],
        [ -9.5165,   7.7475],
        [ -9.3946,   8.0083],
        [ -9.8641,   8.1791],
        [ -9.9868,   8.0698],
        [ -0.0256,  -2.4255],
        [ -9.3601,   7.3246],
        [ -9.9233,   8.4653],
        [ -9.1161,   7.1284],
        [-11.2366,   6.6214],
        [ -8.7413,   7.1468],
        [ -9.8913,   7.8323],
        [ -9.4853,   6.3064],
        [ -3.0116,   1.5251],
        [-10.0748,   8.6761],
        [ -9.7763,   8.2310],
        [ -9.9059,   8.2199],
        [ -9.0399,   7.5932],
        [ -9.4293,   8.0425],
        [-10.8131,   6.9304],
        [ -8.9086,   6.4668],
        [ -9.4539,   6.9953],
        [ -9.9090,   8.0339],
        [ -9.8048,   7.9212],
        [-10.5226,   8.6479],
        [-10.0908,   7.7587],
        [  4.7376,  -6.9973],
        [ -4.8023,   3.1573],
        [ -9.1089,   7.3814],
        [ -9.8714,   7.8140],
        [ -9.7052,   8.2006],
        [ -9.8875,   8.2829],
        [ -9.6669,   8.2805],
        [ -4.9997,   3.4527],
        [ -9.6654,   7.4466],
        [ -4.6364,   2.0714],
        [ -8.9561,   7.2838],
        [ -9.3290,   7.9404],
        [-10.0310,   8.1455],
        [ -9.7633,   8.1752],
        [-10.5144,   8.7854],
        [ -9.5586,   7.2922],
        [ -9.8201,   8.1811],
        [-10.1888,   8.7992],
        [ -9.3592,   7.8370],
        [-10.1310,   8.6722],
        [ -9.6022,   6.9852],
        [ -8.0323,   6.1062],
        [ -9.3398,   7.8956],
        [-11.1831,   6.5679],
        [-10.5600,   6.1620],
        [ -9.9949,   6.9949],
        [ -9.6747,   8.2343],
        [-10.0955,   8.6013],
        [ -9.4892,   8.0944],
        [ -9.0847,   6.7405],
        [-10.3688,   7.7962],
        [ -9.8859,   8.4533],
        [-10.1281,   8.1576],
        [-11.7130,   8.4634],
        [-10.3106,   6.5826],
        [ -8.4645,   6.0641],
        [ -9.5218,   7.8831],
        [ -9.6958,   8.0246],
        [ -9.3030,   6.8886],
        [-10.4508,   8.7299],
        [-10.9382,   8.3022],
        [ -9.7061,   7.5603],
        [ -9.7541,   8.2118],
        [-10.1831,   7.6212],
        [ -9.3587,   7.8489],
        [-10.1632,   8.1293],
        [ -9.8188,   8.0569],
        [-10.3126,   7.2376],
        [ -9.3651,   7.6239],
        [ -9.4713,   8.0520],
        [ -7.0176,   4.8132],
        [ -9.5980,   8.2116],
        [-10.2251,   8.6442],
        [ -9.6657,   5.0505],
        [ -9.4692,   7.7479],
        [ -9.1802,   7.5749],
        [ -9.9219,   8.5151],
        [ -9.2161,   7.7537],
        [ -8.7293,   6.5343],
        [-10.0170,   8.6305],
        [ -9.2710,   7.8475],
        [ -9.8800,   8.4666],
        [-10.0667,   7.5988],
        [ -9.2085,   7.8213],
        [ -8.8803,   7.4682]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 6.1072e-07],
        [1.3219e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0193, 0.9807], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.4306e-05, 2.2029e-01],
         [4.8613e-01, 1.3313e-01]],

        [[3.0333e-01, 7.2168e-02],
         [9.0705e-01, 6.5125e-01]],

        [[8.5043e-01, 2.0498e-01],
         [9.5269e-01, 2.7275e-01]],

        [[2.5599e-01, 2.2061e-01],
         [1.8434e-01, 2.1032e-01]],

        [[6.6940e-01, 1.7828e-01],
         [1.2006e-01, 3.1346e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: -0.013574768645372375
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.0028941180812326324
Average Adjusted Rand Index: -0.0047312454790009355
9930.763321721859
new:  [0.00014555556650875184, -0.0028941180812326324, -0.0020059309140797467, -0.0028941180812326324] [-0.002833281052803375, -0.0047312454790009355, -0.0014017132127863664, -0.0047312454790009355] [9784.84863286151, 9778.863439455858, 9780.543271468046, 9778.827024356357]
prior:  [0.0, 0.0, 0.0032003860414685976, 0.0032003860414685976] [0.0, 0.0, 0.0021007570582299772, 0.0021007570582299772] [nan, 9789.76395091884, 9784.805518665906, 9784.805524273226]
-----------------------------------------------------------------------------------------
This iteration is 1
True Objective function: Loss = -9963.412899975014
Iteration 0: Loss = -15757.628961114482
Iteration 10: Loss = -9884.035459641656
Iteration 20: Loss = -9883.562664298106
Iteration 30: Loss = -9883.29970308052
Iteration 40: Loss = -9883.283287075195
Iteration 50: Loss = -9883.281781986754
Iteration 60: Loss = -9883.27909696099
Iteration 70: Loss = -9883.275615449842
Iteration 80: Loss = -9883.272676514845
Iteration 90: Loss = -9883.270773395978
Iteration 100: Loss = -9883.26974547961
Iteration 110: Loss = -9883.26918164245
Iteration 120: Loss = -9883.268884123865
Iteration 130: Loss = -9883.268747459993
Iteration 140: Loss = -9883.268683774993
Iteration 150: Loss = -9883.268652788109
Iteration 160: Loss = -9883.2686048866
Iteration 170: Loss = -9883.26856334383
Iteration 180: Loss = -9883.26859199342
1
Iteration 190: Loss = -9883.268567538482
2
Iteration 200: Loss = -9883.268555311513
Iteration 210: Loss = -9883.268579693084
1
Iteration 220: Loss = -9883.268558232385
2
Iteration 230: Loss = -9883.268557234203
3
Stopping early at iteration 229 due to no improvement.
pi: tensor([[0.0159, 0.9841],
        [0.0577, 0.9423]], dtype=torch.float64)
alpha: tensor([0.0558, 0.9442])
beta: tensor([[[0.1978, 0.1565],
         [0.9516, 0.1327]],

        [[0.3113, 0.1707],
         [0.3982, 0.3331]],

        [[0.4005, 0.1743],
         [0.0834, 0.4187]],

        [[0.5343, 0.2039],
         [0.6002, 0.0904]],

        [[0.2109, 0.0807],
         [0.2200, 0.6198]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011269512297611439
Average Adjusted Rand Index: -0.0020854854324299475
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16472.01663798387
Iteration 100: Loss = -9889.56552827527
Iteration 200: Loss = -9888.403101182961
Iteration 300: Loss = -9888.047782396345
Iteration 400: Loss = -9887.874792493218
Iteration 500: Loss = -9887.769764043893
Iteration 600: Loss = -9887.710446362807
Iteration 700: Loss = -9887.67114789805
Iteration 800: Loss = -9887.642567389272
Iteration 900: Loss = -9887.619401695332
Iteration 1000: Loss = -9887.596554765016
Iteration 1100: Loss = -9887.570832951178
Iteration 1200: Loss = -9887.540957748339
Iteration 1300: Loss = -9887.506809378638
Iteration 1400: Loss = -9887.464917266398
Iteration 1500: Loss = -9887.402163729757
Iteration 1600: Loss = -9887.277220252561
Iteration 1700: Loss = -9887.006105835817
Iteration 1800: Loss = -9886.644044031267
Iteration 1900: Loss = -9886.226612012955
Iteration 2000: Loss = -9885.800094373142
Iteration 2100: Loss = -9885.495598104579
Iteration 2200: Loss = -9884.972101571037
Iteration 2300: Loss = -9872.803951260184
Iteration 2400: Loss = -9868.122564034838
Iteration 2500: Loss = -9866.34755849532
Iteration 2600: Loss = -9865.300557092565
Iteration 2700: Loss = -9859.456489422511
Iteration 2800: Loss = -9857.64829601735
Iteration 2900: Loss = -9857.279153960777
Iteration 3000: Loss = -9856.565898188881
Iteration 3100: Loss = -9856.532118241053
Iteration 3200: Loss = -9856.358815021762
Iteration 3300: Loss = -9854.875230720721
Iteration 3400: Loss = -9852.53785322508
Iteration 3500: Loss = -9852.377905848969
Iteration 3600: Loss = -9851.320131534201
Iteration 3700: Loss = -9850.235205817904
Iteration 3800: Loss = -9850.23039671081
Iteration 3900: Loss = -9850.234488922337
1
Iteration 4000: Loss = -9850.22858717434
Iteration 4100: Loss = -9850.23475748533
1
Iteration 4200: Loss = -9850.227385944201
Iteration 4300: Loss = -9850.233714356304
1
Iteration 4400: Loss = -9850.226935007524
Iteration 4500: Loss = -9850.226639285518
Iteration 4600: Loss = -9850.226484709965
Iteration 4700: Loss = -9850.224841535699
Iteration 4800: Loss = -9850.224784939131
Iteration 4900: Loss = -9850.224827465061
1
Iteration 5000: Loss = -9850.224698759563
Iteration 5100: Loss = -9850.224667445536
Iteration 5200: Loss = -9850.225572712696
1
Iteration 5300: Loss = -9850.224616240477
Iteration 5400: Loss = -9850.225317443124
1
Iteration 5500: Loss = -9850.224492948506
Iteration 5600: Loss = -9850.224285675396
Iteration 5700: Loss = -9850.155908740957
Iteration 5800: Loss = -9850.155484703018
Iteration 5900: Loss = -9850.13368961296
Iteration 6000: Loss = -9850.120455643828
Iteration 6100: Loss = -9850.119425591645
Iteration 6200: Loss = -9850.135783795473
1
Iteration 6300: Loss = -9849.379538244642
Iteration 6400: Loss = -9849.374552237608
Iteration 6500: Loss = -9849.374586563126
1
Iteration 6600: Loss = -9849.374515580756
Iteration 6700: Loss = -9849.374392599493
Iteration 6800: Loss = -9849.374778367672
1
Iteration 6900: Loss = -9849.382190412169
2
Iteration 7000: Loss = -9849.3739695699
Iteration 7100: Loss = -9849.374748703807
1
Iteration 7200: Loss = -9849.338947332915
Iteration 7300: Loss = -9849.338262748402
Iteration 7400: Loss = -9849.363777166773
1
Iteration 7500: Loss = -9849.338099116467
Iteration 7600: Loss = -9849.341577780866
1
Iteration 7700: Loss = -9849.346708997573
2
Iteration 7800: Loss = -9849.345784840672
3
Iteration 7900: Loss = -9849.339270601633
4
Iteration 8000: Loss = -9849.3384774601
5
Iteration 8100: Loss = -9849.340261119498
6
Iteration 8200: Loss = -9849.338154830919
7
Iteration 8300: Loss = -9849.338334079786
8
Iteration 8400: Loss = -9849.338916952564
9
Iteration 8500: Loss = -9849.342257120665
10
Stopping early at iteration 8500 due to no improvement.
tensor([[-4.9621,  0.3469],
        [-4.5890, -0.0262],
        [-2.2233, -2.3919],
        [-4.5269, -0.0883],
        [-3.6459, -0.9693],
        [-4.0516, -0.5636],
        [-3.7819, -0.8333],
        [-4.4374, -0.1779],
        [-4.6887,  0.0734],
        [-3.6634, -0.9518],
        [-3.0785, -1.5367],
        [-2.9195, -1.6957],
        [-5.1606,  0.5453],
        [-1.2752, -3.3400],
        [-4.5327, -0.0825],
        [ 2.3455, -6.9607],
        [-2.3511, -2.2641],
        [-3.0662, -1.5490],
        [-4.8154,  0.2002],
        [-1.4623, -3.1529],
        [-4.4490, -0.1662],
        [-4.2952, -0.3200],
        [-4.9934,  0.3782],
        [-0.3976, -4.2176],
        [-5.7058,  1.0906],
        [-4.3185, -0.2967],
        [-3.9429, -0.6723],
        [-2.4900, -2.1252],
        [-4.2887, -0.3265],
        [-4.6866,  0.0713],
        [-4.1938, -0.4214],
        [-4.5450, -0.0702],
        [-1.2345, -3.3808],
        [-3.3956, -1.2196],
        [-3.0623, -1.5529],
        [-4.3845, -0.2308],
        [-0.3116, -4.3036],
        [-4.7982,  0.1830],
        [-0.7889, -3.8264],
        [-5.2876,  0.6724],
        [-3.8146, -0.8006],
        [-5.4346,  0.8194],
        [-4.3799, -0.2353],
        [-4.8193,  0.2041],
        [-0.4476, -4.1676],
        [-3.4196, -1.1956],
        [-2.5781, -2.0371],
        [-3.8950, -0.7202],
        [-2.1370, -2.4782],
        [-0.3886, -4.2267],
        [-3.6297, -0.9856],
        [-0.3388, -4.2764],
        [-4.0826, -0.5327],
        [-4.4098, -0.2054],
        [-4.5203, -0.0949],
        [-0.3326, -4.2826],
        [-3.2899, -1.3253],
        [ 0.2712, -4.8864],
        [-5.4827,  0.8674],
        [ 1.3955, -6.0107],
        [-4.6314,  0.0162],
        [-3.9651, -0.6501],
        [-4.4191, -0.1961],
        [-1.0753, -3.5400],
        [ 0.4521, -5.0673],
        [-0.3114, -4.3038],
        [-4.0775, -0.5378],
        [ 0.9722, -5.5874],
        [-3.4671, -1.1481],
        [-3.6945, -0.9207],
        [-2.0155, -2.5997],
        [-0.0711, -4.5441],
        [-5.4183,  0.8031],
        [-3.5559, -1.0593],
        [-5.2331,  0.6179],
        [-5.9275,  1.3123],
        [-3.5624, -1.0528],
        [ 0.1260, -4.7412],
        [-3.8351, -0.7802],
        [-2.3113, -2.3040],
        [-4.0431, -0.5721],
        [-0.4774, -4.1379],
        [-3.4068, -1.2084],
        [-0.4397, -4.1755],
        [-4.1714, -0.4438],
        [-4.8297,  0.2145],
        [-4.2758, -0.3395],
        [-2.6817, -1.9335],
        [-1.7861, -2.8291],
        [-2.0705, -2.5447],
        [-0.3420, -4.2732],
        [-3.6159, -0.9993],
        [-2.3587, -2.2566],
        [-3.4231, -1.1921],
        [-3.8605, -0.7547],
        [-0.8558, -3.7594],
        [-3.5601, -1.0551],
        [-2.6237, -1.9915],
        [-3.4169, -1.1984],
        [-3.8297, -0.7855]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7615, 0.2385],
        [0.2453, 0.7547]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3101, 0.6899], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2296, 0.1044],
         [0.9516, 0.1429]],

        [[0.3113, 0.0976],
         [0.3982, 0.3331]],

        [[0.4005, 0.1002],
         [0.0834, 0.4187]],

        [[0.5343, 0.0981],
         [0.6002, 0.0904]],

        [[0.2109, 0.0833],
         [0.2200, 0.6198]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 27
Adjusted Rand Index: 0.2050659310199179
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 22
Adjusted Rand Index: 0.30670639258136173
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 16
Adjusted Rand Index: 0.4569316639010409
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 12
Adjusted Rand Index: 0.5733494225133597
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6690909090909091
Global Adjusted Rand Index: 0.4291433869560507
Average Adjusted Rand Index: 0.4422288638213178
Iteration 0: Loss = -25799.11813416166
Iteration 10: Loss = -9887.788016333761
Iteration 20: Loss = -9887.788016333761
1
Iteration 30: Loss = -9887.788016334074
2
Iteration 40: Loss = -9887.788016767143
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 2.1602e-11],
        [1.0000e+00, 1.1809e-35]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.9203e-11])
beta: tensor([[[0.1356, 0.1553],
         [0.5772, 0.1905]],

        [[0.7307, 0.1943],
         [0.7359, 0.5593]],

        [[0.6707, 0.1898],
         [0.1386, 0.2513]],

        [[0.6588, 0.2476],
         [0.4221, 0.3164]],

        [[0.6536, 0.0505],
         [0.2800, 0.9560]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25798.597635065184
Iteration 100: Loss = -9916.544607936543
Iteration 200: Loss = -9897.54808323684
Iteration 300: Loss = -9892.043014905908
Iteration 400: Loss = -9890.508932052457
Iteration 500: Loss = -9889.648614676164
Iteration 600: Loss = -9889.100405965975
Iteration 700: Loss = -9888.718982419336
Iteration 800: Loss = -9888.426309149996
Iteration 900: Loss = -9888.136679136887
Iteration 1000: Loss = -9887.95204636332
Iteration 1100: Loss = -9887.82053638523
Iteration 1200: Loss = -9887.71684015281
Iteration 1300: Loss = -9887.617411890935
Iteration 1400: Loss = -9887.528994508704
Iteration 1500: Loss = -9887.355073003146
Iteration 1600: Loss = -9886.562549731447
Iteration 1700: Loss = -9886.043478903304
Iteration 1800: Loss = -9885.700982828977
Iteration 1900: Loss = -9885.433964040361
Iteration 2000: Loss = -9885.196938904923
Iteration 2100: Loss = -9884.958625566913
Iteration 2200: Loss = -9884.719479541001
Iteration 2300: Loss = -9884.50408232761
Iteration 2400: Loss = -9884.311204120024
Iteration 2500: Loss = -9884.139406186096
Iteration 2600: Loss = -9883.989930754038
Iteration 2700: Loss = -9883.867046248204
Iteration 2800: Loss = -9883.766469122145
Iteration 2900: Loss = -9883.676603679167
Iteration 3000: Loss = -9883.59967281964
Iteration 3100: Loss = -9883.53894677699
Iteration 3200: Loss = -9883.486639982984
Iteration 3300: Loss = -9883.439739717054
Iteration 3400: Loss = -9883.399017754715
Iteration 3500: Loss = -9883.36371767563
Iteration 3600: Loss = -9883.331686969215
Iteration 3700: Loss = -9883.30231677326
Iteration 3800: Loss = -9883.276031957088
Iteration 3900: Loss = -9883.25272303057
Iteration 4000: Loss = -9883.23162092251
Iteration 4100: Loss = -9883.211594609382
Iteration 4200: Loss = -9883.192057137527
Iteration 4300: Loss = -9883.172709673241
Iteration 4400: Loss = -9883.15395724881
Iteration 4500: Loss = -9883.134507772564
Iteration 4600: Loss = -9883.115840291486
Iteration 4700: Loss = -9883.098830441724
Iteration 4800: Loss = -9883.083971187016
Iteration 4900: Loss = -9883.25715588699
1
Iteration 5000: Loss = -9883.063687489202
Iteration 5100: Loss = -9883.297381364551
1
Iteration 5200: Loss = -9883.054162349223
Iteration 5300: Loss = -9883.051521491932
Iteration 5400: Loss = -9883.049853121982
Iteration 5500: Loss = -9883.048204412817
Iteration 5600: Loss = -9883.085124993528
1
Iteration 5700: Loss = -9883.04583212579
Iteration 5800: Loss = -9883.044869246629
Iteration 5900: Loss = -9883.047573463138
1
Iteration 6000: Loss = -9883.04309017854
Iteration 6100: Loss = -9883.042365320267
Iteration 6200: Loss = -9883.041538925841
Iteration 6300: Loss = -9883.043368702125
1
Iteration 6400: Loss = -9883.040196937372
Iteration 6500: Loss = -9883.0394483658
Iteration 6600: Loss = -9883.049820591988
1
Iteration 6700: Loss = -9883.036294458176
Iteration 6800: Loss = -9882.930897635051
Iteration 6900: Loss = -9882.912340470357
Iteration 7000: Loss = -9882.908877568425
Iteration 7100: Loss = -9882.907507402786
Iteration 7200: Loss = -9882.91015068166
1
Iteration 7300: Loss = -9882.906405825262
Iteration 7400: Loss = -9882.905951483004
Iteration 7500: Loss = -9882.90551159416
Iteration 7600: Loss = -9882.905092877096
Iteration 7700: Loss = -9882.904682038832
Iteration 7800: Loss = -9882.904369679241
Iteration 7900: Loss = -9882.903941091376
Iteration 8000: Loss = -9882.958791724319
1
Iteration 8100: Loss = -9882.903311321652
Iteration 8200: Loss = -9882.902971682044
Iteration 8300: Loss = -9882.969461312328
1
Iteration 8400: Loss = -9882.902406278392
Iteration 8500: Loss = -9882.902114315442
Iteration 8600: Loss = -9882.902521768481
1
Iteration 8700: Loss = -9882.901632629108
Iteration 8800: Loss = -9882.901408078596
Iteration 8900: Loss = -9883.035055499806
1
Iteration 9000: Loss = -9882.900951727826
Iteration 9100: Loss = -9882.900745075585
Iteration 9200: Loss = -9882.997488124369
1
Iteration 9300: Loss = -9882.900374400548
Iteration 9400: Loss = -9882.9001912041
Iteration 9500: Loss = -9882.974430934026
1
Iteration 9600: Loss = -9882.89988496197
Iteration 9700: Loss = -9882.899698633475
Iteration 9800: Loss = -9882.90449216644
1
Iteration 9900: Loss = -9882.899441762089
Iteration 10000: Loss = -9882.899299269455
Iteration 10100: Loss = -9882.903281086954
1
Iteration 10200: Loss = -9882.899095216739
Iteration 10300: Loss = -9882.898960063485
Iteration 10400: Loss = -9883.096126877897
1
Iteration 10500: Loss = -9882.898763865063
Iteration 10600: Loss = -9882.898653995953
Iteration 10700: Loss = -9882.899329947812
1
Iteration 10800: Loss = -9882.898465191372
Iteration 10900: Loss = -9882.898362881735
Iteration 11000: Loss = -9882.959717742924
1
Iteration 11100: Loss = -9882.898248055457
Iteration 11200: Loss = -9882.898143504995
Iteration 11300: Loss = -9882.961302194553
1
Iteration 11400: Loss = -9882.898034371667
Iteration 11500: Loss = -9882.89795119212
Iteration 11600: Loss = -9882.95205788878
1
Iteration 11700: Loss = -9882.897842039198
Iteration 11800: Loss = -9882.89777105587
Iteration 11900: Loss = -9882.898600978448
1
Iteration 12000: Loss = -9882.897693950337
Iteration 12100: Loss = -9882.897639019147
Iteration 12200: Loss = -9882.897733842547
1
Iteration 12300: Loss = -9882.897568452401
Iteration 12400: Loss = -9883.377203428987
1
Iteration 12500: Loss = -9882.89750431593
Iteration 12600: Loss = -9882.89748721258
Iteration 12700: Loss = -9882.91021511557
1
Iteration 12800: Loss = -9882.897389248888
Iteration 12900: Loss = -9882.897359555547
Iteration 13000: Loss = -9882.899589558023
1
Iteration 13100: Loss = -9882.897309967391
Iteration 13200: Loss = -9882.897275224068
Iteration 13300: Loss = -9882.897438723832
1
Iteration 13400: Loss = -9882.897206396021
Iteration 13500: Loss = -9882.906942541755
1
Iteration 13600: Loss = -9882.897190996127
Iteration 13700: Loss = -9882.89773581737
1
Iteration 13800: Loss = -9882.897137774225
Iteration 13900: Loss = -9882.897112920451
Iteration 14000: Loss = -9882.897134653636
1
Iteration 14100: Loss = -9882.897104361762
Iteration 14200: Loss = -9882.902332816106
1
Iteration 14300: Loss = -9882.897042044782
Iteration 14400: Loss = -9882.900961493457
1
Iteration 14500: Loss = -9882.897002446683
Iteration 14600: Loss = -9882.904583199492
1
Iteration 14700: Loss = -9882.89697530029
Iteration 14800: Loss = -9882.912873074063
1
Iteration 14900: Loss = -9882.897022907422
2
Iteration 15000: Loss = -9882.955093390749
3
Iteration 15100: Loss = -9882.896940881827
Iteration 15200: Loss = -9882.896952252713
1
Iteration 15300: Loss = -9882.897120910244
2
Iteration 15400: Loss = -9882.896909741474
Iteration 15500: Loss = -9882.897686198237
1
Iteration 15600: Loss = -9882.896913530261
2
Iteration 15700: Loss = -9882.908330731922
3
Iteration 15800: Loss = -9882.89690087068
Iteration 15900: Loss = -9882.897383404428
1
Iteration 16000: Loss = -9882.994635162482
2
Iteration 16100: Loss = -9882.89687458573
Iteration 16200: Loss = -9882.8971546695
1
Iteration 16300: Loss = -9882.910198812551
2
Iteration 16400: Loss = -9882.951577868129
3
Iteration 16500: Loss = -9882.94681177428
4
Iteration 16600: Loss = -9882.897039436577
5
Iteration 16700: Loss = -9882.89684421891
Iteration 16800: Loss = -9882.89731296585
1
Iteration 16900: Loss = -9882.899826656845
2
Iteration 17000: Loss = -9882.90528990385
3
Iteration 17100: Loss = -9882.912316264184
4
Iteration 17200: Loss = -9882.896830364454
Iteration 17300: Loss = -9882.89693498937
1
Iteration 17400: Loss = -9882.897299909115
2
Iteration 17500: Loss = -9882.897423319275
3
Iteration 17600: Loss = -9882.897584536982
4
Iteration 17700: Loss = -9882.90084883577
5
Iteration 17800: Loss = -9882.897014019085
6
Iteration 17900: Loss = -9882.897627878652
7
Iteration 18000: Loss = -9882.912041645128
8
Iteration 18100: Loss = -9882.905655661074
9
Iteration 18200: Loss = -9882.92121746058
10
Stopping early at iteration 18200 due to no improvement.
tensor([[ 2.1150e-01, -1.5982e+00],
        [ 6.0728e-01, -2.0131e+00],
        [ 5.6735e-01, -1.9973e+00],
        [ 3.2195e-01, -1.7194e+00],
        [ 2.3076e-01, -1.7204e+00],
        [ 7.2324e-01, -2.3650e+00],
        [ 9.3106e-01, -2.3957e+00],
        [ 8.4345e-01, -2.2453e+00],
        [ 2.2472e-01, -1.6413e+00],
        [ 3.0945e-01, -1.7527e+00],
        [ 4.6721e-01, -1.9117e+00],
        [ 3.8771e-01, -1.7743e+00],
        [ 9.5223e-02, -2.1376e+00],
        [ 5.3483e-01, -2.5053e+00],
        [ 5.5620e-01, -2.0002e+00],
        [-7.0768e-01, -2.3545e+00],
        [ 7.5741e-01, -2.1531e+00],
        [ 5.1595e-01, -2.0316e+00],
        [ 6.7279e-01, -2.0591e+00],
        [ 2.5477e-01, -2.4347e+00],
        [ 2.3631e-01, -2.4938e+00],
        [-1.0641e-01, -2.0182e+00],
        [-2.2221e-01, -2.1706e+00],
        [ 1.7967e-03, -1.9252e+00],
        [-2.2476e-01, -1.5457e+00],
        [ 2.2836e-02, -2.3790e+00],
        [ 3.2474e-01, -2.3340e+00],
        [-7.4078e-01, -3.5749e+00],
        [-3.7062e-01, -2.6175e+00],
        [ 5.7901e-01, -1.9931e+00],
        [ 4.7615e-01, -1.8964e+00],
        [ 8.8686e-02, -1.6574e+00],
        [-1.6316e-01, -2.4262e+00],
        [-3.1161e-02, -1.5619e+00],
        [-1.1885e+00, -3.4267e+00],
        [ 9.1781e-02, -1.5745e+00],
        [ 3.1431e-01, -1.7114e+00],
        [ 3.1447e-01, -1.7515e+00],
        [-4.3799e-01, -1.3708e+00],
        [ 5.4977e-01, -2.0551e+00],
        [-3.2837e-01, -1.5215e+00],
        [ 1.8209e-01, -2.4599e+00],
        [ 1.4287e-01, -1.7032e+00],
        [ 6.9881e-01, -2.0864e+00],
        [ 1.1362e-01, -1.5747e+00],
        [ 5.2618e-01, -1.9154e+00],
        [-2.1804e-01, -1.2393e+00],
        [ 3.0589e-02, -1.4718e+00],
        [ 2.3813e-01, -2.4090e+00],
        [-9.7113e-01, -2.5869e+00],
        [ 1.3395e-01, -1.5993e+00],
        [ 1.2258e-01, -1.6966e+00],
        [ 4.8206e-01, -1.8893e+00],
        [-9.0660e-01, -2.5637e+00],
        [-1.8462e-02, -1.3878e+00],
        [ 3.5101e-02, -1.4276e+00],
        [ 6.0176e-01, -1.9883e+00],
        [ 7.6236e-02, -1.9663e+00],
        [ 1.5077e-01, -1.8696e+00],
        [ 1.4380e-01, -1.5861e+00],
        [ 1.6061e-01, -1.8934e+00],
        [ 5.7464e-01, -2.0109e+00],
        [ 4.1972e-02, -1.4604e+00],
        [ 1.6971e-01, -1.8562e+00],
        [-8.8168e-01, -2.3657e+00],
        [-4.2091e-01, -1.2590e+00],
        [ 5.2730e-01, -1.9995e+00],
        [-6.2670e-01, -2.1215e+00],
        [-8.2486e-02, -2.5351e+00],
        [-6.1071e-01, -2.7978e+00],
        [ 3.1851e-01, -2.5858e+00],
        [ 8.3316e-02, -1.7391e+00],
        [ 7.4167e-02, -1.6251e+00],
        [-5.8586e-01, -2.1136e+00],
        [-1.1672e-01, -1.3356e+00],
        [-5.5471e-01, -2.2401e+00],
        [ 4.8607e-01, -1.8999e+00],
        [ 5.0679e-01, -1.9733e+00],
        [ 7.1515e-01, -2.3732e+00],
        [ 8.9604e-02, -1.5087e+00],
        [-2.8686e-01, -1.4057e+00],
        [-6.7593e-02, -1.3222e+00],
        [ 6.7599e-01, -2.2650e+00],
        [ 7.7769e-02, -1.8366e+00],
        [ 5.3302e-01, -2.2723e+00],
        [ 1.2276e-01, -1.5599e+00],
        [ 2.3396e-01, -1.6252e+00],
        [ 9.2569e-02, -2.0966e+00],
        [ 4.1000e-01, -2.3956e+00],
        [-1.1105e+00, -3.5047e+00],
        [-2.5004e-04, -1.3966e+00],
        [-3.9146e-01, -1.7262e+00],
        [ 7.4278e-01, -2.4744e+00],
        [-1.0085e+00, -3.2914e+00],
        [-5.3354e-01, -2.2213e+00],
        [ 7.4677e-01, -2.1895e+00],
        [ 1.5174e-01, -2.5904e+00],
        [ 2.4019e-01, -1.6280e+00],
        [-3.6081e-01, -1.8443e+00],
        [ 2.2668e-01, -1.7434e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.3482e-01, 6.5184e-02],
        [9.9999e-01, 1.3397e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8810, 0.1190], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1340, 0.1531],
         [0.5772, 0.1884]],

        [[0.7307, 0.1703],
         [0.7359, 0.5593]],

        [[0.6707, 0.1734],
         [0.1386, 0.2513]],

        [[0.6588, 0.2018],
         [0.4221, 0.3164]],

        [[0.6536, 0.0812],
         [0.2800, 0.9560]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011269512297611439
Average Adjusted Rand Index: -0.0020854854324299475
Iteration 0: Loss = -18208.56764952072
Iteration 10: Loss = -9886.00005690626
Iteration 20: Loss = -9885.897108724022
Iteration 30: Loss = -9885.433034205724
Iteration 40: Loss = -9882.30212854678
Iteration 50: Loss = -9879.99742820158
Iteration 60: Loss = -9879.666231785555
Iteration 70: Loss = -9879.45015506379
Iteration 80: Loss = -9879.253552561444
Iteration 90: Loss = -9879.077739721195
Iteration 100: Loss = -9878.923807294977
Iteration 110: Loss = -9878.791856715303
Iteration 120: Loss = -9878.681614715244
Iteration 130: Loss = -9878.592056662557
Iteration 140: Loss = -9878.521508911725
Iteration 150: Loss = -9878.467827970653
Iteration 160: Loss = -9878.428366948216
Iteration 170: Loss = -9878.400418202764
Iteration 180: Loss = -9878.38142367598
Iteration 190: Loss = -9878.369128305854
Iteration 200: Loss = -9878.361711839965
Iteration 210: Loss = -9878.357672837115
Iteration 220: Loss = -9878.355915615215
Iteration 230: Loss = -9878.35566945352
Iteration 240: Loss = -9878.356398542766
1
Iteration 250: Loss = -9878.357635289389
2
Iteration 260: Loss = -9878.35912069195
3
Stopping early at iteration 259 due to no improvement.
pi: tensor([[0.4398, 0.5602],
        [0.2847, 0.7153]], dtype=torch.float64)
alpha: tensor([0.3349, 0.6651])
beta: tensor([[[0.0984, 0.1294],
         [0.8391, 0.1613]],

        [[0.0798, 0.1159],
         [0.4395, 0.7146]],

        [[0.5449, 0.1309],
         [0.2323, 0.5584]],

        [[0.4302, 0.1134],
         [0.1729, 0.2079]],

        [[0.1340, 0.1085],
         [0.1293, 0.3203]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0025406526374637966
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.11714891030445379
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 64
Adjusted Rand Index: 0.06387589263728145
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 75
Adjusted Rand Index: 0.24173797699736327
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 70
Adjusted Rand Index: 0.1528117359413203
Global Adjusted Rand Index: 0.09935500386733852
Average Adjusted Rand Index: 0.11460677264859101
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18208.38004791905
Iteration 100: Loss = -9892.72557789598
Iteration 200: Loss = -9887.470774945647
Iteration 300: Loss = -9884.676892262285
Iteration 400: Loss = -9882.521304210366
Iteration 500: Loss = -9881.631777852173
Iteration 600: Loss = -9880.795381536314
Iteration 700: Loss = -9879.835973814106
Iteration 800: Loss = -9878.403740915373
Iteration 900: Loss = -9875.369171492903
Iteration 1000: Loss = -9863.050722284657
Iteration 1100: Loss = -9861.253310036951
Iteration 1200: Loss = -9861.052593259106
Iteration 1300: Loss = -9860.002800948394
Iteration 1400: Loss = -9859.858794733304
Iteration 1500: Loss = -9859.796857833973
Iteration 1600: Loss = -9859.753900316788
Iteration 1700: Loss = -9859.721272841218
Iteration 1800: Loss = -9859.694889103846
Iteration 1900: Loss = -9859.672799199032
Iteration 2000: Loss = -9859.654251482698
Iteration 2100: Loss = -9859.639076506162
Iteration 2200: Loss = -9859.626308719191
Iteration 2300: Loss = -9859.615305454337
Iteration 2400: Loss = -9859.605554394368
Iteration 2500: Loss = -9859.596841873192
Iteration 2600: Loss = -9859.5891208467
Iteration 2700: Loss = -9859.582397716511
Iteration 2800: Loss = -9859.576512772725
Iteration 2900: Loss = -9859.57132169029
Iteration 3000: Loss = -9859.566802468
Iteration 3100: Loss = -9859.562856073198
Iteration 3200: Loss = -9859.559244268634
Iteration 3300: Loss = -9859.556006055513
Iteration 3400: Loss = -9859.552994202868
Iteration 3500: Loss = -9859.550313682546
Iteration 3600: Loss = -9859.547780422638
Iteration 3700: Loss = -9859.545541651614
Iteration 3800: Loss = -9859.54317474964
Iteration 3900: Loss = -9859.541181041708
Iteration 4000: Loss = -9859.539335239862
Iteration 4100: Loss = -9859.537522175937
Iteration 4200: Loss = -9859.535757923575
Iteration 4300: Loss = -9859.534659245675
Iteration 4400: Loss = -9859.53284230976
Iteration 4500: Loss = -9859.531560595098
Iteration 4600: Loss = -9859.530411781523
Iteration 4700: Loss = -9859.529224761052
Iteration 4800: Loss = -9859.528194867009
Iteration 4900: Loss = -9859.52724148388
Iteration 5000: Loss = -9859.52634660777
Iteration 5100: Loss = -9859.525518947
Iteration 5200: Loss = -9859.524693363437
Iteration 5300: Loss = -9859.52404491038
Iteration 5400: Loss = -9859.523310902809
Iteration 5500: Loss = -9859.522856902946
Iteration 5600: Loss = -9859.523877569372
1
Iteration 5700: Loss = -9859.521496310943
Iteration 5800: Loss = -9859.521031768623
Iteration 5900: Loss = -9859.52076406893
Iteration 6000: Loss = -9859.529359747195
1
Iteration 6100: Loss = -9859.52964360792
2
Iteration 6200: Loss = -9859.520914592056
3
Iteration 6300: Loss = -9859.518985393763
Iteration 6400: Loss = -9859.525233497452
1
Iteration 6500: Loss = -9859.518292996281
Iteration 6600: Loss = -9859.529322146678
1
Iteration 6700: Loss = -9859.517719166979
Iteration 6800: Loss = -9859.535445048106
1
Iteration 6900: Loss = -9859.517163282646
Iteration 7000: Loss = -9859.52009342084
1
Iteration 7100: Loss = -9859.516632637755
Iteration 7200: Loss = -9859.522382351877
1
Iteration 7300: Loss = -9859.5395682841
2
Iteration 7400: Loss = -9859.516003185388
Iteration 7500: Loss = -9859.515738126864
Iteration 7600: Loss = -9859.525393987919
1
Iteration 7700: Loss = -9859.532956041225
2
Iteration 7800: Loss = -9859.516177974356
3
Iteration 7900: Loss = -9859.515200588132
Iteration 8000: Loss = -9859.521872698002
1
Iteration 8100: Loss = -9859.516994203199
2
Iteration 8200: Loss = -9859.539115727832
3
Iteration 8300: Loss = -9859.591499296406
4
Iteration 8400: Loss = -9859.563727939014
5
Iteration 8500: Loss = -9859.513546880518
Iteration 8600: Loss = -9859.513228842892
Iteration 8700: Loss = -9859.518293925972
1
Iteration 8800: Loss = -9859.511590573522
Iteration 8900: Loss = -9859.509397058018
Iteration 9000: Loss = -9859.502538249324
Iteration 9100: Loss = -9859.434256459777
Iteration 9200: Loss = -9858.625550487468
Iteration 9300: Loss = -9858.42590734242
Iteration 9400: Loss = -9858.398465851824
Iteration 9500: Loss = -9858.362983498058
Iteration 9600: Loss = -9858.200510666298
Iteration 9700: Loss = -9858.188601404505
Iteration 9800: Loss = -9858.194423859715
1
Iteration 9900: Loss = -9858.186444155883
Iteration 10000: Loss = -9858.183096522413
Iteration 10100: Loss = -9858.183114372883
1
Iteration 10200: Loss = -9858.178816332234
Iteration 10300: Loss = -9858.176370759118
Iteration 10400: Loss = -9858.174732711204
Iteration 10500: Loss = -9858.173485381241
Iteration 10600: Loss = -9858.172953653077
Iteration 10700: Loss = -9858.17393928551
1
Iteration 10800: Loss = -9858.174354620052
2
Iteration 10900: Loss = -9858.17413216849
3
Iteration 11000: Loss = -9858.209084207512
4
Iteration 11100: Loss = -9858.239955181258
5
Iteration 11200: Loss = -9858.188188290429
6
Iteration 11300: Loss = -9858.174036275994
7
Iteration 11400: Loss = -9858.17335432554
8
Iteration 11500: Loss = -9858.214376148657
9
Iteration 11600: Loss = -9858.172287555022
Iteration 11700: Loss = -9858.172084572512
Iteration 11800: Loss = -9858.172141872346
1
Iteration 11900: Loss = -9858.172301866263
2
Iteration 12000: Loss = -9858.18839106267
3
Iteration 12100: Loss = -9858.225752961464
4
Iteration 12200: Loss = -9858.1863354693
5
Iteration 12300: Loss = -9858.173098284025
6
Iteration 12400: Loss = -9858.172513906364
7
Iteration 12500: Loss = -9858.17811296459
8
Iteration 12600: Loss = -9858.173252438028
9
Iteration 12700: Loss = -9858.178250178193
10
Stopping early at iteration 12700 due to no improvement.
tensor([[ 1.4518, -2.8840],
        [ 1.4668, -2.9444],
        [ 2.9280, -4.5820],
        [-0.7784, -1.1574],
        [ 1.7727, -3.4366],
        [-1.7845, -1.0081],
        [ 3.4770, -5.3392],
        [ 0.0649, -1.6219],
        [ 0.9219, -3.1836],
        [-0.3507, -1.9191],
        [ 2.9790, -4.8042],
        [ 3.0829, -4.4746],
        [-1.4855,  0.0939],
        [ 3.8152, -5.2803],
        [ 0.4366, -2.1777],
        [ 5.8414, -7.5843],
        [ 1.6303, -3.2817],
        [ 2.0424, -4.9882],
        [-1.2495, -0.1404],
        [ 4.4322, -7.3561],
        [-0.3178, -1.0781],
        [ 3.6339, -5.9414],
        [-0.9798, -0.6464],
        [ 4.2218, -5.7112],
        [ 0.1497, -1.9594],
        [-0.7726, -0.6930],
        [ 2.9815, -4.4676],
        [ 5.7745, -7.1678],
        [-0.7325, -0.7139],
        [-2.8573, -1.7579],
        [ 4.9701, -7.7891],
        [ 2.8149, -4.2598],
        [ 4.0382, -5.5090],
        [-0.0133, -1.8344],
        [ 0.4840, -2.3797],
        [-0.1739, -1.8867],
        [ 3.3222, -5.0726],
        [ 0.6265, -2.0360],
        [ 5.7781, -7.2487],
        [-1.6809,  0.2596],
        [ 3.0875, -4.5105],
        [-1.1116, -0.4968],
        [ 4.6800, -8.8728],
        [-0.0832, -1.3215],
        [ 1.0138, -2.4823],
        [ 4.3838, -7.3375],
        [ 2.2569, -4.8513],
        [ 1.1825, -2.5877],
        [ 2.4871, -3.8984],
        [ 2.7065, -4.1677],
        [-0.9477, -0.4611],
        [ 2.3893, -5.2577],
        [-0.7265, -1.1187],
        [ 1.5382, -3.1560],
        [ 3.0134, -4.4487],
        [ 4.8423, -6.7743],
        [-0.2836, -1.3020],
        [ 2.1477, -3.6311],
        [ 0.9235, -2.7372],
        [ 4.3759, -6.4336],
        [-0.7329, -1.0549],
        [-0.6545, -1.0825],
        [ 1.4593, -2.9709],
        [-0.5897, -1.4407],
        [ 1.1469, -2.8033],
        [ 0.5927, -2.4875],
        [ 2.4655, -4.5155],
        [ 4.4886, -6.4490],
        [-0.9385, -0.4489],
        [-0.9124, -1.5825],
        [-0.0518, -1.3563],
        [ 3.0982, -5.6854],
        [-0.0303, -1.3650],
        [ 2.2211, -4.2217],
        [ 1.7908, -3.7072],
        [-0.2679, -1.4381],
        [ 0.8715, -4.2023],
        [ 1.3560, -3.4286],
        [-1.3924, -1.0504],
        [ 4.6296, -6.1536],
        [ 4.8783, -7.6431],
        [ 5.5360, -7.2004],
        [ 1.0203, -2.4820],
        [ 2.6007, -4.0951],
        [ 2.3495, -3.8164],
        [-0.6538, -2.0152],
        [ 0.3022, -1.8907],
        [ 1.6465, -3.9640],
        [ 3.4496, -5.8506],
        [ 2.3314, -4.0729],
        [ 3.7359, -6.1372],
        [ 3.6122, -5.1935],
        [ 3.9079, -5.5281],
        [ 4.6153, -8.5783],
        [-0.1464, -1.4487],
        [ 3.8452, -5.3305],
        [ 1.4354, -2.8279],
        [ 0.1043, -1.5304],
        [-0.0285, -2.8931],
        [ 5.3542, -7.5814]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.3472e-01, 7.6528e-01],
        [1.0000e+00, 1.4968e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8626, 0.1374], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1537, 0.1163],
         [0.8391, 0.1831]],

        [[0.0798, 0.0837],
         [0.4395, 0.7146]],

        [[0.5449, 0.1102],
         [0.2323, 0.5584]],

        [[0.4302, 0.0911],
         [0.1729, 0.2079]],

        [[0.1340, 0.0852],
         [0.1293, 0.3203]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.04410228386827175
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 77
Adjusted Rand Index: 0.28291043908070007
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 24
Adjusted Rand Index: 0.2594517344419904
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 84
Adjusted Rand Index: 0.4568536306748378
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 27
Adjusted Rand Index: 0.2052710677761079
Global Adjusted Rand Index: -0.0022516416841514703
Average Adjusted Rand Index: 0.24971783116838156
Iteration 0: Loss = -11557.458395954241
Iteration 10: Loss = -9881.425747341387
Iteration 20: Loss = -9880.897559207175
Iteration 30: Loss = -9880.592830473772
Iteration 40: Loss = -9880.30489195696
Iteration 50: Loss = -9880.034592685988
Iteration 60: Loss = -9879.784136675422
Iteration 70: Loss = -9879.554431419378
Iteration 80: Loss = -9879.346091302834
Iteration 90: Loss = -9879.159511590635
Iteration 100: Loss = -9878.994936085282
Iteration 110: Loss = -9878.852499269246
Iteration 120: Loss = -9878.731917611782
Iteration 130: Loss = -9878.632536046418
Iteration 140: Loss = -9878.55306692539
Iteration 150: Loss = -9878.491657798346
Iteration 160: Loss = -9878.445732018927
Iteration 170: Loss = -9878.412577129868
Iteration 180: Loss = -9878.389598251932
Iteration 190: Loss = -9878.374341928731
Iteration 200: Loss = -9878.364786372576
Iteration 210: Loss = -9878.359221517558
Iteration 220: Loss = -9878.356546302633
Iteration 230: Loss = -9878.355711397415
Iteration 240: Loss = -9878.355962680404
1
Iteration 250: Loss = -9878.356995871909
2
Iteration 260: Loss = -9878.358413656433
3
Stopping early at iteration 259 due to no improvement.
pi: tensor([[0.7143, 0.2857],
        [0.5591, 0.4409]], dtype=torch.float64)
alpha: tensor([0.6639, 0.3361])
beta: tensor([[[0.1614, 0.1294],
         [0.8156, 0.0985]],

        [[0.2028, 0.1160],
         [0.2965, 0.8500]],

        [[0.6808, 0.1309],
         [0.9467, 0.0944]],

        [[0.3373, 0.1135],
         [0.7485, 0.1354]],

        [[0.4969, 0.1086],
         [0.4273, 0.4923]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0025406526374637966
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 32
Adjusted Rand Index: 0.11714891030445379
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 36
Adjusted Rand Index: 0.06387589263728145
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 25
Adjusted Rand Index: 0.24173797699736327
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 30
Adjusted Rand Index: 0.1528117359413203
Global Adjusted Rand Index: 0.09935500386733852
Average Adjusted Rand Index: 0.11460677264859101
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -11557.215949281728
Iteration 100: Loss = -9885.027595772619
Iteration 200: Loss = -9878.717045913882
Iteration 300: Loss = -9876.488534660251
Iteration 400: Loss = -9855.021594724789
Iteration 500: Loss = -9850.165053511964
Iteration 600: Loss = -9850.023788146036
Iteration 700: Loss = -9849.941911946587
Iteration 800: Loss = -9849.911627891706
Iteration 900: Loss = -9849.723020558722
Iteration 1000: Loss = -9849.223972622352
Iteration 1100: Loss = -9849.206918383548
Iteration 1200: Loss = -9849.202803069207
Iteration 1300: Loss = -9849.200673487967
Iteration 1400: Loss = -9849.199343547656
Iteration 1500: Loss = -9849.198511361572
Iteration 1600: Loss = -9849.197902401234
Iteration 1700: Loss = -9849.197482575775
Iteration 1800: Loss = -9849.197162468805
Iteration 1900: Loss = -9849.196952690525
Iteration 2000: Loss = -9849.196730025375
Iteration 2100: Loss = -9849.19661977561
Iteration 2200: Loss = -9849.196481126319
Iteration 2300: Loss = -9849.196566781198
1
Iteration 2400: Loss = -9849.196295117239
Iteration 2500: Loss = -9849.196255859812
Iteration 2600: Loss = -9849.196199418866
Iteration 2700: Loss = -9849.196114499946
Iteration 2800: Loss = -9849.196117687055
1
Iteration 2900: Loss = -9849.213247487673
2
Iteration 3000: Loss = -9849.196019615516
Iteration 3100: Loss = -9849.24237742518
1
Iteration 3200: Loss = -9849.195943517176
Iteration 3300: Loss = -9849.195907485919
Iteration 3400: Loss = -9849.19610014956
1
Iteration 3500: Loss = -9849.195891330397
Iteration 3600: Loss = -9849.211139813433
1
Iteration 3700: Loss = -9849.196732324244
2
Iteration 3800: Loss = -9849.19595844364
3
Iteration 3900: Loss = -9849.19893610269
4
Iteration 4000: Loss = -9849.199218825597
5
Iteration 4100: Loss = -9849.197047244184
6
Iteration 4200: Loss = -9849.195832525458
Iteration 4300: Loss = -9849.196851118757
1
Iteration 4400: Loss = -9849.195912636755
2
Iteration 4500: Loss = -9849.19578087989
Iteration 4600: Loss = -9849.195953513134
1
Iteration 4700: Loss = -9849.200176295286
2
Iteration 4800: Loss = -9849.222361897759
3
Iteration 4900: Loss = -9849.195698504405
Iteration 5000: Loss = -9849.201411942804
1
Iteration 5100: Loss = -9849.195721842949
2
Iteration 5200: Loss = -9849.196388932005
3
Iteration 5300: Loss = -9849.200147821366
4
Iteration 5400: Loss = -9849.197044950919
5
Iteration 5500: Loss = -9849.196068194846
6
Iteration 5600: Loss = -9849.200781259462
7
Iteration 5700: Loss = -9849.199023193336
8
Iteration 5800: Loss = -9849.206114428618
9
Iteration 5900: Loss = -9849.197233086055
10
Stopping early at iteration 5900 due to no improvement.
tensor([[ 1.2412, -2.6919],
        [ 1.4905, -3.0132],
        [-0.6281, -0.9102],
        [ 0.2295, -2.1615],
        [-1.6185, -0.6101],
        [ 1.0050, -2.7861],
        [ 0.6507, -2.1495],
        [ 1.2862, -2.6758],
        [ 1.5591, -3.0332],
        [ 0.2077, -1.7123],
        [-2.1733,  0.4253],
        [ 0.5222, -1.9237],
        [ 2.0199, -3.4972],
        [-1.6264,  0.0103],
        [ 0.2605, -1.6689],
        [-4.6828,  2.8853],
        [-0.3524, -1.3052],
        [-1.0188, -2.6653],
        [ 2.1643, -3.5525],
        [-2.1056,  0.1251],
        [ 1.1103, -2.5040],
        [-2.5692,  0.8393],
        [ 1.6416, -3.5435],
        [-3.4350,  1.0527],
        [ 1.3548, -2.7459],
        [ 0.3567, -3.2578],
        [ 0.2060, -1.8367],
        [-2.1532,  0.4733],
        [ 1.1987, -2.7237],
        [ 0.6852, -2.8382],
        [-0.6128, -2.7429],
        [-3.0382,  0.9882],
        [-2.4500,  1.0284],
        [-0.8037, -2.8604],
        [-0.6979, -0.8201],
        [-0.5504, -0.8365],
        [-3.5814,  1.8245],
        [ 1.7220, -3.5214],
        [-2.7987,  0.6742],
        [ 1.0976, -4.0030],
        [-1.8684,  0.2594],
        [ 2.7340, -4.2195],
        [-0.3142, -1.0723],
        [ 1.2047, -3.8451],
        [-2.4326,  0.4834],
        [-0.5611, -0.9594],
        [-2.9745,  1.1781],
        [-0.1372, -2.3909],
        [-1.0489, -0.7009],
        [-3.1838,  1.7910],
        [ 0.3570, -2.1642],
        [-3.0753,  0.4349],
        [ 0.2197, -1.6815],
        [-1.0427, -1.1749],
        [-3.7669, -0.1910],
        [-3.4671,  0.9951],
        [ 0.2460, -1.7008],
        [-3.1402,  1.7510],
        [ 1.0949, -3.2700],
        [-4.2547,  2.1669],
        [ 1.4185, -2.8069],
        [ 0.6410, -3.6128],
        [ 1.1326, -2.7213],
        [-1.8621,  0.2818],
        [-2.8088,  1.3999],
        [-1.9482, -0.0508],
        [ 0.2737, -2.5107],
        [-4.5967,  3.2031],
        [-0.7615, -1.9603],
        [ 0.5629, -2.0091],
        [-0.4303, -1.1771],
        [-3.1234,  1.2827],
        [ 1.7286, -3.2605],
        [-0.9617, -0.9713],
        [ 0.8441, -2.2964],
        [ 2.4058, -3.8874],
        [-1.9315, -2.6838],
        [-2.5970,  1.1731],
        [ 0.6285, -2.5862],
        [-2.6352,  0.9264],
        [-1.9696, -0.0054],
        [-2.2939,  0.3683],
        [-0.2546, -1.2139],
        [-3.2147,  1.4128],
        [ 0.6908, -2.1829],
        [ 0.8373, -2.5809],
        [ 0.4744, -2.7055],
        [-1.1589, -2.1111],
        [-2.2982,  0.9106],
        [-0.2462, -1.4971],
        [-2.4305,  1.0442],
        [-0.9061, -0.4808],
        [-0.2054, -1.6265],
        [-1.1507, -0.3649],
        [ 0.5878, -2.4909],
        [-1.7425,  0.1770],
        [-1.7281, -2.8871],
        [-1.9521, -0.1348],
        [-2.9459,  1.5266],
        [ 1.1510, -2.7494]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7526, 0.2474],
        [0.2180, 0.7820]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5663, 0.4337], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1405, 0.1067],
         [0.8156, 0.2148]],

        [[0.2028, 0.0933],
         [0.2965, 0.8500]],

        [[0.6808, 0.1054],
         [0.9467, 0.0944]],

        [[0.3373, 0.0964],
         [0.7485, 0.1354]],

        [[0.4969, 0.0830],
         [0.4273, 0.4923]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 80
Adjusted Rand Index: 0.353778413555379
time is 1
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 86
Adjusted Rand Index: 0.5135632264964799
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 88
Adjusted Rand Index: 0.5733903713149096
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 87
Adjusted Rand Index: 0.5430434321976833
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7025959183673469
Global Adjusted Rand Index: 0.5348971496313834
Average Adjusted Rand Index: 0.5372742723863597
Iteration 0: Loss = -33589.22088353682
Iteration 10: Loss = -9882.257946827021
Iteration 20: Loss = -9880.834788401178
Iteration 30: Loss = -9880.500847927844
Iteration 40: Loss = -9880.220055484944
Iteration 50: Loss = -9879.956255713052
Iteration 60: Loss = -9879.712154191564
Iteration 70: Loss = -9879.48890496228
Iteration 80: Loss = -9879.287157386256
Iteration 90: Loss = -9879.107227003473
Iteration 100: Loss = -9878.949375110325
Iteration 110: Loss = -9878.813601302945
Iteration 120: Loss = -9878.69959077619
Iteration 130: Loss = -9878.606389987923
Iteration 140: Loss = -9878.532625942098
Iteration 150: Loss = -9878.47617475353
Iteration 160: Loss = -9878.434472516237
Iteration 170: Loss = -9878.40462070864
Iteration 180: Loss = -9878.384252758702
Iteration 190: Loss = -9878.37090419023
Iteration 200: Loss = -9878.362756676896
Iteration 210: Loss = -9878.358175753547
Iteration 220: Loss = -9878.356113023674
Iteration 230: Loss = -9878.355659679057
Iteration 240: Loss = -9878.356237373438
1
Iteration 250: Loss = -9878.35738449425
2
Iteration 260: Loss = -9878.358842586615
3
Stopping early at iteration 259 due to no improvement.
pi: tensor([[0.4402, 0.5598],
        [0.2851, 0.7149]], dtype=torch.float64)
alpha: tensor([0.3353, 0.6647])
beta: tensor([[[0.0984, 0.1294],
         [0.6432, 0.1613]],

        [[0.2648, 0.1160],
         [0.6209, 0.3175]],

        [[0.6057, 0.1309],
         [0.7287, 0.4360]],

        [[0.4382, 0.1135],
         [0.8783, 0.0946]],

        [[0.2581, 0.1086],
         [0.9233, 0.6359]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0025406526374637966
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.11714891030445379
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 64
Adjusted Rand Index: 0.06387589263728145
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 75
Adjusted Rand Index: 0.24173797699736327
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 70
Adjusted Rand Index: 0.1528117359413203
Global Adjusted Rand Index: 0.09935500386733852
Average Adjusted Rand Index: 0.11460677264859101
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33589.05463460108
Iteration 100: Loss = -10110.706855453816
Iteration 200: Loss = -10068.266385429131
Iteration 300: Loss = -10042.585192869854
Iteration 400: Loss = -10004.636703423877
Iteration 500: Loss = -9955.218724167458
Iteration 600: Loss = -9906.328137442097
Iteration 700: Loss = -9900.8373806665
Iteration 800: Loss = -9891.403965502963
Iteration 900: Loss = -9890.39491138769
Iteration 1000: Loss = -9889.790660543804
Iteration 1100: Loss = -9889.381770552893
Iteration 1200: Loss = -9889.085512832913
Iteration 1300: Loss = -9888.861045939393
Iteration 1400: Loss = -9888.685443091386
Iteration 1500: Loss = -9888.54466913054
Iteration 1600: Loss = -9888.429606343288
Iteration 1700: Loss = -9888.33409529526
Iteration 1800: Loss = -9888.25372945405
Iteration 1900: Loss = -9888.18514221147
Iteration 2000: Loss = -9888.126438935284
Iteration 2100: Loss = -9888.075659491544
Iteration 2200: Loss = -9888.03135060119
Iteration 2300: Loss = -9887.992388746456
Iteration 2400: Loss = -9887.95798828956
Iteration 2500: Loss = -9887.927472948828
Iteration 2600: Loss = -9887.90026127862
Iteration 2700: Loss = -9887.875855243105
Iteration 2800: Loss = -9887.853914106343
Iteration 2900: Loss = -9887.834085771106
Iteration 3000: Loss = -9887.81610626413
Iteration 3100: Loss = -9887.799826676228
Iteration 3200: Loss = -9887.78495631626
Iteration 3300: Loss = -9887.77138151625
Iteration 3400: Loss = -9887.758974664055
Iteration 3500: Loss = -9887.747602461986
Iteration 3600: Loss = -9887.73712735631
Iteration 3700: Loss = -9887.727473464414
Iteration 3800: Loss = -9887.718585345323
Iteration 3900: Loss = -9887.710372279535
Iteration 4000: Loss = -9887.702783355442
Iteration 4100: Loss = -9887.695694761504
Iteration 4200: Loss = -9887.68919217501
Iteration 4300: Loss = -9887.683104332275
Iteration 4400: Loss = -9887.677437761262
Iteration 4500: Loss = -9887.672173708852
Iteration 4600: Loss = -9887.667252093212
Iteration 4700: Loss = -9887.66267057843
Iteration 4800: Loss = -9887.658339331749
Iteration 4900: Loss = -9887.65434316057
Iteration 5000: Loss = -9887.650553092491
Iteration 5100: Loss = -9887.647064338593
Iteration 5200: Loss = -9887.643728375608
Iteration 5300: Loss = -9887.640637163355
Iteration 5400: Loss = -9887.637669455786
Iteration 5500: Loss = -9887.63484533934
Iteration 5600: Loss = -9887.632190563316
Iteration 5700: Loss = -9887.629767902914
Iteration 5800: Loss = -9887.627443685076
Iteration 5900: Loss = -9887.6333596744
1
Iteration 6000: Loss = -9887.623187461979
Iteration 6100: Loss = -9887.621224961867
Iteration 6200: Loss = -9887.61938267607
Iteration 6300: Loss = -9887.63713819484
1
Iteration 6400: Loss = -9887.61593963872
Iteration 6500: Loss = -9887.614382901078
Iteration 6600: Loss = -9887.61293471175
Iteration 6700: Loss = -9887.61181722906
Iteration 6800: Loss = -9887.610277082968
Iteration 6900: Loss = -9887.609032162782
Iteration 7000: Loss = -9887.923095380593
1
Iteration 7100: Loss = -9887.606483857377
Iteration 7200: Loss = -9887.604902617571
Iteration 7300: Loss = -9887.602206277943
Iteration 7400: Loss = -9887.589084305946
Iteration 7500: Loss = -9887.558268694112
Iteration 7600: Loss = -9887.561340860546
1
Iteration 7700: Loss = -9887.704130255486
2
Iteration 7800: Loss = -9887.552371350399
Iteration 7900: Loss = -9887.551565514521
Iteration 8000: Loss = -9887.550956133027
Iteration 8100: Loss = -9887.550271278325
Iteration 8200: Loss = -9887.549697218448
Iteration 8300: Loss = -9887.611820501092
1
Iteration 8400: Loss = -9887.54870099396
Iteration 8500: Loss = -9887.548181511673
Iteration 8600: Loss = -9887.74627840584
1
Iteration 8700: Loss = -9887.547422986707
Iteration 8800: Loss = -9887.546921493233
Iteration 8900: Loss = -9887.612699163106
1
Iteration 9000: Loss = -9887.546181231743
Iteration 9100: Loss = -9887.54789156917
1
Iteration 9200: Loss = -9887.545654806128
Iteration 9300: Loss = -9887.54513726238
Iteration 9400: Loss = -9887.662598510205
1
Iteration 9500: Loss = -9887.544610031104
Iteration 9600: Loss = -9887.54432396148
Iteration 9700: Loss = -9887.544533127095
1
Iteration 9800: Loss = -9887.543898990303
Iteration 9900: Loss = -9887.543605747635
Iteration 10000: Loss = -9887.543366338456
Iteration 10100: Loss = -9887.559881018526
1
Iteration 10200: Loss = -9887.542909683743
Iteration 10300: Loss = -9887.542767341152
Iteration 10400: Loss = -9887.566627753915
1
Iteration 10500: Loss = -9887.542388353548
Iteration 10600: Loss = -9887.578894184195
1
Iteration 10700: Loss = -9887.54258968788
2
Iteration 10800: Loss = -9887.54451962039
3
Iteration 10900: Loss = -9887.566764818434
4
Iteration 11000: Loss = -9887.54167995689
Iteration 11100: Loss = -9887.541962287996
1
Iteration 11200: Loss = -9887.541523039303
Iteration 11300: Loss = -9887.541291713722
Iteration 11400: Loss = -9887.562295796317
1
Iteration 11500: Loss = -9887.541081051892
Iteration 11600: Loss = -9887.555386808519
1
Iteration 11700: Loss = -9887.541662838898
2
Iteration 11800: Loss = -9887.562142931134
3
Iteration 11900: Loss = -9887.55155688868
4
Iteration 12000: Loss = -9887.540679609032
Iteration 12100: Loss = -9887.58852537448
1
Iteration 12200: Loss = -9887.540503107382
Iteration 12300: Loss = -9887.540455839642
Iteration 12400: Loss = -9887.573894731355
1
Iteration 12500: Loss = -9887.54035618593
Iteration 12600: Loss = -9887.545858248242
1
Iteration 12700: Loss = -9887.540234831606
Iteration 12800: Loss = -9887.540176936396
Iteration 12900: Loss = -9887.541279430448
1
Iteration 13000: Loss = -9887.540525400545
2
Iteration 13100: Loss = -9887.540079979399
Iteration 13200: Loss = -9887.54913401085
1
Iteration 13300: Loss = -9887.53995067619
Iteration 13400: Loss = -9887.569051363937
1
Iteration 13500: Loss = -9887.540054574492
2
Iteration 13600: Loss = -9887.53983173673
Iteration 13700: Loss = -9887.540684283516
1
Iteration 13800: Loss = -9887.539758874245
Iteration 13900: Loss = -9887.540053973677
1
Iteration 14000: Loss = -9887.539703121183
Iteration 14100: Loss = -9887.539678778889
Iteration 14200: Loss = -9887.540705596299
1
Iteration 14300: Loss = -9887.539608717248
Iteration 14400: Loss = -9887.573704342245
1
Iteration 14500: Loss = -9887.53957962629
Iteration 14600: Loss = -9887.698976753834
1
Iteration 14700: Loss = -9887.539572261323
Iteration 14800: Loss = -9887.53957905517
1
Iteration 14900: Loss = -9887.541312067811
2
Iteration 15000: Loss = -9887.539564579427
Iteration 15100: Loss = -9887.53956553467
1
Iteration 15200: Loss = -9887.563414729533
2
Iteration 15300: Loss = -9887.53954035375
Iteration 15400: Loss = -9887.603259597043
1
Iteration 15500: Loss = -9887.544164887155
2
Iteration 15600: Loss = -9887.54057508387
3
Iteration 15700: Loss = -9887.539499401564
Iteration 15800: Loss = -9887.539547774024
1
Iteration 15900: Loss = -9887.539517718335
2
Iteration 16000: Loss = -9887.674526162535
3
Iteration 16100: Loss = -9887.540423110524
4
Iteration 16200: Loss = -9887.539463925816
Iteration 16300: Loss = -9887.646362133835
1
Iteration 16400: Loss = -9887.543492245451
2
Iteration 16500: Loss = -9887.539436346302
Iteration 16600: Loss = -9887.539526373861
1
Iteration 16700: Loss = -9887.539460657386
2
Iteration 16800: Loss = -9887.539428956206
Iteration 16900: Loss = -9887.551761628434
1
Iteration 17000: Loss = -9887.54051273295
2
Iteration 17100: Loss = -9887.539412970882
Iteration 17200: Loss = -9887.574019109028
1
Iteration 17300: Loss = -9887.53940993407
Iteration 17400: Loss = -9887.539417264385
1
Iteration 17500: Loss = -9887.54474627076
2
Iteration 17600: Loss = -9887.539388984003
Iteration 17700: Loss = -9887.544331974988
1
Iteration 17800: Loss = -9887.539421432604
2
Iteration 17900: Loss = -9887.53938909274
3
Iteration 18000: Loss = -9887.540880234023
4
Iteration 18100: Loss = -9887.539385285863
Iteration 18200: Loss = -9887.53938687965
1
Iteration 18300: Loss = -9887.54163394286
2
Iteration 18400: Loss = -9887.539382703188
Iteration 18500: Loss = -9887.539370872402
Iteration 18600: Loss = -9887.540415604852
1
Iteration 18700: Loss = -9887.539388329864
2
Iteration 18800: Loss = -9887.540830878947
3
Iteration 18900: Loss = -9887.539388489546
4
Iteration 19000: Loss = -9887.539456506369
5
Iteration 19100: Loss = -9887.539446484885
6
Iteration 19200: Loss = -9887.539360548415
Iteration 19300: Loss = -9887.590442839954
1
Iteration 19400: Loss = -9887.539335682335
Iteration 19500: Loss = -9887.539337515282
1
Iteration 19600: Loss = -9887.541508864679
2
Iteration 19700: Loss = -9887.425880854786
Iteration 19800: Loss = -9887.416562351513
Iteration 19900: Loss = -9887.414150296576
tensor([[  9.0890, -10.5823],
        [  8.7323, -10.6256],
        [  8.8096, -10.2808],
        [  9.2317, -10.7098],
        [  8.7632, -10.8132],
        [  9.1704, -10.8640],
        [  8.9660, -10.9563],
        [  9.0107, -10.4637],
        [  9.2449, -10.8216],
        [  8.9318, -11.6061],
        [  8.9687, -11.2216],
        [  9.4782, -11.2471],
        [  8.6067, -11.5810],
        [  9.0474, -10.4643],
        [  9.4633, -11.0220],
        [  9.3250, -11.3964],
        [  8.1226, -11.0986],
        [  9.3840, -11.0921],
        [  8.5274,  -9.9767],
        [  9.0192, -10.9714],
        [  8.6320, -10.4596],
        [  8.8003, -11.2231],
        [  9.2442, -10.7184],
        [  9.0482, -10.4445],
        [  8.4989,  -9.9298],
        [  9.6656, -11.4042],
        [  9.0650, -10.4515],
        [  7.4529, -12.0681],
        [  8.5909, -10.3145],
        [  9.3934, -11.2443],
        [  8.3485, -11.4371],
        [  8.2592, -10.7619],
        [  8.8248, -10.9770],
        [  9.0499, -10.4822],
        [  9.3905, -10.8333],
        [  9.3105, -11.3557],
        [  8.9792, -10.6793],
        [  8.1949,  -9.9649],
        [  9.6571, -11.0840],
        [  8.8672, -10.3788],
        [  9.0148, -12.0455],
        [  8.6884, -10.5224],
        [  9.2528, -10.6765],
        [  8.2690, -10.9641],
        [  8.8713, -11.4379],
        [  9.6836, -11.3052],
        [  8.9186, -10.3546],
        [  9.4748, -10.9931],
        [  9.2423, -10.9566],
        [  9.1814, -10.8981],
        [  9.3772, -11.1059],
        [  8.4162, -10.8104],
        [  9.1741, -11.3951],
        [  8.0341, -12.6493],
        [  9.1482, -10.5390],
        [  8.8636, -10.2537],
        [  9.7606, -11.1538],
        [  8.4517, -11.0113],
        [  9.4391, -11.0024],
        [  9.0740, -10.4633],
        [  9.6043, -11.0540],
        [  8.7154, -10.1458],
        [  9.1705, -10.5838],
        [  9.4826, -11.1322],
        [  8.0811, -11.9951],
        [  8.8031, -10.3899],
        [  9.6722, -11.0593],
        [  8.2600, -10.2126],
        [  9.0152, -11.2858],
        [  7.4967, -10.8031],
        [  9.2670, -11.2290],
        [  8.9245, -10.3291],
        [  9.1160, -10.6408],
        [  9.2926, -10.9208],
        [  8.8052, -10.4787],
        [  9.2489, -10.6909],
        [  8.3338, -10.9940],
        [  9.3416, -12.5539],
        [  9.4955, -10.9595],
        [  9.2036, -10.5943],
        [  9.5145, -10.9229],
        [  9.3289, -10.7319],
        [  9.1518, -10.9373],
        [  8.1498,  -9.8805],
        [  9.1356, -10.5225],
        [  9.5635, -11.0329],
        [  9.2194, -10.6286],
        [  9.5223, -11.0697],
        [  8.9758, -10.5536],
        [  8.5372, -10.0537],
        [  9.3490, -10.7355],
        [  9.1523, -10.5740],
        [  9.5727, -11.4552],
        [  8.7727, -12.8322],
        [  9.3185, -10.7500],
        [  9.0605, -11.5912],
        [  9.2161, -11.0558],
        [  9.7518, -11.3894],
        [  9.2268, -10.6240],
        [  8.3021, -10.5643]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.2517e-08, 1.0000e+00],
        [9.9197e-01, 8.0280e-03]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 5.9465e-09], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1374, 0.1424],
         [0.6432, 0.1370]],

        [[0.2648, 0.1340],
         [0.6209, 0.3175]],

        [[0.6057, 0.2226],
         [0.7287, 0.4360]],

        [[0.4382, 0.1867],
         [0.8783, 0.0946]],

        [[0.2581, 0.0802],
         [0.9233, 0.6359]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014116079949687524
Average Adjusted Rand Index: 0.0
9963.412899975014
new:  [-0.0011269512297611439, -0.0022516416841514703, 0.5348971496313834, -0.0014116079949687524] [-0.0020854854324299475, 0.24971783116838156, 0.5372742723863597, 0.0] [9882.92121746058, 9858.178250178193, 9849.197233086055, 9887.398101432133]
prior:  [0.0, 0.09935500386733852, 0.09935500386733852, 0.09935500386733852] [0.0, 0.11460677264859101, 0.11460677264859101, 0.11460677264859101] [9887.788016767143, 9878.35912069195, 9878.358413656433, 9878.358842586615]
-----------------------------------------------------------------------------------------
This iteration is 2
True Objective function: Loss = -10034.962704662514
Iteration 0: Loss = -14176.72849153742
Iteration 10: Loss = -9905.506642604187
Iteration 20: Loss = -9905.369953120662
Iteration 30: Loss = -9905.307492177877
Iteration 40: Loss = -9905.253192114116
Iteration 50: Loss = -9905.178714500218
Iteration 60: Loss = -9905.048414784105
Iteration 70: Loss = -9904.83235648981
Iteration 80: Loss = -9904.604205525557
Iteration 90: Loss = -9904.48512730233
Iteration 100: Loss = -9904.443290876794
Iteration 110: Loss = -9904.425004986608
Iteration 120: Loss = -9904.41347602787
Iteration 130: Loss = -9904.404800844615
Iteration 140: Loss = -9904.397813130665
Iteration 150: Loss = -9904.392110466923
Iteration 160: Loss = -9904.387373892812
Iteration 170: Loss = -9904.383530799652
Iteration 180: Loss = -9904.380354353849
Iteration 190: Loss = -9904.377762634489
Iteration 200: Loss = -9904.375576627865
Iteration 210: Loss = -9904.373838642774
Iteration 220: Loss = -9904.372410162803
Iteration 230: Loss = -9904.371176844044
Iteration 240: Loss = -9904.37015957956
Iteration 250: Loss = -9904.369313542637
Iteration 260: Loss = -9904.36860361295
Iteration 270: Loss = -9904.368076481649
Iteration 280: Loss = -9904.367574655873
Iteration 290: Loss = -9904.367133589905
Iteration 300: Loss = -9904.36677322326
Iteration 310: Loss = -9904.366506488612
Iteration 320: Loss = -9904.36623245314
Iteration 330: Loss = -9904.366034523036
Iteration 340: Loss = -9904.365854778074
Iteration 350: Loss = -9904.365687345906
Iteration 360: Loss = -9904.365556772937
Iteration 370: Loss = -9904.365422885248
Iteration 380: Loss = -9904.365349187332
Iteration 390: Loss = -9904.365263061516
Iteration 400: Loss = -9904.365199060865
Iteration 410: Loss = -9904.365115392284
Iteration 420: Loss = -9904.36505354606
Iteration 430: Loss = -9904.364994877105
Iteration 440: Loss = -9904.36493541401
Iteration 450: Loss = -9904.364915935117
Iteration 460: Loss = -9904.364872711376
Iteration 470: Loss = -9904.364857469756
Iteration 480: Loss = -9904.364823725156
Iteration 490: Loss = -9904.364809147844
Iteration 500: Loss = -9904.36481592949
1
Iteration 510: Loss = -9904.364780827951
Iteration 520: Loss = -9904.36477874125
Iteration 530: Loss = -9904.364767640895
Iteration 540: Loss = -9904.364741634547
Iteration 550: Loss = -9904.36474831164
1
Iteration 560: Loss = -9904.364728663888
Iteration 570: Loss = -9904.364715442442
Iteration 580: Loss = -9904.36470516577
Iteration 590: Loss = -9904.364711060762
1
Iteration 600: Loss = -9904.364717004604
2
Iteration 610: Loss = -9904.364667163314
Iteration 620: Loss = -9904.364690332555
1
Iteration 630: Loss = -9904.364681491847
2
Iteration 640: Loss = -9904.364694997961
3
Stopping early at iteration 639 due to no improvement.
pi: tensor([[0.1175, 0.8825],
        [0.0986, 0.9014]], dtype=torch.float64)
alpha: tensor([0.1003, 0.8997])
beta: tensor([[[0.0984, 0.1249],
         [0.8989, 0.1408]],

        [[0.4632, 0.0977],
         [0.4234, 0.3787]],

        [[0.7977, 0.1218],
         [0.4338, 0.1397]],

        [[0.9828, 0.1059],
         [0.4748, 0.8840]],

        [[0.7259, 0.1365],
         [0.0842, 0.2466]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.451712833749729e-05
Average Adjusted Rand Index: -0.0003077958928485548
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14205.050377666694
Iteration 100: Loss = -9907.396268674973
Iteration 200: Loss = -9906.38883519287
Iteration 300: Loss = -9906.030513754498
Iteration 400: Loss = -9905.906447725603
Iteration 500: Loss = -9905.843505352248
Iteration 600: Loss = -9905.777352010262
Iteration 700: Loss = -9905.664761553891
Iteration 800: Loss = -9905.429894307204
Iteration 900: Loss = -9905.119563672804
Iteration 1000: Loss = -9904.959189564424
Iteration 1100: Loss = -9904.871076931437
Iteration 1200: Loss = -9904.824589667116
Iteration 1300: Loss = -9904.764441618638
Iteration 1400: Loss = -9904.729961642064
Iteration 1500: Loss = -9904.703038543796
Iteration 1600: Loss = -9904.807752320115
1
Iteration 1700: Loss = -9904.667336709601
Iteration 1800: Loss = -9904.658292376813
Iteration 1900: Loss = -9904.652429601856
Iteration 2000: Loss = -9904.60349484387
Iteration 2100: Loss = -9904.156112053452
Iteration 2200: Loss = -9904.05482710772
Iteration 2300: Loss = -9903.956697880962
Iteration 2400: Loss = -9903.923045498153
Iteration 2500: Loss = -9903.841097536386
Iteration 2600: Loss = -9903.839942726705
Iteration 2700: Loss = -9903.83965620147
Iteration 2800: Loss = -9903.841416900517
1
Iteration 2900: Loss = -9903.839488342823
Iteration 3000: Loss = -9904.006527454889
1
Iteration 3100: Loss = -9903.839468590648
Iteration 3200: Loss = -9903.839457977527
Iteration 3300: Loss = -9903.879259801506
1
Iteration 3400: Loss = -9903.839457729451
Iteration 3500: Loss = -9903.839466897816
1
Iteration 3600: Loss = -9903.888785362858
2
Iteration 3700: Loss = -9903.839450376925
Iteration 3800: Loss = -9903.839465567915
1
Iteration 3900: Loss = -9903.84151908195
2
Iteration 4000: Loss = -9903.839500772983
3
Iteration 4100: Loss = -9903.83944611835
Iteration 4200: Loss = -9903.852449955744
1
Iteration 4300: Loss = -9903.839460652813
2
Iteration 4400: Loss = -9903.839488379655
3
Iteration 4500: Loss = -9903.840218083313
4
Iteration 4600: Loss = -9903.839473836899
5
Iteration 4700: Loss = -9903.841236844897
6
Iteration 4800: Loss = -9903.839461963129
7
Iteration 4900: Loss = -9903.8394738107
8
Iteration 5000: Loss = -9903.839524083118
9
Iteration 5100: Loss = -9903.839468830443
10
Stopping early at iteration 5100 due to no improvement.
tensor([[-2.5624, -2.0528],
        [-3.6553, -0.9599],
        [-2.9117, -1.7035],
        [-3.3289, -1.2863],
        [-3.6045, -1.0107],
        [-2.7145, -1.9007],
        [-2.9486, -1.6666],
        [-3.4315, -1.1838],
        [-2.4826, -2.1326],
        [-3.3668, -1.2484],
        [-3.3461, -1.2691],
        [-2.7396, -1.8757],
        [-3.4015, -1.2137],
        [-2.6750, -1.9402],
        [-2.9286, -1.6867],
        [-3.0741, -1.5411],
        [-3.5761, -1.0392],
        [-3.7893, -0.8259],
        [-2.8946, -1.7206],
        [-2.9607, -1.6545],
        [-3.3021, -1.3131],
        [-3.0080, -1.6072],
        [-2.5604, -2.0548],
        [-2.9652, -1.6500],
        [-3.3805, -1.2347],
        [-3.3099, -1.3053],
        [-2.6627, -1.9525],
        [-3.6008, -1.0144],
        [-3.8264, -0.7888],
        [-3.6470, -0.9682],
        [-2.9125, -1.7027],
        [-3.4596, -1.1557],
        [-3.0455, -1.5698],
        [-2.8418, -1.7735],
        [-3.7048, -0.9105],
        [-3.3405, -1.2747],
        [-3.5341, -1.0811],
        [-2.4892, -2.1260],
        [-3.0487, -1.5665],
        [-3.5991, -1.0161],
        [-3.1023, -1.5129],
        [-3.2183, -1.3969],
        [-3.0041, -1.6111],
        [-3.3885, -1.2267],
        [-2.7675, -1.8477],
        [-3.3034, -1.3118],
        [-3.2290, -1.3862],
        [-3.5737, -1.0416],
        [-2.8840, -1.7312],
        [-3.2319, -1.3833],
        [-3.3473, -1.2679],
        [-2.3522, -2.2631],
        [-2.9432, -1.6720],
        [-3.1009, -1.5143],
        [-3.2434, -1.3718],
        [-2.7480, -1.8672],
        [-3.7862, -0.8290],
        [-3.1081, -1.5072],
        [-2.9981, -1.6171],
        [-3.4473, -1.1679],
        [-3.2642, -1.3510],
        [-3.3298, -1.2854],
        [-3.8085, -0.8067],
        [-4.0503, -0.5649],
        [-3.0992, -1.5160],
        [-3.2347, -1.3805],
        [-2.5301, -2.0851],
        [-3.5562, -1.0590],
        [-3.0747, -1.5406],
        [-3.4776, -1.1376],
        [-3.0042, -1.6111],
        [-3.0019, -1.6133],
        [-3.1802, -1.4351],
        [-3.4228, -1.1924],
        [-2.6722, -1.9430],
        [-3.0017, -1.6135],
        [-2.9099, -1.7053],
        [-3.5073, -1.1079],
        [-3.5485, -1.0667],
        [-3.5354, -1.0798],
        [-3.1604, -1.4549],
        [-3.4303, -1.1849],
        [-2.8398, -1.7754],
        [-2.7356, -1.8796],
        [-3.2114, -1.4038],
        [-2.3934, -2.2218],
        [-3.6927, -0.9226],
        [-3.6441, -0.9712],
        [-3.5183, -1.0969],
        [-3.5908, -1.0244],
        [-3.1630, -1.4522],
        [-3.9929, -0.6224],
        [-3.3891, -1.2261],
        [-3.8336, -0.7816],
        [-3.8255, -0.7897],
        [-3.0560, -1.5592],
        [-2.9723, -1.6429],
        [-3.2706, -1.3446],
        [-3.1495, -1.4658],
        [-3.5399, -1.0753]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4750, 0.5250],
        [0.0617, 0.9383]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1644, 0.8356], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0974, 0.1263],
         [0.8989, 0.1442]],

        [[0.4632, 0.1009],
         [0.4234, 0.3787]],

        [[0.7977, 0.1245],
         [0.4338, 0.1397]],

        [[0.9828, 0.1031],
         [0.4748, 0.8840]],

        [[0.7259, 0.1284],
         [0.0842, 0.2466]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.00038912871648324923
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0017351917879060224
Average Adjusted Rand Index: 0.00011227306676611041
Iteration 0: Loss = -23528.562080043877
Iteration 10: Loss = -9906.175102686877
Iteration 20: Loss = -9906.175102686877
1
Iteration 30: Loss = -9906.175102686879
2
Iteration 40: Loss = -9906.17510268688
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.1483e-17, 1.0000e+00],
        [1.7793e-16, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.7158e-16, 1.0000e+00])
beta: tensor([[[0.1316, 0.1411],
         [0.3831, 0.1360]],

        [[0.5591, 0.0742],
         [0.7146, 0.4074]],

        [[0.3957, 0.1273],
         [0.1506, 0.2373]],

        [[0.7931, 0.1676],
         [0.2692, 0.3203]],

        [[0.6582, 0.1783],
         [0.3261, 0.1095]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23529.31064789081
Iteration 100: Loss = -9935.950207864596
Iteration 200: Loss = -9915.8722504447
Iteration 300: Loss = -9909.432888026708
Iteration 400: Loss = -9908.226621539836
Iteration 500: Loss = -9907.561338571502
Iteration 600: Loss = -9907.135874063157
Iteration 700: Loss = -9906.832377616844
Iteration 800: Loss = -9906.592822137123
Iteration 900: Loss = -9906.392266211964
Iteration 1000: Loss = -9906.267966380854
Iteration 1100: Loss = -9906.17679545432
Iteration 1200: Loss = -9906.103967999925
Iteration 1300: Loss = -9906.050232213505
Iteration 1400: Loss = -9906.012385179018
Iteration 1500: Loss = -9905.981330298815
Iteration 1600: Loss = -9905.948522722976
Iteration 1700: Loss = -9905.901158549565
Iteration 1800: Loss = -9905.87160421326
Iteration 1900: Loss = -9905.844184540489
Iteration 2000: Loss = -9905.818589645947
Iteration 2100: Loss = -9905.794772058769
Iteration 2200: Loss = -9905.772320790704
Iteration 2300: Loss = -9905.75061112314
Iteration 2400: Loss = -9905.72896591936
Iteration 2500: Loss = -9905.706866566981
Iteration 2600: Loss = -9905.68401075219
Iteration 2700: Loss = -9905.65999076942
Iteration 2800: Loss = -9905.63439079589
Iteration 2900: Loss = -9905.60617346352
Iteration 3000: Loss = -9905.574743555802
Iteration 3100: Loss = -9905.539132321624
Iteration 3200: Loss = -9905.498436596843
Iteration 3300: Loss = -9905.451731322988
Iteration 3400: Loss = -9905.398593916825
Iteration 3500: Loss = -9905.339295879801
Iteration 3600: Loss = -9905.275839077894
Iteration 3700: Loss = -9905.211439783132
Iteration 3800: Loss = -9905.149790417287
Iteration 3900: Loss = -9905.093795634824
Iteration 4000: Loss = -9905.044352473242
Iteration 4100: Loss = -9905.000230995742
Iteration 4200: Loss = -9904.959173453462
Iteration 4300: Loss = -9904.920085560874
Iteration 4400: Loss = -9904.882446556856
Iteration 4500: Loss = -9904.846287088021
Iteration 4600: Loss = -9904.814088681305
Iteration 4700: Loss = -9904.783592612454
Iteration 4800: Loss = -9904.750061815757
Iteration 4900: Loss = -9904.68636755161
Iteration 5000: Loss = -9904.517255542016
Iteration 5100: Loss = -9904.259616119283
Iteration 5200: Loss = -9904.20673482813
Iteration 5300: Loss = -9904.1913000942
Iteration 5400: Loss = -9904.181837967748
Iteration 5500: Loss = -9904.173639551476
Iteration 5600: Loss = -9904.154539744051
Iteration 5700: Loss = -9904.1518544884
Iteration 5800: Loss = -9904.150045584129
Iteration 5900: Loss = -9904.1483784459
Iteration 6000: Loss = -9904.14665589741
Iteration 6100: Loss = -9904.142758199563
Iteration 6200: Loss = -9904.137848359196
Iteration 6300: Loss = -9904.137022618852
Iteration 6400: Loss = -9904.136195242041
Iteration 6500: Loss = -9904.152915200875
1
Iteration 6600: Loss = -9904.134938523504
Iteration 6700: Loss = -9904.134212336177
Iteration 6800: Loss = -9904.13358050328
Iteration 6900: Loss = -9904.156465172064
1
Iteration 7000: Loss = -9904.132423499186
Iteration 7100: Loss = -9904.13229663343
Iteration 7200: Loss = -9904.131345499132
Iteration 7300: Loss = -9904.130781102867
Iteration 7400: Loss = -9904.13022972739
Iteration 7500: Loss = -9904.129688230309
Iteration 7600: Loss = -9904.132055989032
1
Iteration 7700: Loss = -9904.12860114544
Iteration 7800: Loss = -9904.127963609862
Iteration 7900: Loss = -9904.13524419024
1
Iteration 8000: Loss = -9904.126761330783
Iteration 8100: Loss = -9904.126195637362
Iteration 8200: Loss = -9904.125401140216
Iteration 8300: Loss = -9904.124746078442
Iteration 8400: Loss = -9904.12394153351
Iteration 8500: Loss = -9904.124777299274
1
Iteration 8600: Loss = -9904.122708906778
Iteration 8700: Loss = -9904.12265437207
Iteration 8800: Loss = -9904.121938506207
Iteration 8900: Loss = -9904.121960074584
1
Iteration 9000: Loss = -9904.12151661028
Iteration 9100: Loss = -9904.130787322889
1
Iteration 9200: Loss = -9904.121238313865
Iteration 9300: Loss = -9904.123809695246
1
Iteration 9400: Loss = -9904.121045859127
Iteration 9500: Loss = -9904.134801899561
1
Iteration 9600: Loss = -9904.120856268817
Iteration 9700: Loss = -9904.120784044293
Iteration 9800: Loss = -9904.12082253276
1
Iteration 9900: Loss = -9904.120576733807
Iteration 10000: Loss = -9904.175513747194
1
Iteration 10100: Loss = -9904.120415549554
Iteration 10200: Loss = -9904.120390428609
Iteration 10300: Loss = -9904.122938505838
1
Iteration 10400: Loss = -9904.120237267332
Iteration 10500: Loss = -9904.120173372788
Iteration 10600: Loss = -9904.120184413787
1
Iteration 10700: Loss = -9904.322573175548
2
Iteration 10800: Loss = -9904.119982323708
Iteration 10900: Loss = -9904.367561023346
1
Iteration 11000: Loss = -9904.119892550792
Iteration 11100: Loss = -9904.121552202963
1
Iteration 11200: Loss = -9904.119838948582
Iteration 11300: Loss = -9904.231835855337
1
Iteration 11400: Loss = -9904.119684747084
Iteration 11500: Loss = -9904.413087330999
1
Iteration 11600: Loss = -9904.119641803713
Iteration 11700: Loss = -9904.119583421632
Iteration 11800: Loss = -9904.119567195798
Iteration 11900: Loss = -9904.119556589067
Iteration 12000: Loss = -9904.119795317798
1
Iteration 12100: Loss = -9904.11948818934
Iteration 12200: Loss = -9904.119914638084
1
Iteration 12300: Loss = -9904.119439403368
Iteration 12400: Loss = -9904.119451312357
1
Iteration 12500: Loss = -9904.119557739637
2
Iteration 12600: Loss = -9904.119307670993
Iteration 12700: Loss = -9904.119304704562
Iteration 12800: Loss = -9904.119343225406
1
Iteration 12900: Loss = -9904.119301202358
Iteration 13000: Loss = -9904.142348733147
1
Iteration 13100: Loss = -9904.119236864979
Iteration 13200: Loss = -9904.120054160165
1
Iteration 13300: Loss = -9904.119317323422
2
Iteration 13400: Loss = -9904.126908539274
3
Iteration 13500: Loss = -9904.322998880883
4
Iteration 13600: Loss = -9904.119131785057
Iteration 13700: Loss = -9904.119719971774
1
Iteration 13800: Loss = -9904.119286578793
2
Iteration 13900: Loss = -9904.131236426641
3
Iteration 14000: Loss = -9904.119112438655
Iteration 14100: Loss = -9904.121112595938
1
Iteration 14200: Loss = -9904.307537268236
2
Iteration 14300: Loss = -9904.11929409668
3
Iteration 14400: Loss = -9904.119531016084
4
Iteration 14500: Loss = -9904.122251417857
5
Iteration 14600: Loss = -9904.332924512164
6
Iteration 14700: Loss = -9904.119099127418
Iteration 14800: Loss = -9904.119921032065
1
Iteration 14900: Loss = -9904.12709871386
2
Iteration 15000: Loss = -9904.118990830371
Iteration 15100: Loss = -9904.119265534053
1
Iteration 15200: Loss = -9904.119076174886
2
Iteration 15300: Loss = -9904.122544451327
3
Iteration 15400: Loss = -9904.118975466758
Iteration 15500: Loss = -9904.183498215192
1
Iteration 15600: Loss = -9904.118937212328
Iteration 15700: Loss = -9904.119083515854
1
Iteration 15800: Loss = -9904.118991408253
2
Iteration 15900: Loss = -9904.119720737344
3
Iteration 16000: Loss = -9904.229280946352
4
Iteration 16100: Loss = -9904.118955941953
5
Iteration 16200: Loss = -9904.11911298702
6
Iteration 16300: Loss = -9904.125743563662
7
Iteration 16400: Loss = -9904.119315439744
8
Iteration 16500: Loss = -9904.121092901929
9
Iteration 16600: Loss = -9904.119067717551
10
Stopping early at iteration 16600 due to no improvement.
tensor([[-2.3565,  0.7984],
        [-3.0900,  1.1847],
        [-3.3545,  0.3975],
        [-2.7351,  0.8428],
        [-3.0627,  1.1957],
        [-2.6101,  0.6816],
        [-3.9139, -0.5767],
        [-2.5647,  1.1420],
        [-4.2803, -0.0651],
        [-3.7137,  0.6675],
        [-2.6581,  1.0552],
        [-2.1910,  0.7076],
        [-3.9198,  0.1081],
        [-3.3506,  1.0959],
        [-2.4318,  0.7696],
        [-2.5053,  0.9570],
        [-2.7362,  1.2399],
        [-2.9203,  1.3377],
        [-2.7261,  0.9908],
        [-3.9031, -0.2107],
        [-2.9854,  0.7451],
        [-2.4740,  0.6881],
        [-2.1918,  0.8055],
        [-2.4394,  0.7145],
        [-2.9805,  0.7279],
        [-3.1024,  1.1026],
        [-2.7211,  1.3250],
        [-3.1024,  1.0257],
        [-2.8950,  1.4975],
        [-3.1706,  1.1309],
        [-2.6019,  0.7513],
        [-3.6151,  0.2241],
        [-2.4271,  1.0205],
        [-4.2048, -0.4104],
        [-3.6382,  0.9462],
        [-3.2077,  0.6817],
        [-2.9760,  1.3661],
        [-2.4099,  1.0042],
        [-2.3618,  0.9427],
        [-3.1224,  1.3485],
        [-3.4012,  0.7700],
        [-2.4118,  1.0242],
        [-2.6161,  1.0987],
        [-2.5733,  1.1447],
        [-3.1225,  0.4745],
        [-3.4320,  0.8851],
        [-3.1379,  0.7609],
        [-2.8825,  1.1160],
        [-2.2897,  0.7505],
        [-2.7002,  0.8805],
        [-2.6149,  1.1024],
        [-3.1518,  0.5184],
        [-3.7082, -0.0710],
        [-2.7545,  1.3681],
        [-3.1729,  1.7222],
        [-2.8298,  1.0793],
        [-3.3041,  1.2447],
        [-3.4729,  0.4261],
        [-2.6195,  1.2295],
        [-2.8467,  1.0081],
        [-2.4803,  1.0925],
        [-3.8388,  0.6253],
        [-3.2376,  1.3170],
        [-3.1887,  1.7870],
        [-2.5057,  0.9354],
        [-2.7759,  0.8313],
        [-3.3620,  0.2332],
        [-3.2147,  0.9259],
        [-2.8385,  0.8172],
        [-3.0953,  1.0552],
        [-2.6228,  0.5397],
        [-2.8832,  1.4819],
        [-2.9354,  1.5455],
        [-2.9842,  1.3499],
        [-2.9002,  0.8317],
        [-2.8255,  1.0411],
        [-2.3842,  0.9934],
        [-2.7988,  1.3547],
        [-4.2929, -0.3223],
        [-2.9565,  1.1808],
        [-2.7635,  0.6915],
        [-2.7123,  1.3037],
        [-2.7418,  1.1923],
        [-2.4491,  0.9963],
        [-2.5637,  1.1764],
        [-3.5045,  0.7659],
        [-2.8535,  1.4056],
        [-3.9472,  0.1653],
        [-3.2770,  0.8669],
        [-2.7867,  1.3373],
        [-2.4242,  1.0250],
        [-3.2642,  1.8732],
        [-2.6779,  1.0287],
        [-3.1128,  1.4694],
        [-2.8957,  1.4883],
        [-3.1784,  1.1079],
        [-3.6140,  0.8349],
        [-3.4963,  0.2208],
        [-2.6157,  0.9849],
        [-2.7050,  1.2839]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.7207e-05, 9.9993e-01],
        [8.4648e-02, 9.1535e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0218, 0.9782], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0976, 0.1253],
         [0.3831, 0.1407]],

        [[0.5591, 0.0954],
         [0.7146, 0.4074]],

        [[0.3957, 0.1222],
         [0.1506, 0.2373]],

        [[0.7931, 0.1025],
         [0.2692, 0.3203]],

        [[0.6582, 0.1450],
         [0.3261, 0.1095]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.451712833749729e-05
Average Adjusted Rand Index: -0.0003077958928485548
Iteration 0: Loss = -22370.834145688354
Iteration 10: Loss = -9906.171484922334
Iteration 20: Loss = -9906.139117188925
Iteration 30: Loss = -9906.016715563243
Iteration 40: Loss = -9905.702059774236
Iteration 50: Loss = -9905.33309662227
Iteration 60: Loss = -9905.10821751287
Iteration 70: Loss = -9904.99494589266
Iteration 80: Loss = -9904.921434312395
Iteration 90: Loss = -9904.819680203589
Iteration 100: Loss = -9904.566221420151
Iteration 110: Loss = -9904.419446458
Iteration 120: Loss = -9904.395971689635
Iteration 130: Loss = -9904.38513604179
Iteration 140: Loss = -9904.377778664642
Iteration 150: Loss = -9904.372659575478
Iteration 160: Loss = -9904.369122498507
Iteration 170: Loss = -9904.366708842987
Iteration 180: Loss = -9904.3650001375
Iteration 190: Loss = -9904.363886428742
Iteration 200: Loss = -9904.363184658343
Iteration 210: Loss = -9904.362749022896
Iteration 220: Loss = -9904.362520625431
Iteration 230: Loss = -9904.362395121065
Iteration 240: Loss = -9904.362454254928
1
Iteration 250: Loss = -9904.362483647632
2
Iteration 260: Loss = -9904.362578642073
3
Stopping early at iteration 259 due to no improvement.
pi: tensor([[0.1091, 0.8909],
        [0.0937, 0.9063]], dtype=torch.float64)
alpha: tensor([0.0950, 0.9050])
beta: tensor([[[0.0983, 0.1249],
         [0.3549, 0.1405]],

        [[0.0167, 0.0969],
         [0.4160, 0.9479]],

        [[0.8979, 0.1218],
         [0.0437, 0.6355]],

        [[0.9743, 0.1049],
         [0.1570, 0.6113]],

        [[0.0567, 0.1374],
         [0.5491, 0.3136]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.451712833749729e-05
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22371.193085508472
Iteration 100: Loss = -9918.835804421764
Iteration 200: Loss = -9911.667053330415
Iteration 300: Loss = -9909.106097751703
Iteration 400: Loss = -9907.78916386614
Iteration 500: Loss = -9906.968413010527
Iteration 600: Loss = -9906.37153403748
Iteration 700: Loss = -9906.020872248333
Iteration 800: Loss = -9905.77713567528
Iteration 900: Loss = -9905.586003225244
Iteration 1000: Loss = -9905.410629786018
Iteration 1100: Loss = -9905.297581976343
Iteration 1200: Loss = -9905.206612953902
Iteration 1300: Loss = -9905.131852630066
Iteration 1400: Loss = -9905.069759028902
Iteration 1500: Loss = -9905.017539671338
Iteration 1600: Loss = -9904.972973330774
Iteration 1700: Loss = -9904.9341676078
Iteration 1800: Loss = -9904.900408684698
Iteration 1900: Loss = -9904.870226994846
Iteration 2000: Loss = -9904.84277241825
Iteration 2100: Loss = -9904.817311291867
Iteration 2200: Loss = -9904.79290887119
Iteration 2300: Loss = -9904.767795661628
Iteration 2400: Loss = -9904.738465008912
Iteration 2500: Loss = -9904.694638965484
Iteration 2600: Loss = -9904.601375539078
Iteration 2700: Loss = -9904.405824927591
Iteration 2800: Loss = -9904.266249622386
Iteration 2900: Loss = -9904.229966953251
Iteration 3000: Loss = -9904.213810641482
Iteration 3100: Loss = -9904.204298114102
Iteration 3200: Loss = -9904.195677448357
Iteration 3300: Loss = -9904.188781547346
Iteration 3400: Loss = -9904.182740815575
Iteration 3500: Loss = -9904.177476763502
Iteration 3600: Loss = -9904.172918282984
Iteration 3700: Loss = -9904.168878947476
Iteration 3800: Loss = -9904.165338030885
Iteration 3900: Loss = -9904.162292471765
Iteration 4000: Loss = -9904.159433412935
Iteration 4100: Loss = -9904.156998311946
Iteration 4200: Loss = -9904.155104090067
Iteration 4300: Loss = -9904.152819429173
Iteration 4400: Loss = -9904.151081960572
Iteration 4500: Loss = -9904.149467535346
Iteration 4600: Loss = -9904.148042100342
Iteration 4700: Loss = -9904.148555318074
1
Iteration 4800: Loss = -9904.145579950598
Iteration 4900: Loss = -9904.14445870617
Iteration 5000: Loss = -9904.146183773984
1
Iteration 5100: Loss = -9904.142575305177
Iteration 5200: Loss = -9904.141731565769
Iteration 5300: Loss = -9904.140983481822
Iteration 5400: Loss = -9904.140244649869
Iteration 5500: Loss = -9904.13959642647
Iteration 5600: Loss = -9904.138990312049
Iteration 5700: Loss = -9904.138435399103
Iteration 5800: Loss = -9904.140050881188
1
Iteration 5900: Loss = -9904.137369224076
Iteration 6000: Loss = -9904.19262785196
1
Iteration 6100: Loss = -9904.136453045936
Iteration 6200: Loss = -9904.145052008156
1
Iteration 6300: Loss = -9904.135712531664
Iteration 6400: Loss = -9904.135338673577
Iteration 6500: Loss = -9904.139537158322
1
Iteration 6600: Loss = -9904.134665616752
Iteration 6700: Loss = -9904.134401264604
Iteration 6800: Loss = -9904.134825931575
1
Iteration 6900: Loss = -9904.133905499684
Iteration 7000: Loss = -9904.133640221477
Iteration 7100: Loss = -9904.134140146938
1
Iteration 7200: Loss = -9904.133214100335
Iteration 7300: Loss = -9904.132995926362
Iteration 7400: Loss = -9904.1328205795
Iteration 7500: Loss = -9904.132662324886
Iteration 7600: Loss = -9904.132466817318
Iteration 7700: Loss = -9904.132316505733
Iteration 7800: Loss = -9904.132436080736
1
Iteration 7900: Loss = -9904.132010620402
Iteration 8000: Loss = -9904.13188344953
Iteration 8100: Loss = -9904.13288535325
1
Iteration 8200: Loss = -9904.131622506047
Iteration 8300: Loss = -9904.131498410849
Iteration 8400: Loss = -9904.171194042574
1
Iteration 8500: Loss = -9904.131288686307
Iteration 8600: Loss = -9904.131192237946
Iteration 8700: Loss = -9904.220350785326
1
Iteration 8800: Loss = -9904.131004010485
Iteration 8900: Loss = -9904.130924145427
Iteration 9000: Loss = -9904.153181004964
1
Iteration 9100: Loss = -9904.13075293416
Iteration 9200: Loss = -9904.131146092655
1
Iteration 9300: Loss = -9904.134809436735
2
Iteration 9400: Loss = -9904.130535695274
Iteration 9500: Loss = -9904.13969825704
1
Iteration 9600: Loss = -9904.132062138622
2
Iteration 9700: Loss = -9904.138126911806
3
Iteration 9800: Loss = -9904.134635672277
4
Iteration 9900: Loss = -9904.130736022295
5
Iteration 10000: Loss = -9904.130185460117
Iteration 10100: Loss = -9904.132605958714
1
Iteration 10200: Loss = -9904.130060745027
Iteration 10300: Loss = -9904.13044437592
1
Iteration 10400: Loss = -9904.129985397225
Iteration 10500: Loss = -9904.13048551648
1
Iteration 10600: Loss = -9904.137938731836
2
Iteration 10700: Loss = -9904.13009979192
3
Iteration 10800: Loss = -9904.130042924293
4
Iteration 10900: Loss = -9904.137643123024
5
Iteration 11000: Loss = -9904.157634492825
6
Iteration 11100: Loss = -9904.129785330286
Iteration 11200: Loss = -9904.129773689598
Iteration 11300: Loss = -9904.135947661882
1
Iteration 11400: Loss = -9904.12740037587
Iteration 11500: Loss = -9904.155638439548
1
Iteration 11600: Loss = -9904.127263039005
Iteration 11700: Loss = -9904.127482464735
1
Iteration 11800: Loss = -9904.12841576845
2
Iteration 11900: Loss = -9904.198246823837
3
Iteration 12000: Loss = -9904.12717022196
Iteration 12100: Loss = -9904.184592257674
1
Iteration 12200: Loss = -9904.12715952835
Iteration 12300: Loss = -9904.409665672674
1
Iteration 12400: Loss = -9904.127116667958
Iteration 12500: Loss = -9904.127101513734
Iteration 12600: Loss = -9904.127376633247
1
Iteration 12700: Loss = -9904.127062108446
Iteration 12800: Loss = -9904.128587400483
1
Iteration 12900: Loss = -9904.12704689403
Iteration 13000: Loss = -9904.127369943426
1
Iteration 13100: Loss = -9904.127149411765
2
Iteration 13200: Loss = -9904.27612983176
3
Iteration 13300: Loss = -9904.127974847072
4
Iteration 13400: Loss = -9904.170342052485
5
Iteration 13500: Loss = -9904.127042793958
Iteration 13600: Loss = -9904.127019188094
Iteration 13700: Loss = -9904.127984216188
1
Iteration 13800: Loss = -9904.127076619465
2
Iteration 13900: Loss = -9904.13376035346
3
Iteration 14000: Loss = -9904.226290965295
4
Iteration 14100: Loss = -9904.188384105983
5
Iteration 14200: Loss = -9904.152834576267
6
Iteration 14300: Loss = -9904.12694790116
Iteration 14400: Loss = -9904.127207917218
1
Iteration 14500: Loss = -9904.127253348412
2
Iteration 14600: Loss = -9904.18730027249
3
Iteration 14700: Loss = -9904.127946985576
4
Iteration 14800: Loss = -9904.128253183466
5
Iteration 14900: Loss = -9904.131745267174
6
Iteration 15000: Loss = -9904.136487557325
7
Iteration 15100: Loss = -9904.132728867131
8
Iteration 15200: Loss = -9904.258675517762
9
Iteration 15300: Loss = -9904.126830089213
Iteration 15400: Loss = -9904.126870162718
1
Iteration 15500: Loss = -9904.167932762466
2
Iteration 15600: Loss = -9904.23995263389
3
Iteration 15700: Loss = -9904.129214446406
4
Iteration 15800: Loss = -9904.127609768586
5
Iteration 15900: Loss = -9904.126993034868
6
Iteration 16000: Loss = -9904.127028371671
7
Iteration 16100: Loss = -9904.126270925122
Iteration 16200: Loss = -9904.12453640172
Iteration 16300: Loss = -9904.128679196847
1
Iteration 16400: Loss = -9904.120157676301
Iteration 16500: Loss = -9904.142238796981
1
Iteration 16600: Loss = -9904.176523829725
2
Iteration 16700: Loss = -9904.122460171317
3
Iteration 16800: Loss = -9904.119260074453
Iteration 16900: Loss = -9904.122444145753
1
Iteration 17000: Loss = -9904.179547630622
2
Iteration 17100: Loss = -9904.1188915643
Iteration 17200: Loss = -9904.121176363895
1
Iteration 17300: Loss = -9904.118856744939
Iteration 17400: Loss = -9904.136521339675
1
Iteration 17500: Loss = -9904.133162603302
2
Iteration 17600: Loss = -9904.119046261703
3
Iteration 17700: Loss = -9904.132181460613
4
Iteration 17800: Loss = -9904.173285645238
5
Iteration 17900: Loss = -9904.119373738191
6
Iteration 18000: Loss = -9904.11975336269
7
Iteration 18100: Loss = -9904.11971759785
8
Iteration 18200: Loss = -9904.118934239974
9
Iteration 18300: Loss = -9904.119694666382
10
Stopping early at iteration 18300 due to no improvement.
tensor([[-2.4822,  0.6747],
        [-2.8849,  1.3951],
        [-2.9409,  0.8134],
        [-2.5719,  1.0085],
        [-2.9225,  1.3410],
        [-2.6129,  0.6808],
        [-3.3630, -0.0235],
        [-3.1193,  0.5906],
        [-2.8317,  1.3862],
        [-2.9018,  1.4845],
        [-2.5852,  1.1313],
        [-2.5072,  0.3935],
        [-3.1855,  0.8475],
        [-3.7434,  0.7081],
        [-2.3934,  0.8100],
        [-2.9473,  0.5174],
        [-2.6973,  1.2842],
        [-2.8395,  1.4239],
        [-4.1674, -0.4478],
        [-2.5424,  1.1524],
        [-2.6153,  1.1182],
        [-3.2366, -0.0724],
        [-3.8071, -0.8081],
        [-2.2848,  0.8712],
        [-3.4633,  0.2481],
        [-2.8990,  1.3112],
        [-2.7326,  1.3180],
        [-2.9316,  1.2013],
        [-3.6014,  0.7965],
        [-3.0580,  1.2483],
        [-2.6627,  0.6928],
        [-2.6159,  1.2275],
        [-2.4189,  1.0312],
        [-3.7303,  0.0666],
        [-3.1848,  1.4049],
        [-2.6505,  1.2429],
        [-4.4813, -0.1339],
        [-2.5806,  0.8356],
        [-2.3609,  0.9460],
        [-2.9461,  1.5301],
        [-2.7821,  1.3938],
        [-3.2551,  0.1833],
        [-4.1663, -0.4489],
        [-3.0806,  0.6403],
        [-2.6941,  0.9051],
        [-4.4687, -0.1466],
        [-2.8597,  1.0433],
        [-2.7048,  1.2987],
        [-2.2804,  0.7619],
        [-2.4874,  1.0958],
        [-2.7765,  0.9440],
        [-3.5545,  0.1176],
        [-2.5285,  1.1111],
        [-2.7585,  1.3691],
        [-3.2940,  1.6065],
        [-2.9516,  0.9613],
        [-3.0149,  1.5392],
        [-3.3182,  0.5851],
        [-3.1731,  0.6796],
        [-2.7309,  1.1279],
        [-3.0775,  0.4980],
        [-3.6555,  0.8136],
        [-3.1454,  1.4143],
        [-3.2081,  1.7732],
        [-2.4346,  1.0088],
        [-2.7007,  0.9090],
        [-2.5194,  1.0777],
        [-2.9889,  1.1566],
        [-2.6472,  1.0111],
        [-3.9032,  0.2521],
        [-2.7217,  0.4429],
        [-2.9166,  1.4533],
        [-3.2007,  1.2854],
        [-4.1066,  0.2328],
        [-2.6799,  1.0543],
        [-2.7666,  1.1029],
        [-2.3915,  0.9882],
        [-2.8075,  1.3510],
        [-3.2504,  0.7249],
        [-2.9239,  1.2187],
        [-2.6117,  0.8455],
        [-3.1194,  0.9008],
        [-2.7787,  1.1587],
        [-2.6871,  0.7603],
        [-3.3400,  0.4033],
        [-2.8309,  1.4424],
        [-2.8256,  1.4388],
        [-3.7661,  0.3517],
        [-2.7678,  1.3814],
        [-3.6054,  0.5238],
        [-2.7221,  0.7296],
        [-3.5128,  1.6301],
        [-2.7110,  0.9986],
        [-3.0102,  1.5774],
        [-2.8931,  1.4963],
        [-3.0166,  1.2741],
        [-4.3206,  0.1334],
        [-2.5600,  1.1602],
        [-2.5236,  1.0796],
        [-2.6935,  1.3007]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.1653e-05, 9.9999e-01],
        [8.4422e-02, 9.1558e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0218, 0.9782], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0977, 0.1254],
         [0.3549, 0.1406]],

        [[0.0167, 0.0953],
         [0.4160, 0.9479]],

        [[0.8979, 0.1223],
         [0.0437, 0.6355]],

        [[0.9743, 0.1026],
         [0.1570, 0.6113]],

        [[0.0567, 0.1452],
         [0.5491, 0.3136]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.451712833749729e-05
Average Adjusted Rand Index: -0.0003077958928485548
Iteration 0: Loss = -37468.455407055415
Iteration 10: Loss = -9906.175105747976
Iteration 20: Loss = -9906.175109748448
1
Iteration 30: Loss = -9906.17513614949
2
Iteration 40: Loss = -9906.175216483318
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[2.8452e-16, 1.0000e+00],
        [9.3463e-09, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([9.0127e-09, 1.0000e+00])
beta: tensor([[[0.1316, 0.1411],
         [0.3347, 0.1360]],

        [[0.2048, 0.0742],
         [0.3541, 0.1980]],

        [[0.3727, 0.1273],
         [0.0677, 0.2909]],

        [[0.3733, 0.1676],
         [0.7224, 0.5825]],

        [[0.0373, 0.1783],
         [0.4992, 0.6149]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37468.52707939471
Iteration 100: Loss = -9975.683500102066
Iteration 200: Loss = -9935.200425820502
Iteration 300: Loss = -9920.137327370097
Iteration 400: Loss = -9915.171804749463
Iteration 500: Loss = -9912.512854258843
Iteration 600: Loss = -9910.879038616722
Iteration 700: Loss = -9909.786909409855
Iteration 800: Loss = -9909.019484055056
Iteration 900: Loss = -9908.456613407763
Iteration 1000: Loss = -9908.030373466187
Iteration 1100: Loss = -9907.699410864729
Iteration 1200: Loss = -9907.436665792156
Iteration 1300: Loss = -9907.223859562071
Iteration 1400: Loss = -9907.04839778047
Iteration 1500: Loss = -9906.902023235576
Iteration 1600: Loss = -9906.781479288959
Iteration 1700: Loss = -9906.680408668928
Iteration 1800: Loss = -9906.595535013823
Iteration 1900: Loss = -9906.524362778166
Iteration 2000: Loss = -9906.464081008879
Iteration 2100: Loss = -9906.412616086302
Iteration 2200: Loss = -9906.368280340059
Iteration 2300: Loss = -9906.32966337629
Iteration 2400: Loss = -9906.296185535863
Iteration 2500: Loss = -9906.266820413013
Iteration 2600: Loss = -9906.240885121922
Iteration 2700: Loss = -9906.217861998386
Iteration 2800: Loss = -9906.19703828805
Iteration 2900: Loss = -9906.178383683726
Iteration 3000: Loss = -9906.16264731543
Iteration 3100: Loss = -9906.146325705893
Iteration 3200: Loss = -9906.132516689742
Iteration 3300: Loss = -9906.119946407805
Iteration 3400: Loss = -9906.110223613108
Iteration 3500: Loss = -9906.09791045671
Iteration 3600: Loss = -9906.088253171336
Iteration 3700: Loss = -9906.081936873506
Iteration 3800: Loss = -9906.071194003762
Iteration 3900: Loss = -9906.063587037077
Iteration 4000: Loss = -9906.056601975051
Iteration 4100: Loss = -9906.050175130715
Iteration 4200: Loss = -9906.044003948939
Iteration 4300: Loss = -9906.0383832086
Iteration 4400: Loss = -9906.033085457655
Iteration 4500: Loss = -9906.028490276965
Iteration 4600: Loss = -9906.023493615525
Iteration 4700: Loss = -9906.019176551736
Iteration 4800: Loss = -9906.019620832134
1
Iteration 4900: Loss = -9906.011127016489
Iteration 5000: Loss = -9906.007400324072
Iteration 5100: Loss = -9906.003799443723
Iteration 5200: Loss = -9906.000895982963
Iteration 5300: Loss = -9905.996867614114
Iteration 5400: Loss = -9905.99343526135
Iteration 5500: Loss = -9905.990863469702
Iteration 5600: Loss = -9905.98642572616
Iteration 5700: Loss = -9905.982518354558
Iteration 5800: Loss = -9905.97830679382
Iteration 5900: Loss = -9905.974545166731
Iteration 6000: Loss = -9905.967708893262
Iteration 6100: Loss = -9905.959859097384
Iteration 6200: Loss = -9905.948026327711
Iteration 6300: Loss = -9905.926789744133
Iteration 6400: Loss = -9905.880345301843
Iteration 6500: Loss = -9905.776546401634
Iteration 6600: Loss = -9905.798323555873
1
Iteration 6700: Loss = -9905.429964137824
Iteration 6800: Loss = -9905.247475426479
Iteration 6900: Loss = -9905.122403378577
Iteration 7000: Loss = -9905.039069075967
Iteration 7100: Loss = -9904.963140292586
Iteration 7200: Loss = -9904.909334111524
Iteration 7300: Loss = -9904.825584477692
Iteration 7400: Loss = -9904.29247987758
Iteration 7500: Loss = -9904.197328437489
Iteration 7600: Loss = -9904.18429766674
Iteration 7700: Loss = -9904.175333077488
Iteration 7800: Loss = -9904.162594424488
Iteration 7900: Loss = -9904.145779717603
Iteration 8000: Loss = -9904.152554146425
1
Iteration 8100: Loss = -9904.142628947413
Iteration 8200: Loss = -9904.141532786289
Iteration 8300: Loss = -9904.251165788164
1
Iteration 8400: Loss = -9904.139720092884
Iteration 8500: Loss = -9904.13893677778
Iteration 8600: Loss = -9904.138165352453
Iteration 8700: Loss = -9904.137600164499
Iteration 8800: Loss = -9904.136847171712
Iteration 8900: Loss = -9904.1362347603
Iteration 9000: Loss = -9904.154782500598
1
Iteration 9100: Loss = -9904.134424471187
Iteration 9200: Loss = -9904.13181374577
Iteration 9300: Loss = -9904.138737165491
1
Iteration 9400: Loss = -9904.131015375839
Iteration 9500: Loss = -9904.13075231677
Iteration 9600: Loss = -9904.138294005514
1
Iteration 9700: Loss = -9904.130192089016
Iteration 9800: Loss = -9904.129957663054
Iteration 9900: Loss = -9904.203761259596
1
Iteration 10000: Loss = -9904.129566230113
Iteration 10100: Loss = -9904.12933677097
Iteration 10200: Loss = -9904.12921338735
Iteration 10300: Loss = -9904.12914216768
Iteration 10400: Loss = -9904.128909183506
Iteration 10500: Loss = -9904.12875983775
Iteration 10600: Loss = -9904.128684229121
Iteration 10700: Loss = -9904.12850458413
Iteration 10800: Loss = -9904.128436198778
Iteration 10900: Loss = -9904.128844759949
1
Iteration 11000: Loss = -9904.128287251211
Iteration 11100: Loss = -9904.128180887292
Iteration 11200: Loss = -9904.157614550415
1
Iteration 11300: Loss = -9904.128073223514
Iteration 11400: Loss = -9904.127970658694
Iteration 11500: Loss = -9904.128862281801
1
Iteration 11600: Loss = -9904.12788600328
Iteration 11700: Loss = -9904.127810359172
Iteration 11800: Loss = -9904.13236429064
1
Iteration 11900: Loss = -9904.127756048318
Iteration 12000: Loss = -9904.127716504894
Iteration 12100: Loss = -9904.132491148743
1
Iteration 12200: Loss = -9904.127634277762
Iteration 12300: Loss = -9904.127585531905
Iteration 12400: Loss = -9904.128310118664
1
Iteration 12500: Loss = -9904.127524419348
Iteration 12600: Loss = -9904.12756250984
1
Iteration 12700: Loss = -9904.127574409207
2
Iteration 12800: Loss = -9904.127483934337
Iteration 12900: Loss = -9904.43147193776
1
Iteration 13000: Loss = -9904.127417708773
Iteration 13100: Loss = -9904.127405507694
Iteration 13200: Loss = -9904.619514659402
1
Iteration 13300: Loss = -9904.127383750641
Iteration 13400: Loss = -9904.127342361486
Iteration 13500: Loss = -9904.169730790842
1
Iteration 13600: Loss = -9904.127310182099
Iteration 13700: Loss = -9904.12732566314
1
Iteration 13800: Loss = -9904.151031960044
2
Iteration 13900: Loss = -9904.127257533895
Iteration 14000: Loss = -9904.127303905923
1
Iteration 14100: Loss = -9904.178104980185
2
Iteration 14200: Loss = -9904.127239329417
Iteration 14300: Loss = -9904.128029015823
1
Iteration 14400: Loss = -9904.127244009687
2
Iteration 14500: Loss = -9904.12746279879
3
Iteration 14600: Loss = -9904.137324653298
4
Iteration 14700: Loss = -9904.127242285515
5
Iteration 14800: Loss = -9904.149421119893
6
Iteration 14900: Loss = -9904.12721481407
Iteration 15000: Loss = -9904.12776023975
1
Iteration 15100: Loss = -9904.127553621402
2
Iteration 15200: Loss = -9904.127316568489
3
Iteration 15300: Loss = -9904.130308816264
4
Iteration 15400: Loss = -9904.12739737959
5
Iteration 15500: Loss = -9904.12723456205
6
Iteration 15600: Loss = -9904.1322489249
7
Iteration 15700: Loss = -9904.127156652705
Iteration 15800: Loss = -9904.132155534686
1
Iteration 15900: Loss = -9904.128532858253
2
Iteration 16000: Loss = -9904.12712096754
Iteration 16100: Loss = -9904.127720511857
1
Iteration 16200: Loss = -9904.127349635328
2
Iteration 16300: Loss = -9904.127105993655
Iteration 16400: Loss = -9904.370558743949
1
Iteration 16500: Loss = -9904.138191682447
2
Iteration 16600: Loss = -9904.199798321046
3
Iteration 16700: Loss = -9904.127336264899
4
Iteration 16800: Loss = -9904.129753400513
5
Iteration 16900: Loss = -9904.136084711088
6
Iteration 17000: Loss = -9904.127112601098
7
Iteration 17100: Loss = -9904.12724651769
8
Iteration 17200: Loss = -9904.133436330081
9
Iteration 17300: Loss = -9904.127074575368
Iteration 17400: Loss = -9904.135102493876
1
Iteration 17500: Loss = -9904.127054254932
Iteration 17600: Loss = -9904.12735673132
1
Iteration 17700: Loss = -9904.127131634863
2
Iteration 17800: Loss = -9904.127362454725
3
Iteration 17900: Loss = -9904.207973705326
4
Iteration 18000: Loss = -9904.127181897902
5
Iteration 18100: Loss = -9904.129916216583
6
Iteration 18200: Loss = -9904.168701005452
7
Iteration 18300: Loss = -9904.129383550273
8
Iteration 18400: Loss = -9904.130053986784
9
Iteration 18500: Loss = -9904.144234384868
10
Stopping early at iteration 18500 due to no improvement.
tensor([[-6.9986,  5.2514],
        [-7.2485,  5.1831],
        [-7.7219,  4.6552],
        [-6.8275,  5.4139],
        [-7.1208,  5.2152],
        [-7.3831,  4.9755],
        [-6.8787,  5.4027],
        [-7.0019,  5.2601],
        [-7.3580,  5.4973],
        [-7.1084,  5.4110],
        [-6.8591,  5.4199],
        [-6.7732,  5.3833],
        [-6.9560,  5.3815],
        [-7.1224,  5.5807],
        [-7.8175,  4.4372],
        [-7.2626,  4.9622],
        [-6.8761,  5.4568],
        [-7.0528,  5.2871],
        [-7.0899,  5.2900],
        [-6.8864,  5.4844],
        [-7.1193,  5.1792],
        [-6.8156,  5.3570],
        [-6.8086,  5.4131],
        [-6.7959,  5.3581],
        [-6.8444,  5.4253],
        [-7.1126,  5.2883],
        [-6.9397,  5.5494],
        [-8.2856,  4.0443],
        [-7.0570,  5.3748],
        [-6.9017,  5.5153],
        [-6.8436,  5.4076],
        [-6.9218,  5.3809],
        [-6.9284,  5.3028],
        [-7.4253,  5.0218],
        [-6.9468,  5.5376],
        [-6.8678,  5.4673],
        [-7.3782,  5.1058],
        [-6.9280,  5.5317],
        [-6.9186,  5.2757],
        [-7.5403,  4.8577],
        [-6.9953,  5.5080],
        [-7.4170,  4.8457],
        [-6.9184,  5.4790],
        [-6.8448,  5.4358],
        [-7.5867,  4.7970],
        [-7.0087,  5.5300],
        [-7.0500,  5.3086],
        [-6.9155,  5.4275],
        [-6.8317,  5.3095],
        [-6.8640,  5.3978],
        [-7.0483,  5.2511],
        [-7.3747,  5.2996],
        [-6.8497,  5.4481],
        [-7.0826,  5.3845],
        [-7.4523,  5.2081],
        [-6.9271,  5.5351],
        [-6.9404,  5.5310],
        [-7.1167,  5.2418],
        [-7.0393,  5.3804],
        [-7.3468,  4.9664],
        [-6.9209,  5.3292],
        [-7.0613,  5.4603],
        [-6.9518,  5.4769],
        [-7.2937,  5.3040],
        [-6.8112,  5.4145],
        [-6.8635,  5.4339],
        [-7.3097,  5.1513],
        [-7.3306,  5.0659],
        [-7.1280,  5.1981],
        [-6.9642,  5.4168],
        [-6.9375,  5.2607],
        [-7.0791,  5.6144],
        [-8.6243,  4.0091],
        [-6.9942,  5.4589],
        [-6.9748,  5.5394],
        [-6.9216,  5.4936],
        [-6.8993,  5.3526],
        [-6.9908,  5.4093],
        [-8.1632,  4.1801],
        [-6.9008,  5.4893],
        [-7.3459,  4.8715],
        [-7.2831,  5.0877],
        [-7.2057,  5.2695],
        [-7.9213,  4.4502],
        [-6.8604,  5.4410],
        [-8.8080,  4.1927],
        [-7.0081,  5.3911],
        [-7.4247,  4.8856],
        [-6.8805,  5.4902],
        [-7.3886,  4.9442],
        [-6.8393,  5.4275],
        [-7.0143,  5.5393],
        [-6.8609,  5.4210],
        [-7.8678,  4.6544],
        [-7.7312,  4.7246],
        [-6.9931,  5.6021],
        [-7.3074,  5.3725],
        [-7.1308,  5.1326],
        [-7.3939,  4.8771],
        [-6.8832,  5.4523]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.5059e-05, 9.9998e-01],
        [7.8467e-02, 9.2153e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.2241e-06, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0975, 0.1359],
         [0.3347, 0.1404]],

        [[0.2048, 0.0948],
         [0.3541, 0.1980]],

        [[0.3727, 0.1223],
         [0.0677, 0.2909]],

        [[0.3733, 0.1012],
         [0.7224, 0.5825]],

        [[0.0373, 0.1467],
         [0.4992, 0.6149]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.451712833749729e-05
Average Adjusted Rand Index: -0.0003077958928485548
Iteration 0: Loss = -21121.694266029343
Iteration 10: Loss = -9906.175020986788
Iteration 20: Loss = -9906.173743177924
Iteration 30: Loss = -9906.167940457488
Iteration 40: Loss = -9906.140779809035
Iteration 50: Loss = -9906.026720406455
Iteration 60: Loss = -9905.722432787092
Iteration 70: Loss = -9905.354157702182
Iteration 80: Loss = -9905.125035427202
Iteration 90: Loss = -9905.005942607699
Iteration 100: Loss = -9904.926829282447
Iteration 110: Loss = -9904.82138632002
Iteration 120: Loss = -9904.569005185751
Iteration 130: Loss = -9904.424383952159
Iteration 140: Loss = -9904.399991741291
Iteration 150: Loss = -9904.387792951005
Iteration 160: Loss = -9904.379529380765
Iteration 170: Loss = -9904.373835071123
Iteration 180: Loss = -9904.369866291967
Iteration 190: Loss = -9904.367176482585
Iteration 200: Loss = -9904.365338449981
Iteration 210: Loss = -9904.364130960055
Iteration 220: Loss = -9904.3633211371
Iteration 230: Loss = -9904.362844599003
Iteration 240: Loss = -9904.36254373401
Iteration 250: Loss = -9904.362435939469
Iteration 260: Loss = -9904.362416544172
Iteration 270: Loss = -9904.362466611285
1
Iteration 280: Loss = -9904.362571507063
2
Iteration 290: Loss = -9904.362735322598
3
Stopping early at iteration 289 due to no improvement.
pi: tensor([[0.9059, 0.0941],
        [0.8901, 0.1099]], dtype=torch.float64)
alpha: tensor([0.9045, 0.0955])
beta: tensor([[[0.1405, 0.1249],
         [0.8458, 0.0983]],

        [[0.2575, 0.0969],
         [0.2918, 0.7712]],

        [[0.5303, 0.1218],
         [0.6292, 0.4978]],

        [[0.2356, 0.1050],
         [0.5866, 0.5651]],

        [[0.1597, 0.1373],
         [0.1434, 0.9212]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.451712833749729e-05
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21120.46242012821
Iteration 100: Loss = -9921.681155657063
Iteration 200: Loss = -9907.912117227743
Iteration 300: Loss = -9906.7184193126
Iteration 400: Loss = -9906.319307749154
Iteration 500: Loss = -9906.146365285174
Iteration 600: Loss = -9906.035973503616
Iteration 700: Loss = -9905.958101027694
Iteration 800: Loss = -9905.907640451593
Iteration 900: Loss = -9905.869611382635
Iteration 1000: Loss = -9905.837893827878
Iteration 1100: Loss = -9905.808735230114
Iteration 1200: Loss = -9905.779544591202
Iteration 1300: Loss = -9905.74765634856
Iteration 1400: Loss = -9905.709308549029
Iteration 1500: Loss = -9905.65885258222
Iteration 1600: Loss = -9905.585675357279
Iteration 1700: Loss = -9905.471599075694
Iteration 1800: Loss = -9905.305870846882
Iteration 1900: Loss = -9905.131778467918
Iteration 2000: Loss = -9905.008118881837
Iteration 2100: Loss = -9904.929930295959
Iteration 2200: Loss = -9904.876088713541
Iteration 2300: Loss = -9904.835065164028
Iteration 2400: Loss = -9904.801041318244
Iteration 2500: Loss = -9904.769887245286
Iteration 2600: Loss = -9904.736437782578
Iteration 2700: Loss = -9904.688649037806
Iteration 2800: Loss = -9904.573506011458
Iteration 2900: Loss = -9904.333525912647
Iteration 3000: Loss = -9904.218406442678
Iteration 3100: Loss = -9904.181060840356
Iteration 3200: Loss = -9904.1671160442
Iteration 3300: Loss = -9904.160437223238
Iteration 3400: Loss = -9904.156119602088
Iteration 3500: Loss = -9904.152870758682
Iteration 3600: Loss = -9904.150253242635
Iteration 3700: Loss = -9904.147977421693
Iteration 3800: Loss = -9904.146052493747
Iteration 3900: Loss = -9904.144421871113
Iteration 4000: Loss = -9904.142992440162
Iteration 4100: Loss = -9904.141675669014
Iteration 4200: Loss = -9904.140527766674
Iteration 4300: Loss = -9904.139406384662
Iteration 4400: Loss = -9904.138375025652
Iteration 4500: Loss = -9904.137360502673
Iteration 4600: Loss = -9904.13644099453
Iteration 4700: Loss = -9904.135468761044
Iteration 4800: Loss = -9904.134619439936
Iteration 4900: Loss = -9904.133783089399
Iteration 5000: Loss = -9904.132972592955
Iteration 5100: Loss = -9904.132178286505
Iteration 5200: Loss = -9904.131365886476
Iteration 5300: Loss = -9904.130642858483
Iteration 5400: Loss = -9904.129960190148
Iteration 5500: Loss = -9904.129248847563
Iteration 5600: Loss = -9904.128616261649
Iteration 5700: Loss = -9904.127963542976
Iteration 5800: Loss = -9904.127422589363
Iteration 5900: Loss = -9904.126788713911
Iteration 6000: Loss = -9904.12628365249
Iteration 6100: Loss = -9904.125838145535
Iteration 6200: Loss = -9904.125382579194
Iteration 6300: Loss = -9904.12502212503
Iteration 6400: Loss = -9904.124738131059
Iteration 6500: Loss = -9904.125181230307
1
Iteration 6600: Loss = -9904.124174175884
Iteration 6700: Loss = -9904.135394481655
1
Iteration 6800: Loss = -9904.123644891437
Iteration 6900: Loss = -9904.123983820893
1
Iteration 7000: Loss = -9904.12324118585
Iteration 7100: Loss = -9904.123109943173
Iteration 7200: Loss = -9904.122854232211
Iteration 7300: Loss = -9904.32431948142
1
Iteration 7400: Loss = -9904.122531325247
Iteration 7500: Loss = -9904.122393019403
Iteration 7600: Loss = -9904.276962971644
1
Iteration 7700: Loss = -9904.122070732745
Iteration 7800: Loss = -9904.121919949977
Iteration 7900: Loss = -9904.12820049251
1
Iteration 8000: Loss = -9904.121655415143
Iteration 8100: Loss = -9904.121514943941
Iteration 8200: Loss = -9904.121409800871
Iteration 8300: Loss = -9904.121438362206
1
Iteration 8400: Loss = -9904.121157777508
Iteration 8500: Loss = -9904.12103208465
Iteration 8600: Loss = -9904.128088020267
1
Iteration 8700: Loss = -9904.120845575791
Iteration 8800: Loss = -9904.1207537209
Iteration 8900: Loss = -9904.124715460346
1
Iteration 9000: Loss = -9904.120542695046
Iteration 9100: Loss = -9904.120493134282
Iteration 9200: Loss = -9904.124456399113
1
Iteration 9300: Loss = -9904.120343367018
Iteration 9400: Loss = -9904.120276520629
Iteration 9500: Loss = -9904.120559764646
1
Iteration 9600: Loss = -9904.120118748908
Iteration 9700: Loss = -9904.120070178651
Iteration 9800: Loss = -9904.120344920184
1
Iteration 9900: Loss = -9904.119950077673
Iteration 10000: Loss = -9904.119910436231
Iteration 10100: Loss = -9904.119934591181
1
Iteration 10200: Loss = -9904.119799780912
Iteration 10300: Loss = -9904.119822968096
1
Iteration 10400: Loss = -9904.119790474131
Iteration 10500: Loss = -9904.11971164092
Iteration 10600: Loss = -9904.119629658691
Iteration 10700: Loss = -9904.11970493207
1
Iteration 10800: Loss = -9904.119579637469
Iteration 10900: Loss = -9904.119561537746
Iteration 11000: Loss = -9904.122168024554
1
Iteration 11100: Loss = -9904.119464075202
Iteration 11200: Loss = -9904.119409015279
Iteration 11300: Loss = -9904.132091335552
1
Iteration 11400: Loss = -9904.119368179496
Iteration 11500: Loss = -9904.11934160541
Iteration 11600: Loss = -9904.12258962588
1
Iteration 11700: Loss = -9904.119300152075
Iteration 11800: Loss = -9904.119357098478
1
Iteration 11900: Loss = -9904.11931559472
2
Iteration 12000: Loss = -9904.119248554585
Iteration 12100: Loss = -9904.34777235499
1
Iteration 12200: Loss = -9904.11920547539
Iteration 12300: Loss = -9904.119267346085
1
Iteration 12400: Loss = -9904.11926200978
2
Iteration 12500: Loss = -9904.119147690453
Iteration 12600: Loss = -9904.122150632058
1
Iteration 12700: Loss = -9904.1191293222
Iteration 12800: Loss = -9904.119112682216
Iteration 12900: Loss = -9904.119343249591
1
Iteration 13000: Loss = -9904.119086232915
Iteration 13100: Loss = -9904.119148968854
1
Iteration 13200: Loss = -9904.119060303716
Iteration 13300: Loss = -9904.146655775905
1
Iteration 13400: Loss = -9904.119042577599
Iteration 13500: Loss = -9904.123619487684
1
Iteration 13600: Loss = -9904.139822235473
2
Iteration 13700: Loss = -9904.119691912007
3
Iteration 13800: Loss = -9904.119050505067
4
Iteration 13900: Loss = -9904.129134791656
5
Iteration 14000: Loss = -9904.118984497358
Iteration 14100: Loss = -9904.11914537967
1
Iteration 14200: Loss = -9904.12244482895
2
Iteration 14300: Loss = -9904.141240289797
3
Iteration 14400: Loss = -9904.119365121014
4
Iteration 14500: Loss = -9904.119007552887
5
Iteration 14600: Loss = -9904.120064976674
6
Iteration 14700: Loss = -9904.11896174822
Iteration 14800: Loss = -9904.259815024543
1
Iteration 14900: Loss = -9904.118953867963
Iteration 15000: Loss = -9904.119606569979
1
Iteration 15100: Loss = -9904.140408666199
2
Iteration 15200: Loss = -9904.118920628023
Iteration 15300: Loss = -9904.119874824952
1
Iteration 15400: Loss = -9904.24355027248
2
Iteration 15500: Loss = -9904.121007512254
3
Iteration 15600: Loss = -9904.123698239171
4
Iteration 15700: Loss = -9904.118996312036
5
Iteration 15800: Loss = -9904.118929535893
6
Iteration 15900: Loss = -9904.11899486141
7
Iteration 16000: Loss = -9904.119042292512
8
Iteration 16100: Loss = -9904.12161477686
9
Iteration 16200: Loss = -9904.119752277942
10
Stopping early at iteration 16200 due to no improvement.
tensor([[ 0.8668, -2.2849],
        [ 1.0752, -3.1971],
        [ 1.1670, -2.5822],
        [-0.4047, -3.9798],
        [ 1.3675, -2.8884],
        [ 0.6543, -2.6344],
        [-0.0468, -3.3811],
        [ 1.0978, -2.6062],
        [ 0.8031, -3.4057],
        [ 1.2977, -3.0811],
        [ 0.4228, -3.2878],
        [ 0.4529, -2.4425],
        [ 1.1162, -2.9092],
        [ 1.2546, -3.1888],
        [ 0.8711, -2.3272],
        [ 0.6546, -2.8051],
        [ 1.2823, -2.6914],
        [ 1.1458, -3.1099],
        [ 0.8744, -2.8399],
        [ 0.7574, -2.9325],
        [ 1.1585, -2.5694],
        [ 0.1275, -3.0315],
        [ 0.0891, -2.9048],
        [ 0.8264, -2.3244],
        [ 1.1208, -2.5847],
        [ 1.1168, -3.0858],
        [ 0.8310, -3.2121],
        [ 1.2229, -2.9027],
        [ 0.8150, -3.5752],
        [ 1.4500, -2.8492],
        [ 0.2216, -3.1287],
        [ 1.1566, -2.6796],
        [ 0.8615, -2.5833],
        [ 1.1775, -2.6140],
        [ 0.9847, -3.5974],
        [ 1.2424, -2.6446],
        [ 1.4694, -2.8704],
        [-0.2207, -3.6318],
        [ 0.4470, -2.8546],
        [ 1.5298, -2.9389],
        [ 1.3873, -2.7814],
        [ 0.5550, -2.8781],
        [ 1.1071, -2.6053],
        [ 0.7704, -2.9449],
        [ 0.9956, -2.5986],
        [ 1.4403, -2.8742],
        [ 1.2450, -2.6513],
        [ 1.0664, -2.9297],
        [ 0.7425, -2.2945],
        [ 1.0081, -2.5698],
        [ 0.9696, -2.7451],
        [ 1.1041, -2.5623],
        [ 1.1240, -2.5105],
        [ 1.2761, -2.8441],
        [ 1.4868, -3.4062],
        [ 1.1178, -2.7893],
        [ 1.3799, -3.1667],
        [ 1.1785, -2.7181],
        [ 1.0875, -2.7591],
        [ 0.8394, -3.0129],
        [ 1.0892, -2.4808],
        [ 0.6881, -3.7735],
        [ 1.0238, -3.5286],
        [ 1.7922, -3.1818],
        [ 0.9907, -2.4475],
        [ 0.9079, -2.6967],
        [ 0.8915, -2.7004],
        [ 1.1574, -2.9807],
        [ 1.1175, -2.5357],
        [ 1.1763, -2.9718],
        [ 0.2969, -2.8625],
        [ 0.9768, -3.3854],
        [ 1.4704, -3.0081],
        [ 1.4264, -2.9054],
        [ 0.7240, -3.0047],
        [ 1.2386, -2.6249],
        [ 0.8913, -2.4833],
        [ 1.3819, -2.7692],
        [ 1.2638, -2.7041],
        [ 1.3596, -2.7752],
        [ 1.0063, -2.4459],
        [ 0.3936, -3.6199],
        [ 1.2447, -2.6866],
        [ 0.9145, -2.5279],
        [ 1.1313, -2.6063],
        [ 1.4354, -2.8272],
        [ 0.8866, -3.3701],
        [ 0.6498, -3.4601],
        [ 1.1893, -2.9522],
        [ 1.3675, -2.7539],
        [ 0.3879, -3.0586],
        [ 1.4946, -3.6414],
        [ 0.9929, -2.7109],
        [ 1.4951, -3.0850],
        [ 1.0364, -3.3453],
        [ 1.3015, -2.9816],
        [ 1.5127, -2.9334],
        [ 0.7623, -2.9521],
        [ 0.8193, -2.7787],
        [-0.3143, -4.3009]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.1558e-01, 8.4416e-02],
        [9.9995e-01, 4.8542e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9781, 0.0219], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1407, 0.1253],
         [0.8458, 0.0976]],

        [[0.2575, 0.0951],
         [0.2918, 0.7712]],

        [[0.5303, 0.1221],
         [0.6292, 0.4978]],

        [[0.2356, 0.1025],
         [0.5866, 0.5651]],

        [[0.1597, 0.1450],
         [0.1434, 0.9212]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.451712833749729e-05
Average Adjusted Rand Index: -0.0003077958928485548
10034.962704662514
new:  [6.451712833749729e-05, 6.451712833749729e-05, 6.451712833749729e-05, 6.451712833749729e-05] [-0.0003077958928485548, -0.0003077958928485548, -0.0003077958928485548, -0.0003077958928485548] [9904.119067717551, 9904.119694666382, 9904.144234384868, 9904.119752277942]
prior:  [0.0, 6.451712833749729e-05, 0.0, 6.451712833749729e-05] [0.0, -0.0003077958928485548, 0.0, -0.0003077958928485548] [9906.17510268688, 9904.362578642073, 9906.175216483318, 9904.362735322598]
-----------------------------------------------------------------------------------------
This iteration is 3
True Objective function: Loss = -10088.887365683191
Iteration 0: Loss = -12671.468600288228
Iteration 10: Loss = -9961.124512275323
Iteration 20: Loss = -9961.106914093401
Iteration 30: Loss = -9961.091273656244
Iteration 40: Loss = -9961.075542479417
Iteration 50: Loss = -9961.058119252875
Iteration 60: Loss = -9961.033301468899
Iteration 70: Loss = -9960.986605000371
Iteration 80: Loss = -9960.895862442994
Iteration 90: Loss = -9960.767952403514
Iteration 100: Loss = -9960.643831553683
Iteration 110: Loss = -9960.568726634767
Iteration 120: Loss = -9960.546883019491
Iteration 130: Loss = -9960.544623068672
Iteration 140: Loss = -9960.545735843843
1
Iteration 150: Loss = -9960.546738246967
2
Iteration 160: Loss = -9960.547290776998
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[1.4488e-19, 1.0000e+00],
        [4.7060e-02, 9.5294e-01]], dtype=torch.float64)
alpha: tensor([0.0453, 0.9547])
beta: tensor([[[0.1506, 0.1572],
         [0.8709, 0.1370]],

        [[0.9975, 0.1591],
         [0.1118, 0.5168]],

        [[0.7309, 0.1064],
         [0.7337, 0.6222]],

        [[0.0306, 0.0860],
         [0.9823, 0.1012]],

        [[0.3447, 0.1901],
         [0.7041, 0.1690]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -13168.448229172183
Iteration 100: Loss = -9964.06839176936
Iteration 200: Loss = -9963.14969658294
Iteration 300: Loss = -9962.907283369377
Iteration 400: Loss = -9962.799452752346
Iteration 500: Loss = -9962.72492826415
Iteration 600: Loss = -9962.577136105916
Iteration 700: Loss = -9961.902339395434
Iteration 800: Loss = -9961.747343842537
Iteration 900: Loss = -9961.660135439219
Iteration 1000: Loss = -9961.590672095459
Iteration 1100: Loss = -9961.531931526275
Iteration 1200: Loss = -9961.43187964534
Iteration 1300: Loss = -9955.790341180951
Iteration 1400: Loss = -9955.418793702847
Iteration 1500: Loss = -9954.3125382636
Iteration 1600: Loss = -9954.023875690924
Iteration 1700: Loss = -9953.99123349323
Iteration 1800: Loss = -9953.974950905937
Iteration 1900: Loss = -9953.964300949478
Iteration 2000: Loss = -9953.956374405612
Iteration 2100: Loss = -9953.804304549216
Iteration 2200: Loss = -9953.77569223884
Iteration 2300: Loss = -9953.772230133742
Iteration 2400: Loss = -9953.769564490416
Iteration 2500: Loss = -9953.766383053766
Iteration 2600: Loss = -9953.761095635247
Iteration 2700: Loss = -9953.759465550753
Iteration 2800: Loss = -9953.757984233469
Iteration 2900: Loss = -9953.756420979807
Iteration 3000: Loss = -9953.753547545542
Iteration 3100: Loss = -9953.712186602343
Iteration 3200: Loss = -9953.701698912671
Iteration 3300: Loss = -9953.695675241752
Iteration 3400: Loss = -9953.694585286734
Iteration 3500: Loss = -9953.693277931003
Iteration 3600: Loss = -9953.69125039793
Iteration 3700: Loss = -9953.686024516312
Iteration 3800: Loss = -9953.601841067899
Iteration 3900: Loss = -9953.49846075487
Iteration 4000: Loss = -9953.496641318727
Iteration 4100: Loss = -9953.495899006348
Iteration 4200: Loss = -9953.49517553253
Iteration 4300: Loss = -9953.494696131933
Iteration 4400: Loss = -9953.494488244345
Iteration 4500: Loss = -9953.493865963968
Iteration 4600: Loss = -9953.493554921984
Iteration 4700: Loss = -9953.493263077553
Iteration 4800: Loss = -9953.493008891193
Iteration 4900: Loss = -9953.492703847554
Iteration 5000: Loss = -9953.492543048562
Iteration 5100: Loss = -9953.492262770358
Iteration 5200: Loss = -9953.492350624285
1
Iteration 5300: Loss = -9953.491859185038
Iteration 5400: Loss = -9953.491677670207
Iteration 5500: Loss = -9953.4915520626
Iteration 5600: Loss = -9953.491360510025
Iteration 5700: Loss = -9953.491222342927
Iteration 5800: Loss = -9953.49103067591
Iteration 5900: Loss = -9953.490776395834
Iteration 6000: Loss = -9953.452426002252
Iteration 6100: Loss = -9953.44456710849
Iteration 6200: Loss = -9953.443702147237
Iteration 6300: Loss = -9953.443572011955
Iteration 6400: Loss = -9953.44350531757
Iteration 6500: Loss = -9953.443389928569
Iteration 6600: Loss = -9953.443535026154
1
Iteration 6700: Loss = -9953.443353101231
Iteration 6800: Loss = -9953.444432408665
1
Iteration 6900: Loss = -9953.44313398698
Iteration 7000: Loss = -9953.443805913845
1
Iteration 7100: Loss = -9953.465878565281
2
Iteration 7200: Loss = -9953.442811963369
Iteration 7300: Loss = -9953.436785969378
Iteration 7400: Loss = -9953.433853327777
Iteration 7500: Loss = -9953.467907438579
1
Iteration 7600: Loss = -9953.433760716216
Iteration 7700: Loss = -9953.433769352656
1
Iteration 7800: Loss = -9953.435098877328
2
Iteration 7900: Loss = -9953.433693531533
Iteration 8000: Loss = -9953.433633977993
Iteration 8100: Loss = -9953.535198303023
1
Iteration 8200: Loss = -9953.433580612698
Iteration 8300: Loss = -9953.433557400207
Iteration 8400: Loss = -9953.869791535268
1
Iteration 8500: Loss = -9953.433489550021
Iteration 8600: Loss = -9953.433479622789
Iteration 8700: Loss = -9953.458511334857
1
Iteration 8800: Loss = -9953.433401626933
Iteration 8900: Loss = -9953.433471270808
1
Iteration 9000: Loss = -9953.433412492159
2
Iteration 9100: Loss = -9953.433336054537
Iteration 9200: Loss = -9953.461834493033
1
Iteration 9300: Loss = -9953.433341972162
2
Iteration 9400: Loss = -9953.43349959588
3
Iteration 9500: Loss = -9953.43434795301
4
Iteration 9600: Loss = -9953.43325373359
Iteration 9700: Loss = -9953.43330051521
1
Iteration 9800: Loss = -9953.43444783185
2
Iteration 9900: Loss = -9953.433458104679
3
Iteration 10000: Loss = -9953.433326168619
4
Iteration 10100: Loss = -9953.434015272791
5
Iteration 10200: Loss = -9953.433237125408
Iteration 10300: Loss = -9953.435153577877
1
Iteration 10400: Loss = -9953.491864775966
2
Iteration 10500: Loss = -9953.435152766186
3
Iteration 10600: Loss = -9953.43368081285
4
Iteration 10700: Loss = -9953.44284141025
5
Iteration 10800: Loss = -9953.435446928885
6
Iteration 10900: Loss = -9953.441039745207
7
Iteration 11000: Loss = -9953.43297519443
Iteration 11100: Loss = -9953.628467941113
1
Iteration 11200: Loss = -9953.432943953367
Iteration 11300: Loss = -9953.435593818818
1
Iteration 11400: Loss = -9953.433762844412
2
Iteration 11500: Loss = -9953.43514961387
3
Iteration 11600: Loss = -9953.433362970147
4
Iteration 11700: Loss = -9953.44477601281
5
Iteration 11800: Loss = -9953.432642466594
Iteration 11900: Loss = -9953.432666942515
1
Iteration 12000: Loss = -9953.432682062794
2
Iteration 12100: Loss = -9953.533031918127
3
Iteration 12200: Loss = -9953.433004749897
4
Iteration 12300: Loss = -9953.432633755861
Iteration 12400: Loss = -9953.672338756074
1
Iteration 12500: Loss = -9953.432619701834
Iteration 12600: Loss = -9953.449807458106
1
Iteration 12700: Loss = -9953.432617557475
Iteration 12800: Loss = -9953.432616207618
Iteration 12900: Loss = -9953.432817665931
1
Iteration 13000: Loss = -9953.43765833461
2
Iteration 13100: Loss = -9953.436120783052
3
Iteration 13200: Loss = -9953.435252846431
4
Iteration 13300: Loss = -9953.433185476537
5
Iteration 13400: Loss = -9953.434437194814
6
Iteration 13500: Loss = -9953.56277584223
7
Iteration 13600: Loss = -9953.445309364979
8
Iteration 13700: Loss = -9953.432623419503
9
Iteration 13800: Loss = -9953.438178069093
10
Stopping early at iteration 13800 due to no improvement.
tensor([[-8.8257e+00,  4.2105e+00],
        [-5.3496e+00,  7.3441e-01],
        [-5.6280e+00,  1.0128e+00],
        [-8.3058e+00,  3.6906e+00],
        [-7.6338e+00,  3.0185e+00],
        [-6.6457e+00,  2.0305e+00],
        [-6.6122e+00,  1.9970e+00],
        [-7.4532e+00,  2.8380e+00],
        [-8.8461e+00,  4.2309e+00],
        [-5.6204e+00,  1.0052e+00],
        [-4.7686e+00,  1.5337e-01],
        [-8.9152e-01, -3.7237e+00],
        [-8.7583e+00,  4.1431e+00],
        [-6.7589e+00,  2.1437e+00],
        [-6.8658e+00,  2.2506e+00],
        [-5.9219e+00,  1.3067e+00],
        [-5.9704e+00,  1.3551e+00],
        [-9.7831e+00,  5.1679e+00],
        [-8.1528e+00,  3.5376e+00],
        [-9.1128e-01, -3.7039e+00],
        [-9.6468e+00,  5.0316e+00],
        [-9.7976e+00,  5.1823e+00],
        [-7.1275e+00,  2.5122e+00],
        [-6.4925e+00,  1.8773e+00],
        [-6.8536e+00,  2.2384e+00],
        [-8.9366e+00,  4.3214e+00],
        [-7.4406e+00,  2.8253e+00],
        [-7.3804e+00,  2.7652e+00],
        [-8.0209e+00,  3.4057e+00],
        [-7.1354e+00,  2.5202e+00],
        [-6.0840e+00,  1.4688e+00],
        [-8.3963e+00,  3.7811e+00],
        [-7.0142e+00,  2.3989e+00],
        [-9.2145e+00,  4.5993e+00],
        [-9.2434e+00,  4.6282e+00],
        [-2.5409e+00, -2.0743e+00],
        [-7.1506e+00,  2.5354e+00],
        [-8.6548e+00,  4.0395e+00],
        [-7.3710e+00,  2.7558e+00],
        [-6.1553e+00,  1.5400e+00],
        [-9.9722e+00,  5.3569e+00],
        [-7.7338e+00,  3.1186e+00],
        [-6.3417e+00,  1.7264e+00],
        [-8.9854e+00,  4.3702e+00],
        [-7.6911e+00,  3.0759e+00],
        [-8.9791e+00,  4.3639e+00],
        [-1.0333e+01,  5.7175e+00],
        [-7.9050e+00,  3.2898e+00],
        [-5.7290e+00,  1.1137e+00],
        [-8.7134e+00,  4.0982e+00],
        [-6.5199e+00,  1.9047e+00],
        [-7.1316e+00,  2.5164e+00],
        [-9.4867e+00,  4.8714e+00],
        [-6.0955e+00,  1.4802e+00],
        [-9.3693e+00,  4.7541e+00],
        [-4.2707e+00, -3.4455e-01],
        [-9.2679e+00,  4.6527e+00],
        [-1.0323e+01,  5.7073e+00],
        [-8.2549e+00,  3.6397e+00],
        [-5.5764e+00,  9.6117e-01],
        [-5.9919e+00,  1.3767e+00],
        [-2.8480e+00, -1.7672e+00],
        [-7.5956e+00,  2.9804e+00],
        [-6.0261e+00,  1.4108e+00],
        [-8.9267e+00,  4.3114e+00],
        [-4.2417e+00, -3.7356e-01],
        [-8.4190e+00,  3.8038e+00],
        [-1.0203e+01,  5.5877e+00],
        [-6.4042e+00,  1.7890e+00],
        [-7.4055e+00,  2.7903e+00],
        [-7.0293e+00,  2.4141e+00],
        [-7.1951e+00,  2.5799e+00],
        [-1.0107e+01,  5.4920e+00],
        [-8.2338e+00,  3.6186e+00],
        [-8.4402e+00,  3.8249e+00],
        [-7.6078e+00,  2.9926e+00],
        [-6.6818e+00,  2.0666e+00],
        [-1.5260e+00, -3.0892e+00],
        [-9.0295e+00,  4.4142e+00],
        [-4.3140e+00, -3.0125e-01],
        [-7.9011e+00,  3.2859e+00],
        [-9.2989e+00,  4.6837e+00],
        [-8.7523e+00,  4.1371e+00],
        [-8.3852e+00,  3.7700e+00],
        [-7.8452e+00,  3.2300e+00],
        [-5.2230e+00,  6.0778e-01],
        [-4.6412e+00,  2.5985e-02],
        [-7.2033e+00,  2.5881e+00],
        [-4.1218e+00, -4.9344e-01],
        [-6.3398e+00,  1.7245e+00],
        [-9.3071e+00,  4.6919e+00],
        [-5.1471e+00,  5.3186e-01],
        [-8.5805e+00,  3.9653e+00],
        [-1.0095e+01,  5.4799e+00],
        [-7.5839e+00,  2.9687e+00],
        [-6.6288e+00,  2.0136e+00],
        [-2.9266e+00, -1.6886e+00],
        [-4.6183e+00,  3.0699e-03],
        [-6.6680e+00,  2.0527e+00],
        [-8.6765e+00,  4.0613e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.7833e-06],
        [8.8904e-03, 9.9111e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0371, 0.9629], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3746, 0.1684],
         [0.8709, 0.1367]],

        [[0.9975, 0.1703],
         [0.1118, 0.5168]],

        [[0.7309, 0.1445],
         [0.7337, 0.6222]],

        [[0.0306, 0.0742],
         [0.9823, 0.1012]],

        [[0.3447, 0.1926],
         [0.7041, 0.1690]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0005624593535232805
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.017313838488058987
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: 0.00040339207313405906
Average Adjusted Rand Index: -0.005005487784867221
Iteration 0: Loss = -30825.361942573916
Iteration 10: Loss = -9962.958823176596
Iteration 20: Loss = -9962.958823176934
1
Iteration 30: Loss = -9962.958823187482
2
Iteration 40: Loss = -9962.95882351167
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.1318e-18, 1.0000e+00],
        [1.7361e-11, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.6425e-11, 1.0000e+00])
beta: tensor([[[0.1681, 0.1695],
         [0.5336, 0.1373]],

        [[0.7269, 0.1730],
         [0.3674, 0.9483]],

        [[0.1441, 0.0963],
         [0.6206, 0.7767]],

        [[0.7900, 0.0684],
         [0.9716, 0.7686]],

        [[0.1542, 0.2224],
         [0.8673, 0.8877]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30824.687316362626
Iteration 100: Loss = -9983.98396454918
Iteration 200: Loss = -9967.734505453845
Iteration 300: Loss = -9965.057251903729
Iteration 400: Loss = -9963.976136736663
Iteration 500: Loss = -9963.394073743333
Iteration 600: Loss = -9963.055769102419
Iteration 700: Loss = -9962.852866661651
Iteration 800: Loss = -9962.718654157443
Iteration 900: Loss = -9962.630762051935
Iteration 1000: Loss = -9962.561495601229
Iteration 1100: Loss = -9962.496949854793
Iteration 1200: Loss = -9962.434273864299
Iteration 1300: Loss = -9962.370687152386
Iteration 1400: Loss = -9962.304098733284
Iteration 1500: Loss = -9962.236342219077
Iteration 1600: Loss = -9962.17088782007
Iteration 1700: Loss = -9962.109951855522
Iteration 1800: Loss = -9962.053290001653
Iteration 1900: Loss = -9961.999136962666
Iteration 2000: Loss = -9961.94566900012
Iteration 2100: Loss = -9961.89105618244
Iteration 2200: Loss = -9961.833895893704
Iteration 2300: Loss = -9961.773077649415
Iteration 2400: Loss = -9961.707461162492
Iteration 2500: Loss = -9961.636210282542
Iteration 2600: Loss = -9961.55860431623
Iteration 2700: Loss = -9961.473952994791
Iteration 2800: Loss = -9961.381972402094
Iteration 2900: Loss = -9961.281878087151
Iteration 3000: Loss = -9961.173488163688
Iteration 3100: Loss = -9961.05810901766
Iteration 3200: Loss = -9960.938577424935
Iteration 3300: Loss = -9960.819995747948
Iteration 3400: Loss = -9960.710417723605
Iteration 3500: Loss = -9960.614794022396
Iteration 3600: Loss = -9960.526928635374
Iteration 3700: Loss = -9960.4265137452
Iteration 3800: Loss = -9960.273855770827
Iteration 3900: Loss = -9960.097671341486
Iteration 4000: Loss = -9960.016767680489
Iteration 4100: Loss = -9959.98534878539
Iteration 4200: Loss = -9959.969153133763
Iteration 4300: Loss = -9959.959493742232
Iteration 4400: Loss = -9959.953211343502
Iteration 4500: Loss = -9959.949014255868
Iteration 4600: Loss = -9959.946177077793
Iteration 4700: Loss = -9959.94416997657
Iteration 4800: Loss = -9959.94284660405
Iteration 4900: Loss = -9959.941787725711
Iteration 5000: Loss = -9959.941072834928
Iteration 5100: Loss = -9959.940465461177
Iteration 5200: Loss = -9959.940068009995
Iteration 5300: Loss = -9959.939894186684
Iteration 5400: Loss = -9959.939463519544
Iteration 5500: Loss = -9959.93925830335
Iteration 5600: Loss = -9959.93904926055
Iteration 5700: Loss = -9959.938991116158
Iteration 5800: Loss = -9959.938768815984
Iteration 5900: Loss = -9959.938703949585
Iteration 6000: Loss = -9959.940534755893
1
Iteration 6100: Loss = -9959.93837090967
Iteration 6200: Loss = -9959.938352955145
Iteration 6300: Loss = -9959.938084774582
Iteration 6400: Loss = -9959.937949121739
Iteration 6500: Loss = -9959.937750615367
Iteration 6600: Loss = -9959.937792474202
1
Iteration 6700: Loss = -9959.937430224685
Iteration 6800: Loss = -9959.941068902162
1
Iteration 6900: Loss = -9959.937096688858
Iteration 7000: Loss = -9959.936879820938
Iteration 7100: Loss = -9959.936761738863
Iteration 7200: Loss = -9959.93643461468
Iteration 7300: Loss = -9959.937410713272
1
Iteration 7400: Loss = -9959.935904766826
Iteration 7500: Loss = -9959.946646609216
1
Iteration 7600: Loss = -9959.935402341487
Iteration 7700: Loss = -9959.935235824956
Iteration 7800: Loss = -9959.935229651559
Iteration 7900: Loss = -9959.934597980224
Iteration 8000: Loss = -9959.935126274006
1
Iteration 8100: Loss = -9959.934043559342
Iteration 8200: Loss = -9959.935348693587
1
Iteration 8300: Loss = -9959.933544554404
Iteration 8400: Loss = -9959.9612407251
1
Iteration 8500: Loss = -9960.06526878162
2
Iteration 8600: Loss = -9959.932814344607
Iteration 8700: Loss = -9959.932677856492
Iteration 8800: Loss = -9959.966720366829
1
Iteration 8900: Loss = -9959.932091358365
Iteration 9000: Loss = -9959.934417067774
1
Iteration 9100: Loss = -9959.931655210867
Iteration 9200: Loss = -9959.93153099793
Iteration 9300: Loss = -9959.938832310552
1
Iteration 9400: Loss = -9959.959507484915
2
Iteration 9500: Loss = -9959.931041429236
Iteration 9600: Loss = -9959.931459081881
1
Iteration 9700: Loss = -9959.938145822984
2
Iteration 9800: Loss = -9959.930579209013
Iteration 9900: Loss = -9959.932129907964
1
Iteration 10000: Loss = -9959.945437216918
2
Iteration 10100: Loss = -9959.931173052282
3
Iteration 10200: Loss = -9959.930719781267
4
Iteration 10300: Loss = -9959.93050089788
Iteration 10400: Loss = -9959.951139694987
1
Iteration 10500: Loss = -9959.930468971455
Iteration 10600: Loss = -9959.932396295664
1
Iteration 10700: Loss = -9959.93177450081
2
Iteration 10800: Loss = -9959.929472720763
Iteration 10900: Loss = -9959.929515595762
1
Iteration 11000: Loss = -9959.929417565449
Iteration 11100: Loss = -9959.940013243595
1
Iteration 11200: Loss = -9960.109052421796
2
Iteration 11300: Loss = -9959.960740099961
3
Iteration 11400: Loss = -9959.95428989981
4
Iteration 11500: Loss = -9959.932732745941
5
Iteration 11600: Loss = -9959.929900769244
6
Iteration 11700: Loss = -9959.938921508907
7
Iteration 11800: Loss = -9959.932220860615
8
Iteration 11900: Loss = -9959.932471492404
9
Iteration 12000: Loss = -9959.929371828455
Iteration 12100: Loss = -9960.024514058452
1
Iteration 12200: Loss = -9959.928719712494
Iteration 12300: Loss = -9959.930056843028
1
Iteration 12400: Loss = -9959.928656431064
Iteration 12500: Loss = -9959.936289244104
1
Iteration 12600: Loss = -9959.93108062109
2
Iteration 12700: Loss = -9959.931351288584
3
Iteration 12800: Loss = -9959.928823786111
4
Iteration 12900: Loss = -9959.928795153926
5
Iteration 13000: Loss = -9959.929488056934
6
Iteration 13100: Loss = -9959.928648304865
Iteration 13200: Loss = -9959.928523229917
Iteration 13300: Loss = -9959.929249608882
1
Iteration 13400: Loss = -9959.9286223485
2
Iteration 13500: Loss = -9959.930949852733
3
Iteration 13600: Loss = -9959.928592921591
4
Iteration 13700: Loss = -9959.931065598295
5
Iteration 13800: Loss = -9959.936209271676
6
Iteration 13900: Loss = -9959.930453264697
7
Iteration 14000: Loss = -9959.928352461475
Iteration 14100: Loss = -9959.932717828015
1
Iteration 14200: Loss = -9959.938431530307
2
Iteration 14300: Loss = -9959.930402968706
3
Iteration 14400: Loss = -9959.934828597941
4
Iteration 14500: Loss = -9959.929315886777
5
Iteration 14600: Loss = -9959.953359107301
6
Iteration 14700: Loss = -9959.947590343954
7
Iteration 14800: Loss = -9959.929210959406
8
Iteration 14900: Loss = -9959.94395340395
9
Iteration 15000: Loss = -9959.929355324146
10
Stopping early at iteration 15000 due to no improvement.
tensor([[-0.9819, -0.8892],
        [-1.0561, -0.7571],
        [-1.7928, -1.4636],
        [-1.0453, -0.9526],
        [-1.5938, -1.4640],
        [-0.7054, -0.7138],
        [-0.9693, -0.5975],
        [-1.1392, -1.2492],
        [-0.9583, -1.2201],
        [-0.8475, -0.5756],
        [-0.8381, -0.6267],
        [-0.5877, -0.8018],
        [-0.7573, -0.6494],
        [-0.9683, -0.6161],
        [-0.8019, -0.6399],
        [-0.7105, -0.6807],
        [-0.7861, -0.6090],
        [-2.4211, -2.1941],
        [-0.9998, -0.8152],
        [-0.6265, -0.7639],
        [-1.2258, -1.1516],
        [-1.0725, -1.2351],
        [-1.2418, -1.3366],
        [-0.8264, -0.5648],
        [-0.9371, -0.7668],
        [-2.3602, -2.2550],
        [-0.8857, -0.7496],
        [-0.6916, -0.6975],
        [-1.4859, -1.3207],
        [-0.6810, -0.7075],
        [-0.9508, -0.9602],
        [-0.7308, -0.6561],
        [-0.7717, -0.9863],
        [-1.0372, -1.1012],
        [-1.0839, -0.8801],
        [-1.5129, -1.4611],
        [-1.4886, -1.2021],
        [-0.7960, -0.8601],
        [-2.2507, -2.3646],
        [-2.0773, -2.0069],
        [-0.8766, -0.6301],
        [-0.6731, -0.8235],
        [-1.7600, -1.6384],
        [-0.7934, -0.8427],
        [-1.1326, -0.7493],
        [-1.0284, -0.8047],
        [-0.8314, -0.6136],
        [-0.8052, -1.1481],
        [-0.9479, -0.8090],
        [-0.7200, -0.6697],
        [-0.8088, -0.9891],
        [-1.2284, -1.2441],
        [-0.8707, -0.7010],
        [-0.9881, -0.8016],
        [-1.0218, -0.9377],
        [-2.0212, -1.9670],
        [-1.0334, -0.9252],
        [-1.0200, -0.7946],
        [-1.6709, -1.5312],
        [-1.0276, -1.1868],
        [-1.7970, -2.0095],
        [-1.0473, -0.9692],
        [-0.6216, -0.7694],
        [-2.1957, -2.0575],
        [-1.0165, -0.6554],
        [-1.1358, -1.1298],
        [-0.7719, -0.7558],
        [-0.9063, -0.7720],
        [-0.9975, -1.3164],
        [-0.8623, -0.6851],
        [-1.1973, -1.3387],
        [-0.7967, -0.6760],
        [-1.0491, -0.7889],
        [-2.0926, -1.9328],
        [-0.8171, -0.9998],
        [-1.6590, -1.5183],
        [-0.8221, -0.5773],
        [-0.9036, -0.8366],
        [-0.7695, -0.7183],
        [-0.7129, -0.8593],
        [-1.3282, -1.0803],
        [-1.0906, -1.1358],
        [-1.3203, -1.1220],
        [-1.0044, -0.9513],
        [-1.3718, -1.4422],
        [-1.0400, -1.2411],
        [-0.7217, -0.8849],
        [-1.2321, -1.3950],
        [-0.6078, -0.8075],
        [-0.8341, -0.8099],
        [-1.1804, -0.9929],
        [-0.8740, -0.8580],
        [-1.2227, -1.0430],
        [-0.8750, -0.9433],
        [-0.7091, -0.8042],
        [-0.9866, -1.0717],
        [-0.8191, -0.6224],
        [-0.7005, -0.8795],
        [-0.8628, -0.8805],
        [-0.7533, -0.7304]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.5130e-04, 9.9985e-01],
        [4.8926e-02, 9.5107e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4900, 0.5100], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1493, 0.1436],
         [0.5336, 0.1377]],

        [[0.7269, 0.1692],
         [0.3674, 0.9483]],

        [[0.1441, 0.1073],
         [0.6206, 0.7767]],

        [[0.7900, 0.0852],
         [0.9716, 0.7686]],

        [[0.1542, 0.1913],
         [0.8673, 0.8877]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 34
Adjusted Rand Index: 0.09367422501141903
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.005229581978962136
Average Adjusted Rand Index: 0.018734845002283805
Iteration 0: Loss = -39618.03280434984
Iteration 10: Loss = -9961.092724113534
Iteration 20: Loss = -9961.07201308466
Iteration 30: Loss = -9961.05060308651
Iteration 40: Loss = -9961.024344496036
Iteration 50: Loss = -9960.982210721264
Iteration 60: Loss = -9960.90461658015
Iteration 70: Loss = -9960.79017388361
Iteration 80: Loss = -9960.670488529591
Iteration 90: Loss = -9960.584316879746
Iteration 100: Loss = -9960.552450623109
Iteration 110: Loss = -9960.546413058088
Iteration 120: Loss = -9960.546343471542
Iteration 130: Loss = -9960.546934022397
1
Iteration 140: Loss = -9960.547336375379
2
Iteration 150: Loss = -9960.547592400164
3
Stopping early at iteration 149 due to no improvement.
pi: tensor([[3.3412e-05, 9.9997e-01],
        [4.6967e-02, 9.5303e-01]], dtype=torch.float64)
alpha: tensor([0.0452, 0.9548])
beta: tensor([[[0.1507, 0.1573],
         [0.1891, 0.1370]],

        [[0.8633, 0.1591],
         [0.3885, 0.4831]],

        [[0.9021, 0.1063],
         [0.4864, 0.5419]],

        [[0.5754, 0.0859],
         [0.4299, 0.8100]],

        [[0.4485, 0.1901],
         [0.4570, 0.5126]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39617.73224549484
Iteration 100: Loss = -10045.15073603985
Iteration 200: Loss = -10015.842209328523
Iteration 300: Loss = -10002.381106630619
Iteration 400: Loss = -9994.038089314477
Iteration 500: Loss = -9983.439599954958
Iteration 600: Loss = -9979.368233692321
Iteration 700: Loss = -9979.010934852338
Iteration 800: Loss = -9978.789961462558
Iteration 900: Loss = -9978.716709727265
Iteration 1000: Loss = -9978.548380224282
Iteration 1100: Loss = -9978.516676508105
Iteration 1200: Loss = -9978.506890099228
Iteration 1300: Loss = -9978.499637763709
Iteration 1400: Loss = -9978.494649463191
Iteration 1500: Loss = -9978.491292349443
Iteration 1600: Loss = -9978.488665232499
Iteration 1700: Loss = -9978.48635881559
Iteration 1800: Loss = -9978.484385330665
Iteration 1900: Loss = -9978.482672092827
Iteration 2000: Loss = -9978.481182247051
Iteration 2100: Loss = -9978.479748438787
Iteration 2200: Loss = -9978.478581055562
Iteration 2300: Loss = -9978.477701083604
Iteration 2400: Loss = -9978.476978156268
Iteration 2500: Loss = -9978.47633316225
Iteration 2600: Loss = -9978.475802425612
Iteration 2700: Loss = -9978.475358720198
Iteration 2800: Loss = -9978.475046197607
Iteration 2900: Loss = -9978.474733086021
Iteration 3000: Loss = -9978.474466923235
Iteration 3100: Loss = -9978.474229107484
Iteration 3200: Loss = -9978.474004033249
Iteration 3300: Loss = -9978.47385032907
Iteration 3400: Loss = -9978.473663923702
Iteration 3500: Loss = -9978.473463155755
Iteration 3600: Loss = -9978.473311418911
Iteration 3700: Loss = -9978.473176747077
Iteration 3800: Loss = -9978.473048659958
Iteration 3900: Loss = -9978.472918118152
Iteration 4000: Loss = -9978.47276404424
Iteration 4100: Loss = -9978.472666730602
Iteration 4200: Loss = -9978.472560518243
Iteration 4300: Loss = -9978.472447958708
Iteration 4400: Loss = -9978.474476351537
1
Iteration 4500: Loss = -9978.472233583589
Iteration 4600: Loss = -9978.472273996884
1
Iteration 4700: Loss = -9978.472064676143
Iteration 4800: Loss = -9978.471985226242
Iteration 4900: Loss = -9978.472128887888
1
Iteration 5000: Loss = -9978.471864692441
Iteration 5100: Loss = -9978.471734136223
Iteration 5200: Loss = -9978.471534774322
Iteration 5300: Loss = -9978.471336456994
Iteration 5400: Loss = -9978.474012199076
1
Iteration 5500: Loss = -9978.468931985555
Iteration 5600: Loss = -9978.469168187803
1
Iteration 5700: Loss = -9978.470072942211
2
Iteration 5800: Loss = -9978.46865095846
Iteration 5900: Loss = -9978.46853384058
Iteration 6000: Loss = -9978.468413089044
Iteration 6100: Loss = -9978.468313559328
Iteration 6200: Loss = -9978.46800818508
Iteration 6300: Loss = -9978.468865885545
1
Iteration 6400: Loss = -9978.467914016806
Iteration 6500: Loss = -9978.467946748591
1
Iteration 6600: Loss = -9978.467932925729
2
Iteration 6700: Loss = -9978.467879823296
Iteration 6800: Loss = -9978.469047725368
1
Iteration 6900: Loss = -9978.470751273511
2
Iteration 7000: Loss = -9978.479069442074
3
Iteration 7100: Loss = -9978.468592537765
4
Iteration 7200: Loss = -9978.467927205389
5
Iteration 7300: Loss = -9978.46852108679
6
Iteration 7400: Loss = -9978.468070861805
7
Iteration 7500: Loss = -9978.46791204728
8
Iteration 7600: Loss = -9978.467991339881
9
Iteration 7700: Loss = -9978.4679831475
10
Stopping early at iteration 7700 due to no improvement.
tensor([[-2.7756,  0.6194],
        [-2.1998,  0.7333],
        [-2.2844,  0.0872],
        [-2.6189,  0.8575],
        [-1.0784, -0.3428],
        [-2.1745,  0.7882],
        [ 0.4010, -1.7906],
        [-3.9275,  1.9235],
        [-6.2944,  4.9041],
        [-3.1055,  0.2370],
        [-3.5987,  2.1950],
        [-6.1502,  3.7126],
        [-5.4054,  0.7901],
        [ 1.8173, -3.5735],
        [-4.3841,  2.6459],
        [-2.9478,  1.5539],
        [-2.2323,  0.6388],
        [ 1.2251, -3.0709],
        [ 1.1727, -2.9067],
        [-5.9764,  4.3481],
        [-3.7888,  1.0993],
        [-3.1401,  1.6586],
        [-3.5298,  2.0690],
        [ 0.4502, -2.4149],
        [-2.9365,  1.5394],
        [-3.6888,  1.8063],
        [ 0.7954, -2.4742],
        [-1.5898,  0.0694],
        [-3.1667,  1.7411],
        [ 0.9733, -2.6204],
        [-3.1688,  1.3827],
        [ 0.8049, -3.2787],
        [-6.1655,  4.5274],
        [ 0.7392, -2.4468],
        [-2.8079,  0.8921],
        [-1.8849,  0.3103],
        [ 1.4351, -2.8309],
        [-3.1952,  1.7915],
        [-3.4329,  1.7070],
        [-1.2870, -0.4196],
        [ 0.6136, -2.0082],
        [-5.6417,  3.8759],
        [-3.6126,  2.0933],
        [-0.1169, -1.7909],
        [-0.6275, -0.7669],
        [ 0.8275, -3.0084],
        [-1.0075, -3.6077],
        [-4.9887,  2.7369],
        [-4.2093,  2.8104],
        [-3.2129,  1.4658],
        [-4.9140,  3.2837],
        [-3.9601,  2.3360],
        [-1.0056, -0.9778],
        [-3.5399,  1.9946],
        [-2.8716,  0.0434],
        [-3.5242,  1.8393],
        [ 1.6578, -3.0684],
        [-2.7495, -0.0856],
        [-2.6619,  0.4138],
        [-5.8150,  3.4961],
        [-4.4312,  3.0429],
        [-1.9693,  0.2864],
        [-4.3151,  2.2245],
        [ 1.4239, -3.1120],
        [-1.0655, -2.2631],
        [-3.6786,  2.2507],
        [-3.3147,  1.9211],
        [ 2.7952, -4.2615],
        [-5.7344,  4.2703],
        [-0.6213, -0.7731],
        [-5.7208,  4.3296],
        [-0.0126, -1.4011],
        [-3.4393,  1.0633],
        [-2.5078,  0.7680],
        [-5.0775,  3.5200],
        [-3.4946,  0.9818],
        [ 1.2157, -3.4789],
        [-4.0348,  2.5969],
        [-3.0452,  1.1522],
        [-5.0346,  3.0796],
        [-2.8249,  1.2999],
        [-2.2559,  0.3348],
        [-1.8630, -0.1641],
        [-3.6462,  1.9215],
        [-4.3284,  2.6026],
        [-4.5680,  3.0020],
        [-4.6622,  2.9362],
        [-4.3364,  1.3639],
        [-5.3254,  2.1374],
        [-3.2265, -0.6393],
        [-0.1140, -1.2767],
        [-3.7474,  2.1277],
        [-1.7932, -1.0220],
        [-4.3046,  2.9000],
        [-4.7124,  1.8797],
        [-4.4335,  1.9819],
        [-2.0737,  0.3662],
        [-4.9625,  3.5658],
        [-3.2196,  1.8196],
        [-3.5540,  2.1604]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1756, 0.8244],
        [0.4637, 0.5363]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2363, 0.7637], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.0863],
         [0.1891, 0.1690]],

        [[0.8633, 0.0988],
         [0.3885, 0.4831]],

        [[0.9021, 0.0867],
         [0.4864, 0.5419]],

        [[0.5754, 0.0861],
         [0.4299, 0.8100]],

        [[0.4485, 0.1024],
         [0.4570, 0.5126]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 76
Adjusted Rand Index: 0.2643829797149663
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 79
Adjusted Rand Index: 0.3298938787500459
time is 2
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.010592433514717992
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.010671217994798901
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 28
Adjusted Rand Index: 0.18553113095701562
Global Adjusted Rand Index: 0.023834184508319724
Average Adjusted Rand Index: 0.15594584098838937
Iteration 0: Loss = -15641.481063121879
Iteration 10: Loss = -9961.21016503909
Iteration 20: Loss = -9961.053085276626
Iteration 30: Loss = -9960.96784795852
Iteration 40: Loss = -9960.915888749929
Iteration 50: Loss = -9960.894996390447
Iteration 60: Loss = -9960.893227113917
Iteration 70: Loss = -9960.900009462744
1
Iteration 80: Loss = -9960.910281903509
2
Iteration 90: Loss = -9960.922118489385
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.1194, 0.8806],
        [0.1424, 0.8576]], dtype=torch.float64)
alpha: tensor([0.1412, 0.8588])
beta: tensor([[[0.1748, 0.1555],
         [0.0153, 0.1316]],

        [[0.7896, 0.1571],
         [0.4334, 0.0304]],

        [[0.5470, 0.1424],
         [0.7719, 0.9629]],

        [[0.6923, 0.1341],
         [0.0707, 0.6766]],

        [[0.3025, 0.1665],
         [0.0324, 0.0357]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: -0.00015692302765368048
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15641.216150066763
Iteration 100: Loss = -9978.798872019246
Iteration 200: Loss = -9963.652711887838
Iteration 300: Loss = -9962.558103996324
Iteration 400: Loss = -9962.173028117992
Iteration 500: Loss = -9961.992716049775
Iteration 600: Loss = -9961.865986238607
Iteration 700: Loss = -9961.735895065625
Iteration 800: Loss = -9961.478180710992
Iteration 900: Loss = -9961.313743575738
Iteration 1000: Loss = -9961.149759685253
Iteration 1100: Loss = -9960.99639648228
Iteration 1200: Loss = -9960.848991921059
Iteration 1300: Loss = -9960.690366866304
Iteration 1400: Loss = -9960.549045569795
Iteration 1500: Loss = -9960.40181088925
Iteration 1600: Loss = -9960.27418480255
Iteration 1700: Loss = -9960.202359335857
Iteration 1800: Loss = -9960.153475393561
Iteration 1900: Loss = -9960.113630849097
Iteration 2000: Loss = -9960.080583259187
Iteration 2100: Loss = -9960.05533312049
Iteration 2200: Loss = -9960.036494230812
Iteration 2300: Loss = -9960.022542559696
Iteration 2400: Loss = -9960.012410545454
Iteration 2500: Loss = -9960.006597051091
Iteration 2600: Loss = -9960.00241808966
Iteration 2700: Loss = -9959.99931317671
Iteration 2800: Loss = -9959.996851016704
Iteration 2900: Loss = -9959.994765291976
Iteration 3000: Loss = -9959.992655117454
Iteration 3100: Loss = -9959.989866483626
Iteration 3200: Loss = -9959.982341528012
Iteration 3300: Loss = -9959.967767014094
Iteration 3400: Loss = -9959.96351107698
Iteration 3500: Loss = -9959.961982511815
Iteration 3600: Loss = -9959.960822022113
Iteration 3700: Loss = -9959.959903393503
Iteration 3800: Loss = -9959.958936927129
Iteration 3900: Loss = -9959.966850307488
1
Iteration 4000: Loss = -9959.956232143384
Iteration 4100: Loss = -9959.953997094557
Iteration 4200: Loss = -9959.983393677461
1
Iteration 4300: Loss = -9959.95192689169
Iteration 4400: Loss = -9960.156555803052
1
Iteration 4500: Loss = -9959.95101676203
Iteration 4600: Loss = -9959.950606723716
Iteration 4700: Loss = -9959.950714738188
1
Iteration 4800: Loss = -9959.949836453898
Iteration 4900: Loss = -9959.949512219573
Iteration 5000: Loss = -9959.949197483582
Iteration 5100: Loss = -9959.948798876618
Iteration 5200: Loss = -9959.948470611658
Iteration 5300: Loss = -9959.948258056616
Iteration 5400: Loss = -9959.947927085732
Iteration 5500: Loss = -9959.957413570059
1
Iteration 5600: Loss = -9959.947435831515
Iteration 5700: Loss = -9959.947188478542
Iteration 5800: Loss = -9959.949987577154
1
Iteration 5900: Loss = -9959.946786277009
Iteration 6000: Loss = -9959.9598919252
1
Iteration 6100: Loss = -9959.946609203287
Iteration 6200: Loss = -9959.946211434759
Iteration 6300: Loss = -9959.947669723431
1
Iteration 6400: Loss = -9959.946451306647
2
Iteration 6500: Loss = -9959.956913233174
3
Iteration 6600: Loss = -9959.946291002116
4
Iteration 6700: Loss = -9959.945836346129
Iteration 6800: Loss = -9959.945495115724
Iteration 6900: Loss = -9959.931741065997
Iteration 7000: Loss = -9959.963602038932
1
Iteration 7100: Loss = -9960.024751488267
2
Iteration 7200: Loss = -9959.93025018391
Iteration 7300: Loss = -9959.930172836881
Iteration 7400: Loss = -9960.094404411851
1
Iteration 7500: Loss = -9959.929813412109
Iteration 7600: Loss = -9959.935004060302
1
Iteration 7700: Loss = -9959.930210194143
2
Iteration 7800: Loss = -9959.930756091071
3
Iteration 7900: Loss = -9959.932213168171
4
Iteration 8000: Loss = -9959.92944437977
Iteration 8100: Loss = -9959.930214894264
1
Iteration 8200: Loss = -9959.929495082315
2
Iteration 8300: Loss = -9959.930417524118
3
Iteration 8400: Loss = -9959.929206393905
Iteration 8500: Loss = -9959.929520444097
1
Iteration 8600: Loss = -9959.92923220666
2
Iteration 8700: Loss = -9959.92958530986
3
Iteration 8800: Loss = -9959.931722321144
4
Iteration 8900: Loss = -9959.929014829353
Iteration 9000: Loss = -9959.929419222464
1
Iteration 9100: Loss = -9959.929112748843
2
Iteration 9200: Loss = -9959.928853357236
Iteration 9300: Loss = -9959.973946931412
1
Iteration 9400: Loss = -9959.928707502713
Iteration 9500: Loss = -9959.964746454409
1
Iteration 9600: Loss = -9959.93764317575
2
Iteration 9700: Loss = -9959.992527701685
3
Iteration 9800: Loss = -9959.956678384637
4
Iteration 9900: Loss = -9959.950792053374
5
Iteration 10000: Loss = -9959.96003011886
6
Iteration 10100: Loss = -9959.93036103001
7
Iteration 10200: Loss = -9959.929648998746
8
Iteration 10300: Loss = -9959.933008828828
9
Iteration 10400: Loss = -9959.933067457061
10
Stopping early at iteration 10400 due to no improvement.
tensor([[-0.7451, -0.6598],
        [-1.4858, -1.1952],
        [-1.1341, -0.8135],
        [-0.7446, -0.6593],
        [-0.7546, -0.6325],
        [-0.7750, -0.7903],
        [-1.0427, -0.6801],
        [-0.8070, -0.9236],
        [-0.7074, -0.9747],
        [-1.4330, -1.1696],
        [-0.8300, -0.6263],
        [-1.1699, -1.3898],
        [-0.9526, -0.8516],
        [-0.8691, -0.5261],
        [-1.1331, -0.9789],
        [-1.9127, -1.8902],
        [-0.8014, -0.6324],
        [-0.8739, -0.6553],
        [-1.0755, -0.8990],
        [-0.8780, -1.0216],
        [-0.7460, -0.6793],
        [-0.6352, -0.8040],
        [-1.7529, -1.8542],
        [-0.8961, -0.6431],
        [-0.7873, -0.6249],
        [-0.9587, -0.8608],
        [-1.1927, -1.0644],
        [-0.7080, -0.7211],
        [-0.8213, -0.6636],
        [-0.7258, -0.7594],
        [-0.7231, -0.7396],
        [-1.4532, -1.3860],
        [-0.6103, -0.8307],
        [-0.6790, -0.7500],
        [-0.8008, -0.6050],
        [-0.7417, -0.6971],
        [-0.8330, -0.5553],
        [-0.7794, -0.8502],
        [-0.9705, -1.0909],
        [-0.9873, -0.9242],
        [-1.6025, -1.3643],
        [-0.8268, -0.9835],
        [-1.5385, -1.4244],
        [-0.6850, -0.7413],
        [-1.5649, -1.1911],
        [-0.8079, -0.5926],
        [-0.8735, -0.6641],
        [-0.6614, -1.0092],
        [-1.6823, -1.5506],
        [-1.0577, -1.0147],
        [-1.0075, -1.1937],
        [-0.7563, -0.7790],
        [-1.2736, -1.1120],
        [-2.3514, -2.1725],
        [-0.7617, -0.6851],
        [-0.7705, -0.7236],
        [-0.7453, -0.6447],
        [-0.9856, -0.7685],
        [-0.8675, -0.7352],
        [-1.6204, -1.7857],
        [-0.6226, -0.8408],
        [-1.0154, -0.9446],
        [-0.6310, -0.7851],
        [-2.3728, -2.2424],
        [-1.2025, -0.8505],
        [-1.3159, -1.3169],
        [-1.0986, -1.0895],
        [-1.4650, -1.3385],
        [-0.5677, -0.8918],
        [-0.8462, -0.6769],
        [-1.1076, -1.2553],
        [-1.4812, -1.3682],
        [-0.8475, -0.5959],
        [-1.1838, -1.0319],
        [-0.6809, -0.8696],
        [-0.8807, -0.7471],
        [-1.5433, -1.3072],
        [-0.7450, -0.6854],
        [-1.2048, -1.1608],
        [-0.6196, -0.7721],
        [-2.0785, -1.8388],
        [-1.0977, -1.1498],
        [-1.0261, -0.8357],
        [-0.8010, -0.7551],
        [-0.8953, -0.9725],
        [-0.6089, -0.8160],
        [-0.6290, -0.7983],
        [-1.0534, -1.2225],
        [-0.6726, -0.8782],
        [-0.7225, -0.7056],
        [-0.8715, -0.6921],
        [-0.7316, -0.7227],
        [-0.7803, -0.6086],
        [-1.1988, -1.2738],
        [-0.6578, -0.7594],
        [-0.6885, -0.7802],
        [-1.0709, -0.8824],
        [-0.7946, -0.9795],
        [-0.8108, -0.8357],
        [-2.1514, -2.1357]], dtype=torch.float64, requires_grad=True)
pi: tensor([[4.3850e-04, 9.9956e-01],
        [4.8521e-02, 9.5148e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4919, 0.5081], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1492, 0.1435],
         [0.0153, 0.1377]],

        [[0.7896, 0.1692],
         [0.4334, 0.0304]],

        [[0.5470, 0.1074],
         [0.7719, 0.9629]],

        [[0.6923, 0.0853],
         [0.0707, 0.6766]],

        [[0.3025, 0.1918],
         [0.0324, 0.0357]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 33
Adjusted Rand Index: 0.10694425110023958
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0058403698126631215
Average Adjusted Rand Index: 0.02138885022004792
Iteration 0: Loss = -24515.885247706996
Iteration 10: Loss = -9961.169225357453
Iteration 20: Loss = -9960.82677742642
Iteration 30: Loss = -9960.702457364197
Iteration 40: Loss = -9960.645469353383
Iteration 50: Loss = -9960.604877509031
Iteration 60: Loss = -9960.57330598442
Iteration 70: Loss = -9960.554436424423
Iteration 80: Loss = -9960.549804813609
Iteration 90: Loss = -9960.55774883442
1
Iteration 100: Loss = -9960.575117600072
2
Iteration 110: Loss = -9960.598752682208
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.8009, 0.1991],
        [0.9971, 0.0029]], dtype=torch.float64)
alpha: tensor([0.8351, 0.1649])
beta: tensor([[[0.1432, 0.1367],
         [0.8470, 0.1106]],

        [[0.6961, 0.1326],
         [0.3775, 0.8263]],

        [[0.5047, 0.1145],
         [0.5290, 0.8519]],

        [[0.2532, 0.1104],
         [0.9358, 0.7121]],

        [[0.7299, 0.1315],
         [0.8144, 0.2496]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.008260141115681973
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.847940178382811e-07
Average Adjusted Rand Index: -0.0016520282231363946
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24515.615265771852
Iteration 100: Loss = -9994.854367231683
Iteration 200: Loss = -9974.733769730212
Iteration 300: Loss = -9969.488917789586
Iteration 400: Loss = -9966.928687436026
Iteration 500: Loss = -9965.54667247352
Iteration 600: Loss = -9964.681980107247
Iteration 700: Loss = -9964.073994611326
Iteration 800: Loss = -9963.616119447019
Iteration 900: Loss = -9963.27016250417
Iteration 1000: Loss = -9963.0161279032
Iteration 1100: Loss = -9962.817775652158
Iteration 1200: Loss = -9962.65305603771
Iteration 1300: Loss = -9962.508583821089
Iteration 1400: Loss = -9962.376481687032
Iteration 1500: Loss = -9962.250119058031
Iteration 1600: Loss = -9962.125649252657
Iteration 1700: Loss = -9962.002067803127
Iteration 1800: Loss = -9961.882240445779
Iteration 1900: Loss = -9961.773436315709
Iteration 2000: Loss = -9961.68340616546
Iteration 2100: Loss = -9961.611829986465
Iteration 2200: Loss = -9961.55005528031
Iteration 2300: Loss = -9961.488478099
Iteration 2400: Loss = -9961.415956861809
Iteration 2500: Loss = -9961.315215427097
Iteration 2600: Loss = -9961.139247831474
Iteration 2700: Loss = -9960.862409586121
Iteration 2800: Loss = -9960.707552544498
Iteration 2900: Loss = -9960.658628715955
Iteration 3000: Loss = -9960.622051326689
Iteration 3100: Loss = -9960.602489454914
Iteration 3200: Loss = -9960.587804473435
Iteration 3300: Loss = -9960.57597841612
Iteration 3400: Loss = -9960.56642724217
Iteration 3500: Loss = -9960.558390512033
Iteration 3600: Loss = -9960.55859670347
1
Iteration 3700: Loss = -9960.545275081557
Iteration 3800: Loss = -9960.53979520381
Iteration 3900: Loss = -9960.555279966376
1
Iteration 4000: Loss = -9960.530493342918
Iteration 4100: Loss = -9960.526502867973
Iteration 4200: Loss = -9960.523028142588
Iteration 4300: Loss = -9960.51972010145
Iteration 4400: Loss = -9960.5167091649
Iteration 4500: Loss = -9960.513975930997
Iteration 4600: Loss = -9960.512677007928
Iteration 4700: Loss = -9960.50925263364
Iteration 4800: Loss = -9960.507184554359
Iteration 4900: Loss = -9960.513940849527
1
Iteration 5000: Loss = -9960.503543830837
Iteration 5100: Loss = -9960.501929310245
Iteration 5200: Loss = -9960.539785623261
1
Iteration 5300: Loss = -9960.499127293078
Iteration 5400: Loss = -9960.497856528616
Iteration 5500: Loss = -9960.660293055065
1
Iteration 5600: Loss = -9960.495707496731
Iteration 5700: Loss = -9960.494757094024
Iteration 5800: Loss = -9960.493890336642
Iteration 5900: Loss = -9960.493199409932
Iteration 6000: Loss = -9960.492416290923
Iteration 6100: Loss = -9960.491789079633
Iteration 6200: Loss = -9960.926195854778
1
Iteration 6300: Loss = -9960.490801252156
Iteration 6400: Loss = -9960.490380890433
Iteration 6500: Loss = -9960.489982886817
Iteration 6600: Loss = -9960.48976290636
Iteration 6700: Loss = -9960.489256585906
Iteration 6800: Loss = -9960.48893221525
Iteration 6900: Loss = -9960.552168225367
1
Iteration 7000: Loss = -9960.488343382493
Iteration 7100: Loss = -9960.488068217504
Iteration 7200: Loss = -9960.493114033752
1
Iteration 7300: Loss = -9960.487572460625
Iteration 7400: Loss = -9960.48732300178
Iteration 7500: Loss = -9960.4870783463
Iteration 7600: Loss = -9960.48780321513
1
Iteration 7700: Loss = -9960.486688042582
Iteration 7800: Loss = -9960.486530734897
Iteration 7900: Loss = -9960.52505694903
1
Iteration 8000: Loss = -9960.486225496648
Iteration 8100: Loss = -9960.48605389461
Iteration 8200: Loss = -9960.528404266697
1
Iteration 8300: Loss = -9960.485795527498
Iteration 8400: Loss = -9960.485635596453
Iteration 8500: Loss = -9960.485545312275
Iteration 8600: Loss = -9960.485777346876
1
Iteration 8700: Loss = -9960.485323157884
Iteration 8800: Loss = -9960.485218881036
Iteration 8900: Loss = -9960.485869060469
1
Iteration 9000: Loss = -9960.485037177115
Iteration 9100: Loss = -9960.48495249998
Iteration 9200: Loss = -9960.494439314896
1
Iteration 9300: Loss = -9960.484845209345
Iteration 9400: Loss = -9960.484718405243
Iteration 9500: Loss = -9960.489950813999
1
Iteration 9600: Loss = -9960.484592961458
Iteration 9700: Loss = -9960.484514370082
Iteration 9800: Loss = -9960.589572326437
1
Iteration 9900: Loss = -9960.48444225079
Iteration 10000: Loss = -9960.484355614572
Iteration 10100: Loss = -9960.48565166552
1
Iteration 10200: Loss = -9960.484236074797
Iteration 10300: Loss = -9960.484204625835
Iteration 10400: Loss = -9960.490565355374
1
Iteration 10500: Loss = -9960.484125591729
Iteration 10600: Loss = -9960.74764935863
1
Iteration 10700: Loss = -9960.484034528115
Iteration 10800: Loss = -9960.483954756372
Iteration 10900: Loss = -9960.498160787542
1
Iteration 11000: Loss = -9960.483874874253
Iteration 11100: Loss = -9960.483834882729
Iteration 11200: Loss = -9960.484397063785
1
Iteration 11300: Loss = -9960.483757194925
Iteration 11400: Loss = -9960.484434592614
1
Iteration 11500: Loss = -9960.483695235298
Iteration 11600: Loss = -9960.483630479392
Iteration 11700: Loss = -9960.822248113218
1
Iteration 11800: Loss = -9960.483571819252
Iteration 11900: Loss = -9960.483515993345
Iteration 12000: Loss = -9960.524225240722
1
Iteration 12100: Loss = -9960.483399300321
Iteration 12200: Loss = -9960.483360986023
Iteration 12300: Loss = -9960.50275983171
1
Iteration 12400: Loss = -9960.483238455794
Iteration 12500: Loss = -9960.620611174907
1
Iteration 12600: Loss = -9960.48308783177
Iteration 12700: Loss = -9960.483117208076
1
Iteration 12800: Loss = -9960.482944221068
Iteration 12900: Loss = -9960.482997513293
1
Iteration 13000: Loss = -9960.482616741296
Iteration 13100: Loss = -9960.487207241096
1
Iteration 13200: Loss = -9960.482219251386
Iteration 13300: Loss = -9960.481930761627
Iteration 13400: Loss = -9960.48152875571
Iteration 13500: Loss = -9960.480788426994
Iteration 13600: Loss = -9960.487424169287
1
Iteration 13700: Loss = -9960.477293116077
Iteration 13800: Loss = -9960.468586905281
Iteration 13900: Loss = -9960.424830716733
Iteration 14000: Loss = -9959.95999857635
Iteration 14100: Loss = -9959.930267081878
Iteration 14200: Loss = -9959.933721980758
1
Iteration 14300: Loss = -9959.928665058143
Iteration 14400: Loss = -9959.983935335673
1
Iteration 14500: Loss = -9959.937281370574
2
Iteration 14600: Loss = -9959.928626300367
Iteration 14700: Loss = -9959.931802317045
1
Iteration 14800: Loss = -9959.93785833018
2
Iteration 14900: Loss = -9959.928698713498
3
Iteration 15000: Loss = -9959.933317910461
4
Iteration 15100: Loss = -9959.941479457393
5
Iteration 15200: Loss = -9959.972834284239
6
Iteration 15300: Loss = -9959.936788583445
7
Iteration 15400: Loss = -9959.940041553262
8
Iteration 15500: Loss = -9959.928086124552
Iteration 15600: Loss = -9959.928560178421
1
Iteration 15700: Loss = -9959.936132771416
2
Iteration 15800: Loss = -9959.955159750485
3
Iteration 15900: Loss = -9959.935221611533
4
Iteration 16000: Loss = -9959.940372080437
5
Iteration 16100: Loss = -9959.92809695195
6
Iteration 16200: Loss = -9959.928489734266
7
Iteration 16300: Loss = -9959.928043468148
Iteration 16400: Loss = -9959.932899603611
1
Iteration 16500: Loss = -9959.928946000377
2
Iteration 16600: Loss = -9959.928074327368
3
Iteration 16700: Loss = -9959.972529499864
4
Iteration 16800: Loss = -9959.960865794692
5
Iteration 16900: Loss = -9959.928566660396
6
Iteration 17000: Loss = -9959.931030666714
7
Iteration 17100: Loss = -9959.931206397478
8
Iteration 17200: Loss = -9959.928214127734
9
Iteration 17300: Loss = -9959.92828363665
10
Stopping early at iteration 17300 due to no improvement.
tensor([[-0.6877, -0.7769],
        [-0.5821, -0.8812],
        [-0.6090, -0.9387],
        [-0.6554, -0.7452],
        [-0.6785, -0.8063],
        [-0.7001, -0.6885],
        [-0.6235, -0.9958],
        [-0.8754, -0.7648],
        [-0.9658, -0.7055],
        [-0.5582, -0.8296],
        [-0.7420, -0.9512],
        [-1.4044, -1.1928],
        [-1.3780, -1.4830],
        [-0.5208, -0.8742],
        [-0.6469, -0.8065],
        [-0.9210, -0.9475],
        [-1.7293, -1.9062],
        [-0.6090, -0.8357],
        [-0.6952, -0.8799],
        [-0.9586, -0.8222],
        [-1.6953, -1.7668],
        [-0.8064, -0.6442],
        [-0.7418, -0.6465],
        [-0.9233, -1.1853],
        [-0.8065, -0.9753],
        [-0.6887, -0.7903],
        [-1.6802, -1.8149],
        [-0.8732, -0.8646],
        [-0.6261, -0.7895],
        [-0.8027, -0.7742],
        [-0.8335, -0.8211],
        [-0.6747, -0.7467],
        [-0.8235, -0.6105],
        [-1.1309, -1.0654],
        [-0.7561, -0.9583],
        [-2.0101, -2.0585],
        [-0.6490, -0.9360],
        [-1.7907, -1.7255],
        [-0.8215, -0.7070],
        [-2.2741, -2.3412],
        [-0.6234, -0.8696],
        [-0.8097, -0.6599],
        [-0.8204, -0.9392],
        [-0.9835, -0.9331],
        [-2.0391, -2.4231],
        [-0.7199, -0.9439],
        [-0.8196, -1.0371],
        [-1.4195, -1.0795],
        [-1.0002, -1.1362],
        [-0.9204, -0.9673],
        [-0.7892, -0.6098],
        [-2.1226, -2.1039],
        [-0.6086, -0.7778],
        [-0.7005, -0.8849],
        [-0.8528, -0.9339],
        [-0.7094, -0.7603],
        [-0.6808, -0.7869],
        [-0.7760, -1.0014],
        [-0.6339, -0.7712],
        [-1.3881, -1.2290],
        [-0.8123, -0.6003],
        [-1.3941, -1.4690],
        [-0.8756, -0.7282],
        [-0.6322, -0.7696],
        [-0.5291, -0.8909],
        [-1.5863, -1.5892],
        [-1.7560, -1.7696],
        [-0.6276, -0.7611],
        [-1.5042, -1.1877],
        [-0.6271, -0.8036],
        [-0.9413, -0.8003],
        [-0.8351, -0.9542],
        [-0.6286, -0.8888],
        [-1.0526, -1.2099],
        [-1.4015, -1.2198],
        [-0.6870, -0.8248],
        [-0.5810, -0.8269],
        [-1.6959, -1.7599],
        [-0.7429, -0.7906],
        [-1.1498, -1.0036],
        [-0.8319, -1.0802],
        [-0.7376, -0.6907],
        [-0.6007, -0.7986],
        [-0.6781, -0.7274],
        [-0.9030, -0.8304],
        [-0.9161, -0.7164],
        [-0.8955, -0.7332],
        [-1.2908, -1.1285],
        [-1.6578, -1.4597],
        [-0.7555, -0.7762],
        [-0.9055, -1.0923],
        [-0.6897, -0.7025],
        [-1.1733, -1.3522],
        [-0.7290, -0.6585],
        [-0.7992, -0.7033],
        [-0.9353, -0.8498],
        [-0.5981, -0.7933],
        [-1.2822, -1.1039],
        [-1.0603, -1.0417],
        [-0.7574, -0.7768]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.5096e-01, 4.9040e-02],
        [9.9998e-01, 1.7580e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5127, 0.4873], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1376, 0.1441],
         [0.8470, 0.1493]],

        [[0.6961, 0.1692],
         [0.3775, 0.8263]],

        [[0.5047, 0.1074],
         [0.5290, 0.8519]],

        [[0.2532, 0.0853],
         [0.9358, 0.7121]],

        [[0.7299, 0.1913],
         [0.8144, 0.2496]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 66
Adjusted Rand Index: 0.09367422501141903
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.005229581978962136
Average Adjusted Rand Index: 0.018734845002283805
10088.887365683191
new:  [0.005229581978962136, 0.023834184508319724, 0.0058403698126631215, 0.005229581978962136] [0.018734845002283805, 0.15594584098838937, 0.02138885022004792, 0.018734845002283805] [9959.929355324146, 9978.4679831475, 9959.933067457061, 9959.92828363665]
prior:  [0.0, 0.0, 0.0, 3.847940178382811e-07] [0.0, 0.0, -0.00015692302765368048, -0.0016520282231363946] [9962.95882351167, 9960.547592400164, 9960.922118489385, 9960.598752682208]
-----------------------------------------------------------------------------------------
This iteration is 4
True Objective function: Loss = -10018.531855885527
Iteration 0: Loss = -14060.618081924204
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.8126,    nan]],

        [[0.5926,    nan],
         [0.9025, 0.0953]],

        [[0.5960,    nan],
         [0.5012, 0.1078]],

        [[0.7669,    nan],
         [0.6179, 0.8091]],

        [[0.7795,    nan],
         [0.7854, 0.5315]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15320.375048954691
Iteration 100: Loss = -9888.55935713062
Iteration 200: Loss = -9886.029255665682
Iteration 300: Loss = -9885.151769443893
Iteration 400: Loss = -9884.725299642221
Iteration 500: Loss = -9884.479892253066
Iteration 600: Loss = -9884.324637187561
Iteration 700: Loss = -9884.219895608814
Iteration 800: Loss = -9884.145999772707
Iteration 900: Loss = -9884.092850264178
Iteration 1000: Loss = -9884.055991709729
Iteration 1100: Loss = -9884.02957666963
Iteration 1200: Loss = -9884.009975547398
Iteration 1300: Loss = -9883.995105043294
Iteration 1400: Loss = -9883.983417342439
Iteration 1500: Loss = -9883.973924930518
Iteration 1600: Loss = -9883.966016685252
Iteration 1700: Loss = -9883.959184713027
Iteration 1800: Loss = -9883.953199933425
Iteration 1900: Loss = -9883.947731101089
Iteration 2000: Loss = -9883.942586579748
Iteration 2100: Loss = -9883.93749247299
Iteration 2200: Loss = -9883.932171117736
Iteration 2300: Loss = -9883.92614708224
Iteration 2400: Loss = -9883.91857920363
Iteration 2500: Loss = -9883.90753028122
Iteration 2600: Loss = -9883.887630333367
Iteration 2700: Loss = -9883.83698896267
Iteration 2800: Loss = -9883.456352565318
Iteration 2900: Loss = -9882.792810920144
Iteration 3000: Loss = -9882.65516938763
Iteration 3100: Loss = -9882.626586773113
Iteration 3200: Loss = -9882.61404553515
Iteration 3300: Loss = -9882.605816006288
Iteration 3400: Loss = -9882.599217924626
Iteration 3500: Loss = -9882.593492360667
Iteration 3600: Loss = -9882.588550706136
Iteration 3700: Loss = -9882.584389365038
Iteration 3800: Loss = -9882.580888400964
Iteration 3900: Loss = -9882.577970643046
Iteration 4000: Loss = -9882.575406424789
Iteration 4100: Loss = -9882.573146499306
Iteration 4200: Loss = -9882.571105645822
Iteration 4300: Loss = -9882.569247981968
Iteration 4400: Loss = -9882.567550981503
Iteration 4500: Loss = -9882.56600130626
Iteration 4600: Loss = -9882.56456268919
Iteration 4700: Loss = -9882.568399360576
1
Iteration 4800: Loss = -9882.572541914442
2
Iteration 4900: Loss = -9882.560852027571
Iteration 5000: Loss = -9882.564056256098
1
Iteration 5100: Loss = -9882.55878706153
Iteration 5200: Loss = -9882.570518131095
1
Iteration 5300: Loss = -9882.556983889632
Iteration 5400: Loss = -9882.556197143096
Iteration 5500: Loss = -9882.557041528366
1
Iteration 5600: Loss = -9882.554707869178
Iteration 5700: Loss = -9882.553991579054
Iteration 5800: Loss = -9882.553885947116
Iteration 5900: Loss = -9882.55278425011
Iteration 6000: Loss = -9882.552199725154
Iteration 6100: Loss = -9882.552927135208
1
Iteration 6200: Loss = -9882.551175582272
Iteration 6300: Loss = -9882.550723213397
Iteration 6400: Loss = -9882.566225740356
1
Iteration 6500: Loss = -9882.549865326211
Iteration 6600: Loss = -9882.549464262653
Iteration 6700: Loss = -9882.549103977735
Iteration 6800: Loss = -9882.549456507819
1
Iteration 6900: Loss = -9882.54839216071
Iteration 7000: Loss = -9882.548099162512
Iteration 7100: Loss = -9882.548121633119
1
Iteration 7200: Loss = -9882.547579914431
Iteration 7300: Loss = -9882.547244058727
Iteration 7400: Loss = -9882.546974827177
Iteration 7500: Loss = -9882.707528076038
1
Iteration 7600: Loss = -9882.54646368743
Iteration 7700: Loss = -9882.546278153231
Iteration 7800: Loss = -9882.546067840387
Iteration 7900: Loss = -9882.546118658965
1
Iteration 8000: Loss = -9882.545671037238
Iteration 8100: Loss = -9882.545523895064
Iteration 8200: Loss = -9882.545470859128
Iteration 8300: Loss = -9882.545245678506
Iteration 8400: Loss = -9882.545026399666
Iteration 8500: Loss = -9882.54488378675
Iteration 8600: Loss = -9882.58483299908
1
Iteration 8700: Loss = -9882.54461942237
Iteration 8800: Loss = -9882.544480175531
Iteration 8900: Loss = -9882.544405572698
Iteration 9000: Loss = -9882.544704602658
1
Iteration 9100: Loss = -9882.544165495305
Iteration 9200: Loss = -9882.54409046223
Iteration 9300: Loss = -9882.544004811301
Iteration 9400: Loss = -9882.5441412014
1
Iteration 9500: Loss = -9882.543837427576
Iteration 9600: Loss = -9882.543749310527
Iteration 9700: Loss = -9882.543681669955
Iteration 9800: Loss = -9882.545617778167
1
Iteration 9900: Loss = -9882.543533411113
Iteration 10000: Loss = -9882.543480136075
Iteration 10100: Loss = -9882.7543232645
1
Iteration 10200: Loss = -9882.543345111977
Iteration 10300: Loss = -9882.543274289777
Iteration 10400: Loss = -9882.543281783363
1
Iteration 10500: Loss = -9882.543257417907
Iteration 10600: Loss = -9882.543140214078
Iteration 10700: Loss = -9882.543077793232
Iteration 10800: Loss = -9882.544371706826
1
Iteration 10900: Loss = -9882.542989170965
Iteration 11000: Loss = -9882.542901447672
Iteration 11100: Loss = -9882.602031794593
1
Iteration 11200: Loss = -9882.542008909051
Iteration 11300: Loss = -9882.489638109753
Iteration 11400: Loss = -9882.489466503142
Iteration 11500: Loss = -9882.483148539473
Iteration 11600: Loss = -9882.47882817605
Iteration 11700: Loss = -9882.478754470827
Iteration 11800: Loss = -9882.473685130306
Iteration 11900: Loss = -9882.318423963745
Iteration 12000: Loss = -9881.459834159858
Iteration 12100: Loss = -9881.466403877805
1
Iteration 12200: Loss = -9881.457713480664
Iteration 12300: Loss = -9881.787015533982
1
Iteration 12400: Loss = -9881.457032931297
Iteration 12500: Loss = -9881.456797283787
Iteration 12600: Loss = -9881.45763337406
1
Iteration 12700: Loss = -9881.456462951512
Iteration 12800: Loss = -9881.456359410813
Iteration 12900: Loss = -9881.456959567997
1
Iteration 13000: Loss = -9881.456175604315
Iteration 13100: Loss = -9881.45614284823
Iteration 13200: Loss = -9881.453944935052
Iteration 13300: Loss = -9881.453710915448
Iteration 13400: Loss = -9881.45450147961
1
Iteration 13500: Loss = -9881.471184776738
2
Iteration 13600: Loss = -9881.453620760369
Iteration 13700: Loss = -9881.668571001639
1
Iteration 13800: Loss = -9881.453513011696
Iteration 13900: Loss = -9881.453470391776
Iteration 14000: Loss = -9881.454222186867
1
Iteration 14100: Loss = -9881.453395142347
Iteration 14200: Loss = -9881.454845258477
1
Iteration 14300: Loss = -9881.453464193255
2
Iteration 14400: Loss = -9881.453369864637
Iteration 14500: Loss = -9881.454328717125
1
Iteration 14600: Loss = -9881.453325985238
Iteration 14700: Loss = -9881.454716198868
1
Iteration 14800: Loss = -9881.4532558063
Iteration 14900: Loss = -9881.463802375747
1
Iteration 15000: Loss = -9881.45327368786
2
Iteration 15100: Loss = -9881.656219539027
3
Iteration 15200: Loss = -9881.453223091123
Iteration 15300: Loss = -9881.455762590278
1
Iteration 15400: Loss = -9881.453206201204
Iteration 15500: Loss = -9881.453175067112
Iteration 15600: Loss = -9881.453238646325
1
Iteration 15700: Loss = -9881.45318801824
2
Iteration 15800: Loss = -9881.453162747392
Iteration 15900: Loss = -9881.460330556814
1
Iteration 16000: Loss = -9881.453159615072
Iteration 16100: Loss = -9881.453144565998
Iteration 16200: Loss = -9881.560864814439
1
Iteration 16300: Loss = -9881.45316900398
2
Iteration 16400: Loss = -9881.453176045963
3
Iteration 16500: Loss = -9881.453196566845
4
Iteration 16600: Loss = -9881.453187841902
5
Iteration 16700: Loss = -9881.453207295974
6
Iteration 16800: Loss = -9881.453674946682
7
Iteration 16900: Loss = -9881.507725514772
8
Iteration 17000: Loss = -9881.45318874619
9
Iteration 17100: Loss = -9881.453395405864
10
Stopping early at iteration 17100 due to no improvement.
tensor([[-1.0179e+01,  5.5635e+00],
        [-1.0018e+01,  5.4028e+00],
        [-1.0411e+01,  5.7954e+00],
        [-1.1075e+01,  6.4602e+00],
        [-9.5595e+00,  4.9443e+00],
        [-1.0447e+01,  5.8315e+00],
        [-1.0811e+01,  6.1958e+00],
        [-1.0364e+01,  5.7488e+00],
        [-3.3649e+00, -1.2503e+00],
        [-1.0372e+01,  5.7569e+00],
        [-1.0061e+01,  5.4461e+00],
        [-9.3860e+00,  4.7708e+00],
        [-1.0095e+01,  5.4800e+00],
        [-1.0581e+01,  5.9657e+00],
        [-9.2298e+00,  4.6146e+00],
        [-1.0349e+01,  5.7336e+00],
        [-9.9481e+00,  5.3329e+00],
        [-7.9706e-01, -3.8182e+00],
        [-1.0925e+01,  6.3093e+00],
        [-1.0433e+01,  5.8182e+00],
        [-9.7052e+00,  5.0900e+00],
        [-6.6885e+00,  2.0733e+00],
        [-8.8120e+00,  4.1968e+00],
        [-9.9263e+00,  5.3111e+00],
        [-3.9678e+00, -6.4737e-01],
        [-1.0094e+01,  5.4793e+00],
        [-9.8981e+00,  5.2829e+00],
        [-1.0481e+01,  5.8661e+00],
        [-4.4668e+00, -1.4837e-01],
        [-1.0669e+01,  6.0538e+00],
        [-1.1125e+01,  6.5096e+00],
        [-1.0826e+01,  6.2108e+00],
        [-6.7324e+00,  2.1171e+00],
        [-1.1108e+01,  6.4932e+00],
        [-5.2809e+00,  6.6567e-01],
        [-1.0752e+01,  6.1367e+00],
        [-9.5753e+00,  4.9601e+00],
        [-1.0490e+01,  5.8748e+00],
        [-9.9461e+00,  5.3309e+00],
        [-1.0593e+01,  5.9780e+00],
        [-8.0385e+00,  3.4233e+00],
        [-9.1982e+00,  4.5830e+00],
        [-9.9920e+00,  5.3767e+00],
        [-1.0433e+01,  5.8182e+00],
        [-9.5943e+00,  4.9791e+00],
        [-1.0105e+01,  5.4898e+00],
        [-1.0507e+01,  5.8922e+00],
        [-1.1009e+01,  6.3940e+00],
        [-1.0570e+01,  5.9543e+00],
        [-9.7660e+00,  5.1507e+00],
        [-1.1498e+01,  6.8825e+00],
        [-1.0898e+01,  6.2830e+00],
        [-1.0636e+01,  6.0206e+00],
        [-4.1710e+00, -4.4424e-01],
        [-9.6458e+00,  5.0306e+00],
        [-9.8439e+00,  5.2287e+00],
        [-1.0720e+01,  6.1045e+00],
        [-1.0647e+01,  6.0320e+00],
        [-1.0388e+01,  5.7732e+00],
        [-8.2156e+00,  3.6004e+00],
        [-1.0506e+01,  5.8907e+00],
        [-9.5567e+00,  4.9415e+00],
        [-9.1630e+00,  4.5478e+00],
        [-1.9585e+00, -2.6567e+00],
        [-1.0337e+01,  5.7217e+00],
        [-1.1030e+01,  6.4151e+00],
        [-1.0278e+01,  5.6627e+00],
        [-1.0583e+01,  5.9681e+00],
        [-5.3419e+00,  7.2667e-01],
        [-1.0896e+01,  6.2808e+00],
        [-5.9659e+00,  1.3507e+00],
        [-9.2096e+00,  4.5943e+00],
        [-1.0385e+01,  5.7700e+00],
        [-1.0106e+01,  5.4910e+00],
        [-6.9800e+00,  2.3647e+00],
        [-1.1078e+01,  6.4627e+00],
        [-9.8417e+00,  5.2265e+00],
        [-5.4604e+00,  8.4520e-01],
        [-9.3937e+00,  4.7784e+00],
        [-1.0373e+01,  5.7579e+00],
        [-9.3082e+00,  4.6929e+00],
        [-1.0501e+01,  5.8860e+00],
        [-1.0753e+01,  6.1377e+00],
        [-9.5854e+00,  4.9702e+00],
        [-1.0957e+01,  6.3416e+00],
        [-9.5830e+00,  4.9677e+00],
        [-9.3598e+00,  4.7446e+00],
        [-1.0234e+01,  5.6184e+00],
        [-1.0652e+01,  6.0368e+00],
        [-9.9845e+00,  5.3692e+00],
        [-9.6301e+00,  5.0149e+00],
        [-8.9651e+00,  4.3499e+00],
        [-1.0730e+01,  6.1145e+00],
        [-9.5653e+00,  4.9501e+00],
        [-9.5373e+00,  4.9220e+00],
        [-1.0908e+01,  6.2923e+00],
        [-4.6211e+00,  5.9011e-03],
        [-9.0845e+00,  4.4693e+00],
        [-1.0385e+01,  5.7702e+00],
        [-1.0692e+01,  6.0772e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9996e-01, 3.8291e-05],
        [6.1678e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0181, 0.9819], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1804, 0.0652],
         [0.8126, 0.1369]],

        [[0.5926, 0.2072],
         [0.9025, 0.0953]],

        [[0.5960, 0.1110],
         [0.5012, 0.1078]],

        [[0.7669, 0.1810],
         [0.6179, 0.8091]],

        [[0.7795, 0.1272],
         [0.7854, 0.5315]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: 0.0020601844398504122
Average Adjusted Rand Index: 0.0019421524804575283
Iteration 0: Loss = -21861.644301704135
Iteration 10: Loss = -9884.10650124361
Iteration 20: Loss = -9884.106501243612
1
Iteration 30: Loss = -9884.106501243617
2
Iteration 40: Loss = -9884.106501243643
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 1.5262e-15],
        [1.0000e+00, 1.3632e-30]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.5170e-15])
beta: tensor([[[0.1356, 0.2396],
         [0.4377, 0.1330]],

        [[0.4803, 0.2068],
         [0.8309, 0.6391]],

        [[0.3989, 0.0544],
         [0.3842, 0.9960]],

        [[0.5707, 0.1549],
         [0.6088, 0.0075]],

        [[0.9063, 0.1727],
         [0.4290, 0.0699]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21863.698980016536
Iteration 100: Loss = -9888.279090215869
Iteration 200: Loss = -9885.934578939028
Iteration 300: Loss = -9885.058377503521
Iteration 400: Loss = -9884.625074425941
Iteration 500: Loss = -9884.38888588777
Iteration 600: Loss = -9884.23737101087
Iteration 700: Loss = -9884.132208939945
Iteration 800: Loss = -9884.053227344257
Iteration 900: Loss = -9883.989687471374
Iteration 1000: Loss = -9883.93713725374
Iteration 1100: Loss = -9883.892541496078
Iteration 1200: Loss = -9883.853252281466
Iteration 1300: Loss = -9883.817731159474
Iteration 1400: Loss = -9883.78471401217
Iteration 1500: Loss = -9883.75300951885
Iteration 1600: Loss = -9883.721766283767
Iteration 1700: Loss = -9883.690122564423
Iteration 1800: Loss = -9883.657830516662
Iteration 1900: Loss = -9883.62481592708
Iteration 2000: Loss = -9883.59114845959
Iteration 2100: Loss = -9883.556914711984
Iteration 2200: Loss = -9883.522078581456
Iteration 2300: Loss = -9883.48607759394
Iteration 2400: Loss = -9883.44840893567
Iteration 2500: Loss = -9883.408639229634
Iteration 2600: Loss = -9883.366684161794
Iteration 2700: Loss = -9883.322518864641
Iteration 2800: Loss = -9883.276398828195
Iteration 2900: Loss = -9883.229147324246
Iteration 3000: Loss = -9883.182045581854
Iteration 3100: Loss = -9883.136152555448
Iteration 3200: Loss = -9883.091914805256
Iteration 3300: Loss = -9883.049408444696
Iteration 3400: Loss = -9883.00829580269
Iteration 3500: Loss = -9882.96796564166
Iteration 3600: Loss = -9882.928154468544
Iteration 3700: Loss = -9882.88910349151
Iteration 3800: Loss = -9882.851320685364
Iteration 3900: Loss = -9882.815579242375
Iteration 4000: Loss = -9882.782836910817
Iteration 4100: Loss = -9882.754311914836
Iteration 4200: Loss = -9882.729190058588
Iteration 4300: Loss = -9882.70546139199
Iteration 4400: Loss = -9882.685257675113
Iteration 4500: Loss = -9882.668641392649
Iteration 4600: Loss = -9882.654416184443
Iteration 4700: Loss = -9882.642004583964
Iteration 4800: Loss = -9882.63104258873
Iteration 4900: Loss = -9882.621015145423
Iteration 5000: Loss = -9882.611919771452
Iteration 5100: Loss = -9882.6029492836
Iteration 5200: Loss = -9882.594507790578
Iteration 5300: Loss = -9882.58756614158
Iteration 5400: Loss = -9882.59395860898
1
Iteration 5500: Loss = -9882.581554202896
Iteration 5600: Loss = -9882.558447255358
Iteration 5700: Loss = -9882.557250484417
Iteration 5800: Loss = -9882.541632008193
Iteration 5900: Loss = -9882.523956536312
Iteration 6000: Loss = -9882.511874892549
Iteration 6100: Loss = -9882.501411807092
Iteration 6200: Loss = -9882.491358081257
Iteration 6300: Loss = -9882.436423291423
Iteration 6400: Loss = -9882.092678282868
Iteration 6500: Loss = -9882.06881664765
Iteration 6600: Loss = -9882.052474274642
Iteration 6700: Loss = -9882.04953906461
Iteration 6800: Loss = -9882.045174783003
Iteration 6900: Loss = -9882.057089036281
1
Iteration 7000: Loss = -9882.042777994864
Iteration 7100: Loss = -9882.042124545638
Iteration 7200: Loss = -9882.04353728497
1
Iteration 7300: Loss = -9882.04138526539
Iteration 7400: Loss = -9882.041222957132
Iteration 7500: Loss = -9882.040928605991
Iteration 7600: Loss = -9882.04424701021
1
Iteration 7700: Loss = -9882.040653704986
Iteration 7800: Loss = -9882.040533483758
Iteration 7900: Loss = -9882.041747768291
1
Iteration 8000: Loss = -9882.040344732375
Iteration 8100: Loss = -9882.050158244587
1
Iteration 8200: Loss = -9882.040209707022
Iteration 8300: Loss = -9882.040148954296
Iteration 8400: Loss = -9882.04016880107
1
Iteration 8500: Loss = -9882.040018363023
Iteration 8600: Loss = -9882.095942321777
1
Iteration 8700: Loss = -9882.039862399908
Iteration 8800: Loss = -9882.040027694857
1
Iteration 8900: Loss = -9882.067997857033
2
Iteration 9000: Loss = -9882.03975776487
Iteration 9100: Loss = -9882.040777824572
1
Iteration 9200: Loss = -9882.04120120057
2
Iteration 9300: Loss = -9882.040121011492
3
Iteration 9400: Loss = -9882.04269368709
4
Iteration 9500: Loss = -9882.06725907246
5
Iteration 9600: Loss = -9882.060733043085
6
Iteration 9700: Loss = -9882.03950609928
Iteration 9800: Loss = -9882.073034096984
1
Iteration 9900: Loss = -9882.04381983154
2
Iteration 10000: Loss = -9882.039933920496
3
Iteration 10100: Loss = -9882.049524323511
4
Iteration 10200: Loss = -9882.040793834123
5
Iteration 10300: Loss = -9882.062877656948
6
Iteration 10400: Loss = -9882.039261774195
Iteration 10500: Loss = -9882.040934621336
1
Iteration 10600: Loss = -9882.039328693696
2
Iteration 10700: Loss = -9882.039139346436
Iteration 10800: Loss = -9882.043866654905
1
Iteration 10900: Loss = -9882.039567735288
2
Iteration 11000: Loss = -9882.039084527849
Iteration 11100: Loss = -9882.117926017776
1
Iteration 11200: Loss = -9882.039015371682
Iteration 11300: Loss = -9882.101421183846
1
Iteration 11400: Loss = -9882.038954214846
Iteration 11500: Loss = -9882.042203086023
1
Iteration 11600: Loss = -9882.03894526515
Iteration 11700: Loss = -9882.039011372468
1
Iteration 11800: Loss = -9882.041791490743
2
Iteration 11900: Loss = -9882.040748541684
3
Iteration 12000: Loss = -9882.0396542756
4
Iteration 12100: Loss = -9882.039433465721
5
Iteration 12200: Loss = -9882.038801270066
Iteration 12300: Loss = -9882.039882074963
1
Iteration 12400: Loss = -9882.038738963052
Iteration 12500: Loss = -9882.044005661357
1
Iteration 12600: Loss = -9882.038844795221
2
Iteration 12700: Loss = -9882.039083775373
3
Iteration 12800: Loss = -9882.039490798554
4
Iteration 12900: Loss = -9882.039717748417
5
Iteration 13000: Loss = -9882.042587913887
6
Iteration 13100: Loss = -9882.038632385511
Iteration 13200: Loss = -9882.051621895222
1
Iteration 13300: Loss = -9882.039024237236
2
Iteration 13400: Loss = -9882.038849798695
3
Iteration 13500: Loss = -9882.039270331297
4
Iteration 13600: Loss = -9882.038596967499
Iteration 13700: Loss = -9882.038733498353
1
Iteration 13800: Loss = -9882.038928399104
2
Iteration 13900: Loss = -9882.04286962846
3
Iteration 14000: Loss = -9882.0386180927
4
Iteration 14100: Loss = -9882.039863991036
5
Iteration 14200: Loss = -9882.03853271836
Iteration 14300: Loss = -9882.038814125708
1
Iteration 14400: Loss = -9882.045362226114
2
Iteration 14500: Loss = -9882.039134793415
3
Iteration 14600: Loss = -9882.03855490515
4
Iteration 14700: Loss = -9882.058670781169
5
Iteration 14800: Loss = -9882.038429945655
Iteration 14900: Loss = -9882.040394379024
1
Iteration 15000: Loss = -9882.05138401531
2
Iteration 15100: Loss = -9882.038518703226
3
Iteration 15200: Loss = -9882.038670049466
4
Iteration 15300: Loss = -9882.038430955417
5
Iteration 15400: Loss = -9882.040453123032
6
Iteration 15500: Loss = -9882.246277951437
7
Iteration 15600: Loss = -9882.038375601194
Iteration 15700: Loss = -9882.085751086364
1
Iteration 15800: Loss = -9882.040092313715
2
Iteration 15900: Loss = -9882.068904198428
3
Iteration 16000: Loss = -9882.038386948027
4
Iteration 16100: Loss = -9882.049412091552
5
Iteration 16200: Loss = -9882.03867716477
6
Iteration 16300: Loss = -9882.0450966667
7
Iteration 16400: Loss = -9882.049401849343
8
Iteration 16500: Loss = -9882.038357540532
Iteration 16600: Loss = -9882.038516777026
1
Iteration 16700: Loss = -9882.038400751575
2
Iteration 16800: Loss = -9882.041268708608
3
Iteration 16900: Loss = -9882.04328092414
4
Iteration 17000: Loss = -9882.03838305238
5
Iteration 17100: Loss = -9882.040216307969
6
Iteration 17200: Loss = -9882.03862509272
7
Iteration 17300: Loss = -9882.038366786433
8
Iteration 17400: Loss = -9882.070758515227
9
Iteration 17500: Loss = -9882.055764585157
10
Stopping early at iteration 17500 due to no improvement.
tensor([[ 3.0046, -4.7083],
        [ 1.8643, -3.5924],
        [ 3.2351, -4.6502],
        [ 0.9079, -2.3323],
        [ 4.4190, -5.8583],
        [ 1.9123, -4.7342],
        [ 1.8446, -3.2477],
        [ 2.8405, -4.4472],
        [ 4.2585, -5.6465],
        [ 2.2449, -5.0480],
        [ 3.0011, -5.0060],
        [ 3.4582, -4.9451],
        [ 3.2524, -4.6558],
        [ 2.8949, -4.4560],
        [ 4.0383, -5.8508],
        [ 1.9261, -5.4234],
        [ 2.9817, -4.7101],
        [ 4.7565, -8.1525],
        [ 1.8180, -3.3911],
        [ 2.1017, -4.7101],
        [ 3.9145, -5.7177],
        [ 3.8801, -5.9212],
        [ 4.4477, -5.9062],
        [ 3.4140, -5.5893],
        [ 3.7648, -5.2553],
        [ 2.0758, -4.2597],
        [ 3.2894, -4.7243],
        [ 2.7921, -4.8193],
        [ 3.0936, -5.4069],
        [ 1.4926, -3.9475],
        [ 0.7530, -2.4900],
        [ 1.2785, -3.4938],
        [ 2.7465, -4.2633],
        [ 1.3280, -2.9479],
        [ 3.7134, -6.4571],
        [ 2.1339, -3.6096],
        [ 3.4458, -4.8434],
        [ 2.0870, -4.6795],
        [ 3.3391, -4.8076],
        [ 2.5923, -3.9903],
        [ 4.6963, -6.0964],
        [ 4.2122, -5.7946],
        [ 3.2117, -5.2688],
        [ 2.6358, -4.0951],
        [ 3.3942, -4.9273],
        [ 1.8976, -3.3074],
        [ 2.0822, -4.6708],
        [ 0.4534, -2.0702],
        [ 2.8829, -4.2784],
        [ 2.9226, -5.6091],
        [-1.1896, -0.1994],
        [ 1.7233, -3.2809],
        [ 1.6195, -4.0623],
        [ 4.9359, -7.8291],
        [ 3.1581, -4.5842],
        [ 2.6263, -4.0629],
        [ 1.8583, -3.8689],
        [ 1.6559, -3.8520],
        [ 2.5163, -5.5787],
        [ 4.9967, -6.4008],
        [ 2.9705, -4.6039],
        [ 4.0708, -6.1142],
        [ 3.8342, -5.6276],
        [ 4.7562, -7.9497],
        [ 2.4113, -4.9030],
        [ 1.6665, -3.0531],
        [ 3.0087, -4.4433],
        [ 2.4876, -4.8458],
        [ 3.2190, -4.6724],
        [ 1.4010, -2.7908],
        [ 3.7041, -5.1050],
        [ 4.0503, -5.8266],
        [ 2.7836, -4.3059],
        [ 3.0621, -4.8819],
        [ 5.1434, -6.5536],
        [ 0.4360, -1.8290],
        [ 2.4636, -4.2922],
        [ 3.4128, -4.8330],
        [ 3.2410, -4.6450],
        [ 1.9988, -4.1097],
        [ 2.3388, -5.2727],
        [ 3.0209, -4.4834],
        [ 1.8820, -4.1210],
        [ 3.6324, -6.3977],
        [-0.0105, -4.1538],
        [ 3.6982, -5.1490],
        [ 3.0572, -5.7216],
        [ 3.0514, -4.6162],
        [ 2.4931, -3.9487],
        [ 2.9724, -4.3838],
        [ 3.4737, -5.1384],
        [ 4.4973, -5.9478],
        [ 2.1192, -3.6081],
        [ 3.9182, -5.4603],
        [ 1.8358, -6.4510],
        [ 0.6927, -3.9274],
        [ 3.9913, -5.3800],
        [ 3.2990, -5.7218],
        [ 3.2146, -4.9964],
        [ 1.7264, -3.4740]], dtype=torch.float64, requires_grad=True)
pi: tensor([[4.1967e-01, 5.8033e-01],
        [9.9971e-01, 2.8718e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9888, 0.0112], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1301, 0.2299],
         [0.4377, 0.1520]],

        [[0.4803, 0.1412],
         [0.8309, 0.6391]],

        [[0.3989, 0.1376],
         [0.3842, 0.9960]],

        [[0.5707, 0.1393],
         [0.6088, 0.0075]],

        [[0.9063, 0.1406],
         [0.4290, 0.0699]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.006002224857279073
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0009785334230322305
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0015793426848825644
Global Adjusted Rand Index: 0.00953315716518147
Average Adjusted Rand Index: 0.0020317643976750645
Iteration 0: Loss = -19572.375457012567
Iteration 10: Loss = -9884.106479104546
Iteration 20: Loss = -9884.106413813326
Iteration 30: Loss = -9884.106401159592
Iteration 40: Loss = -9884.105322389812
Iteration 50: Loss = -9884.079085958661
Iteration 60: Loss = -9883.814648739868
Iteration 70: Loss = -9883.500596589638
Iteration 80: Loss = -9883.43176060208
Iteration 90: Loss = -9883.418409625327
Iteration 100: Loss = -9883.415003264749
Iteration 110: Loss = -9883.413921973834
Iteration 120: Loss = -9883.413536092703
Iteration 130: Loss = -9883.41335748167
Iteration 140: Loss = -9883.413358404148
1
Iteration 150: Loss = -9883.413325714715
Iteration 160: Loss = -9883.4133242777
Iteration 170: Loss = -9883.413321652008
Iteration 180: Loss = -9883.41334559947
1
Iteration 190: Loss = -9883.413317394552
Iteration 200: Loss = -9883.413327600076
1
Iteration 210: Loss = -9883.413325063071
2
Iteration 220: Loss = -9883.413306506403
Iteration 230: Loss = -9883.41331640401
1
Iteration 240: Loss = -9883.413312020906
2
Iteration 250: Loss = -9883.413316701324
3
Stopping early at iteration 249 due to no improvement.
pi: tensor([[0.9892, 0.0108],
        [0.9701, 0.0299]], dtype=torch.float64)
alpha: tensor([0.9889, 0.0111])
beta: tensor([[[0.1356, 0.0749],
         [0.8421, 0.1496]],

        [[0.3997, 0.1979],
         [0.5553, 0.7187]],

        [[0.8782, 0.0527],
         [0.4338, 0.6691]],

        [[0.4807, 0.1492],
         [0.1225, 0.1807]],

        [[0.9353, 0.1622],
         [0.4647, 0.6637]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006739575247983711
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19572.744573155156
Iteration 100: Loss = -9896.373570780508
Iteration 200: Loss = -9885.686886210498
Iteration 300: Loss = -9884.636141473957
Iteration 400: Loss = -9884.21104000209
Iteration 500: Loss = -9883.994550601357
Iteration 600: Loss = -9883.842928482372
Iteration 700: Loss = -9883.7282971244
Iteration 800: Loss = -9883.647425682413
Iteration 900: Loss = -9883.590576295994
Iteration 1000: Loss = -9883.545180461366
Iteration 1100: Loss = -9883.50589323137
Iteration 1200: Loss = -9883.46873024428
Iteration 1300: Loss = -9883.433951336465
Iteration 1400: Loss = -9883.399158103837
Iteration 1500: Loss = -9883.36716618478
Iteration 1600: Loss = -9883.334584559832
Iteration 1700: Loss = -9883.30089309915
Iteration 1800: Loss = -9883.262753295177
Iteration 1900: Loss = -9883.213278858148
Iteration 2000: Loss = -9883.17727085984
Iteration 2100: Loss = -9883.132695843615
Iteration 2200: Loss = -9883.07636363756
Iteration 2300: Loss = -9883.03774746966
Iteration 2400: Loss = -9882.99726615171
Iteration 2500: Loss = -9882.955560265405
Iteration 2600: Loss = -9882.918277518758
Iteration 2700: Loss = -9882.884673989955
Iteration 2800: Loss = -9882.847958974047
Iteration 2900: Loss = -9882.776506265913
Iteration 3000: Loss = -9882.741010729622
Iteration 3100: Loss = -9882.711080810102
Iteration 3200: Loss = -9882.68621604696
Iteration 3300: Loss = -9882.665555138261
Iteration 3400: Loss = -9882.648205072666
Iteration 3500: Loss = -9882.63362130407
Iteration 3600: Loss = -9882.620850323565
Iteration 3700: Loss = -9882.609697846261
Iteration 3800: Loss = -9882.59935212238
Iteration 3900: Loss = -9882.589477710613
Iteration 4000: Loss = -9882.579725326474
Iteration 4100: Loss = -9882.57049768877
Iteration 4200: Loss = -9882.56193998844
Iteration 4300: Loss = -9882.548186927037
Iteration 4400: Loss = -9882.536620685698
Iteration 4500: Loss = -9882.524377769962
Iteration 4600: Loss = -9882.513673535132
Iteration 4700: Loss = -9882.503682718496
Iteration 4800: Loss = -9882.496829390702
Iteration 4900: Loss = -9882.492475639674
Iteration 5000: Loss = -9882.489584534263
Iteration 5100: Loss = -9882.48801571876
Iteration 5200: Loss = -9882.48693762755
Iteration 5300: Loss = -9882.486061401034
Iteration 5400: Loss = -9882.48372280739
Iteration 5500: Loss = -9882.050664054306
Iteration 5600: Loss = -9882.044150549162
Iteration 5700: Loss = -9882.041976652792
Iteration 5800: Loss = -9882.041224073935
Iteration 5900: Loss = -9882.041057749935
Iteration 6000: Loss = -9882.040637263972
Iteration 6100: Loss = -9882.040460836733
Iteration 6200: Loss = -9882.040486095555
1
Iteration 6300: Loss = -9882.04020154899
Iteration 6400: Loss = -9882.041588170196
1
Iteration 6500: Loss = -9882.040097932631
Iteration 6600: Loss = -9882.040059298832
Iteration 6700: Loss = -9882.04066093562
1
Iteration 6800: Loss = -9882.039895422085
Iteration 6900: Loss = -9882.039891293964
Iteration 7000: Loss = -9882.039904278254
1
Iteration 7100: Loss = -9882.03985083775
Iteration 7200: Loss = -9882.082068001167
1
Iteration 7300: Loss = -9882.039726130923
Iteration 7400: Loss = -9882.04031856307
1
Iteration 7500: Loss = -9882.039653683125
Iteration 7600: Loss = -9882.039892015384
1
Iteration 7700: Loss = -9882.039553745428
Iteration 7800: Loss = -9882.039800807925
1
Iteration 7900: Loss = -9882.040122967801
2
Iteration 8000: Loss = -9882.048371601975
3
Iteration 8100: Loss = -9882.039451048982
Iteration 8200: Loss = -9882.03952195096
1
Iteration 8300: Loss = -9882.041105510249
2
Iteration 8400: Loss = -9882.093613620742
3
Iteration 8500: Loss = -9882.06186206653
4
Iteration 8600: Loss = -9882.039301006715
Iteration 8700: Loss = -9882.042448462786
1
Iteration 8800: Loss = -9882.124736269367
2
Iteration 8900: Loss = -9882.039194184055
Iteration 9000: Loss = -9882.039314852123
1
Iteration 9100: Loss = -9882.039233274994
2
Iteration 9200: Loss = -9882.042970142387
3
Iteration 9300: Loss = -9882.042973792511
4
Iteration 9400: Loss = -9882.041495845979
5
Iteration 9500: Loss = -9882.042614341914
6
Iteration 9600: Loss = -9882.042063491186
7
Iteration 9700: Loss = -9882.046112395437
8
Iteration 9800: Loss = -9882.140046171739
9
Iteration 9900: Loss = -9882.040248771485
10
Stopping early at iteration 9900 due to no improvement.
tensor([[ 3.1625, -4.5536],
        [ 2.0157, -3.4474],
        [ 2.4348, -5.4597],
        [ 0.9310, -2.3173],
        [ 4.2458, -6.0339],
        [ 2.0042, -4.6442],
        [ 1.0755, -4.0244],
        [ 2.7864, -4.5044],
        [ 4.1811, -5.7201],
        [ 2.9334, -4.3656],
        [ 3.2396, -4.7688],
        [ 3.0248, -5.3747],
        [ 3.2280, -4.6820],
        [ 2.0948, -5.2568],
        [ 4.1321, -5.7570],
        [ 2.9087, -4.4483],
        [ 2.0598, -5.6337],
        [ 5.3330, -6.8868],
        [ 1.8250, -3.3924],
        [ 2.2975, -4.5202],
        [ 4.1085, -5.5252],
        [ 4.2060, -5.5925],
        [ 3.3748, -6.9780],
        [ 3.5721, -5.4370],
        [ 3.8141, -5.2024],
        [ 2.0902, -4.2500],
        [ 2.0398, -5.9725],
        [ 3.1141, -4.5008],
        [ 2.9153, -5.5820],
        [ 1.9779, -3.4666],
        [-0.1144, -3.3647],
        [ 1.3363, -3.4432],
        [ 2.6972, -4.3104],
        [ 0.7662, -3.5179],
        [ 4.3796, -5.7817],
        [ 2.1370, -3.6132],
        [ 3.3732, -4.9147],
        [ 1.9851, -4.7871],
        [ 3.3057, -4.8403],
        [ 2.5963, -3.9897],
        [ 4.6746, -6.1036],
        [ 4.2805, -5.7247],
        [ 3.3252, -5.1578],
        [ 2.6068, -4.1275],
        [ 2.0440, -6.2783],
        [ 1.5277, -3.6812],
        [ 2.2807, -4.4756],
        [ 0.3498, -2.1806],
        [ 2.8615, -4.3035],
        [ 3.1855, -5.3467],
        [-1.1869, -0.2014],
        [ 1.6721, -3.3371],
        [ 2.1351, -3.5494],
        [ 4.7217, -8.0213],
        [ 3.1768, -4.5641],
        [ 1.9727, -4.7212],
        [ 1.2074, -4.5268],
        [ 1.9403, -3.5735],
        [ 3.1898, -4.9125],
        [ 4.6460, -6.7163],
        [ 1.4828, -6.0980],
        [ 3.9522, -6.2332],
        [ 4.0225, -5.4378],
        [ 4.7956, -7.6390],
        [ 2.9279, -4.3919],
        [ 1.6687, -3.0582],
        [ 2.8915, -4.5609],
        [ 2.2815, -5.0569],
        [ 3.0861, -4.8036],
        [ 1.2344, -2.9617],
        [ 3.7086, -5.0993],
        [ 4.0728, -5.7998],
        [ 2.8473, -4.2449],
        [ 3.2675, -4.6772],
        [ 5.0970, -6.5332],
        [-0.8933, -3.1649],
        [ 2.5741, -4.1862],
        [ 3.2818, -4.9637],
        [ 2.4458, -5.4418],
        [ 2.2541, -3.8556],
        [ 1.9505, -5.6637],
        [ 3.0548, -4.4520],
        [ 2.1570, -3.8528],
        [ 4.3234, -5.7103],
        [ 1.2955, -2.8537],
        [ 3.3426, -5.5020],
        [ 3.0125, -5.7690],
        [ 2.9196, -4.7464],
        [ 1.5199, -4.9282],
        [ 2.7982, -4.5581],
        [ 3.4466, -5.1651],
        [ 4.5126, -5.9158],
        [ 2.1018, -3.6324],
        [ 3.8476, -5.5333],
        [ 3.0738, -5.2170],
        [ 1.5501, -3.0781],
        [ 3.8218, -5.5464],
        [ 3.7955, -5.2217],
        [ 3.3351, -4.8776],
        [ 1.7715, -3.4355]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4151, 0.5849],
        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9887, 0.0113], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1296, 0.2290],
         [0.8421, 0.1524]],

        [[0.3997, 0.1408],
         [0.5553, 0.7187]],

        [[0.8782, 0.1380],
         [0.4338, 0.6691]],

        [[0.4807, 0.1393],
         [0.1225, 0.1807]],

        [[0.9353, 0.1410],
         [0.4647, 0.6637]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.011656352658433383
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.005679229558745591
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0015793426848825644
Global Adjusted Rand Index: 0.0087572748668885
Average Adjusted Rand Index: 0.0018310373615503628
Iteration 0: Loss = -22785.741008036854
Iteration 10: Loss = -9884.106501285401
Iteration 20: Loss = -9884.1065013696
1
Iteration 30: Loss = -9884.106501631755
2
Iteration 40: Loss = -9884.106502808181
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 8.5693e-11],
        [1.0000e+00, 5.7971e-11]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 8.5435e-11])
beta: tensor([[[0.1356, 0.0674],
         [0.9243, 0.1141]],

        [[0.4425, 0.2068],
         [0.4121, 0.7174]],

        [[0.3135, 0.0477],
         [0.0795, 0.1631]],

        [[0.0515, 0.1549],
         [0.1286, 0.8546]],

        [[0.7216, 0.1727],
         [0.6752, 0.2489]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22785.32073232244
Iteration 100: Loss = -9890.613020785424
Iteration 200: Loss = -9885.328088135748
Iteration 300: Loss = -9884.69800687828
Iteration 400: Loss = -9884.403923149408
Iteration 500: Loss = -9884.229810176292
Iteration 600: Loss = -9884.111122735747
Iteration 700: Loss = -9884.020504743838
Iteration 800: Loss = -9883.946035330995
Iteration 900: Loss = -9883.884131234316
Iteration 1000: Loss = -9883.835382434687
Iteration 1100: Loss = -9883.798011678671
Iteration 1200: Loss = -9883.768134032774
Iteration 1300: Loss = -9883.742540829042
Iteration 1400: Loss = -9883.71928810782
Iteration 1500: Loss = -9883.697488716578
Iteration 1600: Loss = -9883.676608975195
Iteration 1700: Loss = -9883.654465768332
Iteration 1800: Loss = -9883.62796481964
Iteration 1900: Loss = -9883.592780930827
Iteration 2000: Loss = -9883.543639175132
Iteration 2100: Loss = -9883.481999544227
Iteration 2200: Loss = -9883.420975570576
Iteration 2300: Loss = -9883.36460482344
Iteration 2400: Loss = -9883.309581318561
Iteration 2500: Loss = -9883.254802901367
Iteration 2600: Loss = -9883.20018346948
Iteration 2700: Loss = -9883.146224332528
Iteration 2800: Loss = -9883.094424840016
Iteration 2900: Loss = -9883.046618735903
Iteration 3000: Loss = -9883.002982768165
Iteration 3100: Loss = -9882.963110259396
Iteration 3200: Loss = -9882.926215680975
Iteration 3300: Loss = -9882.890937573888
Iteration 3400: Loss = -9882.857476366691
Iteration 3500: Loss = -9882.825243872738
Iteration 3600: Loss = -9882.794423342313
Iteration 3700: Loss = -9882.767767658626
Iteration 3800: Loss = -9882.744268125707
Iteration 3900: Loss = -9882.72292773993
Iteration 4000: Loss = -9882.703001134729
Iteration 4100: Loss = -9882.684285406489
Iteration 4200: Loss = -9882.666421104337
Iteration 4300: Loss = -9882.649441521557
Iteration 4400: Loss = -9882.633061775778
Iteration 4500: Loss = -9882.617095877747
Iteration 4600: Loss = -9882.600911444008
Iteration 4700: Loss = -9882.58406369385
Iteration 4800: Loss = -9882.56837369986
Iteration 4900: Loss = -9882.547203446613
Iteration 5000: Loss = -9882.529257209266
Iteration 5100: Loss = -9882.511211581323
Iteration 5200: Loss = -9882.510998946707
Iteration 5300: Loss = -9882.495593229121
Iteration 5400: Loss = -9882.493011838085
Iteration 5500: Loss = -9882.491762055379
Iteration 5600: Loss = -9882.491111517611
Iteration 5700: Loss = -9882.490776612787
Iteration 5800: Loss = -9882.490451350923
Iteration 5900: Loss = -9882.494078564023
1
Iteration 6000: Loss = -9882.489971528292
Iteration 6100: Loss = -9882.489690194834
Iteration 6200: Loss = -9882.48968716391
Iteration 6300: Loss = -9882.489309875567
Iteration 6400: Loss = -9882.54786014237
1
Iteration 6500: Loss = -9882.488918619652
Iteration 6600: Loss = -9882.488678095786
Iteration 6700: Loss = -9882.488517340476
Iteration 6800: Loss = -9882.488273418747
Iteration 6900: Loss = -9882.48820605203
Iteration 7000: Loss = -9882.48788245336
Iteration 7100: Loss = -9882.487634085614
Iteration 7200: Loss = -9882.487498799688
Iteration 7300: Loss = -9882.48719217576
Iteration 7400: Loss = -9882.487922384189
1
Iteration 7500: Loss = -9882.486130550904
Iteration 7600: Loss = -9882.158256390907
Iteration 7700: Loss = -9882.041815627661
Iteration 7800: Loss = -9882.040883139895
Iteration 7900: Loss = -9882.041705869427
1
Iteration 8000: Loss = -9882.040407661207
Iteration 8100: Loss = -9882.058527467982
1
Iteration 8200: Loss = -9882.041016420848
2
Iteration 8300: Loss = -9882.040068287113
Iteration 8400: Loss = -9882.040087637197
1
Iteration 8500: Loss = -9882.040944145423
2
Iteration 8600: Loss = -9882.039840273415
Iteration 8700: Loss = -9882.039763912986
Iteration 8800: Loss = -9882.046657735607
1
Iteration 8900: Loss = -9882.062435493748
2
Iteration 9000: Loss = -9882.039655570565
Iteration 9100: Loss = -9882.039533380044
Iteration 9200: Loss = -9882.092207991716
1
Iteration 9300: Loss = -9882.110401056314
2
Iteration 9400: Loss = -9882.040246977911
3
Iteration 9500: Loss = -9882.04000841168
4
Iteration 9600: Loss = -9882.165795566514
5
Iteration 9700: Loss = -9882.039290808247
Iteration 9800: Loss = -9882.039570350973
1
Iteration 9900: Loss = -9882.159347523686
2
Iteration 10000: Loss = -9882.039714217317
3
Iteration 10100: Loss = -9882.03991579075
4
Iteration 10200: Loss = -9882.228210450357
5
Iteration 10300: Loss = -9882.043780240178
6
Iteration 10400: Loss = -9882.054357028555
7
Iteration 10500: Loss = -9882.039429087486
8
Iteration 10600: Loss = -9882.044430815089
9
Iteration 10700: Loss = -9882.04476355738
10
Stopping early at iteration 10700 due to no improvement.
tensor([[ 3.1607, -4.5588],
        [ 1.5444, -3.9199],
        [ 2.9674, -4.9283],
        [ 0.7766, -2.4715],
        [ 3.2420, -7.0345],
        [ 2.3846, -4.2676],
        [ 1.0593, -4.0372],
        [ 2.8544, -4.4372],
        [ 4.1112, -5.7895],
        [ 2.8488, -4.4519],
        [ 3.0098, -5.0014],
        [ 3.4154, -4.9850],
        [ 2.5854, -5.3272],
        [ 2.9417, -4.4114],
        [ 4.1832, -5.6990],
        [ 2.3613, -4.9982],
        [ 2.4538, -5.2377],
        [ 5.3823, -6.8402],
        [ 1.7786, -3.4387],
        [ 2.0253, -4.7953],
        [ 3.6844, -5.9474],
        [ 4.1816, -5.6220],
        [ 3.5574, -6.7938],
        [ 3.7164, -5.2871],
        [ 3.6980, -5.3194],
        [ 2.4438, -3.8991],
        [ 3.3107, -4.7031],
        [ 2.7387, -4.8770],
        [ 3.5459, -4.9541],
        [ 1.4560, -3.9915],
        [ 0.7118, -2.5459],
        [ 0.9748, -3.8053],
        [ 2.7652, -4.2471],
        [ 1.3427, -2.9479],
        [ 4.3742, -5.7976],
        [ 1.8853, -3.8640],
        [ 3.2308, -5.0554],
        [ 2.6850, -4.0887],
        [ 3.3763, -4.7714],
        [ 2.5968, -3.9906],
        [ 4.5967, -6.1820],
        [ 4.3080, -5.6962],
        [ 3.5159, -4.9674],
        [ 2.4110, -4.3238],
        [ 2.7064, -5.6159],
        [ 1.8918, -3.3176],
        [ 2.6645, -4.0942],
        [-0.0172, -2.5538],
        [ 2.8849, -4.2828],
        [ 3.3817, -5.1541],
        [-2.2549, -1.2730],
        [ 0.5486, -4.4675],
        [ 1.9992, -3.6840],
        [ 4.9939, -7.6308],
        [ 3.1772, -4.5664],
        [ 2.6446, -4.0517],
        [ 2.1180, -3.6149],
        [ 2.0626, -3.4508],
        [ 3.3574, -4.7477],
        [ 4.0364, -7.3648],
        [ 2.7086, -4.8723],
        [ 4.3906, -5.7990],
        [ 2.7173, -6.7516],
        [ 5.3446, -6.9953],
        [ 2.6897, -4.6326],
        [ 1.5642, -3.1639],
        [ 2.6478, -4.8068],
        [ 2.9756, -4.3625],
        [ 3.2428, -4.6490],
        [-0.2055, -4.4097],
        [ 3.0251, -5.7841],
        [ 3.7084, -6.1745],
        [ 2.2589, -4.8358],
        [ 2.5404, -5.4057],
        [ 4.9870, -6.6827],
        [ 0.1080, -2.1673],
        [ 2.6867, -4.0742],
        [ 3.2233, -5.0210],
        [ 3.2123, -4.6743],
        [ 1.4115, -4.7027],
        [ 2.9826, -4.6317],
        [ 2.2420, -5.2659],
        [ 1.5043, -4.5054],
        [ 4.2361, -5.7943],
        [ 1.3392, -2.8122],
        [ 3.6862, -5.1592],
        [ 3.6961, -5.0825],
        [ 3.1412, -4.5277],
        [ 1.9962, -4.4495],
        [ 2.9209, -4.4374],
        [ 3.6113, -5.0024],
        [ 4.2592, -6.1865],
        [ 2.1475, -3.5879],
        [ 3.4264, -5.9666],
        [ 3.4001, -4.8929],
        [ 1.6112, -3.0191],
        [ 3.9099, -5.4625],
        [ 3.7471, -5.2732],
        [ 3.3066, -4.9059],
        [ 1.8739, -3.3338]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4164, 0.5836],
        [0.9959, 0.0041]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9887, 0.0113], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1294, 0.2287],
         [0.9243, 0.1526]],

        [[0.4425, 0.1407],
         [0.4121, 0.7174]],

        [[0.3135, 0.1382],
         [0.0795, 0.1631]],

        [[0.0515, 0.1394],
         [0.1286, 0.8546]],

        [[0.7216, 0.1412],
         [0.6752, 0.2489]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.011656352658433383
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0027563198896884866
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.005431979218977636
Global Adjusted Rand Index: 0.010565222052534589
Average Adjusted Rand Index: 0.003186146602180798
Iteration 0: Loss = -23538.837245855626
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.0791,    nan]],

        [[0.2607,    nan],
         [0.8343, 0.5442]],

        [[0.0374,    nan],
         [0.0256, 0.7959]],

        [[0.3720,    nan],
         [0.8609, 0.1028]],

        [[0.3957,    nan],
         [0.0861, 0.1547]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23539.494648958967
Iteration 100: Loss = -9889.569442322632
Iteration 200: Loss = -9887.17543032077
Iteration 300: Loss = -9886.114097888523
Iteration 400: Loss = -9885.490056191069
Iteration 500: Loss = -9885.096530778374
Iteration 600: Loss = -9884.831557942744
Iteration 700: Loss = -9884.6430873048
Iteration 800: Loss = -9884.502964827916
Iteration 900: Loss = -9884.394937526355
Iteration 1000: Loss = -9884.309183022644
Iteration 1100: Loss = -9884.239316454476
Iteration 1200: Loss = -9884.181175670024
Iteration 1300: Loss = -9884.1318885534
Iteration 1400: Loss = -9884.089549222903
Iteration 1500: Loss = -9884.052834588352
Iteration 1600: Loss = -9884.020877519302
Iteration 1700: Loss = -9883.992904251874
Iteration 1800: Loss = -9883.968427236405
Iteration 1900: Loss = -9883.946893489814
Iteration 2000: Loss = -9883.927803228207
Iteration 2100: Loss = -9883.910754224798
Iteration 2200: Loss = -9883.895324618195
Iteration 2300: Loss = -9883.881238594236
Iteration 2400: Loss = -9883.86824472838
Iteration 2500: Loss = -9883.855958824219
Iteration 2600: Loss = -9883.844209153165
Iteration 2700: Loss = -9883.832733261455
Iteration 2800: Loss = -9883.82128375681
Iteration 2900: Loss = -9883.80960781514
Iteration 3000: Loss = -9883.79733127747
Iteration 3100: Loss = -9883.78422405864
Iteration 3200: Loss = -9883.770104895388
Iteration 3300: Loss = -9883.755641124068
Iteration 3400: Loss = -9883.740752874091
Iteration 3500: Loss = -9883.7249190221
Iteration 3600: Loss = -9883.707839481012
Iteration 3700: Loss = -9883.68905191593
Iteration 3800: Loss = -9883.667956366344
Iteration 3900: Loss = -9883.643837952346
Iteration 4000: Loss = -9883.615467148158
Iteration 4100: Loss = -9883.581450073107
Iteration 4200: Loss = -9883.539230360102
Iteration 4300: Loss = -9883.484533529163
Iteration 4400: Loss = -9883.412823179528
Iteration 4500: Loss = -9883.337935634558
Iteration 4600: Loss = -9883.273953859543
Iteration 4700: Loss = -9883.218280151465
Iteration 4800: Loss = -9883.16984583364
Iteration 4900: Loss = -9883.126943499281
Iteration 5000: Loss = -9883.087090151774
Iteration 5100: Loss = -9883.04829040298
Iteration 5200: Loss = -9883.009665705256
Iteration 5300: Loss = -9882.971152100738
Iteration 5400: Loss = -9882.947846048504
Iteration 5500: Loss = -9882.892575169995
Iteration 5600: Loss = -9882.846360838794
Iteration 5700: Loss = -9882.807179092686
Iteration 5800: Loss = -9882.776132366487
Iteration 5900: Loss = -9882.751203776235
Iteration 6000: Loss = -9882.733827941287
Iteration 6100: Loss = -9882.719336546965
Iteration 6200: Loss = -9882.689913111586
Iteration 6300: Loss = -9882.666707609736
Iteration 6400: Loss = -9882.687898839296
1
Iteration 6500: Loss = -9882.648772726501
Iteration 6600: Loss = -9882.621921004658
Iteration 6700: Loss = -9882.61827112923
Iteration 6800: Loss = -9882.59771117434
Iteration 6900: Loss = -9882.581075291131
Iteration 7000: Loss = -9882.570445793985
Iteration 7100: Loss = -9882.550430179375
Iteration 7200: Loss = -9882.534000869577
Iteration 7300: Loss = -9882.519090796784
Iteration 7400: Loss = -9882.508322823065
Iteration 7500: Loss = -9882.499377591212
Iteration 7600: Loss = -9882.510201694344
1
Iteration 7700: Loss = -9882.499035238685
Iteration 7800: Loss = -9882.494862192836
Iteration 7900: Loss = -9882.490416184686
Iteration 8000: Loss = -9882.490032278083
Iteration 8100: Loss = -9882.489912228297
Iteration 8200: Loss = -9882.491190592013
1
Iteration 8300: Loss = -9882.49036782264
2
Iteration 8400: Loss = -9882.521043665607
3
Iteration 8500: Loss = -9882.48925579436
Iteration 8600: Loss = -9882.50068592431
1
Iteration 8700: Loss = -9882.491183268365
2
Iteration 8800: Loss = -9882.489701578823
3
Iteration 8900: Loss = -9882.48842778598
Iteration 9000: Loss = -9882.488336735722
Iteration 9100: Loss = -9882.544263713058
1
Iteration 9200: Loss = -9882.488003731112
Iteration 9300: Loss = -9882.552153835428
1
Iteration 9400: Loss = -9882.488080533156
2
Iteration 9500: Loss = -9882.494972509781
3
Iteration 9600: Loss = -9882.498541163612
4
Iteration 9700: Loss = -9882.48794675501
Iteration 9800: Loss = -9882.487518962993
Iteration 9900: Loss = -9882.487906362583
1
Iteration 10000: Loss = -9882.48723224709
Iteration 10100: Loss = -9882.487508834982
1
Iteration 10200: Loss = -9882.487159091335
Iteration 10300: Loss = -9882.490158402969
1
Iteration 10400: Loss = -9882.48694726088
Iteration 10500: Loss = -9882.49307221014
1
Iteration 10600: Loss = -9882.487007632886
2
Iteration 10700: Loss = -9882.491793727346
3
Iteration 10800: Loss = -9882.488006357196
4
Iteration 10900: Loss = -9882.48746980403
5
Iteration 11000: Loss = -9882.486901110256
Iteration 11100: Loss = -9882.521779894783
1
Iteration 11200: Loss = -9882.548248589828
2
Iteration 11300: Loss = -9882.501632833795
3
Iteration 11400: Loss = -9882.486265300155
Iteration 11500: Loss = -9882.31042387172
Iteration 11600: Loss = -9882.116985676314
Iteration 11700: Loss = -9882.043812852322
Iteration 11800: Loss = -9882.039587511259
Iteration 11900: Loss = -9882.039484879815
Iteration 12000: Loss = -9882.117301138069
1
Iteration 12100: Loss = -9882.039051140406
Iteration 12200: Loss = -9882.040202582055
1
Iteration 12300: Loss = -9882.039456757453
2
Iteration 12400: Loss = -9882.043538828286
3
Iteration 12500: Loss = -9882.109065942672
4
Iteration 12600: Loss = -9882.039022125644
Iteration 12700: Loss = -9882.03893705583
Iteration 12800: Loss = -9882.03899364711
1
Iteration 12900: Loss = -9882.039770884121
2
Iteration 13000: Loss = -9882.038806816641
Iteration 13100: Loss = -9882.040955001414
1
Iteration 13200: Loss = -9882.050538478183
2
Iteration 13300: Loss = -9882.038931479043
3
Iteration 13400: Loss = -9882.080845811368
4
Iteration 13500: Loss = -9882.040806388466
5
Iteration 13600: Loss = -9882.043815981291
6
Iteration 13700: Loss = -9882.038656894518
Iteration 13800: Loss = -9882.068478214085
1
Iteration 13900: Loss = -9882.089268662512
2
Iteration 14000: Loss = -9882.039895499112
3
Iteration 14100: Loss = -9882.049218981158
4
Iteration 14200: Loss = -9882.038979710336
5
Iteration 14300: Loss = -9882.052191796705
6
Iteration 14400: Loss = -9882.039808179012
7
Iteration 14500: Loss = -9882.04315382284
8
Iteration 14600: Loss = -9882.120539587422
9
Iteration 14700: Loss = -9882.039851597316
10
Stopping early at iteration 14700 due to no improvement.
tensor([[ 3.1075, -4.6103],
        [ 1.9906, -3.4733],
        [ 3.0341, -4.8602],
        [ 0.6736, -2.5756],
        [ 4.3507, -5.9270],
        [ 1.0183, -5.6335],
        [ 1.8292, -3.2698],
        [ 2.6584, -4.6324],
        [ 4.2346, -5.6689],
        [ 2.5456, -4.7557],
        [ 3.2775, -4.7313],
        [ 3.3342, -5.0677],
        [ 3.0514, -4.8588],
        [ 2.9557, -4.3964],
        [ 2.8948, -6.9939],
        [ 2.8951, -4.4619],
        [ 2.5082, -5.1836],
        [ 5.3525, -7.3259],
        [ 1.7757, -3.4416],
        [ 2.5993, -4.2187],
        [ 4.0055, -5.6269],
        [ 3.9533, -5.8452],
        [ 3.9492, -6.4006],
        [ 3.7683, -5.2367],
        [ 3.6588, -5.3603],
        [ 1.2324, -5.1103],
        [ 3.2533, -4.7602],
        [ 2.5092, -5.1053],
        [ 2.5785, -5.9260],
        [ 1.9068, -3.5399],
        [ 0.9313, -2.3204],
        [ 1.6604, -3.1207],
        [ 2.7286, -4.2825],
        [ 1.3536, -2.9314],
        [ 4.3896, -5.7767],
        [ 1.5962, -4.1552],
        [ 3.2004, -5.0884],
        [ 2.3162, -4.4573],
        [ 3.3455, -4.8019],
        [ 2.0390, -4.5476],
        [ 4.6986, -6.0921],
        [ 4.2672, -5.7424],
        [ 3.5276, -4.9574],
        [ 2.5135, -4.2216],
        [ 3.2944, -5.0271],
        [ 1.6422, -3.5696],
        [ 2.5231, -4.2348],
        [-1.0416, -3.5736],
        [ 2.5732, -4.5931],
        [ 2.2195, -6.3156],
        [-1.2808, -0.2979],
        [ 1.2585, -3.7531],
        [ 1.9226, -3.7641],
        [ 5.4927, -7.0903],
        [ 2.3171, -5.4288],
        [ 1.6387, -5.0570],
        [ 2.0819, -3.6520],
        [ 2.0386, -3.4765],
        [ 2.9493, -5.1534],
        [ 4.9749, -6.3693],
        [ 2.9678, -4.6125],
        [ 3.7628, -6.4227],
        [ 3.9414, -5.5205],
        [ 5.3901, -7.2239],
        [ 2.8627, -4.4587],
        [ 0.5790, -4.1498],
        [ 3.0336, -4.4200],
        [ 2.9459, -4.3908],
        [ 2.8512, -5.0401],
        [ 1.1871, -3.0126],
        [ 3.7086, -5.0996],
        [ 4.1796, -5.6965],
        [ 2.2491, -4.8444],
        [ 2.4636, -5.4816],
        [ 5.1386, -6.5367],
        [ 0.4205, -1.8528],
        [ 2.6839, -4.0769],
        [ 2.9109, -5.3341],
        [ 3.0462, -4.8393],
        [ 2.1609, -3.9519],
        [ 1.6505, -5.9653],
        [ 2.7342, -4.7726],
        [ 1.9200, -4.0906],
        [ 4.1030, -5.9417],
        [-0.2313, -4.3839],
        [ 3.4306, -5.4212],
        [ 3.6680, -5.1092],
        [ 3.0802, -4.5875],
        [ 2.3771, -4.0697],
        [ 1.8667, -5.4906],
        [ 3.1981, -5.4181],
        [ 4.4940, -5.9516],
        [ 1.8310, -3.9045],
        [ 3.9741, -5.4101],
        [ 3.4507, -4.8430],
        [ 1.3120, -3.3175],
        [ 3.9773, -5.3941],
        [ 3.5210, -5.5112],
        [ 3.0868, -5.1257],
        [ 1.8966, -3.3111]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4180, 0.5820],
        [0.9985, 0.0015]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9887, 0.0113], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1295, 0.2291],
         [0.0791, 0.1526]],

        [[0.2607, 0.1408],
         [0.8343, 0.5442]],

        [[0.0374, 0.1381],
         [0.0256, 0.7959]],

        [[0.3720, 0.1393],
         [0.8609, 0.1028]],

        [[0.3957, 0.1411],
         [0.0861, 0.1547]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0016975114736032565
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0009785334230322305
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.005431979218977636
Global Adjusted Rand Index: 0.00953315716518147
Average Adjusted Rand Index: 0.0019413490277589156
10018.531855885527
new:  [0.00953315716518147, 0.0087572748668885, 0.010565222052534589, 0.00953315716518147] [0.0020317643976750645, 0.0018310373615503628, 0.003186146602180798, 0.0019413490277589156] [9882.055764585157, 9882.040248771485, 9882.04476355738, 9882.039851597316]
prior:  [0.0, -0.0006739575247983711, 0.0, 0.0] [0.0, -0.000982071485668608, 0.0, 0.0] [9884.106501243643, 9883.413316701324, 9884.106502808181, nan]
-----------------------------------------------------------------------------------------
This iteration is 5
True Objective function: Loss = -10236.122192207491
Iteration 0: Loss = -17659.7950710693
Iteration 10: Loss = -10160.75163552731
Iteration 20: Loss = -10158.989017472102
Iteration 30: Loss = -10159.025116831266
1
Iteration 40: Loss = -10159.025488781961
2
Iteration 50: Loss = -10159.025162993426
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.1898, 0.8102],
        [0.0198, 0.9802]], dtype=torch.float64)
alpha: tensor([0.0236, 0.9764])
beta: tensor([[[0.1628, 0.1036],
         [0.9332, 0.1392]],

        [[0.9755, 0.1711],
         [0.0902, 0.4067]],

        [[0.6189, 0.2145],
         [0.2882, 0.0954]],

        [[0.3800, 0.2871],
         [0.2502, 0.6625]],

        [[0.9528, 0.2280],
         [0.0326, 0.9086]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.012494332208171696
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 61
Adjusted Rand Index: -0.015623423336712405
Global Adjusted Rand Index: -0.004215919502146915
Average Adjusted Rand Index: -0.00621622959700246
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17936.357357220764
Iteration 100: Loss = -10166.507329969483
Iteration 200: Loss = -10164.626157124592
Iteration 300: Loss = -10164.044805305575
Iteration 400: Loss = -10163.75051712847
Iteration 500: Loss = -10163.54113413607
Iteration 600: Loss = -10163.401233012688
Iteration 700: Loss = -10163.288659891843
Iteration 800: Loss = -10163.119677982495
Iteration 900: Loss = -10162.575685379456
Iteration 1000: Loss = -10161.621993267769
Iteration 1100: Loss = -10160.393078227436
Iteration 1200: Loss = -10159.544858649662
Iteration 1300: Loss = -10159.139565459747
Iteration 1400: Loss = -10158.88121547924
Iteration 1500: Loss = -10158.701319473177
Iteration 1600: Loss = -10158.591116099993
Iteration 1700: Loss = -10158.507925365546
Iteration 1800: Loss = -10158.450529471053
Iteration 1900: Loss = -10158.4130856926
Iteration 2000: Loss = -10158.389868380837
Iteration 2100: Loss = -10158.36756181778
Iteration 2200: Loss = -10158.344390617915
Iteration 2300: Loss = -10158.317932113969
Iteration 2400: Loss = -10158.287562271036
Iteration 2500: Loss = -10158.254776340355
Iteration 2600: Loss = -10158.234732678957
Iteration 2700: Loss = -10158.236223308213
1
Iteration 2800: Loss = -10158.21876853471
Iteration 2900: Loss = -10158.214753874305
Iteration 3000: Loss = -10158.20797911314
Iteration 3100: Loss = -10158.204296306656
Iteration 3200: Loss = -10158.201282688116
Iteration 3300: Loss = -10158.19854632528
Iteration 3400: Loss = -10158.196259900848
Iteration 3500: Loss = -10158.194021904656
Iteration 3600: Loss = -10158.192080871459
Iteration 3700: Loss = -10158.190240480848
Iteration 3800: Loss = -10158.193594922512
1
Iteration 3900: Loss = -10158.187144539259
Iteration 4000: Loss = -10158.185788839484
Iteration 4100: Loss = -10158.184559368
Iteration 4200: Loss = -10158.183353541483
Iteration 4300: Loss = -10158.522418086988
1
Iteration 4400: Loss = -10158.181325290978
Iteration 4500: Loss = -10158.180393446853
Iteration 4600: Loss = -10158.192170478249
1
Iteration 4700: Loss = -10158.178722563736
Iteration 4800: Loss = -10158.17795498385
Iteration 4900: Loss = -10158.247759190795
1
Iteration 5000: Loss = -10158.176630097385
Iteration 5100: Loss = -10158.176028356962
Iteration 5200: Loss = -10158.19469357455
1
Iteration 5300: Loss = -10158.174954354972
Iteration 5400: Loss = -10158.174459591286
Iteration 5500: Loss = -10158.174648120343
1
Iteration 5600: Loss = -10158.173599488098
Iteration 5700: Loss = -10158.173176288368
Iteration 5800: Loss = -10158.177077598695
1
Iteration 5900: Loss = -10158.17243824705
Iteration 6000: Loss = -10158.172058620647
Iteration 6100: Loss = -10158.441378092279
1
Iteration 6200: Loss = -10158.171438805966
Iteration 6300: Loss = -10158.171130226088
Iteration 6400: Loss = -10158.171598344183
1
Iteration 6500: Loss = -10158.170649176507
Iteration 6600: Loss = -10158.170354092108
Iteration 6700: Loss = -10158.170189596141
Iteration 6800: Loss = -10158.170049263317
Iteration 6900: Loss = -10158.169724706684
Iteration 7000: Loss = -10158.169582012228
Iteration 7100: Loss = -10158.170180921083
1
Iteration 7200: Loss = -10158.169216274588
Iteration 7300: Loss = -10158.169061978371
Iteration 7400: Loss = -10158.168963558886
Iteration 7500: Loss = -10158.168742096064
Iteration 7600: Loss = -10158.168633266718
Iteration 7700: Loss = -10158.168958009348
1
Iteration 7800: Loss = -10158.168384343551
Iteration 7900: Loss = -10158.168261789495
Iteration 8000: Loss = -10158.169732389946
1
Iteration 8100: Loss = -10158.168063918096
Iteration 8200: Loss = -10158.167968617163
Iteration 8300: Loss = -10158.169179856717
1
Iteration 8400: Loss = -10158.167806000236
Iteration 8500: Loss = -10158.167689741817
Iteration 8600: Loss = -10158.172847028345
1
Iteration 8700: Loss = -10158.167533786842
Iteration 8800: Loss = -10158.167451408637
Iteration 8900: Loss = -10158.17505841255
1
Iteration 9000: Loss = -10158.167349842039
Iteration 9100: Loss = -10158.167322779049
Iteration 9200: Loss = -10158.168165856798
1
Iteration 9300: Loss = -10158.1671530105
Iteration 9400: Loss = -10158.167101710558
Iteration 9500: Loss = -10158.167424731355
1
Iteration 9600: Loss = -10158.167052409488
Iteration 9700: Loss = -10158.16696465259
Iteration 9800: Loss = -10158.16772742024
1
Iteration 9900: Loss = -10158.16690907781
Iteration 10000: Loss = -10158.166865134614
Iteration 10100: Loss = -10158.167474273123
1
Iteration 10200: Loss = -10158.166776391494
Iteration 10300: Loss = -10158.16674640435
Iteration 10400: Loss = -10158.166874224922
1
Iteration 10500: Loss = -10158.166700887494
Iteration 10600: Loss = -10158.204884290293
1
Iteration 10700: Loss = -10158.166641874373
Iteration 10800: Loss = -10158.166600879842
Iteration 10900: Loss = -10158.18790541551
1
Iteration 11000: Loss = -10158.166569505449
Iteration 11100: Loss = -10158.166548724932
Iteration 11200: Loss = -10158.174134401
1
Iteration 11300: Loss = -10158.166549875757
2
Iteration 11400: Loss = -10158.226873485148
3
Iteration 11500: Loss = -10158.166467706955
Iteration 11600: Loss = -10158.166457991309
Iteration 11700: Loss = -10158.17294906975
1
Iteration 11800: Loss = -10158.166410637974
Iteration 11900: Loss = -10158.266449251232
1
Iteration 12000: Loss = -10158.166412643352
2
Iteration 12100: Loss = -10158.166601656783
3
Iteration 12200: Loss = -10158.166823045627
4
Iteration 12300: Loss = -10158.176877440821
5
Iteration 12400: Loss = -10158.16638436454
Iteration 12500: Loss = -10158.166484161391
1
Iteration 12600: Loss = -10158.18019932058
2
Iteration 12700: Loss = -10158.251177162785
3
Iteration 12800: Loss = -10158.16966957409
4
Iteration 12900: Loss = -10158.173653169983
5
Iteration 13000: Loss = -10158.166370254772
Iteration 13100: Loss = -10158.36075643102
1
Iteration 13200: Loss = -10158.17062538974
2
Iteration 13300: Loss = -10158.166492142342
3
Iteration 13400: Loss = -10158.173155143675
4
Iteration 13500: Loss = -10158.173941199282
5
Iteration 13600: Loss = -10158.195024217848
6
Iteration 13700: Loss = -10158.166565877238
7
Iteration 13800: Loss = -10158.168956738431
8
Iteration 13900: Loss = -10158.258931097014
9
Iteration 14000: Loss = -10158.16625390193
Iteration 14100: Loss = -10158.166675342245
1
Iteration 14200: Loss = -10158.29267147232
2
Iteration 14300: Loss = -10158.166245716937
Iteration 14400: Loss = -10158.166304230373
1
Iteration 14500: Loss = -10158.167222111251
2
Iteration 14600: Loss = -10158.171824242438
3
Iteration 14700: Loss = -10158.166217514568
Iteration 14800: Loss = -10158.167822144744
1
Iteration 14900: Loss = -10158.171419272854
2
Iteration 15000: Loss = -10158.167347708406
3
Iteration 15100: Loss = -10158.228743552963
4
Iteration 15200: Loss = -10158.166215178915
Iteration 15300: Loss = -10158.166243745867
1
Iteration 15400: Loss = -10158.166680420101
2
Iteration 15500: Loss = -10158.167964385377
3
Iteration 15600: Loss = -10158.16618396074
Iteration 15700: Loss = -10158.166363155584
1
Iteration 15800: Loss = -10158.199060307285
2
Iteration 15900: Loss = -10158.16618834596
3
Iteration 16000: Loss = -10158.229246511488
4
Iteration 16100: Loss = -10158.166208075423
5
Iteration 16200: Loss = -10158.322810851663
6
Iteration 16300: Loss = -10158.18129271015
7
Iteration 16400: Loss = -10158.166565650541
8
Iteration 16500: Loss = -10158.166259783296
9
Iteration 16600: Loss = -10158.166397650384
10
Stopping early at iteration 16600 due to no improvement.
tensor([[-3.4567e+00, -1.1586e+00],
        [-5.4485e+00,  8.3324e-01],
        [-4.0478e+00, -5.6745e-01],
        [-5.1994e+00,  5.8422e-01],
        [-5.3672e+00,  7.5194e-01],
        [-5.4106e+00,  7.9538e-01],
        [-5.8090e+00,  1.1938e+00],
        [-5.2406e+00,  6.2537e-01],
        [-5.0221e+00,  4.0686e-01],
        [-4.8941e+00,  2.7888e-01],
        [-4.8235e+00,  2.0828e-01],
        [-5.4351e+00,  8.1990e-01],
        [-3.8774e+00, -7.3783e-01],
        [-5.0109e+00,  3.9564e-01],
        [-4.2625e+00, -3.5276e-01],
        [-5.7161e+00,  1.1009e+00],
        [-4.4942e+00, -1.2100e-01],
        [-5.6448e+00,  1.0295e+00],
        [-4.9985e+00,  3.8325e-01],
        [-4.6410e+00,  2.5734e-02],
        [-4.0204e+00, -5.9487e-01],
        [-4.0843e+00, -5.3097e-01],
        [-4.2187e+00, -3.9653e-01],
        [-4.0660e+00, -5.4924e-01],
        [-3.6496e+00, -9.6562e-01],
        [-4.2889e+00, -3.2628e-01],
        [-4.0447e+00, -5.7052e-01],
        [-3.6747e+00, -9.4056e-01],
        [-5.6352e+00,  1.0200e+00],
        [-4.8514e+00,  2.3614e-01],
        [-5.0589e+00,  4.4370e-01],
        [-3.9951e+00, -6.2007e-01],
        [-5.1251e+00,  5.0988e-01],
        [-5.1910e+00,  5.7582e-01],
        [-3.6372e+00, -9.7800e-01],
        [-4.2621e+00, -3.5315e-01],
        [-4.2306e+00, -3.8465e-01],
        [-3.2480e+00, -1.3672e+00],
        [-3.9842e+00, -6.3098e-01],
        [-6.1997e+00,  1.5844e+00],
        [-3.4583e+00, -1.1569e+00],
        [-4.5758e+00, -3.9450e-02],
        [-4.7543e+00,  1.3906e-01],
        [-5.3122e+00,  6.9702e-01],
        [-5.3977e+00,  7.8251e-01],
        [-3.7561e+00, -8.5909e-01],
        [-5.0620e+00,  4.4683e-01],
        [-4.4888e+00, -1.2646e-01],
        [-4.4488e+00, -1.6643e-01],
        [-4.1993e+00, -4.1588e-01],
        [-5.5151e+00,  8.9988e-01],
        [-5.9141e+00,  1.2989e+00],
        [-5.3114e+00,  6.9619e-01],
        [-3.7926e+00, -8.2258e-01],
        [-4.4103e+00, -2.0491e-01],
        [-4.2523e+00, -3.6295e-01],
        [-3.8735e+00, -7.4168e-01],
        [-4.2881e+00, -3.2709e-01],
        [-5.3006e+00,  6.8537e-01],
        [-5.8013e+00,  1.1861e+00],
        [-4.6645e+00,  4.9277e-02],
        [-4.9776e+00,  3.6242e-01],
        [-5.4770e+00,  8.6174e-01],
        [-4.6581e+00,  4.2884e-02],
        [-4.4596e+00, -1.5566e-01],
        [-3.8688e+00, -7.4645e-01],
        [-4.4207e+00, -1.9453e-01],
        [-4.2475e+00, -3.6767e-01],
        [-3.6581e+00, -9.5708e-01],
        [-5.0173e+00,  4.0207e-01],
        [-4.6423e+00,  2.7030e-02],
        [-4.9530e+00,  3.3776e-01],
        [-5.0059e+00,  3.9071e-01],
        [-4.8066e+00,  1.9143e-01],
        [-5.3077e+00,  6.9245e-01],
        [-4.6096e+00, -5.5844e-03],
        [-5.0222e+00,  4.0694e-01],
        [-4.0679e+00, -5.4734e-01],
        [-3.4857e+00, -1.1295e+00],
        [-5.0125e+00,  3.9732e-01],
        [-5.8608e+00,  1.2455e+00],
        [-4.0030e+00, -6.1223e-01],
        [-4.6908e+00,  7.5541e-02],
        [-5.1377e+00,  5.2251e-01],
        [-5.2330e+00,  6.1781e-01],
        [-4.2722e+00, -3.4301e-01],
        [-5.4227e+00,  8.0747e-01],
        [-4.8842e+00,  2.6893e-01],
        [-5.3315e+00,  7.1625e-01],
        [-5.5666e+00,  9.5140e-01],
        [-4.2465e+00, -3.6868e-01],
        [-5.2122e+00,  5.9695e-01],
        [-5.0324e+00,  4.1716e-01],
        [-5.3815e+00,  7.6629e-01],
        [-4.3893e+00, -2.2595e-01],
        [-4.8267e+00,  2.1146e-01],
        [-4.8309e+00,  2.1571e-01],
        [-5.2242e+00,  6.0896e-01],
        [-4.6871e+00,  7.1889e-02],
        [-5.4309e+00,  8.1573e-01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.8541e-06, 9.9999e-01],
        [5.9939e-02, 9.4006e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0172, 0.9828], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2182, 0.0984],
         [0.9332, 0.1415]],

        [[0.9755, 0.1697],
         [0.0902, 0.4067]],

        [[0.6189, 0.1927],
         [0.2882, 0.0954]],

        [[0.3800, 0.0651],
         [0.2502, 0.6625]],

        [[0.9528, 0.1937],
         [0.0326, 0.9086]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.012494332208171696
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.007493599095377873
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
Global Adjusted Rand Index: -0.00021554131597433836
Average Adjusted Rand Index: -0.002646871147114132
Iteration 0: Loss = -26620.956658048613
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6939,    nan]],

        [[0.6571,    nan],
         [0.9717, 0.0573]],

        [[0.1340,    nan],
         [0.3387, 0.3111]],

        [[0.1760,    nan],
         [0.2726, 0.9842]],

        [[0.0109,    nan],
         [0.6751, 0.8417]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 37
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26620.67428260257
Iteration 100: Loss = -10169.781809707905
Iteration 200: Loss = -10166.827481068822
Iteration 300: Loss = -10165.630773290222
Iteration 400: Loss = -10164.974689486688
Iteration 500: Loss = -10164.55802112313
Iteration 600: Loss = -10164.27283308931
Iteration 700: Loss = -10164.064096691607
Iteration 800: Loss = -10163.90127251322
Iteration 900: Loss = -10163.766773453792
Iteration 1000: Loss = -10163.646247303615
Iteration 1100: Loss = -10163.523105208104
Iteration 1200: Loss = -10163.373305219638
Iteration 1300: Loss = -10163.177378526685
Iteration 1400: Loss = -10162.94537366219
Iteration 1500: Loss = -10162.690212928283
Iteration 1600: Loss = -10162.352585180448
Iteration 1700: Loss = -10161.908055248374
Iteration 1800: Loss = -10161.473877144117
Iteration 1900: Loss = -10161.095521453099
Iteration 2000: Loss = -10160.749755276796
Iteration 2100: Loss = -10160.414452141962
Iteration 2200: Loss = -10160.078756228293
Iteration 2300: Loss = -10159.745554578582
Iteration 2400: Loss = -10159.430330680878
Iteration 2500: Loss = -10159.155003177368
Iteration 2600: Loss = -10158.93561769806
Iteration 2700: Loss = -10158.771801238641
Iteration 2800: Loss = -10158.650119701257
Iteration 2900: Loss = -10158.555996480129
Iteration 3000: Loss = -10158.48162062604
Iteration 3100: Loss = -10158.422342951868
Iteration 3200: Loss = -10158.37472244307
Iteration 3300: Loss = -10158.336954676377
Iteration 3400: Loss = -10158.307479596393
Iteration 3500: Loss = -10158.284555026605
Iteration 3600: Loss = -10158.266544576883
Iteration 3700: Loss = -10158.252368700974
Iteration 3800: Loss = -10158.2413057226
Iteration 3900: Loss = -10158.232629926795
Iteration 4000: Loss = -10158.225868814718
Iteration 4100: Loss = -10158.22048425669
Iteration 4200: Loss = -10158.216098208646
Iteration 4300: Loss = -10158.21242471592
Iteration 4400: Loss = -10158.209297466687
Iteration 4500: Loss = -10158.206527029244
Iteration 4600: Loss = -10158.204086914162
Iteration 4700: Loss = -10158.201879168757
Iteration 4800: Loss = -10158.199861543806
Iteration 4900: Loss = -10158.197990961662
Iteration 5000: Loss = -10158.196288296069
Iteration 5100: Loss = -10158.194706711496
Iteration 5200: Loss = -10158.193244154063
Iteration 5300: Loss = -10158.191865740579
Iteration 5400: Loss = -10158.190619641298
Iteration 5500: Loss = -10158.189458720844
Iteration 5600: Loss = -10158.188347524352
Iteration 5700: Loss = -10158.18728437534
Iteration 5800: Loss = -10158.186301706386
Iteration 5900: Loss = -10158.185382082946
Iteration 6000: Loss = -10158.18450713027
Iteration 6100: Loss = -10158.183652092
Iteration 6200: Loss = -10158.182855640543
Iteration 6300: Loss = -10158.182095173619
Iteration 6400: Loss = -10158.181331207965
Iteration 6500: Loss = -10158.180664450167
Iteration 6600: Loss = -10158.179986051953
Iteration 6700: Loss = -10158.180221954444
1
Iteration 6800: Loss = -10158.17874621845
Iteration 6900: Loss = -10158.17815212208
Iteration 7000: Loss = -10158.17860900834
1
Iteration 7100: Loss = -10158.177116599201
Iteration 7200: Loss = -10158.176627470431
Iteration 7300: Loss = -10158.176316994452
Iteration 7400: Loss = -10158.175659808156
Iteration 7500: Loss = -10158.175232016254
Iteration 7600: Loss = -10158.189951485843
1
Iteration 7700: Loss = -10158.174419736588
Iteration 7800: Loss = -10158.17402786753
Iteration 7900: Loss = -10158.230731610665
1
Iteration 8000: Loss = -10158.173340206358
Iteration 8100: Loss = -10158.17298063445
Iteration 8200: Loss = -10158.176964809612
1
Iteration 8300: Loss = -10158.172441774741
Iteration 8400: Loss = -10158.172077215035
Iteration 8500: Loss = -10158.171849423257
Iteration 8600: Loss = -10158.172133152757
1
Iteration 8700: Loss = -10158.17133748188
Iteration 8800: Loss = -10158.17108743213
Iteration 8900: Loss = -10158.172819128265
1
Iteration 9000: Loss = -10158.170613794864
Iteration 9100: Loss = -10158.170434467153
Iteration 9200: Loss = -10158.170480777835
1
Iteration 9300: Loss = -10158.170027105161
Iteration 9400: Loss = -10158.169873177223
Iteration 9500: Loss = -10158.169733693236
Iteration 9600: Loss = -10158.169501892788
Iteration 9700: Loss = -10158.169347307079
Iteration 9800: Loss = -10158.169369510277
1
Iteration 9900: Loss = -10158.169050735707
Iteration 10000: Loss = -10158.169009359437
Iteration 10100: Loss = -10158.16885650598
Iteration 10200: Loss = -10158.168659804653
Iteration 10300: Loss = -10158.22370575067
1
Iteration 10400: Loss = -10158.168489640802
Iteration 10500: Loss = -10158.168359842983
Iteration 10600: Loss = -10158.191086176974
1
Iteration 10700: Loss = -10158.168131745582
Iteration 10800: Loss = -10158.16802836666
Iteration 10900: Loss = -10158.168727736329
1
Iteration 11000: Loss = -10158.167854502577
Iteration 11100: Loss = -10158.167757111429
Iteration 11200: Loss = -10158.170684721155
1
Iteration 11300: Loss = -10158.167777454988
2
Iteration 11400: Loss = -10158.21255534204
3
Iteration 11500: Loss = -10158.171400336094
4
Iteration 11600: Loss = -10158.222094967188
5
Iteration 11700: Loss = -10158.170224083746
6
Iteration 11800: Loss = -10158.18450720843
7
Iteration 11900: Loss = -10158.168251903087
8
Iteration 12000: Loss = -10158.18513075016
9
Iteration 12100: Loss = -10158.173613212919
10
Stopping early at iteration 12100 due to no improvement.
tensor([[ 0.3887, -1.9091],
        [ 2.3917, -3.8929],
        [ 1.0363, -2.4453],
        [ 1.6561, -4.1302],
        [ 2.2972, -3.8233],
        [ 2.0371, -4.1729],
        [ 2.4971, -4.5071],
        [ 2.2367, -3.6316],
        [ 1.5766, -3.8562],
        [ 1.5803, -3.5950],
        [ 1.7746, -3.2593],
        [ 2.3306, -3.9275],
        [-0.2331, -3.3724],
        [ 0.3964, -5.0116],
        [ 1.2622, -2.6492],
        [ 2.7147, -4.1071],
        [ 0.9646, -3.4108],
        [ 1.8860, -4.7919],
        [ 1.4280, -3.9556],
        [ 1.2956, -3.3737],
        [ 0.9396, -2.4868],
        [ 0.6995, -2.8546],
        [ 1.1426, -2.6810],
        [ 0.8479, -2.6701],
        [ 0.6459, -2.0383],
        [ 1.0526, -2.9096],
        [ 0.7701, -2.7049],
        [ 0.6129, -2.1212],
        [ 2.5066, -4.1497],
        [ 1.7743, -3.3163],
        [ 1.8638, -3.6428],
        [ 0.9463, -2.4291],
        [ 1.8937, -3.7443],
        [ 2.1895, -3.5809],
        [ 0.3713, -2.2881],
        [ 1.0476, -2.8628],
        [ 1.2294, -2.6175],
        [-0.6579, -2.5377],
        [ 0.6302, -2.7235],
        [ 3.0838, -4.7072],
        [ 0.4124, -1.8885],
        [ 1.4476, -3.0907],
        [ 1.2153, -3.6809],
        [ 1.6570, -4.3551],
        [ 1.8964, -4.2864],
        [ 0.3290, -2.5680],
        [ 1.9242, -3.5885],
        [ 1.1662, -3.1976],
        [ 1.4156, -2.8690],
        [ 1.1974, -2.5874],
        [ 2.4381, -3.9796],
        [ 2.9142, -4.3013],
        [ 1.3423, -4.6685],
        [ 0.3623, -2.6082],
        [-0.2038, -4.4114],
        [ 0.9890, -2.9020],
        [ 0.5707, -2.5615],
        [ 1.2853, -2.6774],
        [ 2.2991, -3.6906],
        [ 2.5000, -4.4949],
        [ 1.4240, -3.2928],
        [ 1.5296, -3.8126],
        [ 2.4246, -3.9151],
        [ 1.3919, -3.3120],
        [ 0.9779, -3.3278],
        [ 0.2715, -2.8509],
        [ 1.2035, -3.0249],
        [ 1.2458, -2.6355],
        [ 0.5346, -2.1665],
        [ 1.7622, -3.6613],
        [ 1.4009, -3.2715],
        [ 1.5008, -3.7928],
        [ 1.8715, -3.5287],
        [ 1.7380, -3.2628],
        [ 2.3084, -3.6950],
        [ 1.5524, -3.0546],
        [ 1.6876, -3.7437],
        [ 0.3698, -3.1518],
        [ 0.4152, -1.9400],
        [ 0.3983, -5.0136],
        [ 2.2534, -4.8552],
        [ 0.7233, -2.6682],
        [ 0.2427, -4.5263],
        [ 1.3105, -4.3526],
        [ 1.8776, -3.9778],
        [ 1.2058, -2.7250],
        [ 2.3235, -3.9092],
        [ 0.7463, -4.4089],
        [ 1.8029, -4.2466],
        [ 1.1891, -5.3324],
        [ 1.2163, -2.6633],
        [ 2.1916, -3.6195],
        [ 1.9554, -3.4983],
        [ 1.7919, -4.3573],
        [ 1.0233, -3.1427],
        [ 1.2471, -3.7940],
        [ 1.7134, -3.3370],
        [ 1.5206, -4.3162],
        [ 1.6438, -3.1180],
        [ 2.3671, -3.8842]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.4017e-01, 5.9827e-02],
        [9.9975e-01, 2.5391e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9828, 0.0172], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1416, 0.0984],
         [0.6939, 0.2181]],

        [[0.6571, 0.1697],
         [0.9717, 0.0573]],

        [[0.1340, 0.1934],
         [0.3387, 0.3111]],

        [[0.1760, 0.0651],
         [0.2726, 0.9842]],

        [[0.0109, 0.1925],
         [0.6751, 0.8417]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.012494332208171696
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.007493599095377873
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: -0.008233622622776837
Global Adjusted Rand Index: -0.00021554131597433836
Average Adjusted Rand Index: -0.002646871147114132
Iteration 0: Loss = -21471.322333649638
Iteration 10: Loss = -10159.305602077766
Iteration 20: Loss = -10159.042623274014
Iteration 30: Loss = -10159.030638068203
Iteration 40: Loss = -10159.026402474741
Iteration 50: Loss = -10159.02534629493
Iteration 60: Loss = -10159.025089435345
Iteration 70: Loss = -10159.025015000041
Iteration 80: Loss = -10159.025007994309
Iteration 90: Loss = -10159.02505201631
1
Iteration 100: Loss = -10159.025011582511
2
Iteration 110: Loss = -10159.025007763765
Iteration 120: Loss = -10159.025008355373
1
Iteration 130: Loss = -10159.025020757175
2
Iteration 140: Loss = -10159.025014859242
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.9802, 0.0198],
        [0.8102, 0.1898]], dtype=torch.float64)
alpha: tensor([0.9764, 0.0236])
beta: tensor([[[0.1392, 0.1036],
         [0.5374, 0.1629]],

        [[0.2456, 0.1711],
         [0.5668, 0.2502]],

        [[0.8371, 0.2145],
         [0.1649, 0.6425]],

        [[0.9338, 0.2871],
         [0.0538, 0.3019]],

        [[0.3507, 0.2280],
         [0.9246, 0.2485]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.012494332208171696
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: -0.015623423336712405
Global Adjusted Rand Index: -0.004215919502146915
Average Adjusted Rand Index: -0.00621622959700246
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21469.95037065303
Iteration 100: Loss = -10217.870357153957
Iteration 200: Loss = -10201.124500489017
Iteration 300: Loss = -10189.03425690706
Iteration 400: Loss = -10179.352258089257
Iteration 500: Loss = -10170.947893876093
Iteration 600: Loss = -10166.414928721868
Iteration 700: Loss = -10163.073916788793
Iteration 800: Loss = -10161.438279214386
Iteration 900: Loss = -10161.045662987744
Iteration 1000: Loss = -10160.749828580581
Iteration 1100: Loss = -10160.510541963933
Iteration 1200: Loss = -10160.304391328744
Iteration 1300: Loss = -10160.104298335535
Iteration 1400: Loss = -10159.885555672734
Iteration 1500: Loss = -10159.636619652027
Iteration 1600: Loss = -10159.362409613143
Iteration 1700: Loss = -10159.087532105921
Iteration 1800: Loss = -10158.842726155766
Iteration 1900: Loss = -10158.64483841929
Iteration 2000: Loss = -10158.493371693366
Iteration 2100: Loss = -10158.379478193663
Iteration 2200: Loss = -10158.293486962826
Iteration 2300: Loss = -10158.227576547173
Iteration 2400: Loss = -10158.176275511945
Iteration 2500: Loss = -10158.135538190993
Iteration 2600: Loss = -10158.102613248504
Iteration 2700: Loss = -10158.07566292925
Iteration 2800: Loss = -10158.053285418975
Iteration 2900: Loss = -10158.034482806434
Iteration 3000: Loss = -10158.01848050415
Iteration 3100: Loss = -10158.004724916851
Iteration 3200: Loss = -10157.992865250175
Iteration 3300: Loss = -10157.982482127418
Iteration 3400: Loss = -10157.973366644646
Iteration 3500: Loss = -10157.965338378164
Iteration 3600: Loss = -10157.958187808927
Iteration 3700: Loss = -10157.951827531948
Iteration 3800: Loss = -10157.946127345682
Iteration 3900: Loss = -10157.941010835057
Iteration 4000: Loss = -10157.936399398219
Iteration 4100: Loss = -10157.932209387582
Iteration 4200: Loss = -10157.928387984244
Iteration 4300: Loss = -10157.925024042097
Iteration 4400: Loss = -10157.930043794235
1
Iteration 4500: Loss = -10157.91960147989
Iteration 4600: Loss = -10157.916344446652
Iteration 4700: Loss = -10157.91500101306
Iteration 4800: Loss = -10157.911624675298
Iteration 4900: Loss = -10157.909591174934
Iteration 5000: Loss = -10157.907675585318
Iteration 5100: Loss = -10157.90642318979
Iteration 5200: Loss = -10157.904349559878
Iteration 5300: Loss = -10157.90291460213
Iteration 5400: Loss = -10157.901657682552
Iteration 5500: Loss = -10157.900296958893
Iteration 5600: Loss = -10157.899144721867
Iteration 5700: Loss = -10157.89810428121
Iteration 5800: Loss = -10157.897115332315
Iteration 5900: Loss = -10157.896184791827
Iteration 6000: Loss = -10157.897768875562
1
Iteration 6100: Loss = -10157.894572271342
Iteration 6200: Loss = -10157.893857719231
Iteration 6300: Loss = -10157.900762913427
1
Iteration 6400: Loss = -10157.892530685742
Iteration 6500: Loss = -10157.891943509712
Iteration 6600: Loss = -10157.907667177296
1
Iteration 6700: Loss = -10157.89088108057
Iteration 6800: Loss = -10157.890422977565
Iteration 6900: Loss = -10157.949797201873
1
Iteration 7000: Loss = -10157.88955092851
Iteration 7100: Loss = -10157.889198649289
Iteration 7200: Loss = -10157.896240721164
1
Iteration 7300: Loss = -10157.88845892501
Iteration 7400: Loss = -10157.888129323426
Iteration 7500: Loss = -10157.88785925608
Iteration 7600: Loss = -10157.887540726737
Iteration 7700: Loss = -10157.887273309749
Iteration 7800: Loss = -10157.887019703498
Iteration 7900: Loss = -10157.887013615524
Iteration 8000: Loss = -10157.886541833133
Iteration 8100: Loss = -10157.886344735201
Iteration 8200: Loss = -10157.887729858201
1
Iteration 8300: Loss = -10157.885948285986
Iteration 8400: Loss = -10157.885784018432
Iteration 8500: Loss = -10157.888102231427
1
Iteration 8600: Loss = -10157.885447874898
Iteration 8700: Loss = -10157.88767314598
1
Iteration 8800: Loss = -10157.885183502052
Iteration 8900: Loss = -10157.88502778019
Iteration 9000: Loss = -10157.89742419169
1
Iteration 9100: Loss = -10157.88480889333
Iteration 9200: Loss = -10157.92985818712
1
Iteration 9300: Loss = -10157.884573656711
Iteration 9400: Loss = -10157.884437225337
Iteration 9500: Loss = -10157.884510394239
1
Iteration 9600: Loss = -10157.88425617025
Iteration 9700: Loss = -10157.914742781568
1
Iteration 9800: Loss = -10157.884075783311
Iteration 9900: Loss = -10157.886869857706
1
Iteration 10000: Loss = -10157.894578753918
2
Iteration 10100: Loss = -10157.884818708233
3
Iteration 10200: Loss = -10157.890429266981
4
Iteration 10300: Loss = -10158.044922735291
5
Iteration 10400: Loss = -10157.884024032976
Iteration 10500: Loss = -10157.883751673273
Iteration 10600: Loss = -10157.913964016365
1
Iteration 10700: Loss = -10157.883669773857
Iteration 10800: Loss = -10157.884385381078
1
Iteration 10900: Loss = -10157.883419779148
Iteration 11000: Loss = -10157.898699432906
1
Iteration 11100: Loss = -10157.88332527718
Iteration 11200: Loss = -10157.883323870106
Iteration 11300: Loss = -10157.88347177013
1
Iteration 11400: Loss = -10157.883213380352
Iteration 11500: Loss = -10157.883792590346
1
Iteration 11600: Loss = -10158.063472475811
2
Iteration 11700: Loss = -10157.883103808263
Iteration 11800: Loss = -10157.883897898362
1
Iteration 11900: Loss = -10157.89078088252
2
Iteration 12000: Loss = -10157.883901822219
3
Iteration 12100: Loss = -10157.883293879158
4
Iteration 12200: Loss = -10157.885604100367
5
Iteration 12300: Loss = -10157.886719440303
6
Iteration 12400: Loss = -10157.883295383244
7
Iteration 12500: Loss = -10157.88299568054
Iteration 12600: Loss = -10157.891320134335
1
Iteration 12700: Loss = -10157.884640023942
2
Iteration 12800: Loss = -10157.882938820758
Iteration 12900: Loss = -10157.883026158219
1
Iteration 13000: Loss = -10157.884104251809
2
Iteration 13100: Loss = -10157.885116461212
3
Iteration 13200: Loss = -10157.895273570039
4
Iteration 13300: Loss = -10157.899378401711
5
Iteration 13400: Loss = -10157.888558427847
6
Iteration 13500: Loss = -10157.882830808465
Iteration 13600: Loss = -10157.882840829598
1
Iteration 13700: Loss = -10157.938273410116
2
Iteration 13800: Loss = -10157.882796978389
Iteration 13900: Loss = -10157.8898696471
1
Iteration 14000: Loss = -10157.909527847341
2
Iteration 14100: Loss = -10157.88325450873
3
Iteration 14200: Loss = -10157.88282384588
4
Iteration 14300: Loss = -10157.883031868982
5
Iteration 14400: Loss = -10157.894433163865
6
Iteration 14500: Loss = -10157.886625854759
7
Iteration 14600: Loss = -10157.882692567484
Iteration 14700: Loss = -10157.882716449707
1
Iteration 14800: Loss = -10157.882678359167
Iteration 14900: Loss = -10157.885301940316
1
Iteration 15000: Loss = -10157.882794846575
2
Iteration 15100: Loss = -10157.884097359385
3
Iteration 15200: Loss = -10157.97494552607
4
Iteration 15300: Loss = -10157.882632019724
Iteration 15400: Loss = -10157.882829970978
1
Iteration 15500: Loss = -10157.883153130924
2
Iteration 15600: Loss = -10157.882841094508
3
Iteration 15700: Loss = -10157.882700404829
4
Iteration 15800: Loss = -10157.884227953617
5
Iteration 15900: Loss = -10157.882755934816
6
Iteration 16000: Loss = -10157.971383199212
7
Iteration 16100: Loss = -10157.882627341833
Iteration 16200: Loss = -10157.898712773276
1
Iteration 16300: Loss = -10157.8917637151
2
Iteration 16400: Loss = -10157.967154646913
3
Iteration 16500: Loss = -10157.888224405024
4
Iteration 16600: Loss = -10157.88258375119
Iteration 16700: Loss = -10157.8826637542
1
Iteration 16800: Loss = -10157.895256793205
2
Iteration 16900: Loss = -10157.883428735324
3
Iteration 17000: Loss = -10157.889135712718
4
Iteration 17100: Loss = -10157.883000300484
5
Iteration 17200: Loss = -10157.959043851328
6
Iteration 17300: Loss = -10157.882600928977
7
Iteration 17400: Loss = -10157.882644269688
8
Iteration 17500: Loss = -10157.902674078128
9
Iteration 17600: Loss = -10157.88254800459
Iteration 17700: Loss = -10157.882849152766
1
Iteration 17800: Loss = -10157.88884729287
2
Iteration 17900: Loss = -10157.882591773061
3
Iteration 18000: Loss = -10157.88272820107
4
Iteration 18100: Loss = -10157.899824117072
5
Iteration 18200: Loss = -10157.886822581275
6
Iteration 18300: Loss = -10157.888836277121
7
Iteration 18400: Loss = -10157.882665616626
8
Iteration 18500: Loss = -10157.882599051776
9
Iteration 18600: Loss = -10157.883006701797
10
Stopping early at iteration 18600 due to no improvement.
tensor([[-6.8564,  5.4055],
        [-8.7041,  4.0889],
        [-7.0512,  5.3901],
        [-7.1069,  5.6128],
        [-8.1125,  4.6331],
        [-7.1313,  5.6015],
        [-7.1331,  5.6892],
        [-7.7879,  4.9571],
        [-8.3223,  4.3834],
        [-7.3197,  5.3067],
        [-7.5121,  5.1092],
        [-7.5632,  5.1728],
        [-7.2758,  5.0590],
        [-7.0543,  5.6158],
        [-6.9456,  5.5284],
        [-7.8112,  5.0072],
        [-6.9674,  5.5534],
        [-7.0842,  5.6102],
        [-7.3221,  5.3419],
        [-7.6363,  4.9635],
        [-7.2696,  5.1867],
        [-7.0905,  5.3295],
        [-6.9425,  5.5497],
        [-7.0214,  5.4135],
        [-7.2790,  5.0895],
        [-7.0473,  5.4566],
        [-7.0578,  5.3841],
        [-7.7058,  4.6357],
        [-7.9442,  4.9340],
        [-7.0259,  5.6164],
        [-7.1687,  5.5060],
        [-8.1557,  4.3105],
        [-7.0695,  5.6742],
        [-7.0732,  5.6602],
        [-6.8454,  5.4374],
        [-6.9694,  5.4617],
        [-6.9636,  5.5342],
        [-6.9177,  5.3403],
        [-6.9081,  5.5207],
        [-7.5169,  5.4687],
        [-7.1495,  5.1705],
        [-7.0112,  5.5570],
        [-7.6622,  4.9878],
        [-7.0356,  5.6368],
        [-7.5486,  5.1767],
        [-7.0056,  5.3933],
        [-7.7289,  4.9713],
        [-7.2143,  5.2925],
        [-7.4289,  5.1225],
        [-6.9598,  5.4533],
        [-7.1511,  5.5950],
        [-7.1041,  5.7081],
        [-7.1842,  5.5604],
        [-8.5096,  3.8944],
        [-7.1007,  5.4166],
        [-6.9233,  5.5216],
        [-7.9370,  4.4374],
        [-7.0266,  5.4791],
        [-7.3161,  5.4266],
        [-7.1572,  5.6789],
        [-6.9874,  5.6011],
        [-7.5549,  4.9278],
        [-7.5964,  5.2028],
        [-7.9984,  4.6072],
        [-6.9815,  5.5659],
        [-7.1220,  5.2639],
        [-7.2368,  5.2975],
        [-6.9797,  5.5038],
        [-6.8834,  5.4698],
        [-8.1943,  4.5096],
        [-7.0068,  5.5866],
        [-7.3235,  5.3238],
        [-7.1177,  5.5606],
        [-7.0151,  5.5977],
        [-7.5058,  5.2224],
        [-7.7622,  4.8418],
        [-7.0797,  5.6156],
        [-7.0721,  5.3640],
        [-7.1864,  5.0457],
        [-7.8818,  4.8227],
        [-7.1774,  5.6870],
        [-6.9285,  5.4770],
        [-7.0517,  5.5188],
        [-7.1148,  5.5915],
        [-7.5490,  5.1254],
        [-6.9553,  5.5482],
        [-7.4877,  5.3042],
        [-7.0232,  5.6160],
        [-8.5433,  4.2571],
        [-7.1252,  5.6371],
        [-6.9825,  5.5015],
        [-7.0642,  5.6561],
        [-7.1159,  5.5807],
        [-7.4567,  5.3205],
        [-6.9277,  5.5411],
        [-7.0163,  5.6188],
        [-6.9900,  5.5904],
        [-7.6481,  5.0884],
        [-7.1001,  5.4924],
        [-8.0928,  4.7064]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.5951e-01, 4.0493e-02],
        [9.9999e-01, 8.5721e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.4409e-06, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1445, 0.1302],
         [0.5374, 0.1358]],

        [[0.2456, 0.1711],
         [0.5668, 0.2502]],

        [[0.8371, 0.2036],
         [0.1649, 0.6425]],

        [[0.9338, 0.0650],
         [0.0538, 0.3019]],

        [[0.3507, 0.2039],
         [0.9246, 0.2485]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: -0.006658343736995423
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: -0.008233622622776837
Global Adjusted Rand Index: 0.015187927434794907
Average Adjusted Rand Index: -0.0023386828990683297
Iteration 0: Loss = -29936.441329155998
Iteration 10: Loss = -10160.095691498349
Iteration 20: Loss = -10159.218766820295
Iteration 30: Loss = -10158.768768872855
Iteration 40: Loss = -10158.709738466567
Iteration 50: Loss = -10158.695397970127
Iteration 60: Loss = -10158.690955364105
Iteration 70: Loss = -10158.68939263434
Iteration 80: Loss = -10158.688774398594
Iteration 90: Loss = -10158.688566350826
Iteration 100: Loss = -10158.688459584333
Iteration 110: Loss = -10158.688371131286
Iteration 120: Loss = -10158.688354293256
Iteration 130: Loss = -10158.688368597215
1
Iteration 140: Loss = -10158.688323097373
Iteration 150: Loss = -10158.688323495531
1
Iteration 160: Loss = -10158.68834736858
2
Iteration 170: Loss = -10158.68834232287
3
Stopping early at iteration 169 due to no improvement.
pi: tensor([[0.0182, 0.9818],
        [0.0492, 0.9508]], dtype=torch.float64)
alpha: tensor([0.0473, 0.9527])
beta: tensor([[[0.1764, 0.1126],
         [0.0415, 0.1407]],

        [[0.6721, 0.1672],
         [0.5006, 0.5698]],

        [[0.2156, 0.1956],
         [0.0597, 0.1507]],

        [[0.0934, 0.0663],
         [0.1401, 0.2268]],

        [[0.5841, 0.1983],
         [0.2942, 0.6369]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.012494332208171696
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
Global Adjusted Rand Index: -0.0013212301113857405
Average Adjusted Rand Index: -0.003505880593303584
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29935.940973653214
Iteration 100: Loss = -10272.316831736316
Iteration 200: Loss = -10222.03945896262
Iteration 300: Loss = -10197.482215562315
Iteration 400: Loss = -10181.76399614903
Iteration 500: Loss = -10175.8495777172
Iteration 600: Loss = -10167.810660898023
Iteration 700: Loss = -10164.123913273006
Iteration 800: Loss = -10163.586761386914
Iteration 900: Loss = -10163.26479475236
Iteration 1000: Loss = -10163.033406292421
Iteration 1100: Loss = -10162.83827556888
Iteration 1200: Loss = -10162.63810389934
Iteration 1300: Loss = -10162.385245206953
Iteration 1400: Loss = -10161.926969022623
Iteration 1500: Loss = -10161.691695969981
Iteration 1600: Loss = -10161.557158019923
Iteration 1700: Loss = -10161.469771084996
Iteration 1800: Loss = -10161.407504828943
Iteration 1900: Loss = -10161.36070787144
Iteration 2000: Loss = -10161.324285577495
Iteration 2100: Loss = -10161.295052836864
Iteration 2200: Loss = -10161.271101416716
Iteration 2300: Loss = -10161.251141578601
Iteration 2400: Loss = -10161.234221191326
Iteration 2500: Loss = -10161.219707114193
Iteration 2600: Loss = -10161.207131021956
Iteration 2700: Loss = -10161.196059915279
Iteration 2800: Loss = -10161.186121016897
Iteration 2900: Loss = -10161.176894070742
Iteration 3000: Loss = -10161.168262034604
Iteration 3100: Loss = -10161.161028273713
Iteration 3200: Loss = -10161.154790255312
Iteration 3300: Loss = -10161.149187721041
Iteration 3400: Loss = -10161.144102832337
Iteration 3500: Loss = -10161.139480170412
Iteration 3600: Loss = -10161.135273913178
Iteration 3700: Loss = -10161.131459472801
Iteration 3800: Loss = -10161.127962697805
Iteration 3900: Loss = -10161.128641141317
1
Iteration 4000: Loss = -10161.121723091195
Iteration 4100: Loss = -10161.118812505336
Iteration 4200: Loss = -10161.115572472496
Iteration 4300: Loss = -10161.109222169238
Iteration 4400: Loss = -10161.103383173273
Iteration 4500: Loss = -10161.047754626197
Iteration 4600: Loss = -10161.015929213581
Iteration 4700: Loss = -10160.983043976525
Iteration 4800: Loss = -10160.94549235922
Iteration 4900: Loss = -10160.919817668524
Iteration 5000: Loss = -10160.906813851127
Iteration 5100: Loss = -10160.899574988578
Iteration 5200: Loss = -10160.894889007366
Iteration 5300: Loss = -10160.891560022745
Iteration 5400: Loss = -10160.889000443993
Iteration 5500: Loss = -10160.886886608312
Iteration 5600: Loss = -10160.885115064428
Iteration 5700: Loss = -10160.883631034121
Iteration 5800: Loss = -10160.882276168491
Iteration 5900: Loss = -10160.881053721509
Iteration 6000: Loss = -10160.880007788777
Iteration 6100: Loss = -10160.878997222084
Iteration 6200: Loss = -10160.878050498162
Iteration 6300: Loss = -10160.877168865147
Iteration 6400: Loss = -10160.876204817538
Iteration 6500: Loss = -10160.867942047671
Iteration 6600: Loss = -10159.991767041742
Iteration 6700: Loss = -10159.987575966628
Iteration 6800: Loss = -10159.986701949225
Iteration 6900: Loss = -10159.986170673541
Iteration 7000: Loss = -10159.985677087447
Iteration 7100: Loss = -10159.985164437292
Iteration 7200: Loss = -10159.984482028158
Iteration 7300: Loss = -10159.886079474403
Iteration 7400: Loss = -10159.751674730709
Iteration 7500: Loss = -10159.660380536261
Iteration 7600: Loss = -10159.624744580842
Iteration 7700: Loss = -10159.225476066993
Iteration 7800: Loss = -10159.167246976529
Iteration 7900: Loss = -10159.121467105017
Iteration 8000: Loss = -10159.118691759217
Iteration 8100: Loss = -10159.114941506115
Iteration 8200: Loss = -10158.974081242965
Iteration 8300: Loss = -10158.982738433926
1
Iteration 8400: Loss = -10158.964897695074
Iteration 8500: Loss = -10158.940446831872
Iteration 8600: Loss = -10158.876793672598
Iteration 8700: Loss = -10158.834148256412
Iteration 8800: Loss = -10158.741042113877
Iteration 8900: Loss = -10158.698706293828
Iteration 9000: Loss = -10158.669038412134
Iteration 9100: Loss = -10158.635798455869
Iteration 9200: Loss = -10158.617255403815
Iteration 9300: Loss = -10158.589428205292
Iteration 9400: Loss = -10158.538938078938
Iteration 9500: Loss = -10158.488092515016
Iteration 9600: Loss = -10158.437518845154
Iteration 9700: Loss = -10158.417161358408
Iteration 9800: Loss = -10158.400457774867
Iteration 9900: Loss = -10158.370362453203
Iteration 10000: Loss = -10158.34740037878
Iteration 10100: Loss = -10158.343249775593
Iteration 10200: Loss = -10158.403294170103
1
Iteration 10300: Loss = -10158.333215290868
Iteration 10400: Loss = -10158.44003692161
1
Iteration 10500: Loss = -10158.323284404589
Iteration 10600: Loss = -10158.32038039906
Iteration 10700: Loss = -10158.319757214054
Iteration 10800: Loss = -10158.316740766944
Iteration 10900: Loss = -10158.315473811577
Iteration 11000: Loss = -10158.317428159737
1
Iteration 11100: Loss = -10158.314895108775
Iteration 11200: Loss = -10158.373041529614
1
Iteration 11300: Loss = -10158.376148909061
2
Iteration 11400: Loss = -10158.340517137523
3
Iteration 11500: Loss = -10158.311801852658
Iteration 11600: Loss = -10158.311449347
Iteration 11700: Loss = -10158.31631541008
1
Iteration 11800: Loss = -10158.3258994007
2
Iteration 11900: Loss = -10158.31050386421
Iteration 12000: Loss = -10158.312245808096
1
Iteration 12100: Loss = -10158.318008136719
2
Iteration 12200: Loss = -10158.31018056853
Iteration 12300: Loss = -10158.310284253426
1
Iteration 12400: Loss = -10158.378167056575
2
Iteration 12500: Loss = -10158.35370182662
3
Iteration 12600: Loss = -10158.31072164962
4
Iteration 12700: Loss = -10158.310142129256
Iteration 12800: Loss = -10158.31074362163
1
Iteration 12900: Loss = -10158.310167636419
2
Iteration 13000: Loss = -10158.313691166948
3
Iteration 13100: Loss = -10158.316211848212
4
Iteration 13200: Loss = -10158.316569388471
5
Iteration 13300: Loss = -10158.320951978478
6
Iteration 13400: Loss = -10158.331113255212
7
Iteration 13500: Loss = -10158.309996151682
Iteration 13600: Loss = -10158.313382072321
1
Iteration 13700: Loss = -10158.309993242334
Iteration 13800: Loss = -10158.310086026328
1
Iteration 13900: Loss = -10158.342780814677
2
Iteration 14000: Loss = -10158.314621130372
3
Iteration 14100: Loss = -10158.309935668447
Iteration 14200: Loss = -10158.311328516365
1
Iteration 14300: Loss = -10158.31140062142
2
Iteration 14400: Loss = -10158.328071856047
3
Iteration 14500: Loss = -10158.318511017762
4
Iteration 14600: Loss = -10158.310040805209
5
Iteration 14700: Loss = -10158.310096888807
6
Iteration 14800: Loss = -10158.31530902219
7
Iteration 14900: Loss = -10158.32601566261
8
Iteration 15000: Loss = -10158.354382703776
9
Iteration 15100: Loss = -10158.309867001859
Iteration 15200: Loss = -10158.309888507898
1
Iteration 15300: Loss = -10158.401609024364
2
Iteration 15400: Loss = -10158.35355588605
3
Iteration 15500: Loss = -10158.309880419782
4
Iteration 15600: Loss = -10158.309990720263
5
Iteration 15700: Loss = -10158.323248339339
6
Iteration 15800: Loss = -10158.316580665338
7
Iteration 15900: Loss = -10158.309970679096
8
Iteration 16000: Loss = -10158.311926293814
9
Iteration 16100: Loss = -10158.313078750938
10
Stopping early at iteration 16100 due to no improvement.
tensor([[ 3.6878, -5.1426],
        [ 3.1516, -5.4820],
        [ 3.6977, -5.0887],
        [ 3.5792, -5.0884],
        [ 3.4203, -5.1582],
        [ 3.2223, -5.4335],
        [ 3.4544, -5.2079],
        [ 3.6098, -5.0256],
        [ 3.2353, -5.4608],
        [ 3.5583, -5.1197],
        [ 3.4346, -5.2877],
        [ 3.2957, -5.3484],
        [ 3.6861, -5.0779],
        [ 3.4993, -5.2011],
        [ 3.5760, -5.1782],
        [ 3.5253, -5.0814],
        [ 3.6023, -5.1641],
        [ 3.2265, -5.4029],
        [ 3.3194, -5.3846],
        [ 3.4000, -5.3031],
        [ 3.5134, -5.2561],
        [ 3.3318, -5.4369],
        [ 3.5599, -5.1898],
        [ 3.4918, -5.2870],
        [ 2.0981, -6.7133],
        [ 3.5730, -5.0839],
        [ 3.1630, -5.6185],
        [ 3.6273, -5.1978],
        [ 3.5798, -5.0436],
        [ 2.8784, -5.8319],
        [ 3.6261, -5.0955],
        [ 3.4571, -5.2797],
        [ 3.4294, -5.2599],
        [ 3.6294, -5.0659],
        [ 3.6549, -5.1611],
        [ 2.8725, -5.8775],
        [ 3.4691, -5.2529],
        [ 3.6300, -5.2033],
        [ 3.6165, -5.1528],
        [ 3.5672, -5.0445],
        [ 3.2869, -5.5268],
        [ 2.2009, -6.4791],
        [ 3.6261, -5.0891],
        [ 3.4892, -5.1680],
        [ 3.6410, -5.0295],
        [ 3.6095, -5.1942],
        [ 2.6284, -6.0761],
        [ 3.6530, -5.0410],
        [ 3.3014, -5.4387],
        [ 3.5594, -5.1761],
        [ 2.0166, -6.6318],
        [ 3.6066, -4.9975],
        [ 3.4269, -5.2214],
        [ 3.6672, -5.1390],
        [ 3.1626, -5.5863],
        [ 3.0987, -5.6646],
        [ 3.1343, -5.6688],
        [ 3.6594, -5.1044],
        [ 3.6513, -5.0400],
        [ 3.6279, -5.0253],
        [ 3.5860, -5.1510],
        [ 3.5725, -5.1375],
        [ 3.5994, -4.9884],
        [ 2.6242, -6.0962],
        [ 3.6223, -5.1044],
        [ 3.6341, -5.1548],
        [ 3.6554, -5.0837],
        [ 3.6730, -5.0598],
        [ 3.3673, -5.4406],
        [ 3.6317, -5.0534],
        [ 3.4531, -5.2870],
        [ 3.4279, -5.1658],
        [ 3.4948, -5.1947],
        [ 2.6132, -6.0840],
        [ 3.4539, -5.2072],
        [ 2.8865, -5.8492],
        [ 2.8120, -5.8704],
        [ 3.6843, -5.0926],
        [ 2.1446, -6.6268],
        [ 3.5542, -5.1412],
        [ 3.5714, -4.9891],
        [ 3.3601, -5.4095],
        [ 3.6527, -5.0400],
        [ 3.5895, -5.0960],
        [ 3.6134, -5.0679],
        [ 2.0673, -6.6825],
        [ 3.2868, -5.3576],
        [ 3.6487, -5.0665],
        [ 2.6642, -6.0070],
        [ 3.5951, -5.0759],
        [ 3.0288, -5.7205],
        [ 2.4931, -6.1427],
        [ 3.5532, -5.1292],
        [ 3.5546, -5.0805],
        [ 3.2885, -5.4661],
        [ 3.6643, -5.0508],
        [ 3.3876, -5.3345],
        [ 3.5498, -5.1397],
        [ 3.6397, -5.0560],
        [ 3.4663, -5.2141]], dtype=torch.float64, requires_grad=True)
pi: tensor([[4.5295e-06, 1.0000e+00],
        [3.5228e-02, 9.6477e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9983e-01, 1.6511e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1358, 0.1374],
         [0.0415, 0.1446]],

        [[0.6721, 0.1717],
         [0.5006, 0.5698]],

        [[0.2156, 0.2106],
         [0.0597, 0.1507]],

        [[0.0934, 0.0637],
         [0.1401, 0.2268]],

        [[0.5841, 0.2071],
         [0.2942, 0.6369]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.012494332208171696
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
Global Adjusted Rand Index: 0.014078065238772875
Average Adjusted Rand Index: -0.003505880593303584
Iteration 0: Loss = -40044.291573143615
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.0221,    nan]],

        [[0.2830,    nan],
         [0.1998, 0.2700]],

        [[0.1525,    nan],
         [0.5737, 0.4321]],

        [[0.3884,    nan],
         [0.1282, 0.2001]],

        [[0.7274,    nan],
         [0.4793, 0.5896]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 37
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40045.37623544441
Iteration 100: Loss = -10293.42332225052
Iteration 200: Loss = -10228.081482993724
Iteration 300: Loss = -10206.034327131376
Iteration 400: Loss = -10190.488260470904
Iteration 500: Loss = -10187.28110971376
Iteration 600: Loss = -10183.957162240902
Iteration 700: Loss = -10173.293876832826
Iteration 800: Loss = -10171.731706274384
Iteration 900: Loss = -10170.828543563068
Iteration 1000: Loss = -10166.635654403437
Iteration 1100: Loss = -10165.647493281369
Iteration 1200: Loss = -10165.170689157283
Iteration 1300: Loss = -10164.766173236632
Iteration 1400: Loss = -10164.403259141274
Iteration 1500: Loss = -10164.100381143051
Iteration 1600: Loss = -10163.855818863216
Iteration 1700: Loss = -10163.65882739432
Iteration 1800: Loss = -10163.4985099737
Iteration 1900: Loss = -10163.363936816242
Iteration 2000: Loss = -10163.248194756512
Iteration 2100: Loss = -10163.147040705157
Iteration 2200: Loss = -10163.057309779631
Iteration 2300: Loss = -10162.976955221366
Iteration 2400: Loss = -10162.904364180331
Iteration 2500: Loss = -10162.838429179299
Iteration 2600: Loss = -10162.778125827435
Iteration 2700: Loss = -10162.722609025279
Iteration 2800: Loss = -10162.671354327307
Iteration 2900: Loss = -10162.624036005664
Iteration 3000: Loss = -10162.579930975786
Iteration 3100: Loss = -10162.536666679684
Iteration 3200: Loss = -10162.487208069528
Iteration 3300: Loss = -10162.435914657013
Iteration 3400: Loss = -10162.396124764331
Iteration 3500: Loss = -10162.363454711467
Iteration 3600: Loss = -10162.334778836825
Iteration 3700: Loss = -10162.308394893613
Iteration 3800: Loss = -10162.282620776929
Iteration 3900: Loss = -10162.252756948272
Iteration 4000: Loss = -10162.172632115244
Iteration 4100: Loss = -10162.09281284106
Iteration 4200: Loss = -10162.05645852808
Iteration 4300: Loss = -10162.019981185373
Iteration 4400: Loss = -10161.981188116471
Iteration 4500: Loss = -10161.93863237699
Iteration 4600: Loss = -10161.890959095583
Iteration 4700: Loss = -10161.837285520181
Iteration 4800: Loss = -10161.778122143689
Iteration 4900: Loss = -10161.716818122804
Iteration 5000: Loss = -10161.652983119502
Iteration 5100: Loss = -10161.55059818145
Iteration 5200: Loss = -10161.518555675051
Iteration 5300: Loss = -10161.504590426115
Iteration 5400: Loss = -10161.49631198122
Iteration 5500: Loss = -10161.490340195833
Iteration 5600: Loss = -10161.48529354362
Iteration 5700: Loss = -10161.48091701735
Iteration 5800: Loss = -10161.47701598853
Iteration 5900: Loss = -10161.473523666631
Iteration 6000: Loss = -10161.470265282387
Iteration 6100: Loss = -10161.467209432327
Iteration 6200: Loss = -10161.46439075751
Iteration 6300: Loss = -10161.461732430897
Iteration 6400: Loss = -10161.459171828199
Iteration 6500: Loss = -10161.45664309976
Iteration 6600: Loss = -10161.453931838543
Iteration 6700: Loss = -10161.45016294699
Iteration 6800: Loss = -10161.436466255822
Iteration 6900: Loss = -10161.263258826591
Iteration 7000: Loss = -10161.25840384116
Iteration 7100: Loss = -10161.25639521872
Iteration 7200: Loss = -10161.25476241484
Iteration 7300: Loss = -10161.27786949147
1
Iteration 7400: Loss = -10161.251687878701
Iteration 7500: Loss = -10161.250214882237
Iteration 7600: Loss = -10161.466905949943
1
Iteration 7700: Loss = -10161.246843810037
Iteration 7800: Loss = -10161.24303443785
Iteration 7900: Loss = -10161.22426798055
Iteration 8000: Loss = -10161.223091939717
Iteration 8100: Loss = -10161.220597043273
Iteration 8200: Loss = -10161.219693542636
Iteration 8300: Loss = -10161.268502287656
1
Iteration 8400: Loss = -10161.218104209858
Iteration 8500: Loss = -10161.217329001995
Iteration 8600: Loss = -10161.216633426586
Iteration 8700: Loss = -10161.216008588408
Iteration 8800: Loss = -10161.215364892698
Iteration 8900: Loss = -10161.214782205252
Iteration 9000: Loss = -10161.232960112487
1
Iteration 9100: Loss = -10161.213693607355
Iteration 9200: Loss = -10161.213202072464
Iteration 9300: Loss = -10161.213438592282
1
Iteration 9400: Loss = -10161.21233937588
Iteration 9500: Loss = -10161.211900209759
Iteration 9600: Loss = -10161.211464452352
Iteration 9700: Loss = -10161.21132191108
Iteration 9800: Loss = -10161.21075697079
Iteration 9900: Loss = -10161.21042705398
Iteration 10000: Loss = -10161.217200338086
1
Iteration 10100: Loss = -10161.209777681497
Iteration 10200: Loss = -10161.209503678665
Iteration 10300: Loss = -10161.543412226554
1
Iteration 10400: Loss = -10161.208948006966
Iteration 10500: Loss = -10161.208636437777
Iteration 10600: Loss = -10161.208254816938
Iteration 10700: Loss = -10161.20733786686
Iteration 10800: Loss = -10161.167983971867
Iteration 10900: Loss = -10161.005218900635
Iteration 11000: Loss = -10161.083259915647
1
Iteration 11100: Loss = -10160.957881511658
Iteration 11200: Loss = -10160.95788018788
Iteration 11300: Loss = -10160.957514975067
Iteration 11400: Loss = -10160.9581416631
1
Iteration 11500: Loss = -10160.957272267633
Iteration 11600: Loss = -10160.957111266638
Iteration 11700: Loss = -10160.959473919653
1
Iteration 11800: Loss = -10160.956825904483
Iteration 11900: Loss = -10160.95668984071
Iteration 12000: Loss = -10161.00611426
1
Iteration 12100: Loss = -10160.956512759582
Iteration 12200: Loss = -10160.956427055566
Iteration 12300: Loss = -10161.278603488136
1
Iteration 12400: Loss = -10160.956245281279
Iteration 12500: Loss = -10160.956144398537
Iteration 12600: Loss = -10160.956257757556
1
Iteration 12700: Loss = -10160.955997559453
Iteration 12800: Loss = -10161.022706892745
1
Iteration 12900: Loss = -10160.955663580513
Iteration 13000: Loss = -10160.834677356437
Iteration 13100: Loss = -10160.742486753028
Iteration 13200: Loss = -10160.720652736025
Iteration 13300: Loss = -10160.721106787609
1
Iteration 13400: Loss = -10160.712971658322
Iteration 13500: Loss = -10160.711988586114
Iteration 13600: Loss = -10160.181977625793
Iteration 13700: Loss = -10160.179602760993
Iteration 13800: Loss = -10160.179804916495
1
Iteration 13900: Loss = -10159.488907050563
Iteration 14000: Loss = -10159.418353583193
Iteration 14100: Loss = -10159.417221864622
Iteration 14200: Loss = -10159.086790382755
Iteration 14300: Loss = -10159.122152780663
1
Iteration 14400: Loss = -10158.89173154786
Iteration 14500: Loss = -10158.886332194092
Iteration 14600: Loss = -10158.885522642204
Iteration 14700: Loss = -10158.884458886534
Iteration 14800: Loss = -10158.8778711675
Iteration 14900: Loss = -10158.652704070304
Iteration 15000: Loss = -10158.592214936558
Iteration 15100: Loss = -10158.588031874355
Iteration 15200: Loss = -10158.512389924379
Iteration 15300: Loss = -10158.396939808334
Iteration 15400: Loss = -10158.318286212187
Iteration 15500: Loss = -10158.239177157122
Iteration 15600: Loss = -10158.175397795747
Iteration 15700: Loss = -10158.100006533474
Iteration 15800: Loss = -10158.055785306005
Iteration 15900: Loss = -10158.03882117313
Iteration 16000: Loss = -10158.061287125638
1
Iteration 16100: Loss = -10157.993997120544
Iteration 16200: Loss = -10157.972384816267
Iteration 16300: Loss = -10157.956793834554
Iteration 16400: Loss = -10157.935554023175
Iteration 16500: Loss = -10157.91348999169
Iteration 16600: Loss = -10157.90571776401
Iteration 16700: Loss = -10157.892050714245
Iteration 16800: Loss = -10157.890224675324
Iteration 16900: Loss = -10157.88294558544
Iteration 17000: Loss = -10157.891282992534
1
Iteration 17100: Loss = -10157.880791307916
Iteration 17200: Loss = -10157.876892163758
Iteration 17300: Loss = -10157.876736290047
Iteration 17400: Loss = -10157.879007366826
1
Iteration 17500: Loss = -10157.921729323138
2
Iteration 17600: Loss = -10157.875433249137
Iteration 17700: Loss = -10157.87488511627
Iteration 17800: Loss = -10157.893387068558
1
Iteration 17900: Loss = -10157.878130082103
2
Iteration 18000: Loss = -10157.876859572463
3
Iteration 18100: Loss = -10157.874767653224
Iteration 18200: Loss = -10157.879910683638
1
Iteration 18300: Loss = -10157.878714968134
2
Iteration 18400: Loss = -10157.876276527744
3
Iteration 18500: Loss = -10157.887489760447
4
Iteration 18600: Loss = -10157.874619630315
Iteration 18700: Loss = -10157.874843561807
1
Iteration 18800: Loss = -10157.878387283648
2
Iteration 18900: Loss = -10157.874528065038
Iteration 19000: Loss = -10157.876156965416
1
Iteration 19100: Loss = -10157.875987599202
2
Iteration 19200: Loss = -10157.89815759941
3
Iteration 19300: Loss = -10157.886815019845
4
Iteration 19400: Loss = -10157.882101350664
5
Iteration 19500: Loss = -10157.876864329473
6
Iteration 19600: Loss = -10157.874989164027
7
Iteration 19700: Loss = -10157.874456278312
Iteration 19800: Loss = -10157.882497009785
1
Iteration 19900: Loss = -10157.87582524867
2
tensor([[-1.8439e+00,  2.8882e-01],
        [-1.5577e+00,  1.6290e-01],
        [-1.7079e+00,  3.1918e-01],
        [-1.6054e+00,  1.7285e-01],
        [-2.0827e+00, -4.1611e-01],
        [-2.0528e+00, -3.0711e-01],
        [-1.8874e+00, -2.6697e-01],
        [-1.7838e+00, -4.3518e-02],
        [-1.8110e+00,  1.3448e-02],
        [-1.8574e+00, -3.1498e-02],
        [-1.7707e+00,  1.0770e-01],
        [-1.7137e+00,  1.8002e-02],
        [-1.7097e+00,  3.1004e-01],
        [-2.2755e+00, -4.4122e-01],
        [-2.6035e+00, -6.3174e-01],
        [-1.9434e+00, -2.7324e-01],
        [-1.6538e+00,  2.6747e-01],
        [-1.7031e+00, -6.8800e-03],
        [-1.8210e+00,  1.8961e-02],
        [-1.8115e+00,  6.3809e-02],
        [-1.7140e+00,  2.9506e-01],
        [-2.7586e+00, -7.5114e-01],
        [-1.8515e+00,  1.1585e-01],
        [-1.9203e+00,  9.7736e-02],
        [-1.9894e+00,  1.0235e-01],
        [-1.6442e+00,  2.1213e-01],
        [-1.7401e+00,  2.8154e-01],
        [-2.1140e+00, -3.0497e-02],
        [-1.5305e+00,  1.4414e-01],
        [-1.7806e+00,  8.3971e-02],
        [-1.6340e+00,  2.1132e-01],
        [-1.6812e+00,  2.8765e-01],
        [-2.2825e+00, -4.7709e-01],
        [-1.6002e+00,  2.1014e-01],
        [-1.7871e+00,  3.1113e-01],
        [-3.2906e+00, -1.3246e+00],
        [-2.7768e+00, -8.4204e-01],
        [-2.3985e+00, -2.4390e-01],
        [-1.6977e+00,  3.1119e-01],
        [-1.5057e+00,  1.1758e-01],
        [-1.7577e+00,  3.5629e-01],
        [-3.2322e+00, -1.3831e+00],
        [-2.2368e+00, -3.6613e-01],
        [-1.5873e+00,  1.7771e-01],
        [-1.7650e+00, -2.1567e-03],
        [-1.8275e+00,  2.3928e-01],
        [-1.6698e+00,  1.6894e-01],
        [-1.6405e+00,  2.4366e-01],
        [-1.9195e+00,  1.7262e-02],
        [-1.6853e+00,  2.6480e-01],
        [-2.1986e+00, -4.6194e-01],
        [-1.5310e+00,  1.1851e-01],
        [-2.0946e+00, -3.3473e-01],
        [-1.7287e+00,  3.4034e-01],
        [-2.1249e+00, -1.7648e-01],
        [-1.8269e+00,  1.5525e-01],
        [-1.8917e+00,  1.7187e-01],
        [-2.9136e+00, -9.3171e-01],
        [-1.9410e+00, -1.3875e-01],
        [-1.7180e+00, -1.0253e-02],
        [-1.6834e+00,  2.3057e-01],
        [-2.2059e+00, -3.5999e-01],
        [-1.5293e+00,  1.3574e-01],
        [-1.6430e+00,  2.5129e-01],
        [-1.9466e+00, -2.4007e-02],
        [-1.7605e+00,  2.8762e-01],
        [-1.8659e+00,  7.1401e-02],
        [-1.8988e+00,  4.8259e-02],
        [-2.3554e+00, -2.6708e-01],
        [-2.6475e+00, -8.2960e-01],
        [-2.7376e+00, -8.2003e-01],
        [-1.6717e+00,  5.0527e-02],
        [-3.2190e+00, -1.3962e+00],
        [-2.0136e+00, -1.6454e-01],
        [-1.5782e+00,  1.9135e-01],
        [-1.7309e+00,  1.8204e-01],
        [-1.6892e+00,  1.2452e-01],
        [-1.9620e+00,  5.5118e-02],
        [-1.9088e+00,  1.5348e-01],
        [-1.7177e+00,  1.1101e-01],
        [-2.3657e+00, -7.6974e-01],
        [-1.8741e+00,  1.3525e-01],
        [-1.6826e+00,  1.7964e-01],
        [-1.7959e+00,  4.2135e-03],
        [-1.9459e+00, -1.5193e-01],
        [-3.2907e+00, -1.3245e+00],
        [-1.5603e+00,  1.7340e-01],
        [-2.2346e+00, -3.6487e-01],
        [-2.2058e+00, -4.4024e-01],
        [-1.9287e+00, -1.8400e-01],
        [-3.2907e+00, -1.3245e+00],
        [-1.6072e+00,  1.3264e-01],
        [-2.8862e+00, -1.0727e+00],
        [-1.7070e+00,  1.4647e-02],
        [-1.8110e+00,  1.4281e-01],
        [-1.8052e+00,  6.4544e-02],
        [-1.6746e+00,  2.0316e-01],
        [-1.9062e+00, -1.0260e-01],
        [-2.0321e+00, -1.6574e-01],
        [-1.6171e+00,  1.5798e-01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.5944e-01, 4.0557e-02],
        [1.0000e+00, 3.8329e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1336, 0.8664], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1444, 0.1385],
         [0.0221, 0.1347]],

        [[0.2830, 0.1731],
         [0.1998, 0.2700]],

        [[0.1525, 0.2038],
         [0.5737, 0.4321]],

        [[0.3884, 0.0652],
         [0.1282, 0.2001]],

        [[0.7274, 0.2042],
         [0.4793, 0.5896]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.012494332208171696
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: -0.008233622622776837
Global Adjusted Rand Index: 0.014078065238772875
Average Adjusted Rand Index: -0.003505880593303584
10236.122192207491
new:  [-0.00021554131597433836, 0.015187927434794907, 0.014078065238772875, 0.014078065238772875] [-0.002646871147114132, -0.0023386828990683297, -0.003505880593303584, -0.003505880593303584] [10158.173613212919, 10157.883006701797, 10158.313078750938, 10157.87827381589]
prior:  [0.0, -0.004215919502146915, -0.0013212301113857405, 0.0] [0.0, -0.00621622959700246, -0.003505880593303584, 0.0] [nan, 10159.025014859242, 10158.68834232287, nan]
-----------------------------------------------------------------------------------------
This iteration is 6
True Objective function: Loss = -10001.991394403987
Iteration 0: Loss = -55702.43249259365
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.5883,    nan]],

        [[0.2801,    nan],
         [0.7594, 0.4044]],

        [[0.1599,    nan],
         [0.6094, 0.2161]],

        [[0.0101,    nan],
         [0.4637, 0.8293]],

        [[0.7289,    nan],
         [0.3913, 0.0720]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -54052.06858184475
Iteration 100: Loss = -9901.254001718808
Iteration 200: Loss = -9896.233573579413
Iteration 300: Loss = -9894.184267800803
Iteration 400: Loss = -9893.200413414143
Iteration 500: Loss = -9892.663089806249
Iteration 600: Loss = -9892.336138644901
Iteration 700: Loss = -9892.119097068009
Iteration 800: Loss = -9891.964183125063
Iteration 900: Loss = -9891.847456508196
Iteration 1000: Loss = -9891.75749689453
Iteration 1100: Loss = -9891.685366449216
Iteration 1200: Loss = -9891.624985854889
Iteration 1300: Loss = -9891.572441822558
Iteration 1400: Loss = -9891.524550235232
Iteration 1500: Loss = -9891.477753760817
Iteration 1600: Loss = -9891.426337979536
Iteration 1700: Loss = -9891.35620089666
Iteration 1800: Loss = -9891.23399446155
Iteration 1900: Loss = -9891.096617599303
Iteration 2000: Loss = -9891.009885436399
Iteration 2100: Loss = -9890.914467175005
Iteration 2200: Loss = -9890.830700828335
Iteration 2300: Loss = -9890.735427088972
Iteration 2400: Loss = -9890.596260939872
Iteration 2500: Loss = -9890.299708934039
Iteration 2600: Loss = -9889.86173200268
Iteration 2700: Loss = -9889.602720006196
Iteration 2800: Loss = -9889.440242333785
Iteration 2900: Loss = -9889.322214903039
Iteration 3000: Loss = -9889.230093275615
Iteration 3100: Loss = -9889.151757164493
Iteration 3200: Loss = -9889.072528275214
Iteration 3300: Loss = -9888.959057137463
Iteration 3400: Loss = -9888.833862221696
Iteration 3500: Loss = -9888.761496923804
Iteration 3600: Loss = -9888.69843193357
Iteration 3700: Loss = -9888.630559228686
Iteration 3800: Loss = -9888.530477417491
Iteration 3900: Loss = -9887.692424608795
Iteration 4000: Loss = -9887.262187134456
Iteration 4100: Loss = -9886.957089265652
Iteration 4200: Loss = -9886.733201161722
Iteration 4300: Loss = -9886.564099061046
Iteration 4400: Loss = -9886.424430609857
Iteration 4500: Loss = -9886.289109700836
Iteration 4600: Loss = -9886.173152943733
Iteration 4700: Loss = -9886.072509502237
Iteration 4800: Loss = -9885.983917050744
Iteration 4900: Loss = -9885.90513317382
Iteration 5000: Loss = -9885.845948069904
Iteration 5100: Loss = -9885.795799374626
Iteration 5200: Loss = -9885.739471341196
Iteration 5300: Loss = -9885.624315824745
Iteration 5400: Loss = -9885.486163554882
Iteration 5500: Loss = -9885.416918550647
Iteration 5600: Loss = -9885.386055759667
Iteration 5700: Loss = -9885.362904878779
Iteration 5800: Loss = -9885.3452810034
Iteration 5900: Loss = -9885.326902326433
Iteration 6000: Loss = -9885.306014154314
Iteration 6100: Loss = -9885.28226422204
Iteration 6200: Loss = -9885.265368527245
Iteration 6300: Loss = -9885.251770525947
Iteration 6400: Loss = -9885.234407355705
Iteration 6500: Loss = -9885.20679384963
Iteration 6600: Loss = -9885.184501706242
Iteration 6700: Loss = -9885.169082645394
Iteration 6800: Loss = -9885.15819883994
Iteration 6900: Loss = -9885.149519661429
Iteration 7000: Loss = -9885.14157698491
Iteration 7100: Loss = -9885.133661155356
Iteration 7200: Loss = -9885.123352180339
Iteration 7300: Loss = -9885.115236696813
Iteration 7400: Loss = -9885.10377099186
Iteration 7500: Loss = -9885.094907097919
Iteration 7600: Loss = -9885.08484310526
Iteration 7700: Loss = -9885.289097498968
1
Iteration 7800: Loss = -9885.06603131603
Iteration 7900: Loss = -9885.055783767317
Iteration 8000: Loss = -9885.04805488785
Iteration 8100: Loss = -9885.0343045441
Iteration 8200: Loss = -9885.04384668721
1
Iteration 8300: Loss = -9885.021064058405
Iteration 8400: Loss = -9885.021965993028
1
Iteration 8500: Loss = -9885.011615520987
Iteration 8600: Loss = -9885.010380995624
Iteration 8700: Loss = -9885.004109429656
Iteration 8800: Loss = -9885.00683207422
1
Iteration 8900: Loss = -9885.008876239921
2
Iteration 9000: Loss = -9884.987922868033
Iteration 9100: Loss = -9885.004542911189
1
Iteration 9200: Loss = -9884.98023633716
Iteration 9300: Loss = -9884.978055635931
Iteration 9400: Loss = -9884.975796691813
Iteration 9500: Loss = -9884.97389511357
Iteration 9600: Loss = -9884.974860799655
1
Iteration 9700: Loss = -9884.970769787253
Iteration 9800: Loss = -9884.968509892313
Iteration 9900: Loss = -9884.971059935406
1
Iteration 10000: Loss = -9884.962503482628
Iteration 10100: Loss = -9884.96138959984
Iteration 10200: Loss = -9884.966358997674
1
Iteration 10300: Loss = -9884.956726132032
Iteration 10400: Loss = -9884.95480678449
Iteration 10500: Loss = -9884.95383962613
Iteration 10600: Loss = -9884.952289567056
Iteration 10700: Loss = -9884.97431911332
1
Iteration 10800: Loss = -9884.949745483455
Iteration 10900: Loss = -9884.947943094263
Iteration 11000: Loss = -9884.944646890686
Iteration 11100: Loss = -9884.961441068626
1
Iteration 11200: Loss = -9884.943295805766
Iteration 11300: Loss = -9884.950209097044
1
Iteration 11400: Loss = -9884.941860880435
Iteration 11500: Loss = -9884.9417600993
Iteration 11600: Loss = -9884.957747594015
1
Iteration 11700: Loss = -9884.941098817073
Iteration 11800: Loss = -9884.94258337125
1
Iteration 11900: Loss = -9885.00714406193
2
Iteration 12000: Loss = -9884.949360020528
3
Iteration 12100: Loss = -9884.939685556563
Iteration 12200: Loss = -9884.942084351413
1
Iteration 12300: Loss = -9884.938908391709
Iteration 12400: Loss = -9884.939096103455
1
Iteration 12500: Loss = -9884.938399221111
Iteration 12600: Loss = -9885.031311689503
1
Iteration 12700: Loss = -9884.938005285014
Iteration 12800: Loss = -9884.937833652813
Iteration 12900: Loss = -9884.938585633865
1
Iteration 13000: Loss = -9884.937469229257
Iteration 13100: Loss = -9884.93729408489
Iteration 13200: Loss = -9884.937247995023
Iteration 13300: Loss = -9884.936974434388
Iteration 13400: Loss = -9884.93725538466
1
Iteration 13500: Loss = -9884.9367494161
Iteration 13600: Loss = -9884.942885740133
1
Iteration 13700: Loss = -9884.93664218885
Iteration 13800: Loss = -9884.94661879245
1
Iteration 13900: Loss = -9884.936470618015
Iteration 14000: Loss = -9884.936911671275
1
Iteration 14100: Loss = -9885.020217928737
2
Iteration 14200: Loss = -9884.936433072011
Iteration 14300: Loss = -9884.936255583269
Iteration 14400: Loss = -9884.939535749074
1
Iteration 14500: Loss = -9884.939102902414
2
Iteration 14600: Loss = -9884.936008015637
Iteration 14700: Loss = -9884.937916456644
1
Iteration 14800: Loss = -9884.935796397365
Iteration 14900: Loss = -9884.935790665866
Iteration 15000: Loss = -9884.935763343281
Iteration 15100: Loss = -9884.935604061997
Iteration 15200: Loss = -9884.935468334901
Iteration 15300: Loss = -9884.93636064523
1
Iteration 15400: Loss = -9884.93530027697
Iteration 15500: Loss = -9884.936621347219
1
Iteration 15600: Loss = -9884.935658794373
2
Iteration 15700: Loss = -9885.018056284653
3
Iteration 15800: Loss = -9884.941465678221
4
Iteration 15900: Loss = -9885.082402463695
5
Iteration 16000: Loss = -9884.941445420794
6
Iteration 16100: Loss = -9884.95405014898
7
Iteration 16200: Loss = -9884.93546468291
8
Iteration 16300: Loss = -9884.935193152502
Iteration 16400: Loss = -9884.934959277643
Iteration 16500: Loss = -9884.934658038366
Iteration 16600: Loss = -9884.934752209061
1
Iteration 16700: Loss = -9884.93456703505
Iteration 16800: Loss = -9884.934623701494
1
Iteration 16900: Loss = -9884.937036062
2
Iteration 17000: Loss = -9884.939801370805
3
Iteration 17100: Loss = -9884.935021648778
4
Iteration 17200: Loss = -9884.947326109235
5
Iteration 17300: Loss = -9884.947438039148
6
Iteration 17400: Loss = -9884.934561723921
Iteration 17500: Loss = -9884.938064180362
1
Iteration 17600: Loss = -9884.981143241985
2
Iteration 17700: Loss = -9884.934516799613
Iteration 17800: Loss = -9884.934605018474
1
Iteration 17900: Loss = -9885.009177742155
2
Iteration 18000: Loss = -9884.934424387771
Iteration 18100: Loss = -9884.945685261271
1
Iteration 18200: Loss = -9884.93438636037
Iteration 18300: Loss = -9884.934751706282
1
Iteration 18400: Loss = -9884.934381547475
Iteration 18500: Loss = -9884.934542501027
1
Iteration 18600: Loss = -9885.073912162708
2
Iteration 18700: Loss = -9884.933985489812
Iteration 18800: Loss = -9884.93499691928
1
Iteration 18900: Loss = -9884.94237828233
2
Iteration 19000: Loss = -9884.934339468256
3
Iteration 19100: Loss = -9884.979084815257
4
Iteration 19200: Loss = -9884.950236930217
5
Iteration 19300: Loss = -9884.933344931864
Iteration 19400: Loss = -9884.93459825599
1
Iteration 19500: Loss = -9884.933129012246
Iteration 19600: Loss = -9884.93415218624
1
Iteration 19700: Loss = -9884.933760147962
2
Iteration 19800: Loss = -9884.933184065538
3
Iteration 19900: Loss = -9884.933086395804
tensor([[ 5.4644e+00, -1.0080e+01],
        [ 4.4005e-01, -5.0553e+00],
        [ 1.8398e+00, -6.4550e+00],
        [ 1.3218e+00, -5.9370e+00],
        [ 3.4711e-01, -4.9623e+00],
        [ 1.4917e+00, -6.1069e+00],
        [ 2.6707e-01, -4.8823e+00],
        [ 5.4003e+00, -1.0016e+01],
        [ 5.4578e+00, -1.0073e+01],
        [ 5.7290e+00, -1.0344e+01],
        [ 5.7797e+00, -1.0395e+01],
        [ 4.1512e+00, -8.7665e+00],
        [-2.8783e+00, -1.7369e+00],
        [ 5.3733e+00, -9.9885e+00],
        [-1.7491e-01, -4.4403e+00],
        [-1.4006e+00, -3.2146e+00],
        [ 1.5198e+00, -6.1350e+00],
        [-4.1854e+00, -4.2979e-01],
        [ 1.4847e+00, -6.0999e+00],
        [ 5.5623e+00, -1.0177e+01],
        [ 5.3120e+00, -9.9273e+00],
        [ 2.2591e+00, -6.8744e+00],
        [ 5.0373e+00, -9.6525e+00],
        [ 5.4866e+00, -1.0102e+01],
        [-2.0247e+00, -2.5905e+00],
        [ 1.9722e+00, -6.5874e+00],
        [ 3.2084e+00, -7.8236e+00],
        [ 3.6738e+00, -8.2891e+00],
        [ 1.7368e+00, -6.3520e+00],
        [-4.0019e+00, -6.1336e-01],
        [ 3.0214e-01, -4.9174e+00],
        [ 1.1637e-01, -4.7316e+00],
        [ 1.6088e+00, -6.2241e+00],
        [ 2.2268e-01, -4.8379e+00],
        [ 4.9232e+00, -9.5384e+00],
        [ 3.1734e-01, -4.9326e+00],
        [ 7.1389e-01, -5.3291e+00],
        [-2.7240e+00, -1.8913e+00],
        [-1.7687e-01, -4.4383e+00],
        [ 8.2938e-01, -5.4446e+00],
        [ 5.5671e+00, -1.0182e+01],
        [ 2.2046e+00, -6.8198e+00],
        [ 5.4444e+00, -1.0060e+01],
        [-1.0500e+00, -3.5652e+00],
        [ 6.9107e-01, -5.3063e+00],
        [ 5.3716e+00, -9.9869e+00],
        [ 3.8427e-01, -4.9995e+00],
        [-7.9346e-01, -3.8218e+00],
        [ 2.1756e+00, -6.7908e+00],
        [ 1.6153e+00, -6.2305e+00],
        [-9.2133e-01, -3.6939e+00],
        [-8.1407e-01, -3.8012e+00],
        [ 4.0387e-01, -5.0191e+00],
        [ 5.0885e+00, -9.7037e+00],
        [-1.2576e+00, -3.3576e+00],
        [ 2.1310e-02, -4.6365e+00],
        [-6.2092e-01, -3.9943e+00],
        [-7.2404e-01, -3.8912e+00],
        [ 5.1427e+00, -9.7579e+00],
        [-8.3635e-01, -3.7789e+00],
        [ 2.2946e-01, -4.8447e+00],
        [ 5.7044e+00, -1.0320e+01],
        [-2.6017e+00, -2.0136e+00],
        [-3.7676e-01, -4.2385e+00],
        [ 5.8394e+00, -1.0455e+01],
        [ 3.3597e-01, -4.9512e+00],
        [-3.6329e+00, -9.8233e-01],
        [ 4.5945e-01, -5.0747e+00],
        [ 7.4347e-01, -5.3587e+00],
        [ 1.4633e+00, -6.0785e+00],
        [ 5.9522e-01, -5.2104e+00],
        [ 5.4650e+00, -1.0080e+01],
        [ 3.5647e-01, -4.9717e+00],
        [-1.4112e+00, -3.2040e+00],
        [-1.1514e+00, -3.4638e+00],
        [-3.6602e+00, -9.5503e-01],
        [-7.9360e-01, -3.8216e+00],
        [ 1.3593e+00, -5.9746e+00],
        [ 6.6339e-01, -5.2786e+00],
        [ 5.4375e+00, -1.0053e+01],
        [ 6.6886e-03, -4.6219e+00],
        [ 4.7621e+00, -9.3773e+00],
        [ 7.9167e-01, -5.4069e+00],
        [ 1.6258e+00, -6.2410e+00],
        [ 5.5063e+00, -1.0122e+01],
        [ 5.5555e+00, -1.0171e+01],
        [-2.4820e-01, -4.3670e+00],
        [-1.1935e+00, -3.4217e+00],
        [ 8.6698e-02, -4.7019e+00],
        [ 5.7292e+00, -1.0344e+01],
        [-3.2307e-01, -4.2921e+00],
        [ 2.0804e-01, -4.8233e+00],
        [ 1.2754e+00, -5.8906e+00],
        [ 1.2503e+00, -5.8655e+00],
        [ 1.1488e+00, -5.7640e+00],
        [ 1.5752e+00, -6.1904e+00],
        [-9.6724e-01, -3.6480e+00],
        [-4.4453e+00, -1.6994e-01],
        [ 2.3034e+00, -6.9186e+00],
        [ 5.4433e+00, -1.0059e+01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.7659e-08],
        [1.7005e-05, 9.9998e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9152, 0.0848], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1387, 0.0734],
         [0.5883, 0.1684]],

        [[0.2801, 0.1203],
         [0.7594, 0.4044]],

        [[0.1599, 0.1139],
         [0.6094, 0.2161]],

        [[0.0101, 0.1893],
         [0.4637, 0.8293]],

        [[0.7289, 0.1381],
         [0.3913, 0.0720]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0019137235069308285
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.006981610987683993
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0023115331287292215
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002508217918958077
Global Adjusted Rand Index: 0.002367361438221298
Average Adjusted Rand Index: -0.0009651534012890775
Iteration 0: Loss = -37195.63822906543
Iteration 10: Loss = -9891.468164739312
Iteration 20: Loss = -9891.46816761692
1
Iteration 30: Loss = -9891.467652380456
Iteration 40: Loss = -9891.324715136821
Iteration 50: Loss = -9890.405731109357
Iteration 60: Loss = -9887.645507215568
Iteration 70: Loss = -9886.800483473966
Iteration 80: Loss = -9885.527262084192
Iteration 90: Loss = -9885.402113923132
Iteration 100: Loss = -9885.389772131273
Iteration 110: Loss = -9885.330520284988
Iteration 120: Loss = -9885.331426009885
1
Iteration 130: Loss = -9885.330697545527
2
Iteration 140: Loss = -9885.330337927928
Iteration 150: Loss = -9885.330201517912
Iteration 160: Loss = -9885.33015288953
Iteration 170: Loss = -9885.33011462444
Iteration 180: Loss = -9885.33009000195
Iteration 190: Loss = -9885.330101083991
1
Iteration 200: Loss = -9885.33005858052
Iteration 210: Loss = -9885.330088897075
1
Iteration 220: Loss = -9885.33008325403
2
Iteration 230: Loss = -9885.330082841054
3
Stopping early at iteration 229 due to no improvement.
pi: tensor([[0.9511, 0.0489],
        [0.9879, 0.0121]], dtype=torch.float64)
alpha: tensor([0.9523, 0.0477])
beta: tensor([[[0.1383, 0.0632],
         [0.0865, 0.0575]],

        [[0.9549, 0.0861],
         [0.8606, 0.5765]],

        [[0.3624, 0.1072],
         [0.7789, 0.1645]],

        [[0.7055, 0.2243],
         [0.9677, 0.5499]],

        [[0.3357, 0.0869],
         [0.6069, 0.7668]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.0003905551290195141
Average Adjusted Rand Index: -0.001964494053204748
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37194.2186143368
Iteration 100: Loss = -9977.658882823827
Iteration 200: Loss = -9936.741065481036
Iteration 300: Loss = -9914.488274108186
Iteration 400: Loss = -9901.587728568353
Iteration 500: Loss = -9894.944175391343
Iteration 600: Loss = -9893.31440145964
Iteration 700: Loss = -9892.718521760477
Iteration 800: Loss = -9892.344524836835
Iteration 900: Loss = -9892.089717539971
Iteration 1000: Loss = -9891.90620143944
Iteration 1100: Loss = -9891.767652619788
Iteration 1200: Loss = -9891.658892476516
Iteration 1300: Loss = -9891.571160509317
Iteration 1400: Loss = -9891.498503168792
Iteration 1500: Loss = -9891.436448627823
Iteration 1600: Loss = -9891.380679685415
Iteration 1700: Loss = -9891.323418747217
Iteration 1800: Loss = -9891.218578448801
Iteration 1900: Loss = -9889.872000441355
Iteration 2000: Loss = -9889.429997685644
Iteration 2100: Loss = -9889.257618840113
Iteration 2200: Loss = -9889.134732916908
Iteration 2300: Loss = -9889.023155605817
Iteration 2400: Loss = -9888.893767618783
Iteration 2500: Loss = -9888.770844557544
Iteration 2600: Loss = -9888.677499787847
Iteration 2700: Loss = -9888.598093040606
Iteration 2800: Loss = -9888.525957941654
Iteration 2900: Loss = -9888.457550570372
Iteration 3000: Loss = -9888.38851570117
Iteration 3100: Loss = -9888.313729945532
Iteration 3200: Loss = -9888.234281684823
Iteration 3300: Loss = -9888.162664769703
Iteration 3400: Loss = -9888.1054055185
Iteration 3500: Loss = -9888.059858023922
Iteration 3600: Loss = -9888.022658799377
Iteration 3700: Loss = -9887.991549794342
Iteration 3800: Loss = -9887.965207211008
Iteration 3900: Loss = -9887.942586538718
Iteration 4000: Loss = -9887.923127943126
Iteration 4100: Loss = -9887.906310285287
Iteration 4200: Loss = -9887.891732663773
Iteration 4300: Loss = -9887.879129003968
Iteration 4400: Loss = -9887.868135532262
Iteration 4500: Loss = -9887.858603063965
Iteration 4600: Loss = -9887.85025088428
Iteration 4700: Loss = -9887.843030722723
Iteration 4800: Loss = -9887.836626377755
Iteration 4900: Loss = -9887.831017017548
Iteration 5000: Loss = -9887.826037445444
Iteration 5100: Loss = -9887.821596491503
Iteration 5200: Loss = -9887.817546375203
Iteration 5300: Loss = -9887.813798180141
Iteration 5400: Loss = -9887.810148463184
Iteration 5500: Loss = -9887.806288591883
Iteration 5600: Loss = -9887.801059689276
Iteration 5700: Loss = -9887.789379897575
Iteration 5800: Loss = -9887.716044122048
Iteration 5900: Loss = -9887.425274813191
Iteration 6000: Loss = -9887.388494020883
Iteration 6100: Loss = -9887.377345005096
Iteration 6200: Loss = -9887.371402861503
Iteration 6300: Loss = -9887.367533466271
Iteration 6400: Loss = -9887.364670078903
Iteration 6500: Loss = -9887.362446170479
Iteration 6600: Loss = -9887.360563246908
Iteration 6700: Loss = -9887.35897787652
Iteration 6800: Loss = -9887.357555637507
Iteration 6900: Loss = -9887.35624797827
Iteration 7000: Loss = -9887.354996406893
Iteration 7100: Loss = -9887.353655591933
Iteration 7200: Loss = -9887.352208323256
Iteration 7300: Loss = -9887.350291484732
Iteration 7400: Loss = -9887.338404506974
Iteration 7500: Loss = -9887.321088085775
Iteration 7600: Loss = -9887.192567443108
Iteration 7700: Loss = -9887.160229269533
Iteration 7800: Loss = -9887.097318639051
Iteration 7900: Loss = -9887.050974474854
Iteration 8000: Loss = -9886.923324216721
Iteration 8100: Loss = -9886.882186782914
Iteration 8200: Loss = -9886.834597625986
Iteration 8300: Loss = -9886.79393108348
Iteration 8400: Loss = -9886.735230346094
Iteration 8500: Loss = -9886.67948146616
Iteration 8600: Loss = -9886.574523181025
Iteration 8700: Loss = -9886.551312811496
Iteration 8800: Loss = -9886.481451707949
Iteration 8900: Loss = -9886.394154733469
Iteration 9000: Loss = -9886.328097301595
Iteration 9100: Loss = -9886.270547798696
Iteration 9200: Loss = -9886.217175789398
Iteration 9300: Loss = -9886.13596668439
Iteration 9400: Loss = -9886.113861180074
Iteration 9500: Loss = -9886.010143839496
Iteration 9600: Loss = -9885.915920061901
Iteration 9700: Loss = -9885.79587400602
Iteration 9800: Loss = -9885.670843573578
Iteration 9900: Loss = -9885.550685246017
Iteration 10000: Loss = -9885.365627971918
Iteration 10100: Loss = -9885.204106790978
Iteration 10200: Loss = -9885.029398480168
Iteration 10300: Loss = -9884.952905060876
Iteration 10400: Loss = -9884.951173338477
Iteration 10500: Loss = -9884.950864184126
Iteration 10600: Loss = -9884.952077201888
1
Iteration 10700: Loss = -9884.979719716302
2
Iteration 10800: Loss = -9884.950301869369
Iteration 10900: Loss = -9884.980139963398
1
Iteration 11000: Loss = -9884.940679194573
Iteration 11100: Loss = -9885.01323619692
1
Iteration 11200: Loss = -9884.940564391598
Iteration 11300: Loss = -9884.940521061098
Iteration 11400: Loss = -9884.941148717624
1
Iteration 11500: Loss = -9884.940415330653
Iteration 11600: Loss = -9884.940741803219
1
Iteration 11700: Loss = -9884.940436649766
2
Iteration 11800: Loss = -9884.94030901252
Iteration 11900: Loss = -9884.943808347545
1
Iteration 12000: Loss = -9884.940248266794
Iteration 12100: Loss = -9884.940222152076
Iteration 12200: Loss = -9884.991705692468
1
Iteration 12300: Loss = -9884.9401510008
Iteration 12400: Loss = -9885.000057933934
1
Iteration 12500: Loss = -9884.940133584005
Iteration 12600: Loss = -9885.124294284367
1
Iteration 12700: Loss = -9884.940076054636
Iteration 12800: Loss = -9884.941043703137
1
Iteration 12900: Loss = -9885.018315267009
2
Iteration 13000: Loss = -9884.987068923732
3
Iteration 13100: Loss = -9884.942020101886
4
Iteration 13200: Loss = -9884.940419576495
5
Iteration 13300: Loss = -9885.014920107657
6
Iteration 13400: Loss = -9884.939968876344
Iteration 13500: Loss = -9884.939944701639
Iteration 13600: Loss = -9884.941120189835
1
Iteration 13700: Loss = -9884.94010056915
2
Iteration 13800: Loss = -9884.953031294459
3
Iteration 13900: Loss = -9884.939904138811
Iteration 14000: Loss = -9885.135864974709
1
Iteration 14100: Loss = -9884.940437538262
2
Iteration 14200: Loss = -9884.94008660211
3
Iteration 14300: Loss = -9884.940382393215
4
Iteration 14400: Loss = -9884.946309376543
5
Iteration 14500: Loss = -9884.940117728715
6
Iteration 14600: Loss = -9884.940060500738
7
Iteration 14700: Loss = -9884.941656190194
8
Iteration 14800: Loss = -9885.112704456453
9
Iteration 14900: Loss = -9884.942600579467
10
Stopping early at iteration 14900 due to no improvement.
tensor([[ 4.2101, -5.9784],
        [ 2.0735, -3.8510],
        [ 6.4355, -9.4616],
        [ 2.3641, -6.9793],
        [ 0.6047, -2.7254],
        [ 2.2617, -3.6859],
        [ 1.4270, -3.0901],
        [ 3.1130, -4.6691],
        [ 2.1519, -5.2263],
        [ 6.4935, -8.2569],
        [ 4.9307, -6.4441],
        [ 2.6951, -4.2574],
        [-1.7490, -0.8408],
        [ 3.3435, -4.7385],
        [ 2.6988, -4.1105],
        [ 0.5617, -2.0334],
        [ 3.6156, -5.0216],
        [ 0.4123, -2.1517],
        [ 1.9584, -4.1837],
        [ 2.9959, -5.7641],
        [ 3.4401, -5.2926],
        [ 2.6918, -4.9677],
        [ 2.5333, -4.3551],
        [ 3.9871, -5.4725],
        [ 2.5479, -3.9343],
        [ 7.0151, -8.4370],
        [ 3.9296, -5.3179],
        [ 3.5323, -4.9244],
        [ 3.2649, -4.6719],
        [-0.9267, -1.8384],
        [ 2.2422, -3.7107],
        [ 1.1938, -4.1011],
        [ 1.6381, -4.2396],
        [ 3.3158, -4.7059],
        [ 1.6480, -5.4057],
        [ 2.0573, -4.7217],
        [ 1.9007, -3.3812],
        [ 0.7511, -2.5794],
        [ 1.7441, -3.5953],
        [ 5.7974, -9.6160],
        [ 6.7588, -8.1902],
        [ 0.5173, -3.0401],
        [ 3.4447, -4.8451],
        [ 1.0861, -2.4743],
        [ 7.0838, -8.4884],
        [ 2.4682, -6.2928],
        [ 2.5818, -4.4209],
        [-0.2254, -1.1661],
        [ 5.1949, -6.7885],
        [ 3.6704, -6.9123],
        [-0.3776, -2.3959],
        [ 0.6990, -2.4600],
        [ 3.5952, -5.2034],
        [ 2.3354, -4.6380],
        [ 1.6736, -3.6487],
        [ 0.6382, -3.5459],
        [ 1.8460, -5.1016],
        [ 0.5634, -2.0200],
        [ 1.9609, -4.0608],
        [-1.1084, -0.3728],
        [ 1.7404, -3.2592],
        [ 3.4697, -7.8052],
        [ 1.7591, -3.1867],
        [ 0.3090, -3.2423],
        [ 5.1479, -6.9734],
        [ 2.2842, -3.8358],
        [-2.1931,  0.4192],
        [-0.5743, -1.7193],
        [ 0.8304, -2.2827],
        [ 0.7798, -2.7370],
        [ 2.6933, -4.0824],
        [ 2.0035, -6.6187],
        [ 6.1826, -8.6678],
        [-1.1936, -0.2457],
        [ 0.9674, -2.8450],
        [-0.0143, -2.6477],
        [ 0.1165, -2.5725],
        [ 1.6057, -3.5326],
        [ 0.4356, -2.6850],
        [ 3.7903, -5.7906],
        [ 1.5164, -2.9088],
        [ 2.9713, -5.9602],
        [ 1.5706, -3.2180],
        [ 2.6485, -4.1194],
        [ 3.8053, -5.2065],
        [ 3.5655, -4.9751],
        [ 2.7110, -4.1102],
        [-0.7014, -0.8844],
        [ 2.4910, -4.3195],
        [ 4.3606, -5.7993],
        [ 0.1872, -1.6095],
        [ 3.3431, -5.0352],
        [ 2.3361, -3.7628],
        [ 4.6893, -6.5785],
        [ 6.4457, -8.1424],
        [ 1.8112, -3.2312],
        [ 0.0630, -2.0158],
        [ 0.1448, -1.5518],
        [ 3.5997, -5.0585],
        [ 2.9727, -4.8250]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.4896e-01, 5.1043e-02],
        [9.9999e-01, 1.3396e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9437, 0.0563], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1400, 0.0640],
         [0.0865, 0.0687]],

        [[0.9549, 0.0875],
         [0.8606, 0.5765]],

        [[0.3624, 0.1104],
         [0.7789, 0.1645]],

        [[0.7055, 0.2230],
         [0.9677, 0.5499]],

        [[0.3357, 0.0883],
         [0.6069, 0.7668]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.00039315811977370555
Average Adjusted Rand Index: -0.001500082540753576
Iteration 0: Loss = -19494.21739131216
Iteration 10: Loss = -9891.179825276604
Iteration 20: Loss = -9887.784719423089
Iteration 30: Loss = -9886.866168905472
Iteration 40: Loss = -9885.957508702537
Iteration 50: Loss = -9885.3275724462
Iteration 60: Loss = -9885.328646429844
1
Iteration 70: Loss = -9885.330446562884
2
Iteration 80: Loss = -9885.330349469348
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.0121, 0.9879],
        [0.0487, 0.9513]], dtype=torch.float64)
alpha: tensor([0.0476, 0.9524])
beta: tensor([[[0.0574, 0.0631],
         [0.1605, 0.1383]],

        [[0.3777, 0.0861],
         [0.4017, 0.5610]],

        [[0.7352, 0.1072],
         [0.3405, 0.2326]],

        [[0.0131, 0.2244],
         [0.4704, 0.9987]],

        [[0.4588, 0.0868],
         [0.3991, 0.6268]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.0003905551290195141
Average Adjusted Rand Index: -0.001964494053204748
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19494.360218119233
Iteration 100: Loss = -9905.408960028788
Iteration 200: Loss = -9893.827891984914
Iteration 300: Loss = -9891.769324727693
Iteration 400: Loss = -9890.732859333213
Iteration 500: Loss = -9890.30981644341
Iteration 600: Loss = -9890.0386326436
Iteration 700: Loss = -9889.7684396435
Iteration 800: Loss = -9889.4337931988
Iteration 900: Loss = -9888.332724574453
Iteration 1000: Loss = -9887.673512874826
Iteration 1100: Loss = -9887.36135849087
Iteration 1200: Loss = -9887.168531015424
Iteration 1300: Loss = -9887.031547319157
Iteration 1400: Loss = -9886.92580258591
Iteration 1500: Loss = -9886.84384420067
Iteration 1600: Loss = -9886.774699802838
Iteration 1700: Loss = -9886.71222010916
Iteration 1800: Loss = -9886.652267105548
Iteration 1900: Loss = -9886.59508020073
Iteration 2000: Loss = -9886.541992406123
Iteration 2100: Loss = -9886.488475963359
Iteration 2200: Loss = -9886.430044413299
Iteration 2300: Loss = -9886.359916790832
Iteration 2400: Loss = -9886.26892719357
Iteration 2500: Loss = -9886.148483456318
Iteration 2600: Loss = -9886.009562890484
Iteration 2700: Loss = -9885.907144460305
Iteration 2800: Loss = -9885.844328683268
Iteration 2900: Loss = -9885.808366266343
Iteration 3000: Loss = -9885.784860956608
Iteration 3100: Loss = -9885.766666750364
Iteration 3200: Loss = -9885.755077048263
Iteration 3300: Loss = -9885.747551244416
Iteration 3400: Loss = -9885.741113884751
Iteration 3500: Loss = -9885.735104398073
Iteration 3600: Loss = -9885.729748149004
Iteration 3700: Loss = -9885.725076128154
Iteration 3800: Loss = -9885.721222123722
Iteration 3900: Loss = -9885.717203573133
Iteration 4000: Loss = -9885.711573815683
Iteration 4100: Loss = -9885.70782348821
Iteration 4200: Loss = -9885.70535479068
Iteration 4300: Loss = -9885.702718398397
Iteration 4400: Loss = -9885.698655944503
Iteration 4500: Loss = -9885.693319545617
Iteration 4600: Loss = -9885.685185089722
Iteration 4700: Loss = -9885.674499326002
Iteration 4800: Loss = -9885.674993508159
1
Iteration 4900: Loss = -9885.668331493378
Iteration 5000: Loss = -9885.66519343105
Iteration 5100: Loss = -9885.667107425701
1
Iteration 5200: Loss = -9885.660161471269
Iteration 5300: Loss = -9885.65854830524
Iteration 5400: Loss = -9885.65969507028
1
Iteration 5500: Loss = -9885.656004361212
Iteration 5600: Loss = -9885.654559019997
Iteration 5700: Loss = -9885.649671905936
Iteration 5800: Loss = -9885.64822643364
Iteration 5900: Loss = -9885.647068681057
Iteration 6000: Loss = -9885.64940507456
1
Iteration 6100: Loss = -9885.658881959047
2
Iteration 6200: Loss = -9885.643428344727
Iteration 6300: Loss = -9885.641408760717
Iteration 6400: Loss = -9885.639744914664
Iteration 6500: Loss = -9885.636245568452
Iteration 6600: Loss = -9885.636284714536
1
Iteration 6700: Loss = -9885.617499732653
Iteration 6800: Loss = -9885.61444426514
Iteration 6900: Loss = -9885.610903373894
Iteration 7000: Loss = -9885.606870156222
Iteration 7100: Loss = -9885.6047263614
Iteration 7200: Loss = -9885.603130203266
Iteration 7300: Loss = -9885.601756362508
Iteration 7400: Loss = -9885.638578026954
1
Iteration 7500: Loss = -9885.599591365426
Iteration 7600: Loss = -9885.604223184338
1
Iteration 7700: Loss = -9885.597898149294
Iteration 7800: Loss = -9885.597230848995
Iteration 7900: Loss = -9885.597223696406
Iteration 8000: Loss = -9885.596073266772
Iteration 8100: Loss = -9885.595866264315
Iteration 8200: Loss = -9885.595223356091
Iteration 8300: Loss = -9885.594806842846
Iteration 8400: Loss = -9885.645393773331
1
Iteration 8500: Loss = -9885.594147839887
Iteration 8600: Loss = -9885.593862425178
Iteration 8700: Loss = -9885.625431965389
1
Iteration 8800: Loss = -9885.59335552277
Iteration 8900: Loss = -9885.59311555511
Iteration 9000: Loss = -9885.617973883895
1
Iteration 9100: Loss = -9885.592696363756
Iteration 9200: Loss = -9885.592482049522
Iteration 9300: Loss = -9885.595191366836
1
Iteration 9400: Loss = -9885.592104417328
Iteration 9500: Loss = -9885.591919916891
Iteration 9600: Loss = -9885.594311521596
1
Iteration 9700: Loss = -9885.591526492384
Iteration 9800: Loss = -9885.593271859463
1
Iteration 9900: Loss = -9885.59114696413
Iteration 10000: Loss = -9885.59090533356
Iteration 10100: Loss = -9885.590258732114
Iteration 10200: Loss = -9885.590274823815
1
Iteration 10300: Loss = -9885.591104704728
2
Iteration 10400: Loss = -9885.591104710307
3
Iteration 10500: Loss = -9885.58996178514
Iteration 10600: Loss = -9885.589948163015
Iteration 10700: Loss = -9885.589785198577
Iteration 10800: Loss = -9885.598509126268
1
Iteration 10900: Loss = -9885.58955949474
Iteration 11000: Loss = -9885.599183721419
1
Iteration 11100: Loss = -9885.589350717399
Iteration 11200: Loss = -9885.58931690943
Iteration 11300: Loss = -9885.58936066358
1
Iteration 11400: Loss = -9885.58910281438
Iteration 11500: Loss = -9885.664070727402
1
Iteration 11600: Loss = -9885.590281334536
2
Iteration 11700: Loss = -9885.587859270268
Iteration 11800: Loss = -9885.58766755685
Iteration 11900: Loss = -9885.591714276752
1
Iteration 12000: Loss = -9885.587640150796
Iteration 12100: Loss = -9885.587662002838
1
Iteration 12200: Loss = -9885.612190212492
2
Iteration 12300: Loss = -9885.587483739042
Iteration 12400: Loss = -9885.614205571048
1
Iteration 12500: Loss = -9885.65956589499
2
Iteration 12600: Loss = -9885.587604581486
3
Iteration 12700: Loss = -9885.588713606978
4
Iteration 12800: Loss = -9885.600455050853
5
Iteration 12900: Loss = -9885.587251658164
Iteration 13000: Loss = -9885.591290409495
1
Iteration 13100: Loss = -9885.587229501763
Iteration 13200: Loss = -9885.592247404773
1
Iteration 13300: Loss = -9885.58716680942
Iteration 13400: Loss = -9885.593451473158
1
Iteration 13500: Loss = -9885.587837140323
2
Iteration 13600: Loss = -9885.587263177289
3
Iteration 13700: Loss = -9885.5872676501
4
Iteration 13800: Loss = -9885.587294360783
5
Iteration 13900: Loss = -9885.587033194377
Iteration 14000: Loss = -9885.587208131414
1
Iteration 14100: Loss = -9885.589765008472
2
Iteration 14200: Loss = -9885.75590822468
3
Iteration 14300: Loss = -9885.586971204171
Iteration 14400: Loss = -9885.588348399455
1
Iteration 14500: Loss = -9885.58856383216
2
Iteration 14600: Loss = -9885.589152585402
3
Iteration 14700: Loss = -9885.621506364183
4
Iteration 14800: Loss = -9885.586934935825
Iteration 14900: Loss = -9885.587175411703
1
Iteration 15000: Loss = -9885.593571705904
2
Iteration 15100: Loss = -9885.587311315276
3
Iteration 15200: Loss = -9885.589038951022
4
Iteration 15300: Loss = -9885.587278013529
5
Iteration 15400: Loss = -9885.588553266336
6
Iteration 15500: Loss = -9885.590190513685
7
Iteration 15600: Loss = -9885.586066110547
Iteration 15700: Loss = -9885.586914570058
1
Iteration 15800: Loss = -9885.58602033812
Iteration 15900: Loss = -9885.592925276442
1
Iteration 16000: Loss = -9885.624175011215
2
Iteration 16100: Loss = -9885.586562693126
3
Iteration 16200: Loss = -9885.586102750256
4
Iteration 16300: Loss = -9885.61992258678
5
Iteration 16400: Loss = -9885.759264785573
6
Iteration 16500: Loss = -9885.586842433811
7
Iteration 16600: Loss = -9885.58956235717
8
Iteration 16700: Loss = -9885.585963348743
Iteration 16800: Loss = -9885.586003588496
1
Iteration 16900: Loss = -9885.593077842545
2
Iteration 17000: Loss = -9885.585965490242
3
Iteration 17100: Loss = -9885.614268318683
4
Iteration 17200: Loss = -9885.588440783986
5
Iteration 17300: Loss = -9885.58596039935
Iteration 17400: Loss = -9885.585962219811
1
Iteration 17500: Loss = -9885.586100894852
2
Iteration 17600: Loss = -9885.586354891635
3
Iteration 17700: Loss = -9885.58537632977
Iteration 17800: Loss = -9885.585404050136
1
Iteration 17900: Loss = -9885.596385617173
2
Iteration 18000: Loss = -9885.62880785614
3
Iteration 18100: Loss = -9885.591323905
4
Iteration 18200: Loss = -9885.581772155316
Iteration 18300: Loss = -9885.582721737097
1
Iteration 18400: Loss = -9885.604365178864
2
Iteration 18500: Loss = -9885.581514134246
Iteration 18600: Loss = -9885.588764869552
1
Iteration 18700: Loss = -9885.729350674124
2
Iteration 18800: Loss = -9885.583979391322
3
Iteration 18900: Loss = -9885.58205973454
4
Iteration 19000: Loss = -9885.58171958063
5
Iteration 19100: Loss = -9885.58163376543
6
Iteration 19200: Loss = -9885.612354011155
7
Iteration 19300: Loss = -9885.583917859409
8
Iteration 19400: Loss = -9885.582074553196
9
Iteration 19500: Loss = -9885.581545274676
10
Stopping early at iteration 19500 due to no improvement.
tensor([[-10.5384,   8.4051],
        [ -3.6067,   2.1683],
        [ -4.2989,   2.4556],
        [-11.2491,   7.1251],
        [ -4.0077,   2.4991],
        [ -4.2613,   2.7718],
        [ -4.2991,   2.9128],
        [ -9.6169,   7.6597],
        [ -5.4067,   3.9948],
        [ -9.9228,   8.2939],
        [ -7.1634,   5.7459],
        [ -5.5373,   4.0968],
        [ -1.8994,   0.0486],
        [ -9.0313,   7.5831],
        [ -9.6691,   7.5853],
        [ -4.1046,   2.6162],
        [ -9.7688,   8.3622],
        [ -3.6622,  -0.9531],
        [ -9.3015,   7.5706],
        [ -9.6087,   8.2153],
        [ -7.4397,   5.0084],
        [-10.7098,   7.1244],
        [ -9.4267,   7.9671],
        [ -9.3727,   7.9780],
        [ -2.9131,   1.5072],
        [ -6.5624,   3.3343],
        [-10.0255,   7.9913],
        [ -9.8439,   8.0984],
        [-10.1389,   7.5882],
        [ -1.0001,  -1.1382],
        [ -3.8886,   2.4988],
        [ -3.9333,   1.6995],
        [ -5.2862,   3.8947],
        [ -9.4584,   7.9205],
        [ -9.5151,   8.1075],
        [ -6.4603,   2.9997],
        [ -5.0506,   3.3839],
        [ -2.3096,   0.8823],
        [ -3.9742,   2.5345],
        [ -9.8800,   8.4090],
        [-10.2160,   8.3102],
        [ -5.0777,   3.0793],
        [ -9.3785,   7.9404],
        [ -2.1945,   0.3462],
        [ -4.0967,   2.6960],
        [ -9.9422,   8.5404],
        [ -4.3371,   2.6579],
        [ -2.0745,   0.6789],
        [ -6.3283,   4.9155],
        [ -9.6527,   7.7490],
        [ -5.1486,   0.5334],
        [ -3.5806,   2.1290],
        [ -9.7184,   8.2296],
        [ -5.1495,   3.2903],
        [ -3.2029,   1.7869],
        [ -5.2580,   2.5467],
        [ -9.7043,   7.4974],
        [ -3.7068,   1.9693],
        [ -5.3961,   4.0055],
        [ -1.4919,   0.0463],
        [ -6.1434,   2.4544],
        [ -8.0668,   6.6613],
        [ -3.8200,   2.3599],
        [ -2.7520,   0.9789],
        [-11.8468,   8.4971],
        [ -5.1128,   3.4369],
        [ -0.1230,  -1.4232],
        [ -2.8384,   1.3423],
        [ -2.7606,   1.3738],
        [ -5.6139,   2.2007],
        [ -9.3306,   7.9439],
        [ -5.9390,   4.5125],
        [ -9.5063,   7.3283],
        [ -0.1578,  -2.1315],
        [ -4.1380,   2.7351],
        [ -3.6801,   0.6340],
        [ -1.7801,   0.1696],
        [ -4.2817,   2.8900],
        [ -4.3802,   2.9938],
        [-10.9931,   7.5935],
        [ -2.7790,   0.8482],
        [ -9.9278,   8.5360],
        [ -4.6211,   2.9677],
        [ -5.0960,   3.4959],
        [-10.3903,   7.6048],
        [ -9.7541,   8.3383],
        [ -4.1852,   2.7338],
        [ -2.9874,   0.4165],
        [ -4.6092,   3.0681],
        [-10.0839,   8.6481],
        [ -3.0967,   1.2424],
        [ -6.3193,   4.3034],
        [ -6.3292,   4.9061],
        [ -9.9366,   8.5311],
        [ -3.8249,   2.3410],
        [ -5.2892,   2.5721],
        [ -2.4080,   0.8936],
        [ -1.8267,  -1.3483],
        [ -9.8608,   8.2893],
        [ -9.6982,   8.3096]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9997e-01, 2.5911e-05],
        [2.3283e-02, 9.7672e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0347, 0.9653], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1706, 0.0615],
         [0.1605, 0.1346]],

        [[0.3777, 0.1440],
         [0.4017, 0.5610]],

        [[0.7352, 0.1442],
         [0.3405, 0.2326]],

        [[0.0131, 0.1942],
         [0.4704, 0.9987]],

        [[0.4588, 0.1471],
         [0.3991, 0.6268]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.018772080923839488
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.006809752538456861
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0028959952356207414
Global Adjusted Rand Index: 2.598777693236343e-05
Average Adjusted Rand Index: 0.0035283340891933126
Iteration 0: Loss = -27110.677558393632
Iteration 10: Loss = -9891.46816574806
Iteration 20: Loss = -9891.468191540342
1
Iteration 30: Loss = -9891.463848862208
Iteration 40: Loss = -9891.042670034129
Iteration 50: Loss = -9889.532812960282
Iteration 60: Loss = -9887.228404508223
Iteration 70: Loss = -9886.540108732815
Iteration 80: Loss = -9885.434863253873
Iteration 90: Loss = -9885.396608383158
Iteration 100: Loss = -9885.38959970019
Iteration 110: Loss = -9885.387368280566
Iteration 120: Loss = -9885.386556430592
Iteration 130: Loss = -9885.341901141202
Iteration 140: Loss = -9885.330256482406
Iteration 150: Loss = -9885.330584286106
1
Iteration 160: Loss = -9885.330331728104
2
Iteration 170: Loss = -9885.330190703398
Iteration 180: Loss = -9885.330141748494
Iteration 190: Loss = -9885.33011779859
Iteration 200: Loss = -9885.330100983952
Iteration 210: Loss = -9885.330093313809
Iteration 220: Loss = -9885.330057153617
Iteration 230: Loss = -9885.330096428495
1
Iteration 240: Loss = -9885.33008414956
2
Iteration 250: Loss = -9885.33008922542
3
Stopping early at iteration 249 due to no improvement.
pi: tensor([[0.0121, 0.9879],
        [0.0489, 0.9511]], dtype=torch.float64)
alpha: tensor([0.0477, 0.9523])
beta: tensor([[[0.0575, 0.0632],
         [0.7446, 0.1383]],

        [[0.5495, 0.0861],
         [0.7775, 0.5218]],

        [[0.6817, 0.1072],
         [0.5032, 0.6577]],

        [[0.5629, 0.2243],
         [0.9271, 0.6863]],

        [[0.3771, 0.0869],
         [0.2031, 0.1424]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.0003905551290195141
Average Adjusted Rand Index: -0.001964494053204748
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27110.064685501355
Iteration 100: Loss = -9905.739307683754
Iteration 200: Loss = -9894.766165722032
Iteration 300: Loss = -9892.649509902674
Iteration 400: Loss = -9891.82566994149
Iteration 500: Loss = -9891.40093746717
Iteration 600: Loss = -9891.115545010092
Iteration 700: Loss = -9890.943762334693
Iteration 800: Loss = -9890.83169939074
Iteration 900: Loss = -9890.750120807963
Iteration 1000: Loss = -9890.681004208096
Iteration 1100: Loss = -9890.61577553112
Iteration 1200: Loss = -9890.556266757245
Iteration 1300: Loss = -9890.49654760713
Iteration 1400: Loss = -9890.432899919528
Iteration 1500: Loss = -9890.359171753193
Iteration 1600: Loss = -9890.263611198794
Iteration 1700: Loss = -9890.125502668872
Iteration 1800: Loss = -9889.944015809013
Iteration 1900: Loss = -9889.745684457646
Iteration 2000: Loss = -9889.499826864934
Iteration 2100: Loss = -9888.871821130424
Iteration 2200: Loss = -9887.889024461536
Iteration 2300: Loss = -9887.48746596818
Iteration 2400: Loss = -9887.224630530241
Iteration 2500: Loss = -9887.051738614033
Iteration 2600: Loss = -9886.92600871556
Iteration 2700: Loss = -9886.831884787674
Iteration 2800: Loss = -9886.758449041788
Iteration 2900: Loss = -9886.700841342037
Iteration 3000: Loss = -9886.656180969761
Iteration 3100: Loss = -9886.606870063493
Iteration 3200: Loss = -9886.552993065648
Iteration 3300: Loss = -9886.533505128828
Iteration 3400: Loss = -9886.518773071168
Iteration 3500: Loss = -9886.5071592104
Iteration 3600: Loss = -9886.497836600272
Iteration 3700: Loss = -9886.489974045078
Iteration 3800: Loss = -9886.48296819765
Iteration 3900: Loss = -9886.476859344146
Iteration 4000: Loss = -9886.471207533748
Iteration 4100: Loss = -9886.466243311006
Iteration 4200: Loss = -9886.460352165306
Iteration 4300: Loss = -9886.44838464572
Iteration 4400: Loss = -9886.378043212333
Iteration 4500: Loss = -9885.685565964255
Iteration 4600: Loss = -9885.28720373585
Iteration 4700: Loss = -9885.133217360932
Iteration 4800: Loss = -9885.061871859869
Iteration 4900: Loss = -9885.023070496802
Iteration 5000: Loss = -9884.999390880432
Iteration 5100: Loss = -9884.983692502934
Iteration 5200: Loss = -9884.972500118029
Iteration 5300: Loss = -9884.964176083697
Iteration 5400: Loss = -9884.95766720873
Iteration 5500: Loss = -9884.952468283429
Iteration 5600: Loss = -9884.948177392156
Iteration 5700: Loss = -9884.944578387764
Iteration 5800: Loss = -9884.941494489982
Iteration 5900: Loss = -9884.938840159448
Iteration 6000: Loss = -9884.936506831265
Iteration 6100: Loss = -9884.934511136007
Iteration 6200: Loss = -9884.93363109342
Iteration 6300: Loss = -9884.931898683722
Iteration 6400: Loss = -9884.93336271243
1
Iteration 6500: Loss = -9884.928370835312
Iteration 6600: Loss = -9884.927175194096
Iteration 6700: Loss = -9884.926065019248
Iteration 6800: Loss = -9884.927127557254
1
Iteration 6900: Loss = -9884.924148774202
Iteration 7000: Loss = -9884.923314340094
Iteration 7100: Loss = -9884.923348005717
1
Iteration 7200: Loss = -9884.921800533619
Iteration 7300: Loss = -9884.921158527944
Iteration 7400: Loss = -9884.954834728187
1
Iteration 7500: Loss = -9884.919960780342
Iteration 7600: Loss = -9884.919430087828
Iteration 7700: Loss = -9884.9525317229
1
Iteration 7800: Loss = -9884.918428540726
Iteration 7900: Loss = -9884.917997211684
Iteration 8000: Loss = -9884.924116894963
1
Iteration 8100: Loss = -9884.917201132675
Iteration 8200: Loss = -9884.916845805736
Iteration 8300: Loss = -9884.942873383556
1
Iteration 8400: Loss = -9884.916177228915
Iteration 8500: Loss = -9884.915857101294
Iteration 8600: Loss = -9884.915585755907
Iteration 8700: Loss = -9884.930768836413
1
Iteration 8800: Loss = -9884.915084429487
Iteration 8900: Loss = -9884.914840797703
Iteration 9000: Loss = -9884.914856226133
1
Iteration 9100: Loss = -9884.914404727664
Iteration 9200: Loss = -9884.91420331304
Iteration 9300: Loss = -9884.913989706683
Iteration 9400: Loss = -9884.913831618143
Iteration 9500: Loss = -9884.913656425695
Iteration 9600: Loss = -9884.913505147251
Iteration 9700: Loss = -9884.916476565237
1
Iteration 9800: Loss = -9884.913198769274
Iteration 9900: Loss = -9884.913072870084
Iteration 10000: Loss = -9884.912948477853
Iteration 10100: Loss = -9884.913872392346
1
Iteration 10200: Loss = -9884.912677140814
Iteration 10300: Loss = -9884.912591189097
Iteration 10400: Loss = -9884.913667832989
1
Iteration 10500: Loss = -9884.912370954155
Iteration 10600: Loss = -9884.912277085248
Iteration 10700: Loss = -9884.918788398432
1
Iteration 10800: Loss = -9884.91209576635
Iteration 10900: Loss = -9884.912034809442
Iteration 11000: Loss = -9884.933501590443
1
Iteration 11100: Loss = -9884.911856619017
Iteration 11200: Loss = -9884.911796366005
Iteration 11300: Loss = -9884.952539884118
1
Iteration 11400: Loss = -9884.91167279095
Iteration 11500: Loss = -9884.91162391526
Iteration 11600: Loss = -9884.940433555623
1
Iteration 11700: Loss = -9884.911762040358
2
Iteration 11800: Loss = -9884.911424999687
Iteration 11900: Loss = -9884.930348157339
1
Iteration 12000: Loss = -9884.911373099549
Iteration 12100: Loss = -9884.938532278315
1
Iteration 12200: Loss = -9884.911309051875
Iteration 12300: Loss = -9884.911381695218
1
Iteration 12400: Loss = -9884.911314522979
2
Iteration 12500: Loss = -9884.911152384318
Iteration 12600: Loss = -9884.913242660235
1
Iteration 12700: Loss = -9884.912539650986
2
Iteration 12800: Loss = -9884.911270707997
3
Iteration 12900: Loss = -9885.13672245737
4
Iteration 13000: Loss = -9884.9111023081
Iteration 13100: Loss = -9884.912131405115
1
Iteration 13200: Loss = -9884.912011360091
2
Iteration 13300: Loss = -9884.91209777194
3
Iteration 13400: Loss = -9884.911014041467
Iteration 13500: Loss = -9884.915678024154
1
Iteration 13600: Loss = -9884.910857945419
Iteration 13700: Loss = -9884.920201220755
1
Iteration 13800: Loss = -9884.910839934126
Iteration 13900: Loss = -9884.910804082554
Iteration 14000: Loss = -9884.911830349523
1
Iteration 14100: Loss = -9884.910792933922
Iteration 14200: Loss = -9884.914880974535
1
Iteration 14300: Loss = -9884.91584159016
2
Iteration 14400: Loss = -9884.91256525443
3
Iteration 14500: Loss = -9884.912784128248
4
Iteration 14600: Loss = -9884.910763848215
Iteration 14700: Loss = -9884.911190848667
1
Iteration 14800: Loss = -9884.91085368164
2
Iteration 14900: Loss = -9884.910781086317
3
Iteration 15000: Loss = -9884.966243439549
4
Iteration 15100: Loss = -9884.91087093633
5
Iteration 15200: Loss = -9884.912821476659
6
Iteration 15300: Loss = -9884.918286999731
7
Iteration 15400: Loss = -9884.910763993657
8
Iteration 15500: Loss = -9884.941711190999
9
Iteration 15600: Loss = -9884.916275800391
10
Stopping early at iteration 15600 due to no improvement.
tensor([[-7.1360,  2.8199],
        [-4.7367,  1.0278],
        [-2.9677,  1.5271],
        [-5.4798,  3.6442],
        [-2.7387,  0.4938],
        [-3.6043,  2.1766],
        [-3.0530,  1.3476],
        [-4.4956,  3.1006],
        [-4.2989,  2.8931],
        [-6.5904,  5.1722],
        [-6.2432,  4.8521],
        [-4.0869,  2.6902],
        [-0.5732, -1.4976],
        [-4.6957,  3.2042],
        [-4.8753,  1.7657],
        [-1.9519,  0.5559],
        [-4.9866,  3.4327],
        [-1.9455,  0.5433],
        [-3.7442,  2.2497],
        [-5.2575,  3.3054],
        [-5.4089,  3.1138],
        [-4.5053,  2.9669],
        [-4.0587,  2.6596],
        [-5.4223,  3.8183],
        [-5.4707,  0.8555],
        [-4.2020,  2.7338],
        [-5.2948,  3.7581],
        [-4.8209,  3.4311],
        [-4.5989,  3.1584],
        [-1.1500, -0.2896],
        [-3.6171,  2.2007],
        [-3.2794,  1.8810],
        [-3.5991,  2.1178],
        [-4.9251,  2.9121],
        [-4.4215,  2.4656],
        [-4.0851,  2.5214],
        [-3.4241,  1.7231],
        [-2.7425,  0.4987],
        [-3.3923,  1.8118],
        [-5.6681,  4.2712],
        [-6.1575,  3.0240],
        [-2.5410,  0.9153],
        [-5.3430,  2.7447],
        [-2.4810,  0.9793],
        [-3.7658,  2.0805],
        [-4.9672,  3.5737],
        [-4.2495,  2.5852],
        [-2.7534, -1.8618],
        [-6.6536,  5.0215],
        [-5.8733,  4.4862],
        [-1.7257,  0.2250],
        [-2.2468,  0.8274],
        [-5.3931,  3.1906],
        [-4.2049,  2.5932],
        [-3.2901,  1.9031],
        [-3.2771,  0.7894],
        [-4.0929,  2.6846],
        [-2.1964,  0.3011],
        [-4.1609,  1.7087],
        [-0.3427, -1.0962],
        [-3.1399,  1.7177],
        [-6.2603,  4.7149],
        [-4.7211,  0.1058],
        [-2.8877,  0.5637],
        [-6.8896,  4.8951],
        [-4.0828,  1.8875],
        [ 0.4061, -2.1863],
        [-1.4889, -0.3887],
        [-3.7851, -0.7621],
        [-2.8543,  0.5609],
        [-4.6095,  1.9982],
        [-5.2138,  3.2077],
        [-3.8376,  2.1096],
        [-0.2206, -1.1931],
        [-2.5955,  1.1079],
        [-2.3813,  0.1650],
        [-2.9067, -0.3031],
        [-3.7393,  1.2634],
        [-2.7556,  0.2758],
        [-5.9617,  3.4052],
        [-3.2381,  1.0704],
        [-5.0564,  3.6679],
        [-3.2287,  1.4518],
        [-4.0504,  2.5439],
        [-5.2244,  3.5848],
        [-4.8821,  3.4560],
        [-4.0332,  2.6194],
        [-0.8739, -0.7207],
        [-4.0570,  2.5857],
        [-5.6638,  4.2691],
        [-1.7459, -0.0173],
        [-4.9713,  3.2253],
        [-3.7044,  2.2346],
        [-6.2476,  4.7137],
        [-2.8633,  1.4420],
        [-3.4921,  1.4106],
        [-1.7414,  0.2798],
        [-1.5085,  0.1201],
        [-4.9682,  3.4963],
        [-4.6136,  2.9990]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.2402e-05, 9.9999e-01],
        [5.0886e-02, 9.4911e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0578, 0.9422], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0685, 0.0649],
         [0.7446, 0.1398]],

        [[0.5495, 0.0869],
         [0.7775, 0.5218]],

        [[0.6817, 0.1104],
         [0.5032, 0.6577]],

        [[0.5629, 0.2229],
         [0.9271, 0.6863]],

        [[0.3771, 0.0880],
         [0.2031, 0.1424]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.00039315811977370555
Average Adjusted Rand Index: -0.001500082540753576
Iteration 0: Loss = -31374.89074319831
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.7619,    nan]],

        [[0.4276,    nan],
         [0.2389, 0.5406]],

        [[0.3374,    nan],
         [0.9833, 0.3356]],

        [[0.8766,    nan],
         [0.8111, 0.8819]],

        [[0.9531,    nan],
         [0.3740, 0.3711]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31376.002372890944
Iteration 100: Loss = -9901.524143277567
Iteration 200: Loss = -9894.912081711003
Iteration 300: Loss = -9893.64938196733
Iteration 400: Loss = -9892.950846033027
Iteration 500: Loss = -9892.507548690157
Iteration 600: Loss = -9892.203038974729
Iteration 700: Loss = -9891.979197972
Iteration 800: Loss = -9891.803047956764
Iteration 900: Loss = -9891.654785524423
Iteration 1000: Loss = -9891.52440049286
Iteration 1100: Loss = -9891.410801725287
Iteration 1200: Loss = -9891.316688426577
Iteration 1300: Loss = -9891.240877327458
Iteration 1400: Loss = -9891.179753793134
Iteration 1500: Loss = -9891.130056321208
Iteration 1600: Loss = -9891.089052024006
Iteration 1700: Loss = -9891.054515238426
Iteration 1800: Loss = -9891.024832978592
Iteration 1900: Loss = -9890.998804818566
Iteration 2000: Loss = -9890.975361750738
Iteration 2100: Loss = -9890.95382083588
Iteration 2200: Loss = -9890.93339020003
Iteration 2300: Loss = -9890.913379633552
Iteration 2400: Loss = -9890.892957857099
Iteration 2500: Loss = -9890.871306802677
Iteration 2600: Loss = -9890.847212983863
Iteration 2700: Loss = -9890.819296832155
Iteration 2800: Loss = -9890.786874118869
Iteration 2900: Loss = -9890.748490820044
Iteration 3000: Loss = -9890.699573507924
Iteration 3100: Loss = -9890.610496734009
Iteration 3200: Loss = -9889.80544097343
Iteration 3300: Loss = -9889.258713025803
Iteration 3400: Loss = -9889.000367548395
Iteration 3500: Loss = -9888.771677747214
Iteration 3600: Loss = -9888.528690111933
Iteration 3700: Loss = -9888.242497065827
Iteration 3800: Loss = -9887.882863568742
Iteration 3900: Loss = -9887.408419952548
Iteration 4000: Loss = -9886.783608579872
Iteration 4100: Loss = -9886.112431001638
Iteration 4200: Loss = -9885.652547125797
Iteration 4300: Loss = -9885.348562482555
Iteration 4400: Loss = -9885.180644526225
Iteration 4500: Loss = -9885.094323864982
Iteration 4600: Loss = -9885.047985332698
Iteration 4700: Loss = -9885.021188186778
Iteration 4800: Loss = -9885.003081718942
Iteration 4900: Loss = -9884.989609359543
Iteration 5000: Loss = -9884.97892909591
Iteration 5100: Loss = -9884.970201325767
Iteration 5200: Loss = -9884.963142669365
Iteration 5300: Loss = -9884.957350676088
Iteration 5400: Loss = -9884.952536211445
Iteration 5500: Loss = -9884.948564038019
Iteration 5600: Loss = -9884.945163280097
Iteration 5700: Loss = -9884.942334061721
Iteration 5800: Loss = -9884.93979868536
Iteration 5900: Loss = -9884.93760843031
Iteration 6000: Loss = -9884.935628018895
Iteration 6100: Loss = -9884.93389681763
Iteration 6200: Loss = -9884.932283828091
Iteration 6300: Loss = -9884.93090087291
Iteration 6400: Loss = -9884.929636882203
Iteration 6500: Loss = -9884.928445334583
Iteration 6600: Loss = -9884.927490947393
Iteration 6700: Loss = -9884.926391961271
Iteration 6800: Loss = -9884.925474385733
Iteration 6900: Loss = -9884.92466468916
Iteration 7000: Loss = -9884.923852940667
Iteration 7100: Loss = -9884.923119718105
Iteration 7200: Loss = -9884.922440309503
Iteration 7300: Loss = -9884.921755334482
Iteration 7400: Loss = -9884.921176650667
Iteration 7500: Loss = -9884.923270782225
1
Iteration 7600: Loss = -9884.92008003545
Iteration 7700: Loss = -9884.919610333654
Iteration 7800: Loss = -9884.919104767172
Iteration 7900: Loss = -9884.918693565709
Iteration 8000: Loss = -9884.918249560315
Iteration 8100: Loss = -9884.917866678285
Iteration 8200: Loss = -9884.917429796684
Iteration 8300: Loss = -9884.917246907076
Iteration 8400: Loss = -9884.916768988303
Iteration 8500: Loss = -9884.91646898621
Iteration 8600: Loss = -9884.917974879721
1
Iteration 8700: Loss = -9884.984734160573
2
Iteration 8800: Loss = -9884.91560338283
Iteration 8900: Loss = -9884.915376047295
Iteration 9000: Loss = -9884.915078868571
Iteration 9100: Loss = -9884.914961916336
Iteration 9200: Loss = -9884.914642255759
Iteration 9300: Loss = -9885.091527770664
1
Iteration 9400: Loss = -9884.914271998536
Iteration 9500: Loss = -9884.914013810281
Iteration 9600: Loss = -9884.94299348567
1
Iteration 9700: Loss = -9884.913701497266
Iteration 9800: Loss = -9884.91353666336
Iteration 9900: Loss = -9884.913380572703
Iteration 10000: Loss = -9884.91344887124
1
Iteration 10100: Loss = -9884.913086240593
Iteration 10200: Loss = -9884.912978271033
Iteration 10300: Loss = -9885.114541057312
1
Iteration 10400: Loss = -9884.912737946946
Iteration 10500: Loss = -9884.91263416995
Iteration 10600: Loss = -9884.91252784463
Iteration 10700: Loss = -9884.917160814864
1
Iteration 10800: Loss = -9884.912329669674
Iteration 10900: Loss = -9884.912229654383
Iteration 11000: Loss = -9885.19438868058
1
Iteration 11100: Loss = -9884.912028473767
Iteration 11200: Loss = -9884.911973280328
Iteration 11300: Loss = -9885.196343549744
1
Iteration 11400: Loss = -9884.91184148918
Iteration 11500: Loss = -9884.911793303118
Iteration 11600: Loss = -9884.911732130025
Iteration 11700: Loss = -9884.911634720576
Iteration 11800: Loss = -9884.97711897188
1
Iteration 11900: Loss = -9884.91157559788
Iteration 12000: Loss = -9884.911493050575
Iteration 12100: Loss = -9885.034007392867
1
Iteration 12200: Loss = -9884.91141934056
Iteration 12300: Loss = -9884.918916704544
1
Iteration 12400: Loss = -9884.911285681668
Iteration 12500: Loss = -9884.921694069268
1
Iteration 12600: Loss = -9884.914686359341
2
Iteration 12700: Loss = -9884.913350503784
3
Iteration 12800: Loss = -9884.911236282844
Iteration 12900: Loss = -9884.911888060355
1
Iteration 13000: Loss = -9884.945408952808
2
Iteration 13100: Loss = -9884.911414326201
3
Iteration 13200: Loss = -9884.911213877136
Iteration 13300: Loss = -9884.990302235768
1
Iteration 13400: Loss = -9884.914446932931
2
Iteration 13500: Loss = -9884.911041694873
Iteration 13600: Loss = -9884.911247059095
1
Iteration 13700: Loss = -9884.912976971638
2
Iteration 13800: Loss = -9884.911895388266
3
Iteration 13900: Loss = -9884.91085435414
Iteration 14000: Loss = -9885.128008033718
1
Iteration 14100: Loss = -9884.911450803162
2
Iteration 14200: Loss = -9884.913690935162
3
Iteration 14300: Loss = -9884.91409798899
4
Iteration 14400: Loss = -9884.913526814433
5
Iteration 14500: Loss = -9884.910907385798
6
Iteration 14600: Loss = -9884.910777695233
Iteration 14700: Loss = -9884.911819722407
1
Iteration 14800: Loss = -9884.910710823895
Iteration 14900: Loss = -9884.920149046573
1
Iteration 15000: Loss = -9884.910696944029
Iteration 15100: Loss = -9884.910860792992
1
Iteration 15200: Loss = -9884.9107333131
2
Iteration 15300: Loss = -9884.912047343623
3
Iteration 15400: Loss = -9884.910878732531
4
Iteration 15500: Loss = -9884.934440152914
5
Iteration 15600: Loss = -9884.911995134915
6
Iteration 15700: Loss = -9884.913407821145
7
Iteration 15800: Loss = -9884.910617994608
Iteration 15900: Loss = -9884.910646299737
1
Iteration 16000: Loss = -9884.954432192177
2
Iteration 16100: Loss = -9884.910585582978
Iteration 16200: Loss = -9884.910707728435
1
Iteration 16300: Loss = -9885.012819915328
2
Iteration 16400: Loss = -9884.910671571159
3
Iteration 16500: Loss = -9884.91171764561
4
Iteration 16600: Loss = -9884.910819700033
5
Iteration 16700: Loss = -9884.911357854578
6
Iteration 16800: Loss = -9884.918024707651
7
Iteration 16900: Loss = -9884.911612826312
8
Iteration 17000: Loss = -9884.913923553957
9
Iteration 17100: Loss = -9884.910572957473
Iteration 17200: Loss = -9884.910831213916
1
Iteration 17300: Loss = -9884.910543370148
Iteration 17400: Loss = -9885.016788300361
1
Iteration 17500: Loss = -9884.910563256077
2
Iteration 17600: Loss = -9884.911559876437
3
Iteration 17700: Loss = -9884.910547150268
4
Iteration 17800: Loss = -9884.91120984749
5
Iteration 17900: Loss = -9884.91054316955
Iteration 18000: Loss = -9884.910608073707
1
Iteration 18100: Loss = -9884.913191232601
2
Iteration 18200: Loss = -9884.910663169283
3
Iteration 18300: Loss = -9884.914161670144
4
Iteration 18400: Loss = -9884.916413081457
5
Iteration 18500: Loss = -9884.911560881483
6
Iteration 18600: Loss = -9884.923987886445
7
Iteration 18700: Loss = -9884.910663784822
8
Iteration 18800: Loss = -9884.910548545888
9
Iteration 18900: Loss = -9884.91247796265
10
Stopping early at iteration 18900 due to no improvement.
tensor([[-5.7192e+00,  4.2273e+00],
        [-3.6230e+00,  2.1460e+00],
        [-3.3705e+00,  1.1291e+00],
        [-6.0813e+00,  3.0469e+00],
        [-2.7444e+00,  4.9488e-01],
        [-3.8110e+00,  1.9714e+00],
        [-2.9009e+00,  1.5053e+00],
        [-4.5243e+00,  3.0650e+00],
        [-4.3206e+00,  2.8743e+00],
        [-6.8274e+00,  4.9249e+00],
        [-6.6138e+00,  4.4823e+00],
        [-4.2351e+00,  2.5426e+00],
        [-8.4181e-01, -1.7593e+00],
        [-5.0053e+00,  2.8978e+00],
        [-4.8451e+00,  1.7970e+00],
        [-1.9543e+00,  5.6034e-01],
        [-5.5889e+00,  2.8331e+00],
        [-2.3467e+00,  1.4897e-01],
        [-3.7371e+00,  2.2585e+00],
        [-4.9818e+00,  3.5809e+00],
        [-4.9798e+00,  3.5415e+00],
        [-4.5910e+00,  2.8795e+00],
        [-4.1405e+00,  2.5809e+00],
        [-5.8620e+00,  3.3750e+00],
        [-3.8888e+00,  2.4437e+00],
        [-4.2128e+00,  2.7265e+00],
        [-5.2791e+00,  3.7682e+00],
        [-5.0839e+00,  3.1679e+00],
        [-4.9346e+00,  2.8243e+00],
        [-1.1863e+00, -3.1884e-01],
        [-3.6063e+00,  2.2146e+00],
        [-3.4277e+00,  1.7392e+00],
        [-3.6387e+00,  2.0809e+00],
        [-4.6097e+00,  3.2209e+00],
        [-5.4184e+00,  1.4717e+00],
        [-5.1578e+00,  1.4513e+00],
        [-3.9189e+00,  1.2326e+00],
        [-2.3855e+00,  8.6251e-01],
        [-3.5487e+00,  1.6605e+00],
        [-7.0500e+00,  2.8912e+00],
        [-5.4685e+00,  3.7060e+00],
        [-3.8121e+00, -3.4975e-01],
        [-4.9793e+00,  3.1061e+00],
        [-2.5164e+00,  9.5049e-01],
        [-3.7400e+00,  2.1086e+00],
        [-4.9806e+00,  3.5619e+00],
        [-4.3420e+00,  2.4927e+00],
        [-1.1427e+00, -2.4406e-01],
        [-6.5956e+00,  5.0970e+00],
        [-7.1649e+00,  3.1873e+00],
        [-1.8112e+00,  1.4633e-01],
        [-2.2958e+00,  7.8424e-01],
        [-5.1748e+00,  3.4082e+00],
        [-4.1854e+00,  2.6147e+00],
        [-3.6046e+00,  1.5916e+00],
        [-3.0979e+00,  9.7522e-01],
        [-4.3015e+00,  2.4766e+00],
        [-2.1123e+00,  3.9206e-01],
        [-3.7624e+00,  2.1108e+00],
        [-1.8474e+00, -2.5935e+00],
        [-3.1695e+00,  1.6918e+00],
        [-6.2651e+00,  4.7254e+00],
        [-3.1567e+00,  1.6738e+00],
        [-3.0704e+00,  3.8748e-01],
        [-6.7057e+00,  5.0764e+00],
        [-3.9823e+00,  1.9904e+00],
        [-2.5350e-01, -2.8390e+00],
        [-1.2526e+00, -1.4524e-01],
        [-2.6433e+00,  3.8694e-01],
        [-3.6624e+00, -2.4049e-01],
        [-4.0299e+00,  2.5803e+00],
        [-4.9341e+00,  3.4889e+00],
        [-3.9700e+00,  1.9789e+00],
        [-3.3853e-01, -1.3040e+00],
        [-2.5481e+00,  1.1616e+00],
        [-2.0051e+00,  5.4806e-01],
        [-2.2311e+00,  3.7935e-01],
        [-3.1983e+00,  1.8098e+00],
        [-2.7531e+00,  2.8541e-01],
        [-5.4407e+00,  3.9237e+00],
        [-3.2527e+00,  1.0625e+00],
        [-5.1307e+00,  3.5910e+00],
        [-3.0901e+00,  1.5965e+00],
        [-5.6061e+00,  9.9088e-01],
        [-5.1160e+00,  3.6922e+00],
        [-5.6338e+00,  2.7049e+00],
        [-4.0355e+00,  2.6188e+00],
        [-1.8946e+00, -1.7342e+00],
        [-4.0154e+00,  2.6283e+00],
        [-5.7499e+00,  4.1759e+00],
        [-1.7305e+00,  5.0449e-03],
        [-4.8737e+00,  3.3216e+00],
        [-3.7220e+00,  2.2196e+00],
        [-6.5439e+00,  4.4208e+00],
        [-2.9293e+00,  1.3814e+00],
        [-3.2280e+00,  1.6777e+00],
        [-1.7876e+00,  2.4099e-01],
        [-1.8236e+00, -1.8802e-01],
        [-5.8484e+00,  2.6167e+00],
        [-4.6237e+00,  2.9909e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.7905e-06, 1.0000e+00],
        [5.1106e-02, 9.4889e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0582, 0.9418], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0685, 0.0651],
         [0.7619, 0.1402]],

        [[0.4276, 0.0868],
         [0.2389, 0.5406]],

        [[0.3374, 0.1100],
         [0.9833, 0.3356]],

        [[0.8766, 0.2230],
         [0.8111, 0.8819]],

        [[0.9531, 0.0880],
         [0.3740, 0.3711]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.00039315811977370555
Average Adjusted Rand Index: -0.001500082540753576
10001.991394403987
new:  [-0.00039315811977370555, 2.598777693236343e-05, -0.00039315811977370555, -0.00039315811977370555] [-0.001500082540753576, 0.0035283340891933126, -0.001500082540753576, -0.001500082540753576] [9884.942600579467, 9885.581545274676, 9884.916275800391, 9884.91247796265]
prior:  [-0.0003905551290195141, -0.0003905551290195141, -0.0003905551290195141, 0.0] [-0.001964494053204748, -0.001964494053204748, -0.001964494053204748, 0.0] [9885.330082841054, 9885.330349469348, 9885.33008922542, nan]
-----------------------------------------------------------------------------------------
This iteration is 7
True Objective function: Loss = -10059.846719487165
Iteration 0: Loss = -32888.168199023596
Iteration 10: Loss = -9994.034022278567
Iteration 20: Loss = -9994.022989919817
Iteration 30: Loss = -9994.001048395969
Iteration 40: Loss = -9993.952000943102
Iteration 50: Loss = -9993.831753610239
Iteration 60: Loss = -9993.537572013063
Iteration 70: Loss = -9993.022492978256
Iteration 80: Loss = -9992.619526579521
Iteration 90: Loss = -9992.456995154504
Iteration 100: Loss = -9992.374007767761
Iteration 110: Loss = -9992.304310847221
Iteration 120: Loss = -9992.231525241223
Iteration 130: Loss = -9992.146036450027
Iteration 140: Loss = -9992.03287432705
Iteration 150: Loss = -9991.863280808919
Iteration 160: Loss = -9991.59101555414
Iteration 170: Loss = -9991.213007500883
Iteration 180: Loss = -9990.868541360072
Iteration 190: Loss = -9990.641003963083
Iteration 200: Loss = -9990.468990337817
Iteration 210: Loss = -9990.114205560658
Iteration 220: Loss = -9989.789884015094
Iteration 230: Loss = -9989.769773291471
Iteration 240: Loss = -9989.64758172393
Iteration 250: Loss = -9989.320928735217
Iteration 260: Loss = -9989.177722832985
Iteration 270: Loss = -9989.169105244768
Iteration 280: Loss = -9989.16999233122
1
Iteration 290: Loss = -9989.170368418032
2
Iteration 300: Loss = -9989.170430223734
3
Stopping early at iteration 299 due to no improvement.
pi: tensor([[0.9438, 0.0562],
        [0.9635, 0.0365]], dtype=torch.float64)
alpha: tensor([0.9445, 0.0555])
beta: tensor([[[0.1407, 0.1144],
         [0.6179, 0.1502]],

        [[0.1262, 0.1477],
         [0.8878, 0.4826]],

        [[0.6583, 0.0611],
         [0.6532, 0.0495]],

        [[0.0107, 0.0674],
         [0.1035, 0.0613]],

        [[0.5417, 0.1811],
         [0.9998, 0.3174]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.017056499655107544
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 36
Adjusted Rand Index: 0.03609912235729749
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00879578788541848
Average Adjusted Rand Index: 0.010631124402481007
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32883.649906964616
Iteration 100: Loss = -9996.417065005124
Iteration 200: Loss = -9995.81657344357
Iteration 300: Loss = -9995.671014723628
Iteration 400: Loss = -9995.570330488792
Iteration 500: Loss = -9995.337775883749
Iteration 600: Loss = -9994.61569087443
Iteration 700: Loss = -9994.018232717695
Iteration 800: Loss = -9992.789236265053
Iteration 900: Loss = -9991.955543169297
Iteration 1000: Loss = -9991.308564485214
Iteration 1100: Loss = -9991.028911042149
Iteration 1200: Loss = -9990.84076299666
Iteration 1300: Loss = -9990.674427570468
Iteration 1400: Loss = -9990.532742577654
Iteration 1500: Loss = -9990.098604129897
Iteration 1600: Loss = -9960.280662069232
Iteration 1700: Loss = -9955.29767597192
Iteration 1800: Loss = -9955.252694665236
Iteration 1900: Loss = -9955.242891789287
Iteration 2000: Loss = -9955.216910172241
Iteration 2100: Loss = -9955.214697488667
Iteration 2200: Loss = -9955.213056401577
Iteration 2300: Loss = -9955.211476738998
Iteration 2400: Loss = -9955.20948735672
Iteration 2500: Loss = -9955.206739605272
Iteration 2600: Loss = -9955.202256185994
Iteration 2700: Loss = -9955.214785505399
1
Iteration 2800: Loss = -9955.164086214769
Iteration 2900: Loss = -9954.949360561131
Iteration 3000: Loss = -9954.892160412077
Iteration 3100: Loss = -9954.86343742608
Iteration 3200: Loss = -9954.74750279218
Iteration 3300: Loss = -9954.68119450701
Iteration 3400: Loss = -9954.65957805424
Iteration 3500: Loss = -9954.652855825785
Iteration 3600: Loss = -9954.647251526327
Iteration 3700: Loss = -9954.57208285291
Iteration 3800: Loss = -9954.544900177581
Iteration 3900: Loss = -9947.57445142121
Iteration 4000: Loss = -9947.468199558609
Iteration 4100: Loss = -9947.46626040434
Iteration 4200: Loss = -9947.465613694398
Iteration 4300: Loss = -9947.465526654145
Iteration 4400: Loss = -9947.465334933733
Iteration 4500: Loss = -9947.464355872291
Iteration 4600: Loss = -9947.461992688855
Iteration 4700: Loss = -9947.461658616703
Iteration 4800: Loss = -9947.445720148771
Iteration 4900: Loss = -9947.438777722935
Iteration 5000: Loss = -9947.438418746557
Iteration 5100: Loss = -9947.440258301442
1
Iteration 5200: Loss = -9947.439321773358
2
Iteration 5300: Loss = -9947.44376212206
3
Iteration 5400: Loss = -9947.44029924048
4
Iteration 5500: Loss = -9947.438812689152
5
Iteration 5600: Loss = -9947.443133391545
6
Iteration 5700: Loss = -9947.438603663808
7
Iteration 5800: Loss = -9947.441945307883
8
Iteration 5900: Loss = -9947.449142643785
9
Iteration 6000: Loss = -9947.438899728098
10
Stopping early at iteration 6000 due to no improvement.
tensor([[ 6.9234e-01, -5.3076e+00],
        [-5.5189e+00,  9.0363e-01],
        [-4.0637e+00, -5.5152e-01],
        [-4.1660e+00, -4.4920e-01],
        [-4.2681e+00, -3.4716e-01],
        [-2.9370e+00, -1.6783e+00],
        [-1.3122e+00, -3.3030e+00],
        [-3.9140e+00, -7.0123e-01],
        [-4.5700e+00, -4.5188e-02],
        [-5.6563e+00,  1.0411e+00],
        [-2.2850e+00, -2.3302e+00],
        [-7.0129e-01, -3.9139e+00],
        [-4.4739e+00, -1.4133e-01],
        [ 4.4550e-01, -5.0607e+00],
        [-4.7985e+00,  1.8332e-01],
        [-2.8037e+00, -1.8115e+00],
        [-5.7338e+00,  1.1185e+00],
        [ 4.3335e-01, -5.0486e+00],
        [-3.7395e+00, -8.7572e-01],
        [-4.4015e+00, -2.1375e-01],
        [ 1.5368e+00, -6.1521e+00],
        [-4.4734e+00, -1.4187e-01],
        [-3.1857e+00, -1.4296e+00],
        [-5.0597e-01, -4.1092e+00],
        [-1.4630e+00, -3.1522e+00],
        [-5.4886e+00,  8.7337e-01],
        [-4.5105e+00, -1.0467e-01],
        [-1.2331e-01, -4.4919e+00],
        [-1.3502e+00, -3.2650e+00],
        [-4.3722e+00, -2.4298e-01],
        [-1.6825e+00, -2.9327e+00],
        [ 1.0203e+00, -5.6355e+00],
        [-4.6644e+00,  4.9138e-02],
        [-5.5305e+00,  9.1523e-01],
        [-9.0707e-01, -3.7081e+00],
        [-5.4367e+00,  8.2148e-01],
        [-5.1616e+00,  5.4635e-01],
        [-6.7927e+00,  2.1774e+00],
        [-4.9496e+00,  3.3438e-01],
        [-4.1466e+00, -4.6858e-01],
        [-2.6881e+00, -1.9271e+00],
        [-5.4086e-01, -4.0744e+00],
        [ 3.6249e-01, -4.9777e+00],
        [-4.3767e+00, -2.3852e-01],
        [-8.0426e-01, -3.8110e+00],
        [-4.2205e+00, -3.9474e-01],
        [-3.2256e+00, -1.3896e+00],
        [-5.0800e+00,  4.6477e-01],
        [-4.3796e+00, -2.3562e-01],
        [-3.0604e+00, -1.5549e+00],
        [-5.5582e+00,  9.4296e-01],
        [-3.6756e+00, -9.3958e-01],
        [ 1.0595e+00, -5.6747e+00],
        [-3.4374e+00, -1.1779e+00],
        [-4.3755e+00, -2.3972e-01],
        [-3.1850e+00, -1.4302e+00],
        [-5.3942e+00,  7.7898e-01],
        [ 1.0586e+00, -5.6739e+00],
        [-7.4511e-01, -3.8701e+00],
        [-5.7139e+00,  1.0987e+00],
        [-3.3142e+00, -1.3010e+00],
        [-3.1963e+00, -1.4190e+00],
        [-4.8180e+00,  2.0278e-01],
        [-3.5986e+00, -1.0166e+00],
        [-5.6512e+00,  1.0359e+00],
        [-3.2085e+00, -1.4068e+00],
        [-4.0654e+00, -5.4981e-01],
        [-3.7185e+00, -8.9672e-01],
        [-4.6190e+00,  3.7734e-03],
        [-2.8466e+00, -1.7686e+00],
        [-6.3583e-01, -3.9794e+00],
        [ 6.4034e-01, -5.2556e+00],
        [-6.3994e+00,  1.7842e+00],
        [ 1.4914e+00, -6.1066e+00],
        [-2.9497e+00, -1.6655e+00],
        [-1.5043e+00, -3.1109e+00],
        [-4.9382e+00,  3.2294e-01],
        [-6.1360e+00,  1.5208e+00],
        [-4.5257e+00, -8.9492e-02],
        [-3.0899e+00, -1.5253e+00],
        [-1.5109e-01, -4.4641e+00],
        [-4.2814e+00, -3.3385e-01],
        [-4.2505e+00, -3.6470e-01],
        [-6.4742e+00,  1.8590e+00],
        [-5.2491e+00,  6.3392e-01],
        [-9.0356e-01, -3.7117e+00],
        [-3.2645e+00, -1.3507e+00],
        [-4.6391e+00,  2.3903e-02],
        [-7.2269e-01, -3.8925e+00],
        [-1.4062e-03, -4.6138e+00],
        [ 3.9001e-02, -4.6542e+00],
        [-4.8411e+00,  2.2592e-01],
        [-3.5012e+00, -1.1141e+00],
        [ 8.4799e-02, -4.7000e+00],
        [-4.0594e+00, -5.5583e-01],
        [-1.8674e-01, -4.4285e+00],
        [-2.0493e+00, -2.5659e+00],
        [-3.5238e+00, -1.0914e+00],
        [-4.1591e+00, -4.5616e-01],
        [-3.4436e+00, -1.1716e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7812, 0.2188],
        [0.2776, 0.7224]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3351, 0.6649], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2224, 0.0931],
         [0.6179, 0.1507]],

        [[0.1262, 0.0995],
         [0.8878, 0.4826]],

        [[0.6583, 0.0869],
         [0.6532, 0.0495]],

        [[0.0107, 0.0867],
         [0.1035, 0.0613]],

        [[0.5417, 0.1065],
         [0.9998, 0.3174]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 30
Adjusted Rand Index: 0.15254928176615415
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 12
Adjusted Rand Index: 0.5733577800500688
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 9
Adjusted Rand Index: 0.6690584936388014
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 10
Adjusted Rand Index: 0.6364547952958964
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 18
Adjusted Rand Index: 0.40361796286827584
Global Adjusted Rand Index: 0.4667794422350858
Average Adjusted Rand Index: 0.4870076627238393
Iteration 0: Loss = -19895.88669493878
Iteration 10: Loss = -9993.024492526356
Iteration 20: Loss = -9992.895459828904
Iteration 30: Loss = -9992.797963927082
Iteration 40: Loss = -9992.703728578583
Iteration 50: Loss = -9992.616861081031
Iteration 60: Loss = -9992.53745525919
Iteration 70: Loss = -9992.46436645424
Iteration 80: Loss = -9992.395261809941
Iteration 90: Loss = -9992.3275240918
Iteration 100: Loss = -9992.256921096188
Iteration 110: Loss = -9992.17683499349
Iteration 120: Loss = -9992.075158064421
Iteration 130: Loss = -9991.92879161457
Iteration 140: Loss = -9991.696030577012
Iteration 150: Loss = -9991.344471785234
Iteration 160: Loss = -9990.970779905363
Iteration 170: Loss = -9990.7055855875
Iteration 180: Loss = -9990.529074371503
Iteration 190: Loss = -9990.27858604042
Iteration 200: Loss = -9989.82276256174
Iteration 210: Loss = -9989.781432393485
Iteration 220: Loss = -9989.708692434637
Iteration 230: Loss = -9989.436967738642
Iteration 240: Loss = -9989.196405626177
Iteration 250: Loss = -9989.169250297495
Iteration 260: Loss = -9989.169711535485
1
Iteration 270: Loss = -9989.170303483523
2
Iteration 280: Loss = -9989.170459029283
3
Stopping early at iteration 279 due to no improvement.
pi: tensor([[0.9438, 0.0562],
        [0.9635, 0.0365]], dtype=torch.float64)
alpha: tensor([0.9445, 0.0555])
beta: tensor([[[0.1407, 0.1144],
         [0.7094, 0.1502]],

        [[0.1502, 0.1477],
         [0.4955, 0.2843]],

        [[0.0025, 0.0611],
         [0.2648, 0.3747]],

        [[0.0222, 0.0675],
         [0.5589, 0.3590]],

        [[0.3725, 0.1811],
         [0.7753, 0.0698]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.017056499655107544
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 36
Adjusted Rand Index: 0.03609912235729749
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00879578788541848
Average Adjusted Rand Index: 0.010631124402481007
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19895.929797252535
Iteration 100: Loss = -10007.921229629826
Iteration 200: Loss = -9996.514817247742
Iteration 300: Loss = -9995.44673168554
Iteration 400: Loss = -9995.030203650234
Iteration 500: Loss = -9994.83705087726
Iteration 600: Loss = -9994.718148724036
Iteration 700: Loss = -9994.630622366814
Iteration 800: Loss = -9994.565870870712
Iteration 900: Loss = -9994.520581890194
Iteration 1000: Loss = -9994.484868117304
Iteration 1100: Loss = -9994.452122128061
Iteration 1200: Loss = -9994.420789680376
Iteration 1300: Loss = -9994.3899228066
Iteration 1400: Loss = -9994.358961152875
Iteration 1500: Loss = -9994.32612887374
Iteration 1600: Loss = -9994.289898719735
Iteration 1700: Loss = -9994.247560370013
Iteration 1800: Loss = -9994.19597331868
Iteration 1900: Loss = -9994.13194014621
Iteration 2000: Loss = -9994.051722817912
Iteration 2100: Loss = -9993.951994756873
Iteration 2200: Loss = -9993.814396262787
Iteration 2300: Loss = -9993.615798690862
Iteration 2400: Loss = -9993.334330030031
Iteration 2500: Loss = -9992.875104622932
Iteration 2600: Loss = -9991.02729174056
Iteration 2700: Loss = -9987.92405371614
Iteration 2800: Loss = -9987.779052907741
Iteration 2900: Loss = -9987.730035047964
Iteration 3000: Loss = -9987.710446240757
Iteration 3100: Loss = -9987.698256620764
Iteration 3200: Loss = -9987.689619479206
Iteration 3300: Loss = -9987.68275889434
Iteration 3400: Loss = -9987.67653411962
Iteration 3500: Loss = -9987.669633666863
Iteration 3600: Loss = -9987.658822139872
Iteration 3700: Loss = -9987.633218896177
Iteration 3800: Loss = -9987.564676534852
Iteration 3900: Loss = -9987.513540210386
Iteration 4000: Loss = -9987.498047380113
Iteration 4100: Loss = -9987.491238828148
Iteration 4200: Loss = -9987.487621201297
Iteration 4300: Loss = -9987.485366517376
Iteration 4400: Loss = -9987.483901287243
Iteration 4500: Loss = -9987.482789359163
Iteration 4600: Loss = -9987.481964103148
Iteration 4700: Loss = -9987.481363712754
Iteration 4800: Loss = -9987.480930584501
Iteration 4900: Loss = -9987.480588524768
Iteration 5000: Loss = -9987.480314392615
Iteration 5100: Loss = -9987.480116088633
Iteration 5200: Loss = -9987.47996653179
Iteration 5300: Loss = -9987.479759520007
Iteration 5400: Loss = -9987.480005674684
1
Iteration 5500: Loss = -9987.48395931808
2
Iteration 5600: Loss = -9987.505338754083
3
Iteration 5700: Loss = -9987.479349557647
Iteration 5800: Loss = -9987.479416338114
1
Iteration 5900: Loss = -9987.550995410564
2
Iteration 6000: Loss = -9987.479110781745
Iteration 6100: Loss = -9987.501346101093
1
Iteration 6200: Loss = -9987.478989243476
Iteration 6300: Loss = -9987.499642152887
1
Iteration 6400: Loss = -9987.478845938907
Iteration 6500: Loss = -9987.47878074997
Iteration 6600: Loss = -9987.479347064706
1
Iteration 6700: Loss = -9987.478674383901
Iteration 6800: Loss = -9987.478611052284
Iteration 6900: Loss = -9987.478645573674
1
Iteration 7000: Loss = -9987.47851047944
Iteration 7100: Loss = -9987.478491946651
Iteration 7200: Loss = -9987.480797355502
1
Iteration 7300: Loss = -9987.478461639575
Iteration 7400: Loss = -9987.478459324057
Iteration 7500: Loss = -9987.511196272946
1
Iteration 7600: Loss = -9987.478391632732
Iteration 7700: Loss = -9987.478363050654
Iteration 7800: Loss = -9987.47838111337
1
Iteration 7900: Loss = -9987.486235081398
2
Iteration 8000: Loss = -9987.478333952497
Iteration 8100: Loss = -9987.478318474065
Iteration 8200: Loss = -9987.478316474722
Iteration 8300: Loss = -9987.478312686228
Iteration 8400: Loss = -9987.478269084973
Iteration 8500: Loss = -9987.47828729932
1
Iteration 8600: Loss = -9987.478517058265
2
Iteration 8700: Loss = -9987.490130521308
3
Iteration 8800: Loss = -9987.478238494492
Iteration 8900: Loss = -9987.480550667347
1
Iteration 9000: Loss = -9987.478269649226
2
Iteration 9100: Loss = -9987.47832052762
3
Iteration 9200: Loss = -9987.577349461533
4
Iteration 9300: Loss = -9987.478227608415
Iteration 9400: Loss = -9987.479845377793
1
Iteration 9500: Loss = -9987.47823448625
2
Iteration 9600: Loss = -9987.60075144143
3
Iteration 9700: Loss = -9987.478186240784
Iteration 9800: Loss = -9987.478156055558
Iteration 9900: Loss = -9987.479713423068
1
Iteration 10000: Loss = -9987.478125119793
Iteration 10100: Loss = -9987.4781687412
1
Iteration 10200: Loss = -9987.479551588856
2
Iteration 10300: Loss = -9987.478131208523
3
Iteration 10400: Loss = -9987.478144637937
4
Iteration 10500: Loss = -9987.48004309761
5
Iteration 10600: Loss = -9987.478147129244
6
Iteration 10700: Loss = -9987.47880203486
7
Iteration 10800: Loss = -9987.478152168484
8
Iteration 10900: Loss = -9987.478122586614
Iteration 11000: Loss = -9987.478226178393
1
Iteration 11100: Loss = -9987.478128606193
2
Iteration 11200: Loss = -9987.482804148693
3
Iteration 11300: Loss = -9987.478129508632
4
Iteration 11400: Loss = -9987.478137631857
5
Iteration 11500: Loss = -9987.515360592186
6
Iteration 11600: Loss = -9987.47812764657
7
Iteration 11700: Loss = -9987.478109799882
Iteration 11800: Loss = -9987.48742316757
1
Iteration 11900: Loss = -9987.478147085745
2
Iteration 12000: Loss = -9987.478130720921
3
Iteration 12100: Loss = -9987.490752452964
4
Iteration 12200: Loss = -9987.478127875464
5
Iteration 12300: Loss = -9987.588174048176
6
Iteration 12400: Loss = -9987.47815136223
7
Iteration 12500: Loss = -9987.47812761583
8
Iteration 12600: Loss = -9987.478299198208
9
Iteration 12700: Loss = -9987.478097595167
Iteration 12800: Loss = -9987.498709472058
1
Iteration 12900: Loss = -9987.478117221044
2
Iteration 13000: Loss = -9987.478128225526
3
Iteration 13100: Loss = -9987.478209904964
4
Iteration 13200: Loss = -9987.478104204141
5
Iteration 13300: Loss = -9987.478129102105
6
Iteration 13400: Loss = -9987.479768407042
7
Iteration 13500: Loss = -9987.478147357686
8
Iteration 13600: Loss = -9987.486990755144
9
Iteration 13700: Loss = -9987.478125539814
10
Stopping early at iteration 13700 due to no improvement.
tensor([[ 1.6418, -3.2312],
        [ 1.4294, -3.4440],
        [ 1.6134, -3.2117],
        [ 0.3431, -4.4821],
        [ 1.6985, -3.0859],
        [ 1.5947, -3.2365],
        [ 1.6419, -3.1427],
        [ 1.5373, -3.2017],
        [ 1.6571, -3.1453],
        [ 1.7128, -3.0994],
        [ 1.6102, -3.1895],
        [ 1.6715, -3.1458],
        [ 1.5894, -3.1426],
        [ 1.4098, -3.3610],
        [ 1.4106, -3.4325],
        [ 1.7056, -3.1151],
        [ 1.5707, -3.1719],
        [ 1.5990, -3.2514],
        [ 1.6898, -3.1174],
        [ 1.5744, -3.2393],
        [ 1.7164, -3.1502],
        [ 1.6576, -3.1526],
        [ 1.4039, -3.4177],
        [ 1.6981, -3.1271],
        [ 1.1546, -3.6949],
        [ 1.2288, -3.6452],
        [ 0.3804, -3.7852],
        [ 1.4521, -3.4147],
        [ 0.4886, -4.3239],
        [ 1.6160, -3.2334],
        [ 1.5744, -3.2328],
        [ 1.7227, -3.1140],
        [ 1.5820, -3.2689],
        [ 1.4433, -3.2180],
        [ 1.7161, -3.1195],
        [ 1.4677, -3.4239],
        [ 1.6000, -3.0884],
        [ 1.7331, -3.1210],
        [ 1.7135, -3.1400],
        [ 1.7014, -3.1307],
        [ 1.4178, -2.9903],
        [ 1.7052, -3.1613],
        [ 1.6142, -3.1986],
        [ 1.4842, -3.3179],
        [ 1.3287, -3.5006],
        [ 1.4603, -3.3197],
        [ 1.6003, -3.1484],
        [ 1.7254, -3.1151],
        [ 1.6886, -3.0910],
        [ 1.7456, -3.1396],
        [ 0.9938, -3.8568],
        [ 1.6830, -3.1970],
        [ 1.7284, -3.1195],
        [ 1.4845, -2.9186],
        [ 1.3776, -3.4444],
        [ 0.8166, -3.8780],
        [ 1.4551, -3.3303],
        [ 1.5852, -3.2734],
        [ 1.6597, -3.1601],
        [ 1.6049, -2.9958],
        [ 1.2289, -3.5748],
        [ 0.1013, -4.7165],
        [ 1.6829, -3.1601],
        [ 0.9002, -3.9186],
        [ 1.3811, -3.4482],
        [ 1.7058, -3.1117],
        [ 1.4499, -3.3693],
        [ 1.4292, -3.3564],
        [ 1.6263, -3.2028],
        [ 0.9229, -3.9115],
        [ 1.0829, -3.7530],
        [ 1.6310, -3.2319],
        [ 0.4969, -4.3238],
        [ 1.7343, -3.1315],
        [ 1.6303, -3.1935],
        [ 0.1102, -4.7254],
        [ 1.0360, -3.7844],
        [ 1.7025, -3.0888],
        [ 1.3826, -3.4155],
        [ 1.3574, -3.4412],
        [ 1.7136, -3.1155],
        [ 1.3487, -2.8563],
        [ 1.5914, -3.2327],
        [ 1.6159, -3.0472],
        [ 1.6455, -3.1374],
        [ 1.5579, -3.2200],
        [ 0.9998, -3.8115],
        [ 1.7238, -3.1102],
        [ 1.3903, -3.3461],
        [ 1.7045, -3.1280],
        [ 0.5881, -4.2017],
        [ 1.6835, -3.1431],
        [ 1.6489, -3.0548],
        [ 1.4333, -3.4254],
        [ 1.7068, -3.1006],
        [ 1.5727, -3.2451],
        [ 1.3710, -3.4387],
        [ 1.5829, -3.2580],
        [ 1.6772, -3.1799],
        [ 1.1072, -3.7398]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0160, 0.9840],
        [0.0665, 0.9335]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9917, 0.0083], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1337, 0.1328],
         [0.7094, 0.1458]],

        [[0.1502, 0.0835],
         [0.4955, 0.2843]],

        [[0.0025, 0.0623],
         [0.2648, 0.3747]],

        [[0.0222, 0.0702],
         [0.5589, 0.3590]],

        [[0.3725, 0.1622],
         [0.7753, 0.0698]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.022741429840595545
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 66
Adjusted Rand Index: 0.0639954496618846
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.016616913647884465
Average Adjusted Rand Index: 0.01734737590049603
Iteration 0: Loss = -30840.803414181362
Iteration 10: Loss = -9995.780675878874
Iteration 20: Loss = -9995.780675878874
1
Iteration 30: Loss = -9995.780675878874
2
Iteration 40: Loss = -9995.780675878876
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[8.9313e-19, 1.0000e+00],
        [9.8066e-17, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([9.3499e-17, 1.0000e+00])
beta: tensor([[[0.2129, 0.1068],
         [0.1988, 0.1380]],

        [[0.7681, 0.1617],
         [0.2373, 0.6017]],

        [[0.6072, 0.0652],
         [0.7834, 0.8215]],

        [[0.2784, 0.1668],
         [0.1493, 0.3798]],

        [[0.0701, 0.2159],
         [0.7299, 0.5573]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30840.867832390944
Iteration 100: Loss = -10035.38530338107
Iteration 200: Loss = -10016.520316472304
Iteration 300: Loss = -10001.638576460424
Iteration 400: Loss = -9999.25350997337
Iteration 500: Loss = -9998.183632753913
Iteration 600: Loss = -9997.561205191958
Iteration 700: Loss = -9997.151043624535
Iteration 800: Loss = -9996.860118304734
Iteration 900: Loss = -9996.643910252793
Iteration 1000: Loss = -9996.477712506172
Iteration 1100: Loss = -9996.346361602795
Iteration 1200: Loss = -9996.240300631795
Iteration 1300: Loss = -9996.153005432128
Iteration 1400: Loss = -9996.080201605377
Iteration 1500: Loss = -9996.018586913087
Iteration 1600: Loss = -9995.96582740036
Iteration 1700: Loss = -9995.92001977171
Iteration 1800: Loss = -9995.879955029535
Iteration 1900: Loss = -9995.844466556317
Iteration 2000: Loss = -9995.81265549188
Iteration 2100: Loss = -9995.783626563105
Iteration 2200: Loss = -9995.756483704494
Iteration 2300: Loss = -9995.729986959577
Iteration 2400: Loss = -9995.701951559407
Iteration 2500: Loss = -9995.667285041267
Iteration 2600: Loss = -9995.62384721786
Iteration 2700: Loss = -9995.588368811521
Iteration 2800: Loss = -9995.556930692585
Iteration 2900: Loss = -9995.53876046261
Iteration 3000: Loss = -9995.523478480818
Iteration 3100: Loss = -9995.503112325581
Iteration 3200: Loss = -9995.480395128492
Iteration 3300: Loss = -9995.451977997716
Iteration 3400: Loss = -9995.42047607131
Iteration 3500: Loss = -9995.37371556315
Iteration 3600: Loss = -9995.307458360438
Iteration 3700: Loss = -9995.205368004348
Iteration 3800: Loss = -9995.002775586408
Iteration 3900: Loss = -9994.491335603412
Iteration 4000: Loss = -9994.272600969456
Iteration 4100: Loss = -9994.130066477825
Iteration 4200: Loss = -9993.898304952516
Iteration 4300: Loss = -9993.04903743953
Iteration 4400: Loss = -9992.665882922069
Iteration 4500: Loss = -9992.382024925011
Iteration 4600: Loss = -9992.226230542625
Iteration 4700: Loss = -9992.182125193589
Iteration 4800: Loss = -9992.113562661963
Iteration 4900: Loss = -9991.998620338889
Iteration 5000: Loss = -9991.857984152117
Iteration 5100: Loss = -9991.72860855897
Iteration 5200: Loss = -9991.63223232083
Iteration 5300: Loss = -9991.561521807107
Iteration 5400: Loss = -9991.504702433875
Iteration 5500: Loss = -9991.44228998375
Iteration 5600: Loss = -9991.385564279524
Iteration 5700: Loss = -9991.342274891822
Iteration 5800: Loss = -9991.310358486628
Iteration 5900: Loss = -9991.280742262152
Iteration 6000: Loss = -9991.260065761857
Iteration 6100: Loss = -9991.243574658263
Iteration 6200: Loss = -9991.228076976036
Iteration 6300: Loss = -9991.200797759328
Iteration 6400: Loss = -9991.189604036894
Iteration 6500: Loss = -9991.182263580136
Iteration 6600: Loss = -9991.175547696821
Iteration 6700: Loss = -9991.170600849522
Iteration 6800: Loss = -9991.167041340492
Iteration 6900: Loss = -9991.164687511084
Iteration 7000: Loss = -9991.163098913861
Iteration 7100: Loss = -9991.1621714608
Iteration 7200: Loss = -9991.161579026877
Iteration 7300: Loss = -9991.160938724419
Iteration 7400: Loss = -9991.16015517388
Iteration 7500: Loss = -9991.16121123175
1
Iteration 7600: Loss = -9991.159522054026
Iteration 7700: Loss = -9991.190236782822
1
Iteration 7800: Loss = -9991.15919287289
Iteration 7900: Loss = -9991.159382560492
1
Iteration 8000: Loss = -9991.159103963519
Iteration 8100: Loss = -9991.159073127035
Iteration 8200: Loss = -9991.159013092758
Iteration 8300: Loss = -9991.160931741719
1
Iteration 8400: Loss = -9991.158973680534
Iteration 8500: Loss = -9991.17047515547
1
Iteration 8600: Loss = -9991.158924327947
Iteration 8700: Loss = -9991.158897578236
Iteration 8800: Loss = -9991.158968632208
1
Iteration 8900: Loss = -9991.158876108302
Iteration 9000: Loss = -9991.158878625345
1
Iteration 9100: Loss = -9991.15929838286
2
Iteration 9200: Loss = -9991.158855138432
Iteration 9300: Loss = -9991.158832498664
Iteration 9400: Loss = -9991.15915954699
1
Iteration 9500: Loss = -9991.158819858427
Iteration 9600: Loss = -9991.158837676621
1
Iteration 9700: Loss = -9991.1591842686
2
Iteration 9800: Loss = -9991.158817280862
Iteration 9900: Loss = -9991.158815246985
Iteration 10000: Loss = -9991.160144121459
1
Iteration 10100: Loss = -9991.158791718593
Iteration 10200: Loss = -9991.15882095887
1
Iteration 10300: Loss = -9991.19496458615
2
Iteration 10400: Loss = -9991.158795140847
3
Iteration 10500: Loss = -9991.15879614813
4
Iteration 10600: Loss = -9991.159577898716
5
Iteration 10700: Loss = -9991.15879173053
6
Iteration 10800: Loss = -9991.158767373186
Iteration 10900: Loss = -9991.158762746309
Iteration 11000: Loss = -9991.158994286605
1
Iteration 11100: Loss = -9991.158765336442
2
Iteration 11200: Loss = -9991.158728486018
Iteration 11300: Loss = -9991.168116823277
1
Iteration 11400: Loss = -9991.15870119817
Iteration 11500: Loss = -9991.158701742117
1
Iteration 11600: Loss = -9991.158712770986
2
Iteration 11700: Loss = -9991.16390992886
3
Iteration 11800: Loss = -9991.158684735208
Iteration 11900: Loss = -9991.158671791392
Iteration 12000: Loss = -9991.162928857746
1
Iteration 12100: Loss = -9991.158700601776
2
Iteration 12200: Loss = -9991.158739527966
3
Iteration 12300: Loss = -9991.15914346062
4
Iteration 12400: Loss = -9991.158679455328
5
Iteration 12500: Loss = -9991.258241052828
6
Iteration 12600: Loss = -9991.158709305422
7
Iteration 12700: Loss = -9991.158704661217
8
Iteration 12800: Loss = -9991.161659489717
9
Iteration 12900: Loss = -9991.158682617119
10
Stopping early at iteration 12900 due to no improvement.
tensor([[-3.9675,  2.5717],
        [-3.7801,  2.3691],
        [-3.2541,  1.6126],
        [-2.9182,  1.5317],
        [-3.0598,  1.6677],
        [-3.0318,  1.6276],
        [-3.1271,  0.5268],
        [-3.0399,  1.4299],
        [-2.8210,  0.9914],
        [-3.8890,  0.3773],
        [-2.5125,  1.0603],
        [-2.9974,  1.2349],
        [-2.9384,  1.5469],
        [-3.6188,  2.1004],
        [-3.4769,  1.2546],
        [-3.1383,  1.0516],
        [-3.7271,  0.9600],
        [-3.2529,  1.8119],
        [-3.0554,  1.6690],
        [-2.8929,  1.3432],
        [-4.0702,  1.6195],
        [-2.7223,  1.3331],
        [-3.0204,  1.6334],
        [-3.6637,  0.8189],
        [-3.2417,  1.8501],
        [-5.0680,  0.4528],
        [-3.3074,  0.3694],
        [-3.7447,  1.6974],
        [-3.6639,  0.3770],
        [-3.2473,  1.8470],
        [-3.3540,  1.1430],
        [-3.4235,  1.1822],
        [-3.2757,  1.8018],
        [-3.7084,  0.6148],
        [-3.2706,  1.3638],
        [-4.5784,  1.7337],
        [-3.2458,  1.8530],
        [-3.3611,  1.9566],
        [-3.3949,  1.8609],
        [-3.0366,  1.6215],
        [-3.1619,  1.7596],
        [-4.1669,  1.7998],
        [-2.7356,  1.2548],
        [-2.9353,  0.8965],
        [-2.9098,  1.5103],
        [-2.9839,  1.5032],
        [-2.5288,  1.1414],
        [-3.9145,  0.9583],
        [-2.5042,  0.9150],
        [-3.9643,  2.1169],
        [-3.5012,  1.5209],
        [-3.6393,  2.2524],
        [-4.1529,  0.9238],
        [-3.2084,  1.7328],
        [-2.9138,  1.2843],
        [-3.8322,  1.4917],
        [-3.1380,  1.7517],
        [-3.5946,  1.6512],
        [-2.8057,  1.3998],
        [-3.7202,  2.0463],
        [-3.4319,  0.4202],
        [-2.8534,  1.4229],
        [-4.7219,  0.3787],
        [-2.8385,  1.4052],
        [-3.9614,  0.9340],
        [-3.4379,  1.4487],
        [-2.8108,  1.4101],
        [-3.1327,  1.7465],
        [-3.1076,  1.5390],
        [-3.0264,  1.6327],
        [-3.1384,  1.4778],
        [-3.5198,  1.9952],
        [-3.2720,  0.9573],
        [-3.4036,  1.9704],
        [-2.9991,  1.4574],
        [-3.1397,  1.4945],
        [-3.0768,  1.3636],
        [-3.7710,  1.3269],
        [-2.8073,  0.8333],
        [-2.8722,  0.9949],
        [-3.0764,  1.3443],
        [-3.7390,  1.1468],
        [-2.9449,  1.5052],
        [-3.1287,  1.5746],
        [-3.2083,  1.7194],
        [-3.3180,  0.5477],
        [-2.6939,  1.3055],
        [-3.0808,  1.5636],
        [-4.3725,  2.3980],
        [-3.4242,  1.2523],
        [-2.7588,  1.2943],
        [-3.7786,  0.9095],
        [-2.9860,  0.2679],
        [-3.3386,  1.8808],
        [-2.8291,  1.4427],
        [-2.8625,  1.3563],
        [-3.1519,  0.8839],
        [-4.6803,  0.2177],
        [-3.4157,  1.8665],
        [-3.2436,  1.8395]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1189, 0.8811],
        [0.0617, 0.9383]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0109, 0.9891], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2260, 0.1149],
         [0.1988, 0.1385]],

        [[0.7681, 0.1608],
         [0.2373, 0.6017]],

        [[0.6072, 0.0594],
         [0.7834, 0.8215]],

        [[0.2784, 0.1634],
         [0.1493, 0.3798]],

        [[0.0701, 0.1871],
         [0.7299, 0.5573]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.017056499655107544
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.004680504024354113
Average Adjusted Rand Index: 0.0028186214429958686
Iteration 0: Loss = -17174.032656568055
Iteration 10: Loss = -9995.780630017634
Iteration 20: Loss = -9995.77790137848
Iteration 30: Loss = -9995.7192395635
Iteration 40: Loss = -9994.865799433726
Iteration 50: Loss = -9992.520512343852
Iteration 60: Loss = -9991.727571717363
Iteration 70: Loss = -9991.72639973567
Iteration 80: Loss = -9991.730217003165
1
Iteration 90: Loss = -9991.731061417044
2
Iteration 100: Loss = -9991.731168707358
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.9363, 0.0637],
        [0.9319, 0.0681]], dtype=torch.float64)
alpha: tensor([0.9370, 0.0630])
beta: tensor([[[0.1378, 0.1240],
         [0.0522, 0.1858]],

        [[0.3576, 0.1556],
         [0.9922, 0.8960]],

        [[0.0041, 0.0608],
         [0.3841, 0.9599]],

        [[0.4386, 0.1545],
         [0.1431, 0.2628]],

        [[0.3185, 0.1840],
         [0.9705, 0.7213]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.017056499655107544
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00557637714542361
Average Adjusted Rand Index: 0.0034112999310215086
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17174.490833499036
Iteration 100: Loss = -9996.441577291822
Iteration 200: Loss = -9994.940781938305
Iteration 300: Loss = -9994.396138657155
Iteration 400: Loss = -9994.00689827502
Iteration 500: Loss = -9993.770495492048
Iteration 600: Loss = -9993.57319993874
Iteration 700: Loss = -9993.3840713986
Iteration 800: Loss = -9993.189549121527
Iteration 900: Loss = -9992.980708506258
Iteration 1000: Loss = -9992.73776642621
Iteration 1100: Loss = -9992.396191724509
Iteration 1200: Loss = -9991.862970497677
Iteration 1300: Loss = -9991.202666311994
Iteration 1400: Loss = -9990.680740112395
Iteration 1500: Loss = -9990.162593281197
Iteration 1600: Loss = -9963.807228404556
Iteration 1700: Loss = -9957.223568136913
Iteration 1800: Loss = -9955.379189816698
Iteration 1900: Loss = -9955.271353060785
Iteration 2000: Loss = -9955.227274392948
Iteration 2100: Loss = -9955.196546868043
Iteration 2200: Loss = -9955.170489821996
Iteration 2300: Loss = -9955.145967573537
Iteration 2400: Loss = -9955.109411001446
Iteration 2500: Loss = -9955.052256213075
Iteration 2600: Loss = -9954.943605179546
Iteration 2700: Loss = -9954.842665399368
Iteration 2800: Loss = -9954.796562836758
Iteration 2900: Loss = -9954.774352106338
Iteration 3000: Loss = -9954.734896195669
Iteration 3100: Loss = -9954.578385102304
Iteration 3200: Loss = -9954.57260963836
Iteration 3300: Loss = -9954.555040765652
Iteration 3400: Loss = -9954.550309337508
Iteration 3500: Loss = -9954.536886795246
Iteration 3600: Loss = -9954.429360078124
Iteration 3700: Loss = -9950.890619532882
Iteration 3800: Loss = -9949.548685869731
Iteration 3900: Loss = -9949.543055850843
Iteration 4000: Loss = -9949.541324218935
Iteration 4100: Loss = -9949.540491598562
Iteration 4200: Loss = -9949.540611151837
1
Iteration 4300: Loss = -9949.539594791824
Iteration 4400: Loss = -9949.54227258421
1
Iteration 4500: Loss = -9949.539072559028
Iteration 4600: Loss = -9949.542038392878
1
Iteration 4700: Loss = -9949.538675901298
Iteration 4800: Loss = -9949.541754270871
1
Iteration 4900: Loss = -9949.53843962105
Iteration 5000: Loss = -9949.5384991503
1
Iteration 5100: Loss = -9949.538260634276
Iteration 5200: Loss = -9949.538292912152
1
Iteration 5300: Loss = -9949.538167365972
Iteration 5400: Loss = -9949.561150560594
1
Iteration 5500: Loss = -9949.538042214615
Iteration 5600: Loss = -9949.548765573434
1
Iteration 5700: Loss = -9949.540386108369
2
Iteration 5800: Loss = -9949.54450374547
3
Iteration 5900: Loss = -9949.563304784037
4
Iteration 6000: Loss = -9949.537883708063
Iteration 6100: Loss = -9949.537860735774
Iteration 6200: Loss = -9949.538722190964
1
Iteration 6300: Loss = -9949.563616647763
2
Iteration 6400: Loss = -9949.549066264673
3
Iteration 6500: Loss = -9949.565432742611
4
Iteration 6600: Loss = -9949.537754217816
Iteration 6700: Loss = -9949.538007344494
1
Iteration 6800: Loss = -9949.537955599433
2
Iteration 6900: Loss = -9949.537723424666
Iteration 7000: Loss = -9949.537764233242
1
Iteration 7100: Loss = -9949.53858318578
2
Iteration 7200: Loss = -9949.5377188226
Iteration 7300: Loss = -9949.539129856064
1
Iteration 7400: Loss = -9949.543471058963
2
Iteration 7500: Loss = -9949.548074264707
3
Iteration 7600: Loss = -9949.541567838172
4
Iteration 7700: Loss = -9949.541186428587
5
Iteration 7800: Loss = -9949.538633578408
6
Iteration 7900: Loss = -9949.549507567463
7
Iteration 8000: Loss = -9949.540575963681
8
Iteration 8100: Loss = -9949.575478892937
9
Iteration 8200: Loss = -9949.538968944771
10
Stopping early at iteration 8200 due to no improvement.
tensor([[-5.1447,  3.7578],
        [-2.8814,  1.4248],
        [ 2.6557, -4.1792],
        [ 1.2824, -2.6694],
        [-2.7050,  1.2759],
        [-0.0481, -1.3385],
        [-0.8948, -0.6677],
        [ 1.0521, -2.5091],
        [ 1.0929, -3.0025],
        [ 1.3802, -2.7728],
        [ 0.0267, -1.5965],
        [-1.1723, -0.3250],
        [ 1.6937, -3.5233],
        [-2.8715,  1.4679],
        [ 1.2463, -2.6329],
        [-1.1401, -0.5834],
        [ 1.6825, -4.3235],
        [-4.9132,  0.4014],
        [-1.5555, -1.8822],
        [ 1.8034, -3.6801],
        [-3.7504,  1.9273],
        [ 0.9585, -3.2331],
        [ 1.0321, -2.4193],
        [-1.9688, -2.3043],
        [-3.9338,  2.5434],
        [ 0.9257, -2.8062],
        [ 0.2130, -1.7672],
        [-3.3986,  1.7579],
        [-2.3104,  0.5587],
        [-2.4814,  0.8469],
        [ 0.2624, -1.9618],
        [-1.5159,  0.1295],
        [-2.5788,  1.1340],
        [ 0.9634, -2.6016],
        [-1.5571, -0.0824],
        [-3.6784,  2.2854],
        [-1.1262, -0.2842],
        [ 0.9787, -2.6541],
        [ 0.9106, -4.4705],
        [-0.1254, -2.2066],
        [-2.0252,  0.6382],
        [-4.6966,  3.2765],
        [-2.9078,  1.0382],
        [ 1.4085, -3.1413],
        [-1.0497, -0.5901],
        [-1.5259, -0.0131],
        [ 0.2941, -3.1111],
        [ 0.5026, -2.0768],
        [ 1.8895, -3.4074],
        [-4.1373,  0.9316],
        [ 1.9089, -3.3474],
        [-3.0299,  1.6433],
        [-2.5173,  0.8729],
        [-0.1792, -1.5763],
        [-1.1850, -0.7251],
        [ 0.4487, -1.8440],
        [ 2.6181, -4.1074],
        [-1.9494,  0.4194],
        [-1.7014, -0.8248],
        [ 1.4721, -3.1862],
        [ 0.1291, -2.1690],
        [-0.4023, -1.0432],
        [-3.0105,  1.6069],
        [-1.8954,  0.4410],
        [ 2.6273, -4.0156],
        [-1.9703,  0.2613],
        [ 1.5323, -2.9191],
        [ 1.0017, -2.9740],
        [ 0.9017, -3.2990],
        [ 0.7062, -3.3584],
        [-0.5060, -0.8999],
        [-4.1039,  2.1838],
        [ 1.2511, -2.6584],
        [-4.4715,  2.1902],
        [ 0.9771, -2.4624],
        [-3.6429,  0.0325],
        [-1.1439, -2.3985],
        [ 3.0739, -4.4608],
        [-2.3970, -0.8230],
        [ 0.0636, -3.2168],
        [-0.5169, -0.8792],
        [-2.5459,  0.0517],
        [ 1.1128, -2.6349],
        [ 1.0717, -3.0663],
        [ 0.0568, -2.4818],
        [-1.6316,  0.2431],
        [ 1.9443, -3.3331],
        [-2.9130, -0.3380],
        [-3.7263,  0.4845],
        [-2.5048, -0.1819],
        [-2.7322,  0.7426],
        [ 2.0448, -3.4351],
        [-1.0066, -1.6829],
        [-2.3241,  0.7724],
        [ 0.3301, -1.7482],
        [-0.6908, -0.7848],
        [-2.0175,  0.4635],
        [ 0.5636, -1.9790],
        [ 0.1669, -1.5608],
        [-1.5176, -0.4038]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7405, 0.2595],
        [0.1971, 0.8029]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5429, 0.4571], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1521, 0.0994],
         [0.0522, 0.2110]],

        [[0.3576, 0.0946],
         [0.9922, 0.8960]],

        [[0.0041, 0.0866],
         [0.3841, 0.9599]],

        [[0.4386, 0.0863],
         [0.1431, 0.2628]],

        [[0.3185, 0.1062],
         [0.9705, 0.7213]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 89
Adjusted Rand Index: 0.6044444444444445
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 91
Adjusted Rand Index: 0.6691246144255645
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369480537608971
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 90
Adjusted Rand Index: 0.6364547952958964
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 84
Adjusted Rand Index: 0.4569625163087556
Global Adjusted Rand Index: 0.6201877148306101
Average Adjusted Rand Index: 0.6207868848471116
Iteration 0: Loss = -19649.490635391343
Iteration 10: Loss = -9993.282488840741
Iteration 20: Loss = -9992.56281376
Iteration 30: Loss = -9992.225592999705
Iteration 40: Loss = -9992.081637838153
Iteration 50: Loss = -9991.930537217044
Iteration 60: Loss = -9991.698330486095
Iteration 70: Loss = -9991.34768802205
Iteration 80: Loss = -9990.973513334726
Iteration 90: Loss = -9990.70728930544
Iteration 100: Loss = -9990.530456582808
Iteration 110: Loss = -9990.281989645322
Iteration 120: Loss = -9989.82444881861
Iteration 130: Loss = -9989.781604755874
Iteration 140: Loss = -9989.709883050838
Iteration 150: Loss = -9989.440066798881
Iteration 160: Loss = -9989.197128411932
Iteration 170: Loss = -9989.169273900341
Iteration 180: Loss = -9989.169706153647
1
Iteration 190: Loss = -9989.170284471973
2
Iteration 200: Loss = -9989.170410241923
3
Stopping early at iteration 199 due to no improvement.
pi: tensor([[0.9438, 0.0562],
        [0.9635, 0.0365]], dtype=torch.float64)
alpha: tensor([0.9445, 0.0555])
beta: tensor([[[0.1407, 0.1144],
         [0.7577, 0.1502]],

        [[0.7237, 0.1477],
         [0.5461, 0.7892]],

        [[0.5270, 0.0611],
         [0.3868, 0.3063]],

        [[0.1920, 0.0675],
         [0.6815, 0.6210]],

        [[0.4408, 0.1811],
         [0.7655, 0.0742]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.017056499655107544
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 36
Adjusted Rand Index: 0.03609912235729749
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00879578788541848
Average Adjusted Rand Index: 0.010631124402481007
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19649.199097235618
Iteration 100: Loss = -10071.890481596245
Iteration 200: Loss = -10020.059556067976
Iteration 300: Loss = -9998.452126387023
Iteration 400: Loss = -9996.49560857867
Iteration 500: Loss = -9995.827686914534
Iteration 600: Loss = -9995.471709548543
Iteration 700: Loss = -9995.249393783324
Iteration 800: Loss = -9995.096083320697
Iteration 900: Loss = -9994.978517973002
Iteration 1000: Loss = -9994.8773109896
Iteration 1100: Loss = -9994.776149521414
Iteration 1200: Loss = -9994.647719848665
Iteration 1300: Loss = -9994.436981878373
Iteration 1400: Loss = -9994.152783066322
Iteration 1500: Loss = -9993.867456158463
Iteration 1600: Loss = -9993.621279618987
Iteration 1700: Loss = -9993.355888648503
Iteration 1800: Loss = -9993.18997897228
Iteration 1900: Loss = -9993.001962213544
Iteration 2000: Loss = -9992.83599547786
Iteration 2100: Loss = -9992.693806333158
Iteration 2200: Loss = -9992.571309785733
Iteration 2300: Loss = -9992.47339622918
Iteration 2400: Loss = -9992.40304266937
Iteration 2500: Loss = -9992.307682730061
Iteration 2600: Loss = -9992.209792627722
Iteration 2700: Loss = -9992.106607158148
Iteration 2800: Loss = -9991.984381687707
Iteration 2900: Loss = -9991.88673967879
Iteration 3000: Loss = -9991.747890283807
Iteration 3100: Loss = -9991.400187812993
Iteration 3200: Loss = -9988.058020997729
Iteration 3300: Loss = -9987.849030083229
Iteration 3400: Loss = -9987.811880378935
Iteration 3500: Loss = -9987.7791711029
Iteration 3600: Loss = -9987.763231384424
Iteration 3700: Loss = -9987.758301764512
Iteration 3800: Loss = -9987.754700803569
Iteration 3900: Loss = -9987.75177077686
Iteration 4000: Loss = -9987.750142591956
Iteration 4100: Loss = -9987.746925028852
Iteration 4200: Loss = -9987.745007781947
Iteration 4300: Loss = -9987.774429639152
1
Iteration 4400: Loss = -9987.739639961523
Iteration 4500: Loss = -9987.702523847565
Iteration 4600: Loss = -9987.670882353914
Iteration 4700: Loss = -9987.706775228948
1
Iteration 4800: Loss = -9987.668441945454
Iteration 4900: Loss = -9987.668290186524
Iteration 5000: Loss = -9987.66667409203
Iteration 5100: Loss = -9987.666836564294
1
Iteration 5200: Loss = -9987.66826566125
2
Iteration 5300: Loss = -9987.665322337527
Iteration 5400: Loss = -9987.685505224372
1
Iteration 5500: Loss = -9987.667510102969
2
Iteration 5600: Loss = -9987.67470676824
3
Iteration 5700: Loss = -9987.664842726123
Iteration 5800: Loss = -9987.66173742503
Iteration 5900: Loss = -9987.66961247586
1
Iteration 6000: Loss = -9987.66996348359
2
Iteration 6100: Loss = -9987.661172183443
Iteration 6200: Loss = -9987.668971457972
1
Iteration 6300: Loss = -9987.660332404967
Iteration 6400: Loss = -9987.660360760638
1
Iteration 6500: Loss = -9987.790242657373
2
Iteration 6600: Loss = -9987.675047259043
3
Iteration 6700: Loss = -9987.659039153581
Iteration 6800: Loss = -9987.660484411346
1
Iteration 6900: Loss = -9987.658881891788
Iteration 7000: Loss = -9987.658376905953
Iteration 7100: Loss = -9987.658585120997
1
Iteration 7200: Loss = -9987.658020498087
Iteration 7300: Loss = -9987.658213328677
1
Iteration 7400: Loss = -9987.661354094169
2
Iteration 7500: Loss = -9987.657506654868
Iteration 7600: Loss = -9987.65778779968
1
Iteration 7700: Loss = -9987.704108184425
2
Iteration 7800: Loss = -9987.6570689448
Iteration 7900: Loss = -9987.82303220463
1
Iteration 8000: Loss = -9987.656849947443
Iteration 8100: Loss = -9987.656761571046
Iteration 8200: Loss = -9987.657042473973
1
Iteration 8300: Loss = -9987.656521851355
Iteration 8400: Loss = -9987.659530566825
1
Iteration 8500: Loss = -9987.656385588149
Iteration 8600: Loss = -9987.659553219059
1
Iteration 8700: Loss = -9987.657473350791
2
Iteration 8800: Loss = -9987.725632391124
3
Iteration 8900: Loss = -9987.656054178091
Iteration 9000: Loss = -9987.655952975245
Iteration 9100: Loss = -9987.66083558351
1
Iteration 9200: Loss = -9987.657475604481
2
Iteration 9300: Loss = -9987.658743726255
3
Iteration 9400: Loss = -9987.656378064978
4
Iteration 9500: Loss = -9987.658020095845
5
Iteration 9600: Loss = -9987.657004365288
6
Iteration 9700: Loss = -9987.677176281164
7
Iteration 9800: Loss = -9987.655456141112
Iteration 9900: Loss = -9987.655500221945
1
Iteration 10000: Loss = -9987.655355240797
Iteration 10100: Loss = -9987.655363010786
1
Iteration 10200: Loss = -9987.65526295924
Iteration 10300: Loss = -9987.655233651634
Iteration 10400: Loss = -9987.655159261649
Iteration 10500: Loss = -9987.65713735639
1
Iteration 10600: Loss = -9987.75038953512
2
Iteration 10700: Loss = -9987.65512788057
Iteration 10800: Loss = -9987.655318998603
1
Iteration 10900: Loss = -9987.655256113874
2
Iteration 11000: Loss = -9987.655009311547
Iteration 11100: Loss = -9987.877775169725
1
Iteration 11200: Loss = -9987.65492318169
Iteration 11300: Loss = -9987.654913772836
Iteration 11400: Loss = -9987.654895796639
Iteration 11500: Loss = -9987.65486296644
Iteration 11600: Loss = -9987.696798101393
1
Iteration 11700: Loss = -9987.6548288876
Iteration 11800: Loss = -9987.654833202367
1
Iteration 11900: Loss = -9987.678844962309
2
Iteration 12000: Loss = -9987.65474865713
Iteration 12100: Loss = -9987.654724486749
Iteration 12200: Loss = -9987.70833682085
1
Iteration 12300: Loss = -9987.654712245354
Iteration 12400: Loss = -9987.654799968439
1
Iteration 12500: Loss = -9987.662801782113
2
Iteration 12600: Loss = -9987.654678175866
Iteration 12700: Loss = -9987.654683078297
1
Iteration 12800: Loss = -9987.654863837346
2
Iteration 12900: Loss = -9987.654902360451
3
Iteration 13000: Loss = -9987.660507845572
4
Iteration 13100: Loss = -9987.654592422707
Iteration 13200: Loss = -9987.659853099603
1
Iteration 13300: Loss = -9987.654579582675
Iteration 13400: Loss = -9987.656720200659
1
Iteration 13500: Loss = -9987.65458189081
2
Iteration 13600: Loss = -9987.655261160226
3
Iteration 13700: Loss = -9987.654780353736
4
Iteration 13800: Loss = -9987.654677295763
5
Iteration 13900: Loss = -9987.656801354233
6
Iteration 14000: Loss = -9987.663101948063
7
Iteration 14100: Loss = -9987.6545095024
Iteration 14200: Loss = -9987.79919653585
1
Iteration 14300: Loss = -9987.654471949218
Iteration 14400: Loss = -9987.654903072826
1
Iteration 14500: Loss = -9987.654521290771
2
Iteration 14600: Loss = -9987.654433911726
Iteration 14700: Loss = -9987.654679574896
1
Iteration 14800: Loss = -9987.654452947078
2
Iteration 14900: Loss = -9987.657725595913
3
Iteration 15000: Loss = -9987.717860861034
4
Iteration 15100: Loss = -9987.654500842442
5
Iteration 15200: Loss = -9987.6545129241
6
Iteration 15300: Loss = -9987.654481736326
7
Iteration 15400: Loss = -9987.654446900386
8
Iteration 15500: Loss = -9987.6826576955
9
Iteration 15600: Loss = -9987.654420459545
Iteration 15700: Loss = -9987.65770277703
1
Iteration 15800: Loss = -9987.654434281894
2
Iteration 15900: Loss = -9987.656723409627
3
Iteration 16000: Loss = -9987.654462420784
4
Iteration 16100: Loss = -9987.73692159681
5
Iteration 16200: Loss = -9987.654433212967
6
Iteration 16300: Loss = -9987.654419149409
Iteration 16400: Loss = -9987.657610652308
1
Iteration 16500: Loss = -9987.654380911985
Iteration 16600: Loss = -9987.65441283205
1
Iteration 16700: Loss = -9987.66396108394
2
Iteration 16800: Loss = -9987.654433824175
3
Iteration 16900: Loss = -9987.654394527714
4
Iteration 17000: Loss = -9987.654372920446
Iteration 17100: Loss = -9987.654737923323
1
Iteration 17200: Loss = -9987.65437405644
2
Iteration 17300: Loss = -9987.654414445302
3
Iteration 17400: Loss = -9987.654535413107
4
Iteration 17500: Loss = -9987.654416202007
5
Iteration 17600: Loss = -9987.654382093617
6
Iteration 17700: Loss = -9987.674290481686
7
Iteration 17800: Loss = -9987.654366735705
Iteration 17900: Loss = -9987.654393618384
1
Iteration 18000: Loss = -9987.682200700847
2
Iteration 18100: Loss = -9987.654387583689
3
Iteration 18200: Loss = -9987.654368873425
4
Iteration 18300: Loss = -9987.92214680151
5
Iteration 18400: Loss = -9987.654374577192
6
Iteration 18500: Loss = -9987.654364885013
Iteration 18600: Loss = -9987.86132748546
1
Iteration 18700: Loss = -9987.654388339737
2
Iteration 18800: Loss = -9987.65436109919
Iteration 18900: Loss = -9987.668701641516
1
Iteration 19000: Loss = -9987.654362066192
2
Iteration 19100: Loss = -9987.66869443626
3
Iteration 19200: Loss = -9987.654340438243
Iteration 19300: Loss = -9987.655474722877
1
Iteration 19400: Loss = -9987.654785724193
2
Iteration 19500: Loss = -9987.730252298785
3
Iteration 19600: Loss = -9987.65438329716
4
Iteration 19700: Loss = -9987.654435211105
5
Iteration 19800: Loss = -9987.654373497096
6
Iteration 19900: Loss = -9987.654551214855
7
tensor([[ 6.1908, -7.5936],
        [ 5.6268, -8.1482],
        [ 6.1410, -7.5672],
        [ 5.6274, -8.0972],
        [ 5.8426, -7.8397],
        [ 6.0999, -7.6143],
        [ 6.0316, -7.5998],
        [ 6.0797, -7.6495],
        [ 6.1549, -7.5630],
        [ 6.0203, -7.6686],
        [ 5.9427, -7.7393],
        [ 6.0821, -7.6105],
        [ 6.0656, -7.5664],
        [ 5.5905, -8.1556],
        [ 6.0648, -7.7372],
        [ 6.1514, -7.5692],
        [ 5.7008, -8.0384],
        [ 6.0512, -7.7158],
        [ 6.0560, -7.7121],
        [ 6.1437, -7.5457],
        [ 5.6377, -8.1543],
        [ 6.0717, -7.6527],
        [ 6.1131, -7.6685],
        [ 5.5671, -8.1914],
        [ 6.1112, -7.6479],
        [ 5.9554, -7.8881],
        [ 5.8786, -7.6139],
        [ 6.2112, -7.5980],
        [ 6.1060, -7.5834],
        [ 5.8887, -7.8833],
        [ 6.0972, -7.6071],
        [ 5.6644, -8.0939],
        [ 4.5991, -9.2143],
        [ 6.1768, -7.5687],
        [ 5.8634, -7.8844],
        [ 5.7939, -8.0494],
        [ 6.0663, -7.7360],
        [ 6.1432, -7.6263],
        [ 6.0968, -7.6652],
        [ 5.9526, -7.8117],
        [ 6.0776, -7.5352],
        [ 6.1517, -7.6557],
        [ 5.7906, -7.9418],
        [ 5.8328, -7.8301],
        [ 5.9424, -7.7769],
        [ 5.9350, -7.7357],
        [ 6.1183, -7.5192],
        [ 6.0993, -7.6528],
        [ 6.0220, -7.5741],
        [ 4.7114, -9.1837],
        [ 6.1084, -7.6808],
        [ 6.2212, -7.6145],
        [ 5.8806, -7.8807],
        [ 6.1456, -7.5956],
        [ 6.1334, -7.5915],
        [ 5.6746, -8.0213],
        [ 6.1548, -7.5418],
        [ 4.5957, -9.2109],
        [ 6.1632, -7.5523],
        [ 5.4678, -8.2385],
        [ 6.0442, -7.6206],
        [ 6.0297, -7.6548],
        [ 5.9814, -7.7685],
        [ 6.0378, -7.6472],
        [ 6.1579, -7.5518],
        [ 6.1549, -7.5449],
        [ 5.9365, -7.7694],
        [ 6.1731, -7.6147],
        [ 6.0731, -7.6337],
        [ 6.0556, -7.6828],
        [ 6.0180, -7.7241],
        [ 5.1917, -8.6038],
        [ 6.1406, -7.5932],
        [ 5.9054, -7.9262],
        [ 6.1189, -7.5748],
        [ 6.1648, -7.5786],
        [ 5.2918, -8.4153],
        [ 6.0801, -7.5948],
        [ 5.8173, -7.8470],
        [ 6.0834, -7.5727],
        [ 6.1380, -7.5969],
        [ 5.9900, -7.6044],
        [ 6.1506, -7.5491],
        [ 6.0922, -7.6735],
        [ 5.6746, -8.0215],
        [ 5.5916, -8.0444],
        [ 5.6897, -7.9974],
        [ 6.1524, -7.5594],
        [ 6.1202, -7.7554],
        [ 6.1283, -7.6015],
        [ 5.5424, -8.1064],
        [ 5.7772, -7.9134],
        [ 5.4034, -8.1708],
        [ 4.9648, -8.8444],
        [ 5.0462, -8.6323],
        [ 5.8603, -7.8393],
        [ 4.8608, -8.8752],
        [ 6.1545, -7.5983],
        [ 5.7204, -8.0702],
        [ 5.7811, -7.9836]], dtype=torch.float64, requires_grad=True)
pi: tensor([[8.0350e-07, 1.0000e+00],
        [6.2906e-02, 9.3709e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.0946e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1338, 0.1319],
         [0.7577, 0.1448]],

        [[0.7237, 0.1256],
         [0.5461, 0.7892]],

        [[0.5270, 0.0620],
         [0.3868, 0.3063]],

        [[0.1920, 0.0688],
         [0.6815, 0.6210]],

        [[0.4408, 0.1694],
         [0.7655, 0.0742]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.022741429840595545
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 64
Adjusted Rand Index: 0.03609912235729749
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.014303822700957536
Average Adjusted Rand Index: 0.011768110439578606
10059.846719487165
new:  [0.016616913647884465, 0.004680504024354113, 0.6201877148306101, 0.014303822700957536] [0.01734737590049603, 0.0028186214429958686, 0.6207868848471116, 0.011768110439578606] [9987.478125539814, 9991.158682617119, 9949.538968944771, 9987.654365737037]
prior:  [0.00879578788541848, 0.0, 0.00557637714542361, 0.00879578788541848] [0.010631124402481007, 0.0, 0.0034112999310215086, 0.010631124402481007] [9989.170459029283, 9995.780675878876, 9991.731168707358, 9989.170410241923]
-----------------------------------------------------------------------------------------
This iteration is 8
True Objective function: Loss = -9915.876623495691
Iteration 0: Loss = -17035.991087463655
Iteration 10: Loss = -9770.918637892704
Iteration 20: Loss = -9770.93291913584
1
Iteration 30: Loss = -9770.935277352892
2
Iteration 40: Loss = -9770.935686403462
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.1345, 0.8655],
        [0.0286, 0.9714]], dtype=torch.float64)
alpha: tensor([0.0328, 0.9672])
beta: tensor([[[0.1654, 0.1563],
         [0.6280, 0.1310]],

        [[0.1885, 0.1918],
         [0.2838, 0.9948]],

        [[0.9341, 0.2167],
         [0.1724, 0.4012]],

        [[0.2324, 0.1119],
         [0.2281, 0.9959]],

        [[0.1493, 0.1452],
         [0.0873, 0.0558]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.009987515605493134
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.004294123202807246
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000857612811303036
Average Adjusted Rand Index: 0.0011386784805371775
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16656.864430357557
Iteration 100: Loss = -9774.54680569483
Iteration 200: Loss = -9772.07425740875
Iteration 300: Loss = -9771.411859172202
Iteration 400: Loss = -9771.122025586406
Iteration 500: Loss = -9770.863233869079
Iteration 600: Loss = -9770.580628097507
Iteration 700: Loss = -9770.300910482692
Iteration 800: Loss = -9770.002576274086
Iteration 900: Loss = -9769.74246496965
Iteration 1000: Loss = -9769.57772262373
Iteration 1100: Loss = -9769.469554826963
Iteration 1200: Loss = -9769.386685957157
Iteration 1300: Loss = -9769.318702325498
Iteration 1400: Loss = -9769.26172394464
Iteration 1500: Loss = -9769.212637340945
Iteration 1600: Loss = -9769.169834601442
Iteration 1700: Loss = -9769.132951079322
Iteration 1800: Loss = -9769.099504587777
Iteration 1900: Loss = -9769.070088204284
Iteration 2000: Loss = -9769.043871361922
Iteration 2100: Loss = -9769.020506627725
Iteration 2200: Loss = -9768.999780260141
Iteration 2300: Loss = -9768.981458464363
Iteration 2400: Loss = -9768.965256578578
Iteration 2500: Loss = -9768.950876138777
Iteration 2600: Loss = -9768.938004854741
Iteration 2700: Loss = -9768.937877955006
Iteration 2800: Loss = -9768.91673738179
Iteration 2900: Loss = -9769.001185563802
1
Iteration 3000: Loss = -9768.9002982584
Iteration 3100: Loss = -9768.893478340447
Iteration 3200: Loss = -9768.887246577277
Iteration 3300: Loss = -9768.88169197028
Iteration 3400: Loss = -9768.894468079405
1
Iteration 3500: Loss = -9768.87236248832
Iteration 3600: Loss = -9768.868360672803
Iteration 3700: Loss = -9768.867841965748
Iteration 3800: Loss = -9768.861540847423
Iteration 3900: Loss = -9768.858572882049
Iteration 4000: Loss = -9768.858511124925
Iteration 4100: Loss = -9768.853470854832
Iteration 4200: Loss = -9768.85119796289
Iteration 4300: Loss = -9768.857952598295
1
Iteration 4400: Loss = -9768.84727771096
Iteration 4500: Loss = -9768.845584694707
Iteration 4600: Loss = -9768.843960832348
Iteration 4700: Loss = -9768.842705867686
Iteration 4800: Loss = -9768.841883383266
Iteration 4900: Loss = -9768.84035268068
Iteration 5000: Loss = -9768.838713001634
Iteration 5100: Loss = -9768.847967991278
1
Iteration 5200: Loss = -9768.836584423356
Iteration 5300: Loss = -9768.836049936172
Iteration 5400: Loss = -9768.84037534987
1
Iteration 5500: Loss = -9768.834486836236
Iteration 5600: Loss = -9768.833356995181
Iteration 5700: Loss = -9768.837430379428
1
Iteration 5800: Loss = -9768.831828039534
Iteration 5900: Loss = -9768.831292172705
Iteration 6000: Loss = -9768.848523205743
1
Iteration 6100: Loss = -9768.830066705796
Iteration 6200: Loss = -9768.829610242592
Iteration 6300: Loss = -9768.82906024896
Iteration 6400: Loss = -9768.92357614017
1
Iteration 6500: Loss = -9768.828136025382
Iteration 6600: Loss = -9768.859740382737
1
Iteration 6700: Loss = -9768.829420641663
2
Iteration 6800: Loss = -9768.827073867675
Iteration 6900: Loss = -9768.85403589576
1
Iteration 7000: Loss = -9768.826325107115
Iteration 7100: Loss = -9768.828590541403
1
Iteration 7200: Loss = -9768.869012067571
2
Iteration 7300: Loss = -9768.82547163262
Iteration 7400: Loss = -9768.82536451211
Iteration 7500: Loss = -9768.824958307023
Iteration 7600: Loss = -9768.826255565333
1
Iteration 7700: Loss = -9768.824648896078
Iteration 7800: Loss = -9768.871898143498
1
Iteration 7900: Loss = -9768.827538950767
2
Iteration 8000: Loss = -9768.824180025806
Iteration 8100: Loss = -9768.82712596305
1
Iteration 8200: Loss = -9768.823803468844
Iteration 8300: Loss = -9768.823536191698
Iteration 8400: Loss = -9768.823421428659
Iteration 8500: Loss = -9768.823375667576
Iteration 8600: Loss = -9768.827808372418
1
Iteration 8700: Loss = -9768.911242697213
2
Iteration 8800: Loss = -9768.822802223303
Iteration 8900: Loss = -9768.825016664574
1
Iteration 9000: Loss = -9768.82259523773
Iteration 9100: Loss = -9768.83345428433
1
Iteration 9200: Loss = -9768.859925194152
2
Iteration 9300: Loss = -9768.822408124102
Iteration 9400: Loss = -9768.822818120474
1
Iteration 9500: Loss = -9768.837717253562
2
Iteration 9600: Loss = -9768.822967655293
3
Iteration 9700: Loss = -9768.892294581852
4
Iteration 9800: Loss = -9768.821918157908
Iteration 9900: Loss = -9768.823293429123
1
Iteration 10000: Loss = -9768.822059421362
2
Iteration 10100: Loss = -9768.822342303502
3
Iteration 10200: Loss = -9768.822633498712
4
Iteration 10300: Loss = -9768.855057987017
5
Iteration 10400: Loss = -9768.831438641064
6
Iteration 10500: Loss = -9768.837078714809
7
Iteration 10600: Loss = -9768.822237524299
8
Iteration 10700: Loss = -9768.82146599006
Iteration 10800: Loss = -9768.821762320784
1
Iteration 10900: Loss = -9768.821395754818
Iteration 11000: Loss = -9768.821440096417
1
Iteration 11100: Loss = -9768.87509309768
2
Iteration 11200: Loss = -9768.82142881842
3
Iteration 11300: Loss = -9768.821436862361
4
Iteration 11400: Loss = -9768.844602377781
5
Iteration 11500: Loss = -9768.821181963349
Iteration 11600: Loss = -9769.117985971088
1
Iteration 11700: Loss = -9768.821117170419
Iteration 11800: Loss = -9768.821143397568
1
Iteration 11900: Loss = -9768.82110010398
Iteration 12000: Loss = -9768.838116795876
1
Iteration 12100: Loss = -9768.821732401979
2
Iteration 12200: Loss = -9768.821268745301
3
Iteration 12300: Loss = -9768.821853528307
4
Iteration 12400: Loss = -9768.826667880829
5
Iteration 12500: Loss = -9768.822043895167
6
Iteration 12600: Loss = -9768.827558426834
7
Iteration 12700: Loss = -9768.822496523127
8
Iteration 12800: Loss = -9768.82206041536
9
Iteration 12900: Loss = -9768.878866096491
10
Stopping early at iteration 12900 due to no improvement.
tensor([[-3.8433, -0.7719],
        [-5.0064,  0.3912],
        [-4.4928, -0.1224],
        [-4.2779, -0.3373],
        [-5.6271,  1.0119],
        [-5.3893,  0.7740],
        [-3.4103, -1.2049],
        [-3.5068, -1.1085],
        [-3.7868, -0.8284],
        [-1.8572, -2.7580],
        [-4.6047, -0.0105],
        [-3.0755, -1.5397],
        [-4.6850,  0.0698],
        [-4.6800,  0.0648],
        [-4.4002, -0.2150],
        [-3.0657, -1.5495],
        [-5.4657,  0.8505],
        [-4.2196, -0.3956],
        [-4.8761,  0.2609],
        [-2.8473, -1.7679],
        [-3.4282, -1.1870],
        [-5.7984,  1.1832],
        [-4.6732,  0.0580],
        [-5.8035,  1.1882],
        [-3.4856, -1.1296],
        [-4.2244, -0.3908],
        [-3.9656, -0.6496],
        [-3.7839, -0.8313],
        [-6.5560,  1.9407],
        [-4.4746, -0.1406],
        [-4.8308,  0.2156],
        [-5.6416,  1.0263],
        [-3.9962, -0.6190],
        [-3.1658, -1.4494],
        [-3.3071, -1.3081],
        [-4.3347, -0.2805],
        [-4.9404,  0.3252],
        [-3.8303, -0.7849],
        [-4.2336, -0.3817],
        [-4.5296, -0.0856],
        [-2.5910, -2.0242],
        [-2.3331, -2.2822],
        [-4.8165,  0.2013],
        [-3.5210, -1.0942],
        [-4.1360, -0.4792],
        [-3.8492, -0.7660],
        [-2.7518, -1.8634],
        [-2.7884, -1.8268],
        [-4.3732, -0.2420],
        [-4.1171, -0.4981],
        [-3.3522, -1.2630],
        [-3.2856, -1.3297],
        [-3.9179, -0.6973],
        [-3.7336, -0.8816],
        [-2.3315, -2.2837],
        [-3.1905, -1.4248],
        [-4.7154,  0.1002],
        [-4.5687, -0.0466],
        [-4.1378, -0.4775],
        [-2.6617, -1.9535],
        [-4.0983, -0.5170],
        [-3.0434, -1.5719],
        [-2.4154, -2.1999],
        [-7.0848,  2.4695],
        [-3.7508, -0.8644],
        [-5.7844,  1.1692],
        [-3.9353, -0.6800],
        [-3.8506, -0.7646],
        [-4.0773, -0.5379],
        [-2.6937, -1.9215],
        [-2.7073, -1.9079],
        [-4.9646,  0.3493],
        [-4.1076, -0.5076],
        [-3.8725, -0.7428],
        [-3.4554, -1.1598],
        [-4.3943, -0.2209],
        [-5.7192,  1.1040],
        [-5.4543,  0.8391],
        [-5.5439,  0.9287],
        [-3.5789, -1.0363],
        [-2.8523, -1.7629],
        [-5.5923,  0.9771],
        [-5.4674,  0.8521],
        [-3.9283, -0.6869],
        [-3.8214, -0.7938],
        [-4.8096,  0.1944],
        [-3.4960, -1.1192],
        [-5.3693,  0.7541],
        [-4.4555, -0.1597],
        [-4.6871,  0.0719],
        [-3.9134, -0.7018],
        [-3.7921, -0.8232],
        [-2.2991, -2.3161],
        [-3.5021, -1.1132],
        [-4.6416,  0.0264],
        [-5.7088,  1.0936],
        [-6.4259,  1.8107],
        [-2.8377, -1.7775],
        [-3.6332, -0.9820],
        [-3.4229, -1.1923]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9991e-01, 8.9511e-05],
        [3.3053e-02, 9.6695e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0848, 0.9152], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1074, 0.1401],
         [0.6280, 0.1401]],

        [[0.1885, 0.0944],
         [0.2838, 0.9948]],

        [[0.9341, 0.1319],
         [0.1724, 0.4012]],

        [[0.2324, 0.1071],
         [0.2281, 0.9959]],

        [[0.1493, 0.1203],
         [0.0873, 0.0558]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.015161593975525573
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.020655664058706252
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0171891264223423
Global Adjusted Rand Index: -0.0076964837724290795
Average Adjusted Rand Index: -0.009218660358715925
Iteration 0: Loss = -26817.351332511145
Iteration 10: Loss = -9773.022070820265
Iteration 20: Loss = -9773.022070820272
1
Iteration 30: Loss = -9773.022070820727
2
Iteration 40: Loss = -9773.022070837165
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[3.9196e-01, 6.0804e-01],
        [4.8526e-13, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([7.3625e-13, 1.0000e+00])
beta: tensor([[[0.1474, 0.1568],
         [0.9083, 0.1332]],

        [[0.9642, 0.2268],
         [0.1480, 0.2979]],

        [[0.8620, 0.2229],
         [0.3864, 0.9169]],

        [[0.3749, 0.1116],
         [0.0381, 0.7109]],

        [[0.3218, 0.1266],
         [0.9096, 0.9456]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26817.81445956811
Iteration 100: Loss = -9815.494720561961
Iteration 200: Loss = -9797.339275636843
Iteration 300: Loss = -9787.09671472615
Iteration 400: Loss = -9778.091205664994
Iteration 500: Loss = -9775.366989591776
Iteration 600: Loss = -9774.668258458953
Iteration 700: Loss = -9774.249910353392
Iteration 800: Loss = -9773.967313012545
Iteration 900: Loss = -9773.75871849335
Iteration 1000: Loss = -9773.597993439924
Iteration 1100: Loss = -9773.46989602604
Iteration 1200: Loss = -9773.363977795334
Iteration 1300: Loss = -9773.269826530206
Iteration 1400: Loss = -9773.155022190997
Iteration 1500: Loss = -9772.829137349037
Iteration 1600: Loss = -9772.65775384039
Iteration 1700: Loss = -9772.425380437819
Iteration 1800: Loss = -9771.731921992425
Iteration 1900: Loss = -9771.468511364388
Iteration 2000: Loss = -9771.27312000539
Iteration 2100: Loss = -9771.120569957724
Iteration 2200: Loss = -9770.972278283987
Iteration 2300: Loss = -9770.818717146733
Iteration 2400: Loss = -9770.666189773556
Iteration 2500: Loss = -9770.424347864024
Iteration 2600: Loss = -9770.243522344974
Iteration 2700: Loss = -9769.870072329633
Iteration 2800: Loss = -9769.677265419896
Iteration 2900: Loss = -9769.520377572626
Iteration 3000: Loss = -9769.395429121378
Iteration 3100: Loss = -9769.297051538091
Iteration 3200: Loss = -9769.217386125376
Iteration 3300: Loss = -9769.159371290505
Iteration 3400: Loss = -9769.09650363125
Iteration 3500: Loss = -9769.049852257163
Iteration 3600: Loss = -9769.009916474704
Iteration 3700: Loss = -9768.975333599312
Iteration 3800: Loss = -9768.94509654797
Iteration 3900: Loss = -9768.918178340025
Iteration 4000: Loss = -9768.89633807776
Iteration 4100: Loss = -9768.864201540784
Iteration 4200: Loss = -9768.705790745367
Iteration 4300: Loss = -9768.286150849195
Iteration 4400: Loss = -9768.265671370129
Iteration 4500: Loss = -9768.250297017781
Iteration 4600: Loss = -9768.237366926896
Iteration 4700: Loss = -9768.266492039991
1
Iteration 4800: Loss = -9768.216175924957
Iteration 4900: Loss = -9768.207232021192
Iteration 5000: Loss = -9768.199165056569
Iteration 5100: Loss = -9768.195066199834
Iteration 5200: Loss = -9768.185254697963
Iteration 5300: Loss = -9768.17920422468
Iteration 5400: Loss = -9768.173657466845
Iteration 5500: Loss = -9768.168629797632
Iteration 5600: Loss = -9768.163907896169
Iteration 5700: Loss = -9768.15956766864
Iteration 5800: Loss = -9768.158557032259
Iteration 5900: Loss = -9768.151828813778
Iteration 6000: Loss = -9768.148344838139
Iteration 6100: Loss = -9768.145116095404
Iteration 6200: Loss = -9768.142198260379
Iteration 6300: Loss = -9768.139212735021
Iteration 6400: Loss = -9768.13655088672
Iteration 6500: Loss = -9768.140890716537
1
Iteration 6600: Loss = -9768.13175987297
Iteration 6700: Loss = -9768.129563545126
Iteration 6800: Loss = -9768.127489820776
Iteration 6900: Loss = -9768.126129845754
Iteration 7000: Loss = -9768.123776948685
Iteration 7100: Loss = -9768.12206149279
Iteration 7200: Loss = -9768.127535564716
1
Iteration 7300: Loss = -9768.118942300496
Iteration 7400: Loss = -9768.117515627417
Iteration 7500: Loss = -9768.116203633595
Iteration 7600: Loss = -9768.127867826626
1
Iteration 7700: Loss = -9768.11374735829
Iteration 7800: Loss = -9768.11261351626
Iteration 7900: Loss = -9768.111555995678
Iteration 8000: Loss = -9768.134374570984
1
Iteration 8100: Loss = -9768.109574762337
Iteration 8200: Loss = -9768.10868106088
Iteration 8300: Loss = -9768.107832481977
Iteration 8400: Loss = -9768.107511583952
Iteration 8500: Loss = -9768.10629287336
Iteration 8600: Loss = -9768.10556320724
Iteration 8700: Loss = -9768.104884302531
Iteration 8800: Loss = -9768.104484139965
Iteration 8900: Loss = -9768.103611176097
Iteration 9000: Loss = -9768.103017343632
Iteration 9100: Loss = -9768.223729797308
1
Iteration 9200: Loss = -9768.101963323152
Iteration 9300: Loss = -9768.101436520114
Iteration 9400: Loss = -9768.10099588521
Iteration 9500: Loss = -9768.104899320859
1
Iteration 9600: Loss = -9768.100121926705
Iteration 9700: Loss = -9768.09973697738
Iteration 9800: Loss = -9768.09939414575
Iteration 9900: Loss = -9768.09903620928
Iteration 10000: Loss = -9768.098735562155
Iteration 10100: Loss = -9768.23569224177
1
Iteration 10200: Loss = -9768.098088908762
Iteration 10300: Loss = -9768.09782331201
Iteration 10400: Loss = -9768.107569435813
1
Iteration 10500: Loss = -9768.097295397401
Iteration 10600: Loss = -9768.097089574181
Iteration 10700: Loss = -9768.097446580821
1
Iteration 10800: Loss = -9768.096636304479
Iteration 10900: Loss = -9768.097601076928
1
Iteration 11000: Loss = -9768.096244355596
Iteration 11100: Loss = -9768.096232380994
Iteration 11200: Loss = -9768.09583439733
Iteration 11300: Loss = -9768.095877563963
1
Iteration 11400: Loss = -9768.095525996834
Iteration 11500: Loss = -9768.095407710825
Iteration 11600: Loss = -9768.095312692689
Iteration 11700: Loss = -9768.095108340858
Iteration 11800: Loss = -9768.094982450126
Iteration 11900: Loss = -9768.100086705617
1
Iteration 12000: Loss = -9768.094758440457
Iteration 12100: Loss = -9768.09467239016
Iteration 12200: Loss = -9768.099560682766
1
Iteration 12300: Loss = -9768.094468941556
Iteration 12400: Loss = -9768.094351344536
Iteration 12500: Loss = -9768.098895156183
1
Iteration 12600: Loss = -9768.13275442802
2
Iteration 12700: Loss = -9768.094088721344
Iteration 12800: Loss = -9768.096910165852
1
Iteration 12900: Loss = -9768.093965652224
Iteration 13000: Loss = -9768.09430093252
1
Iteration 13100: Loss = -9768.093845620398
Iteration 13200: Loss = -9768.0945556374
1
Iteration 13300: Loss = -9768.09368089242
Iteration 13400: Loss = -9768.093663977545
Iteration 13500: Loss = -9768.09362959144
Iteration 13600: Loss = -9768.093527274661
Iteration 13700: Loss = -9768.093558259787
1
Iteration 13800: Loss = -9768.093496502139
Iteration 13900: Loss = -9768.09340862088
Iteration 14000: Loss = -9768.093374944196
Iteration 14100: Loss = -9768.093905127675
1
Iteration 14200: Loss = -9768.09329809703
Iteration 14300: Loss = -9768.093250595917
Iteration 14400: Loss = -9768.099090243091
1
Iteration 14500: Loss = -9768.093206550555
Iteration 14600: Loss = -9768.093193554414
Iteration 14700: Loss = -9768.093593624033
1
Iteration 14800: Loss = -9768.093129914581
Iteration 14900: Loss = -9768.09307475849
Iteration 15000: Loss = -9768.253430611894
1
Iteration 15100: Loss = -9768.092856863786
Iteration 15200: Loss = -9768.090576005678
Iteration 15300: Loss = -9768.28241050842
1
Iteration 15400: Loss = -9768.090504726159
Iteration 15500: Loss = -9768.090493516154
Iteration 15600: Loss = -9768.447339774673
1
Iteration 15700: Loss = -9768.090481283289
Iteration 15800: Loss = -9768.090441563292
Iteration 15900: Loss = -9768.127416691597
1
Iteration 16000: Loss = -9768.090393646453
Iteration 16100: Loss = -9768.090378737414
Iteration 16200: Loss = -9768.091384447695
1
Iteration 16300: Loss = -9768.102176697643
2
Iteration 16400: Loss = -9768.091137209685
3
Iteration 16500: Loss = -9768.090355021657
Iteration 16600: Loss = -9768.090994041338
1
Iteration 16700: Loss = -9768.28055488391
2
Iteration 16800: Loss = -9768.090325646908
Iteration 16900: Loss = -9768.09301901981
1
Iteration 17000: Loss = -9768.090296397428
Iteration 17100: Loss = -9768.090330826302
1
Iteration 17200: Loss = -9768.090430455395
2
Iteration 17300: Loss = -9768.090295921847
Iteration 17400: Loss = -9768.144292925941
1
Iteration 17500: Loss = -9768.08919522074
Iteration 17600: Loss = -9768.089161074038
Iteration 17700: Loss = -9768.096378949502
1
Iteration 17800: Loss = -9768.089152186572
Iteration 17900: Loss = -9768.089162297107
1
Iteration 18000: Loss = -9768.08923978394
2
Iteration 18100: Loss = -9768.08915850309
3
Iteration 18200: Loss = -9768.0906079222
4
Iteration 18300: Loss = -9768.089145944014
Iteration 18400: Loss = -9768.123121541423
1
Iteration 18500: Loss = -9768.089138174433
Iteration 18600: Loss = -9768.08913731612
Iteration 18700: Loss = -9768.089287197656
1
Iteration 18800: Loss = -9768.089149929723
2
Iteration 18900: Loss = -9768.089144718779
3
Iteration 19000: Loss = -9768.089307494683
4
Iteration 19100: Loss = -9768.089139908398
5
Iteration 19200: Loss = -9768.08914481814
6
Iteration 19300: Loss = -9768.08921269466
7
Iteration 19400: Loss = -9768.089138802585
8
Iteration 19500: Loss = -9768.089172571941
9
Iteration 19600: Loss = -9768.08914118292
10
Stopping early at iteration 19600 due to no improvement.
tensor([[ -9.6503,   8.1840],
        [ -8.5719,   6.8267],
        [-11.4407,   7.5758],
        [ -8.5138,   7.0943],
        [ -8.9098,   7.1964],
        [ -9.9438,   7.4281],
        [-11.7286,   7.1406],
        [-10.3066,   7.4241],
        [ -9.2462,   7.8579],
        [-10.0663,   6.8590],
        [ -9.2428,   7.8421],
        [ -8.9080,   7.5195],
        [ -8.8198,   7.3963],
        [ -9.6413,   7.8735],
        [ -9.0151,   7.5010],
        [ -8.2660,   6.8170],
        [ -9.2363,   7.4085],
        [ -9.5270,   8.0048],
        [ -3.6838,   2.2974],
        [ -9.8337,   7.6131],
        [ -9.2242,   7.7855],
        [-10.7086,   7.4417],
        [-10.5384,   7.0863],
        [ -9.0896,   7.7033],
        [-11.6745,   7.0593],
        [ -9.9999,   7.9515],
        [ -8.8548,   7.3799],
        [ -8.6244,   7.2357],
        [  1.2757,  -3.2748],
        [ -8.8782,   7.2760],
        [ -9.6669,   8.2159],
        [ -9.2259,   7.8124],
        [ -9.7029,   7.9457],
        [ -9.3890,   7.2847],
        [ -9.7303,   7.9712],
        [-11.9222,   8.2783],
        [-10.2754,   6.0734],
        [ -9.3755,   7.9762],
        [-10.1212,   8.1846],
        [-10.0855,   8.0245],
        [ -8.0764,   6.5100],
        [ -8.4544,   6.9233],
        [ -9.3132,   7.3097],
        [ -8.7752,   7.3870],
        [ -9.2880,   7.7364],
        [ -9.0128,   7.5241],
        [ -9.4204,   7.5181],
        [ -9.6203,   8.1336],
        [ -9.4255,   7.6180],
        [ -8.6077,   7.2203],
        [ -9.4972,   7.7645],
        [ -8.9069,   7.4176],
        [ -8.9753,   7.4244],
        [ -9.2193,   7.6037],
        [ -9.2165,   7.4914],
        [-10.3117,   7.4644],
        [-11.3922,   7.2773],
        [ -9.3806,   7.9324],
        [ -8.9794,   7.5527],
        [ -9.7995,   8.1011],
        [-10.1743,   6.8314],
        [ -9.9017,   7.5930],
        [ -9.7499,   8.2740],
        [ -5.6016,   2.6670],
        [ -9.7003,   8.0941],
        [-10.9474,   7.6584],
        [ -8.9324,   7.4785],
        [ -8.5059,   6.7231],
        [ -9.5583,   7.6772],
        [ -9.3190,   7.9151],
        [ -8.8963,   7.5070],
        [ -8.8840,   7.4873],
        [ -9.0248,   7.5386],
        [-10.2165,   7.7437],
        [ -9.7293,   8.2849],
        [ -8.8426,   7.4453],
        [-10.2400,   7.9797],
        [ -9.6478,   8.2581],
        [-10.0723,   7.1301],
        [ -9.4673,   8.0159],
        [ -9.5649,   7.8480],
        [ -0.3763,  -1.1869],
        [ -9.6244,   6.8366],
        [ -9.6470,   7.6029],
        [ -9.0924,   7.5619],
        [ -9.4301,   7.9440],
        [ -8.4514,   7.0620],
        [-10.5830,   7.3131],
        [ -9.2143,   6.6641],
        [ -9.6769,   7.9190],
        [ -8.8537,   7.4265],
        [ -8.3993,   6.8907],
        [ -8.8936,   7.4000],
        [ -9.8727,   7.0810],
        [ -8.9206,   7.2458],
        [ -4.7207,   2.0924],
        [ -9.4295,   8.0432],
        [ -9.4280,   7.7362],
        [-10.4912,   7.4072],
        [ -8.7932,   7.2414]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 2.4335e-06],
        [1.8508e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0168, 0.9832], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2014, 0.0834],
         [0.9083, 0.1340]],

        [[0.9642, 0.2337],
         [0.1480, 0.2979]],

        [[0.8620, 0.2091],
         [0.3864, 0.9169]],

        [[0.3749, 0.0995],
         [0.0381, 0.7109]],

        [[0.3218, 0.1246],
         [0.9096, 0.9456]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.0207567131845433
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.001829943009345947
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
Global Adjusted Rand Index: -0.0005790689491107601
Average Adjusted Rand Index: 0.0016754390265663315
Iteration 0: Loss = -37799.93184258565
Iteration 10: Loss = -9772.790519959197
Iteration 20: Loss = -9771.159375416724
Iteration 30: Loss = -9770.95151114952
Iteration 40: Loss = -9770.934866478678
Iteration 50: Loss = -9770.934635295851
Iteration 60: Loss = -9770.935298815726
1
Iteration 70: Loss = -9770.935591343674
2
Iteration 80: Loss = -9770.935697971707
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.1344, 0.8656],
        [0.0286, 0.9714]], dtype=torch.float64)
alpha: tensor([0.0328, 0.9672])
beta: tensor([[[0.1654, 0.1563],
         [0.0773, 0.1310]],

        [[0.1278, 0.1918],
         [0.2765, 0.6493]],

        [[0.1196, 0.2167],
         [0.9079, 0.2810]],

        [[0.8660, 0.1119],
         [0.6148, 0.6433]],

        [[0.5494, 0.1452],
         [0.7723, 0.3831]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.009987515605493134
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.004294123202807246
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000857612811303036
Average Adjusted Rand Index: 0.0011386784805371775
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37799.305840706795
Iteration 100: Loss = -9896.642506488352
Iteration 200: Loss = -9829.418596469615
Iteration 300: Loss = -9803.753938273736
Iteration 400: Loss = -9783.713259422098
Iteration 500: Loss = -9778.792040165068
Iteration 600: Loss = -9777.157638073586
Iteration 700: Loss = -9776.165774185023
Iteration 800: Loss = -9775.498825511779
Iteration 900: Loss = -9775.021877885905
Iteration 1000: Loss = -9774.665852692131
Iteration 1100: Loss = -9774.391350065478
Iteration 1200: Loss = -9774.174248993028
Iteration 1300: Loss = -9773.998876935837
Iteration 1400: Loss = -9773.85479025534
Iteration 1500: Loss = -9773.73462618682
Iteration 1600: Loss = -9773.633027863196
Iteration 1700: Loss = -9773.546233037108
Iteration 1800: Loss = -9773.471455375698
Iteration 1900: Loss = -9773.406618846479
Iteration 2000: Loss = -9773.350154220529
Iteration 2100: Loss = -9773.300757699935
Iteration 2200: Loss = -9773.257262172221
Iteration 2300: Loss = -9773.21875007279
Iteration 2400: Loss = -9773.184510247314
Iteration 2500: Loss = -9773.154052298685
Iteration 2600: Loss = -9773.126893217808
Iteration 2700: Loss = -9773.102612899622
Iteration 2800: Loss = -9773.080817011723
Iteration 2900: Loss = -9773.061046499848
Iteration 3000: Loss = -9773.043036669658
Iteration 3100: Loss = -9773.026498576637
Iteration 3200: Loss = -9773.01131222262
Iteration 3300: Loss = -9772.99725354109
Iteration 3400: Loss = -9772.984178407616
Iteration 3500: Loss = -9772.971981898334
Iteration 3600: Loss = -9772.960509704359
Iteration 3700: Loss = -9772.949742858378
Iteration 3800: Loss = -9772.939761693075
Iteration 3900: Loss = -9772.9308371878
Iteration 4000: Loss = -9772.922969361198
Iteration 4100: Loss = -9772.915915971891
Iteration 4200: Loss = -9772.909433803461
Iteration 4300: Loss = -9772.9034470509
Iteration 4400: Loss = -9772.897902313409
Iteration 4500: Loss = -9772.89271488201
Iteration 4600: Loss = -9772.887892499988
Iteration 4700: Loss = -9772.883356955674
Iteration 4800: Loss = -9772.879164355165
Iteration 4900: Loss = -9772.875214878048
Iteration 5000: Loss = -9772.871513513952
Iteration 5100: Loss = -9772.86802950655
Iteration 5200: Loss = -9772.86471777771
Iteration 5300: Loss = -9772.861535345804
Iteration 5400: Loss = -9772.858452975055
Iteration 5500: Loss = -9772.855238326341
Iteration 5600: Loss = -9772.851075197354
Iteration 5700: Loss = -9772.84205752116
Iteration 5800: Loss = -9772.837353758174
Iteration 5900: Loss = -9772.83283406847
Iteration 6000: Loss = -9772.826672081124
Iteration 6100: Loss = -9772.812886326818
Iteration 6200: Loss = -9772.695308315342
Iteration 6300: Loss = -9772.587330198608
Iteration 6400: Loss = -9772.460332579105
Iteration 6500: Loss = -9772.426037393332
Iteration 6600: Loss = -9772.403806963506
Iteration 6700: Loss = -9772.395809861757
Iteration 6800: Loss = -9772.239036478635
Iteration 6900: Loss = -9772.199079066175
Iteration 7000: Loss = -9772.196541977344
Iteration 7100: Loss = -9772.190336374515
Iteration 7200: Loss = -9771.451801833267
Iteration 7300: Loss = -9771.334426986321
Iteration 7400: Loss = -9771.310292370299
Iteration 7500: Loss = -9771.299015984894
Iteration 7600: Loss = -9771.290687051413
Iteration 7700: Loss = -9771.280027740517
Iteration 7800: Loss = -9771.262963196008
Iteration 7900: Loss = -9771.254813897092
Iteration 8000: Loss = -9771.246993109387
Iteration 8100: Loss = -9771.243766416721
Iteration 8200: Loss = -9771.243123913164
Iteration 8300: Loss = -9771.242244093906
Iteration 8400: Loss = -9771.241337215943
Iteration 8500: Loss = -9771.244900791753
1
Iteration 8600: Loss = -9771.238556460632
Iteration 8700: Loss = -9771.236629032011
Iteration 8800: Loss = -9771.230887608875
Iteration 8900: Loss = -9771.227446763549
Iteration 9000: Loss = -9771.226120820455
Iteration 9100: Loss = -9771.335749197238
1
Iteration 9200: Loss = -9771.22425169846
Iteration 9300: Loss = -9771.223549739942
Iteration 9400: Loss = -9771.237383319096
1
Iteration 9500: Loss = -9771.222535115092
Iteration 9600: Loss = -9771.221918312489
Iteration 9700: Loss = -9771.221354383697
Iteration 9800: Loss = -9771.220864025307
Iteration 9900: Loss = -9771.22043634343
Iteration 10000: Loss = -9771.220023472122
Iteration 10100: Loss = -9771.224245869962
1
Iteration 10200: Loss = -9771.219423159697
Iteration 10300: Loss = -9771.219211185407
Iteration 10400: Loss = -9771.347580829482
1
Iteration 10500: Loss = -9771.21867640368
Iteration 10600: Loss = -9771.23598584549
1
Iteration 10700: Loss = -9771.234911907393
2
Iteration 10800: Loss = -9771.219986278697
3
Iteration 10900: Loss = -9771.220710909427
4
Iteration 11000: Loss = -9771.221245424957
5
Iteration 11100: Loss = -9771.216007081828
Iteration 11200: Loss = -9771.254022133402
1
Iteration 11300: Loss = -9771.162745707543
Iteration 11400: Loss = -9770.703914252308
Iteration 11500: Loss = -9770.459693965602
Iteration 11600: Loss = -9770.206280707824
Iteration 11700: Loss = -9770.107676391215
Iteration 11800: Loss = -9769.984831984752
Iteration 11900: Loss = -9769.923558891742
Iteration 12000: Loss = -9769.888120978456
Iteration 12100: Loss = -9769.88025826635
Iteration 12200: Loss = -9769.85633359634
Iteration 12300: Loss = -9769.842826227878
Iteration 12400: Loss = -9769.813809582274
Iteration 12500: Loss = -9769.829269093918
1
Iteration 12600: Loss = -9769.803615250827
Iteration 12700: Loss = -9769.798027188079
Iteration 12800: Loss = -9769.818092862191
1
Iteration 12900: Loss = -9769.792205937614
Iteration 13000: Loss = -9769.793704335185
1
Iteration 13100: Loss = -9769.779875444683
Iteration 13200: Loss = -9769.780509807002
1
Iteration 13300: Loss = -9769.798980902224
2
Iteration 13400: Loss = -9769.77989714703
3
Iteration 13500: Loss = -9769.78840220021
4
Iteration 13600: Loss = -9769.78069893989
5
Iteration 13700: Loss = -9769.776759155418
Iteration 13800: Loss = -9769.777789984779
1
Iteration 13900: Loss = -9769.780319023752
2
Iteration 14000: Loss = -9769.784353271363
3
Iteration 14100: Loss = -9769.804803886147
4
Iteration 14200: Loss = -9769.78906801082
5
Iteration 14300: Loss = -9769.776014950523
Iteration 14400: Loss = -9769.776278612328
1
Iteration 14500: Loss = -9769.78515424901
2
Iteration 14600: Loss = -9769.779095350614
3
Iteration 14700: Loss = -9769.775922748155
Iteration 14800: Loss = -9769.785865611939
1
Iteration 14900: Loss = -9769.786338511176
2
Iteration 15000: Loss = -9769.831215595363
3
Iteration 15100: Loss = -9769.775825184557
Iteration 15200: Loss = -9769.777604490173
1
Iteration 15300: Loss = -9769.803214735928
2
Iteration 15400: Loss = -9769.775769047406
Iteration 15500: Loss = -9769.776007924138
1
Iteration 15600: Loss = -9769.776052332203
2
Iteration 15700: Loss = -9769.775866193871
3
Iteration 15800: Loss = -9769.795141112269
4
Iteration 15900: Loss = -9769.806489858152
5
Iteration 16000: Loss = -9769.787942877898
6
Iteration 16100: Loss = -9769.786008553738
7
Iteration 16200: Loss = -9769.775725304808
Iteration 16300: Loss = -9769.775945976022
1
Iteration 16400: Loss = -9769.78035383996
2
Iteration 16500: Loss = -9769.788880905275
3
Iteration 16600: Loss = -9769.776751894493
4
Iteration 16700: Loss = -9769.779053001756
5
Iteration 16800: Loss = -9769.789953572297
6
Iteration 16900: Loss = -9769.775920890608
7
Iteration 17000: Loss = -9769.775663055287
Iteration 17100: Loss = -9769.77577108178
1
Iteration 17200: Loss = -9769.778020129635
2
Iteration 17300: Loss = -9769.778272008387
3
Iteration 17400: Loss = -9769.776467444583
4
Iteration 17500: Loss = -9769.897077520767
5
Iteration 17600: Loss = -9769.7803589362
6
Iteration 17700: Loss = -9769.777634123067
7
Iteration 17800: Loss = -9769.782220861776
8
Iteration 17900: Loss = -9769.777959021321
9
Iteration 18000: Loss = -9769.775893265904
10
Stopping early at iteration 18000 due to no improvement.
tensor([[ 4.4721, -6.3052],
        [ 4.6591, -6.0543],
        [ 4.2229, -6.5294],
        [ 3.3109, -7.4062],
        [ 4.5569, -6.1645],
        [ 4.5281, -6.2012],
        [ 4.5029, -6.2475],
        [ 4.6749, -6.0624],
        [ 3.7916, -6.9789],
        [ 4.3622, -5.9462],
        [ 4.2004, -6.5517],
        [ 3.8542, -6.8952],
        [ 4.4882, -6.2368],
        [ 4.6143, -6.1444],
        [ 3.6386, -7.1068],
        [ 4.4873, -6.2408],
        [ 3.0637, -7.6789],
        [ 4.3961, -6.2840],
        [ 4.6241, -6.0928],
        [ 4.0439, -6.7138],
        [ 4.4361, -6.3081],
        [ 4.6574, -6.0675],
        [ 4.0821, -6.6554],
        [ 4.6454, -6.0860],
        [ 4.6605, -6.0895],
        [ 4.3757, -6.3665],
        [ 4.4411, -6.2860],
        [ 4.5045, -6.2067],
        [ 4.6559, -6.0578],
        [ 4.1563, -6.5855],
        [ 4.6846, -6.0744],
        [ 4.3351, -6.4183],
        [ 4.6325, -6.1277],
        [ 4.3406, -6.4081],
        [ 4.6786, -6.1183],
        [ 4.4764, -6.3013],
        [ 4.6678, -6.0609],
        [ 4.4898, -6.2419],
        [ 4.6428, -6.1112],
        [ 4.4722, -6.2755],
        [ 4.0981, -6.6105],
        [ 4.6574, -6.0833],
        [ 4.4205, -6.3096],
        [ 4.6329, -6.1095],
        [ 4.6579, -6.0771],
        [ 4.6175, -6.1151],
        [ 3.7123, -6.7986],
        [ 3.5509, -7.2401],
        [ 4.6305, -6.1064],
        [ 3.7794, -6.9378],
        [ 4.0451, -6.6905],
        [ 4.6611, -6.0673],
        [ 4.5933, -6.1415],
        [ 4.6755, -6.0632],
        [ 4.0926, -6.0129],
        [ 4.6468, -6.0946],
        [ 4.6154, -6.1256],
        [ 4.6572, -6.0936],
        [ 4.6409, -6.1047],
        [ 4.6764, -6.0836],
        [ 3.8529, -6.8759],
        [ 4.5437, -6.2212],
        [ 4.6218, -6.1456],
        [ 4.6522, -6.0868],
        [ 4.5215, -6.2332],
        [ 4.5889, -6.1534],
        [ 4.4297, -6.3163],
        [ 4.5608, -6.1370],
        [ 4.4059, -6.3359],
        [ 3.6290, -7.1375],
        [ 4.4302, -6.1939],
        [ 4.6417, -6.1003],
        [ 4.6639, -6.0721],
        [ 4.5895, -6.1604],
        [ 4.6339, -6.1252],
        [ 4.6263, -6.1048],
        [ 3.2580, -7.5127],
        [ 4.6767, -6.0805],
        [ 4.6580, -6.0767],
        [ 4.5400, -6.2246],
        [ 4.6809, -6.0718],
        [ 3.9545, -6.7527],
        [ 3.2029, -7.5299],
        [ 4.2786, -6.4590],
        [ 4.1359, -6.5936],
        [ 4.2386, -6.5177],
        [ 4.2769, -6.4436],
        [ 4.2551, -6.4820],
        [ 4.6632, -6.0550],
        [ 4.1068, -6.6347],
        [ 4.6067, -6.1356],
        [ 4.3228, -6.3879],
        [ 4.3761, -6.1006],
        [ 4.4561, -6.2828],
        [ 4.5892, -6.1329],
        [ 4.5368, -6.2012],
        [ 4.6648, -6.0974],
        [ 3.0500, -7.6652],
        [ 4.5257, -6.2111],
        [ 4.6477, -6.0580]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0173, 0.9827],
        [0.0430, 0.9570]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9998e-01, 2.2145e-05], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1405, 0.1400],
         [0.0773, 0.1324]],

        [[0.1278, 0.0604],
         [0.2765, 0.6493]],

        [[0.1196, 0.2099],
         [0.9079, 0.2810]],

        [[0.8660, 0.1085],
         [0.6148, 0.6433]],

        [[0.5494, 0.1423],
         [0.7723, 0.3831]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: -0.006658343736995423
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.004294123202807246
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.004558957675867883
Average Adjusted Rand Index: -0.002190493387960534
Iteration 0: Loss = -21291.402936222687
Iteration 10: Loss = -9773.021106515922
Iteration 20: Loss = -9772.955854005613
Iteration 30: Loss = -9771.979285675996
Iteration 40: Loss = -9771.037381181166
Iteration 50: Loss = -9770.941637841723
Iteration 60: Loss = -9770.934640872407
Iteration 70: Loss = -9770.934980996582
1
Iteration 80: Loss = -9770.93543871967
2
Iteration 90: Loss = -9770.935644608744
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.1345, 0.8655],
        [0.0286, 0.9714]], dtype=torch.float64)
alpha: tensor([0.0327, 0.9673])
beta: tensor([[[0.1654, 0.1563],
         [0.2594, 0.1310]],

        [[0.7668, 0.1919],
         [0.5243, 0.1186]],

        [[0.3463, 0.2167],
         [0.4047, 0.1734]],

        [[0.5462, 0.1119],
         [0.6920, 0.6690]],

        [[0.5772, 0.1452],
         [0.9168, 0.1993]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.009987515605493134
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.004294123202807246
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000857612811303036
Average Adjusted Rand Index: 0.0011386784805371775
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21290.842405057545
Iteration 100: Loss = -9807.678297319719
Iteration 200: Loss = -9782.996737601385
Iteration 300: Loss = -9775.492654151027
Iteration 400: Loss = -9774.425764616735
Iteration 500: Loss = -9773.85748633758
Iteration 600: Loss = -9773.457781409135
Iteration 700: Loss = -9773.083065989731
Iteration 800: Loss = -9772.603482545448
Iteration 900: Loss = -9772.176127279503
Iteration 1000: Loss = -9771.898402964995
Iteration 1100: Loss = -9771.697431694367
Iteration 1200: Loss = -9771.524378190115
Iteration 1300: Loss = -9771.344841490733
Iteration 1400: Loss = -9771.108789709964
Iteration 1500: Loss = -9770.94569154114
Iteration 1600: Loss = -9770.825234103922
Iteration 1700: Loss = -9770.72972114657
Iteration 1800: Loss = -9770.649993750558
Iteration 1900: Loss = -9770.56956070016
Iteration 2000: Loss = -9770.460688123201
Iteration 2100: Loss = -9770.303559036063
Iteration 2200: Loss = -9770.091522359984
Iteration 2300: Loss = -9769.870529059894
Iteration 2400: Loss = -9769.675161650972
Iteration 2500: Loss = -9769.508911573637
Iteration 2600: Loss = -9769.368861857314
Iteration 2700: Loss = -9769.238799158818
Iteration 2800: Loss = -9768.83590352255
Iteration 2900: Loss = -9768.623429990477
Iteration 3000: Loss = -9768.522816213648
Iteration 3100: Loss = -9768.452954346365
Iteration 3200: Loss = -9768.399950136409
Iteration 3300: Loss = -9768.358198460977
Iteration 3400: Loss = -9768.324626805414
Iteration 3500: Loss = -9768.297345151335
Iteration 3600: Loss = -9768.274685605516
Iteration 3700: Loss = -9768.255582300255
Iteration 3800: Loss = -9768.239279315014
Iteration 3900: Loss = -9768.225310135153
Iteration 4000: Loss = -9768.213145462387
Iteration 4100: Loss = -9768.202566259873
Iteration 4200: Loss = -9768.193196318576
Iteration 4300: Loss = -9768.185460113606
Iteration 4400: Loss = -9768.177562563293
Iteration 4500: Loss = -9768.170970861183
Iteration 4600: Loss = -9768.165026981682
Iteration 4700: Loss = -9768.159655404217
Iteration 4800: Loss = -9768.154699093999
Iteration 4900: Loss = -9768.150250254128
Iteration 5000: Loss = -9768.23084164694
1
Iteration 5100: Loss = -9768.142497279861
Iteration 5200: Loss = -9768.139092120226
Iteration 5300: Loss = -9768.135928730739
Iteration 5400: Loss = -9768.13303920937
Iteration 5500: Loss = -9768.1303864015
Iteration 5600: Loss = -9768.127919018416
Iteration 5700: Loss = -9768.125721071436
Iteration 5800: Loss = -9768.123509729701
Iteration 5900: Loss = -9768.121559922107
Iteration 6000: Loss = -9768.119723680536
Iteration 6100: Loss = -9768.139780004205
1
Iteration 6200: Loss = -9768.116413657632
Iteration 6300: Loss = -9768.11493620941
Iteration 6400: Loss = -9768.113605025295
Iteration 6500: Loss = -9768.118945313428
1
Iteration 6600: Loss = -9768.111083143109
Iteration 6700: Loss = -9768.109916188412
Iteration 6800: Loss = -9768.108895182066
Iteration 6900: Loss = -9768.108253291526
Iteration 7000: Loss = -9768.106956487374
Iteration 7100: Loss = -9768.106093988386
Iteration 7200: Loss = -9768.23735807242
1
Iteration 7300: Loss = -9768.10449574255
Iteration 7400: Loss = -9768.10375218216
Iteration 7500: Loss = -9768.10308709556
Iteration 7600: Loss = -9768.11154143641
1
Iteration 7700: Loss = -9768.101827706867
Iteration 7800: Loss = -9768.101229643074
Iteration 7900: Loss = -9768.100691759957
Iteration 8000: Loss = -9768.100603745785
Iteration 8100: Loss = -9768.099701487497
Iteration 8200: Loss = -9768.099253910277
Iteration 8300: Loss = -9768.124072995714
1
Iteration 8400: Loss = -9768.098425154194
Iteration 8500: Loss = -9768.097994305937
Iteration 8600: Loss = -9768.097676322788
Iteration 8700: Loss = -9768.098821238837
1
Iteration 8800: Loss = -9768.09699224433
Iteration 8900: Loss = -9768.096686770175
Iteration 9000: Loss = -9768.170707283189
1
Iteration 9100: Loss = -9768.09608357478
Iteration 9200: Loss = -9768.095831184553
Iteration 9300: Loss = -9768.095559434909
Iteration 9400: Loss = -9768.09546190255
Iteration 9500: Loss = -9768.095087313817
Iteration 9600: Loss = -9768.094922691926
Iteration 9700: Loss = -9768.094736284214
Iteration 9800: Loss = -9768.094539739497
Iteration 9900: Loss = -9768.094323121824
Iteration 10000: Loss = -9768.094161084964
Iteration 10100: Loss = -9768.09534258586
1
Iteration 10200: Loss = -9768.093829104486
Iteration 10300: Loss = -9768.093701212594
Iteration 10400: Loss = -9768.093550591218
Iteration 10500: Loss = -9768.093608594581
1
Iteration 10600: Loss = -9768.093301069412
Iteration 10700: Loss = -9768.093159625509
Iteration 10800: Loss = -9768.475690149382
1
Iteration 10900: Loss = -9768.092929723392
Iteration 11000: Loss = -9768.092832359189
Iteration 11100: Loss = -9768.092732999874
Iteration 11200: Loss = -9768.092670395667
Iteration 11300: Loss = -9768.092555818172
Iteration 11400: Loss = -9768.092478259057
Iteration 11500: Loss = -9768.096015639021
1
Iteration 11600: Loss = -9768.092309127973
Iteration 11700: Loss = -9768.092223193222
Iteration 11800: Loss = -9768.381645950958
1
Iteration 11900: Loss = -9768.092094917165
Iteration 12000: Loss = -9768.09200487873
Iteration 12100: Loss = -9768.091953131778
Iteration 12200: Loss = -9768.185349156334
1
Iteration 12300: Loss = -9768.091808936506
Iteration 12400: Loss = -9768.091713917498
Iteration 12500: Loss = -9768.091656782717
Iteration 12600: Loss = -9768.110382689503
1
Iteration 12700: Loss = -9768.091573117867
Iteration 12800: Loss = -9768.091517243576
Iteration 12900: Loss = -9768.091452353392
Iteration 13000: Loss = -9768.099590629381
1
Iteration 13100: Loss = -9768.091390340105
Iteration 13200: Loss = -9768.091354937262
Iteration 13300: Loss = -9768.091322864066
Iteration 13400: Loss = -9768.091447941208
1
Iteration 13500: Loss = -9768.091289422835
Iteration 13600: Loss = -9768.091234130516
Iteration 13700: Loss = -9768.462767091834
1
Iteration 13800: Loss = -9768.091196043284
Iteration 13900: Loss = -9768.091166088681
Iteration 14000: Loss = -9768.091133743415
Iteration 14100: Loss = -9768.092056209369
1
Iteration 14200: Loss = -9768.091106196469
Iteration 14300: Loss = -9768.091109439554
1
Iteration 14400: Loss = -9768.169083646904
2
Iteration 14500: Loss = -9768.09106215076
Iteration 14600: Loss = -9768.091036860736
Iteration 14700: Loss = -9768.091030553727
Iteration 14800: Loss = -9768.091146448667
1
Iteration 14900: Loss = -9768.091009241707
Iteration 15000: Loss = -9768.090959395871
Iteration 15100: Loss = -9768.091561936308
1
Iteration 15200: Loss = -9768.090960839105
2
Iteration 15300: Loss = -9768.090961482376
3
Iteration 15400: Loss = -9768.109403478787
4
Iteration 15500: Loss = -9768.090934074482
Iteration 15600: Loss = -9768.090922124198
Iteration 15700: Loss = -9768.090890551186
Iteration 15800: Loss = -9768.091766389267
1
Iteration 15900: Loss = -9768.090886798369
Iteration 16000: Loss = -9768.09085051805
Iteration 16100: Loss = -9768.09260701675
1
Iteration 16200: Loss = -9768.08814775395
Iteration 16300: Loss = -9768.088934913883
1
Iteration 16400: Loss = -9768.088227093513
2
Iteration 16500: Loss = -9768.088705304066
3
Iteration 16600: Loss = -9768.088152744562
4
Iteration 16700: Loss = -9768.088396597492
5
Iteration 16800: Loss = -9768.08813946221
Iteration 16900: Loss = -9768.088394758443
1
Iteration 17000: Loss = -9768.088137405924
Iteration 17100: Loss = -9768.148709653458
1
Iteration 17200: Loss = -9768.088143727562
2
Iteration 17300: Loss = -9768.088125530812
Iteration 17400: Loss = -9768.121433190867
1
Iteration 17500: Loss = -9768.088133734354
2
Iteration 17600: Loss = -9768.088451873336
3
Iteration 17700: Loss = -9768.088957907707
4
Iteration 17800: Loss = -9768.095742690826
5
Iteration 17900: Loss = -9768.088907318162
6
Iteration 18000: Loss = -9768.088416042872
7
Iteration 18100: Loss = -9768.089675113586
8
Iteration 18200: Loss = -9768.088419755766
9
Iteration 18300: Loss = -9768.105185269558
10
Stopping early at iteration 18300 due to no improvement.
tensor([[ -9.7411,   8.3464],
        [ -8.7177,   6.5700],
        [-10.0128,   8.0450],
        [ -8.6349,   7.2051],
        [ -3.6801,   2.2167],
        [ -9.8092,   7.1381],
        [ -9.4623,   8.0760],
        [ -9.3778,   7.5869],
        [-10.0542,   7.8530],
        [ -8.7457,   7.2999],
        [-10.2874,   7.7743],
        [-10.6532,   7.6631],
        [ -5.2289,   3.7299],
        [ -9.7712,   8.2529],
        [ -9.7439,   7.6746],
        [ -8.8476,   6.6517],
        [ -9.0779,   7.6657],
        [ -9.6811,   7.7448],
        [ -8.5750,   7.0705],
        [ -8.8560,   7.4625],
        [ -9.0203,   7.5903],
        [ -9.0510,   7.6357],
        [-10.3997,   7.1422],
        [ -4.2028,   2.4071],
        [ -9.2590,   7.7116],
        [ -9.3804,   7.9648],
        [ -9.2724,   6.6079],
        [ -8.4259,   6.9364],
        [  1.3074,  -3.2579],
        [ -9.1825,   7.3679],
        [-10.0015,   7.9698],
        [ -4.8821,   3.4327],
        [ -9.7741,   7.8611],
        [ -8.8834,   7.4063],
        [-12.3309,   7.7157],
        [ -6.1691,   4.7798],
        [ -8.4021,   7.0156],
        [ -9.5574,   7.9432],
        [-10.7117,   7.5883],
        [ -9.0731,   7.6854],
        [ -7.9576,   6.4531],
        [ -8.4600,   6.9406],
        [ -8.9148,   7.3540],
        [-10.2031,   6.8113],
        [ -9.2666,   7.7049],
        [-10.3759,   7.0049],
        [-10.5577,   7.8071],
        [ -9.4548,   7.2570],
        [-10.6775,   7.2049],
        [ -8.8312,   6.8268],
        [ -9.1044,   7.3703],
        [ -8.8363,   7.4316],
        [ -8.9014,   7.4874],
        [ -9.3853,   7.9729],
        [ -9.2857,   7.7084],
        [ -9.2205,   7.8330],
        [ -9.1667,   7.6634],
        [-10.2141,   7.5960],
        [ -9.1522,   7.7545],
        [ -9.2329,   7.5596],
        [-10.2488,   6.2528],
        [ -8.9908,   7.5901],
        [ -9.6036,   7.9159],
        [ -4.8562,   3.4073],
        [ -9.2930,   7.8344],
        [ -9.0137,   7.6193],
        [ -9.4393,   7.3235],
        [ -8.1262,   6.6411],
        [ -9.4346,   7.8323],
        [ -9.6184,   7.8772],
        [ -9.7256,   7.9250],
        [ -8.9416,   7.5553],
        [ -9.7780,   6.8519],
        [ -9.4001,   7.7978],
        [ -9.4595,   8.0011],
        [-10.0746,   7.0842],
        [ -9.9762,   7.5786],
        [ -9.2185,   7.7563],
        [ -8.8240,   7.4206],
        [ -8.9897,   7.6031],
        [ -9.0237,   7.4881],
        [ -0.2960,  -1.1131],
        [ -9.1853,   6.7765],
        [ -9.4798,   7.5259],
        [ -8.9018,   7.4075],
        [ -9.8618,   7.7136],
        [ -8.6667,   7.1991],
        [ -9.2394,   7.2460],
        [ -8.1973,   6.8063],
        [ -9.7516,   7.2498],
        [ -8.8781,   7.4724],
        [ -9.0770,   6.6031],
        [ -8.8175,   6.6421],
        [ -4.9848,   3.5965],
        [ -9.2412,   7.2151],
        [ -9.0087,   7.1593],
        [ -9.0689,   7.4350],
        [ -9.5972,   7.4490],
        [ -9.9460,   7.4226],
        [ -9.2617,   7.2449]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 2.8650e-06],
        [1.7823e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0168, 0.9832], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1995, 0.0835],
         [0.2594, 0.1345]],

        [[0.7668, 0.2336],
         [0.5243, 0.1186]],

        [[0.3463, 0.2089],
         [0.4047, 0.1734]],

        [[0.5462, 0.0996],
         [0.6920, 0.6690]],

        [[0.5772, 0.1248],
         [0.9168, 0.1993]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.0207567131845433
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.001829943009345947
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
Global Adjusted Rand Index: -0.0005790689491107601
Average Adjusted Rand Index: 0.0016754390265663315
Iteration 0: Loss = -37293.09113102316
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6180,    nan]],

        [[0.5373,    nan],
         [0.6614, 0.7942]],

        [[0.2354,    nan],
         [0.0164, 0.2671]],

        [[0.9171,    nan],
         [0.3770, 0.2708]],

        [[0.0891,    nan],
         [0.7737, 0.6773]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37292.18450456056
Iteration 100: Loss = -9806.336635449637
Iteration 200: Loss = -9786.794683622797
Iteration 300: Loss = -9778.70160407551
Iteration 400: Loss = -9776.786876710863
Iteration 500: Loss = -9775.703605076142
Iteration 600: Loss = -9775.018203167165
Iteration 700: Loss = -9774.552050725273
Iteration 800: Loss = -9774.21900059849
Iteration 900: Loss = -9773.97248233628
Iteration 1000: Loss = -9773.783773283185
Iteration 1100: Loss = -9773.635264079647
Iteration 1200: Loss = -9773.515679012877
Iteration 1300: Loss = -9773.417155203211
Iteration 1400: Loss = -9773.333877021803
Iteration 1500: Loss = -9773.260594282887
Iteration 1600: Loss = -9773.18915070201
Iteration 1700: Loss = -9773.098229339419
Iteration 1800: Loss = -9772.939380950378
Iteration 1900: Loss = -9772.8022316183
Iteration 2000: Loss = -9772.195603431941
Iteration 2100: Loss = -9771.607709646247
Iteration 2200: Loss = -9771.470706131442
Iteration 2300: Loss = -9771.368798128966
Iteration 2400: Loss = -9771.284445080253
Iteration 2500: Loss = -9771.203684639017
Iteration 2600: Loss = -9771.125963932003
Iteration 2700: Loss = -9771.068594295011
Iteration 2800: Loss = -9771.013802714146
Iteration 2900: Loss = -9770.96127243927
Iteration 3000: Loss = -9770.909501941214
Iteration 3100: Loss = -9770.844975589187
Iteration 3200: Loss = -9770.781797638358
Iteration 3300: Loss = -9770.7013939496
Iteration 3400: Loss = -9770.660819642726
Iteration 3500: Loss = -9770.628745380467
Iteration 3600: Loss = -9770.600516905597
Iteration 3700: Loss = -9770.574728432863
Iteration 3800: Loss = -9770.55040208312
Iteration 3900: Loss = -9770.52723580028
Iteration 4000: Loss = -9770.50649423864
Iteration 4100: Loss = -9770.489087638685
Iteration 4200: Loss = -9770.473726019445
Iteration 4300: Loss = -9770.459741837809
Iteration 4400: Loss = -9770.446825046418
Iteration 4500: Loss = -9770.434798496095
Iteration 4600: Loss = -9770.423441935334
Iteration 4700: Loss = -9770.412534634524
Iteration 4800: Loss = -9770.40180754622
Iteration 4900: Loss = -9770.390519492254
Iteration 5000: Loss = -9770.37881607756
Iteration 5100: Loss = -9770.367858745436
Iteration 5200: Loss = -9770.357368697361
Iteration 5300: Loss = -9770.344806706287
Iteration 5400: Loss = -9770.331016992266
Iteration 5500: Loss = -9770.31887272042
Iteration 5600: Loss = -9770.306662932815
Iteration 5700: Loss = -9770.294052857465
Iteration 5800: Loss = -9770.279904747133
Iteration 5900: Loss = -9770.264924563859
Iteration 6000: Loss = -9770.248944231626
Iteration 6100: Loss = -9770.229827218574
Iteration 6200: Loss = -9770.198953337604
Iteration 6300: Loss = -9770.167041938903
Iteration 6400: Loss = -9770.12365761048
Iteration 6500: Loss = -9770.075019467275
Iteration 6600: Loss = -9770.017115796596
Iteration 6700: Loss = -9769.944246340756
Iteration 6800: Loss = -9769.865244724808
Iteration 6900: Loss = -9769.759250197676
Iteration 7000: Loss = -9768.908620242279
Iteration 7100: Loss = -9768.787023457271
Iteration 7200: Loss = -9768.748355368574
Iteration 7300: Loss = -9768.728370561494
Iteration 7400: Loss = -9768.715879806416
Iteration 7500: Loss = -9768.707027439052
Iteration 7600: Loss = -9768.700400935915
Iteration 7700: Loss = -9768.695160249494
Iteration 7800: Loss = -9768.690916214968
Iteration 7900: Loss = -9768.687377217344
Iteration 8000: Loss = -9768.684314141055
Iteration 8100: Loss = -9768.681701117537
Iteration 8200: Loss = -9768.679462504013
Iteration 8300: Loss = -9768.677476207824
Iteration 8400: Loss = -9768.675675603976
Iteration 8500: Loss = -9768.674092270074
Iteration 8600: Loss = -9768.672651445988
Iteration 8700: Loss = -9768.671366524452
Iteration 8800: Loss = -9768.67017158429
Iteration 8900: Loss = -9768.669151966935
Iteration 9000: Loss = -9768.668146165852
Iteration 9100: Loss = -9768.667228782693
Iteration 9200: Loss = -9768.826738299977
1
Iteration 9300: Loss = -9768.665654018909
Iteration 9400: Loss = -9768.664943599459
Iteration 9500: Loss = -9768.664294543156
Iteration 9600: Loss = -9768.663687961924
Iteration 9700: Loss = -9768.663118948562
Iteration 9800: Loss = -9768.663876126428
1
Iteration 9900: Loss = -9768.663743673607
2
Iteration 10000: Loss = -9768.66162721976
Iteration 10100: Loss = -9768.66124180652
Iteration 10200: Loss = -9768.732159992307
1
Iteration 10300: Loss = -9768.659886011015
Iteration 10400: Loss = -9768.634889702265
Iteration 10500: Loss = -9768.099557433523
Iteration 10600: Loss = -9768.118954804437
1
Iteration 10700: Loss = -9768.09834369741
Iteration 10800: Loss = -9768.097970087172
Iteration 10900: Loss = -9768.114039474987
1
Iteration 11000: Loss = -9768.097439084337
Iteration 11100: Loss = -9768.097170042982
Iteration 11200: Loss = -9768.09693984947
Iteration 11300: Loss = -9768.120718290742
1
Iteration 11400: Loss = -9768.096522120411
Iteration 11500: Loss = -9768.09632228394
Iteration 11600: Loss = -9768.096158947486
Iteration 11700: Loss = -9768.096115082428
Iteration 11800: Loss = -9768.095809595969
Iteration 11900: Loss = -9768.0956854702
Iteration 12000: Loss = -9768.095505674697
Iteration 12100: Loss = -9768.095393428965
Iteration 12200: Loss = -9768.095261102939
Iteration 12300: Loss = -9768.096508930836
1
Iteration 12400: Loss = -9768.095022167268
Iteration 12500: Loss = -9768.124977413256
1
Iteration 12600: Loss = -9768.094816285184
Iteration 12700: Loss = -9768.094704061496
Iteration 12800: Loss = -9768.09656412954
1
Iteration 12900: Loss = -9768.094584410352
Iteration 13000: Loss = -9768.094524219963
Iteration 13100: Loss = -9768.095816598763
1
Iteration 13200: Loss = -9768.094284459272
Iteration 13300: Loss = -9768.095449315219
1
Iteration 13400: Loss = -9768.094112085582
Iteration 13500: Loss = -9768.094083845894
Iteration 13600: Loss = -9768.094021764588
Iteration 13700: Loss = -9768.095420452486
1
Iteration 13800: Loss = -9768.093901234934
Iteration 13900: Loss = -9768.277069523903
1
Iteration 14000: Loss = -9768.09379124158
Iteration 14100: Loss = -9768.09376060139
Iteration 14200: Loss = -9768.11275653633
1
Iteration 14300: Loss = -9768.093669230802
Iteration 14400: Loss = -9768.093614486645
Iteration 14500: Loss = -9768.314176691074
1
Iteration 14600: Loss = -9768.093570216543
Iteration 14700: Loss = -9768.093780231828
1
Iteration 14800: Loss = -9768.095100936027
2
Iteration 14900: Loss = -9768.093470615548
Iteration 15000: Loss = -9768.093444986034
Iteration 15100: Loss = -9768.09375463293
1
Iteration 15200: Loss = -9768.095803875995
2
Iteration 15300: Loss = -9768.09867534883
3
Iteration 15400: Loss = -9768.093397960014
Iteration 15500: Loss = -9768.094259579024
1
Iteration 15600: Loss = -9768.093264619614
Iteration 15700: Loss = -9768.096984674337
1
Iteration 15800: Loss = -9768.09320825391
Iteration 15900: Loss = -9768.093206982203
Iteration 16000: Loss = -9768.09365182491
1
Iteration 16100: Loss = -9768.093180914528
Iteration 16200: Loss = -9768.093175595206
Iteration 16300: Loss = -9768.09351251704
1
Iteration 16400: Loss = -9768.093166306262
Iteration 16500: Loss = -9768.179157841772
1
Iteration 16600: Loss = -9768.093144249562
Iteration 16700: Loss = -9768.093123632214
Iteration 16800: Loss = -9768.156682868954
1
Iteration 16900: Loss = -9768.093094966222
Iteration 17000: Loss = -9768.09309066489
Iteration 17100: Loss = -9768.103016130639
1
Iteration 17200: Loss = -9768.09411324591
2
Iteration 17300: Loss = -9768.094403439422
3
Iteration 17400: Loss = -9768.093799942406
4
Iteration 17500: Loss = -9768.094176495231
5
Iteration 17600: Loss = -9768.091773335382
Iteration 17700: Loss = -9768.091764790772
Iteration 17800: Loss = -9768.091723614283
Iteration 17900: Loss = -9768.091718593756
Iteration 18000: Loss = -9768.100637616702
1
Iteration 18100: Loss = -9768.091721846495
2
Iteration 18200: Loss = -9768.092048010609
3
Iteration 18300: Loss = -9768.091730946953
4
Iteration 18400: Loss = -9768.091710501036
Iteration 18500: Loss = -9768.123098971993
1
Iteration 18600: Loss = -9768.091702680405
Iteration 18700: Loss = -9768.091719536633
1
Iteration 18800: Loss = -9768.152071907216
2
Iteration 18900: Loss = -9768.091699321065
Iteration 19000: Loss = -9768.090967559974
Iteration 19100: Loss = -9768.092308811789
1
Iteration 19200: Loss = -9768.090967597787
2
Iteration 19300: Loss = -9768.120017284096
3
Iteration 19400: Loss = -9768.092103848685
4
Iteration 19500: Loss = -9768.093825787144
5
Iteration 19600: Loss = -9768.090938639365
Iteration 19700: Loss = -9768.091878123576
1
Iteration 19800: Loss = -9768.090940256778
2
Iteration 19900: Loss = -9768.22846584893
3
tensor([[ -9.3546,   7.9335],
        [ -8.6775,   7.1391],
        [ -9.4586,   7.8826],
        [ -9.3614,   6.4876],
        [ -8.8103,   7.3464],
        [-10.2352,   6.0731],
        [ -9.2693,   7.8829],
        [ -9.8151,   7.4374],
        [ -9.3688,   7.9750],
        [ -8.7253,   7.3175],
        [ -9.1389,   7.7500],
        [ -9.0974,   7.7024],
        [ -8.8856,   7.4956],
        [-10.1021,   7.0897],
        [ -8.9098,   7.4095],
        [ -9.1981,   6.4557],
        [ -9.2272,   7.8271],
        [ -9.9980,   7.7343],
        [ -8.6989,   7.2639],
        [ -8.8438,   7.4226],
        [ -9.2478,   7.6588],
        [-10.8212,   7.4634],
        [ -9.0672,   7.6650],
        [ -8.9436,   7.4917],
        [ -8.7831,   7.3960],
        [ -9.3527,   7.8327],
        [ -9.6125,   6.9234],
        [ -4.0701,   2.5887],
        [  1.4833,  -3.0641],
        [-10.3696,   6.3384],
        [-11.6651,   7.5525],
        [ -9.3823,   7.9074],
        [ -9.1291,   7.6751],
        [ -8.8181,   7.4081],
        [ -9.0759,   7.5251],
        [ -9.1384,   7.7037],
        [ -9.2493,   6.7339],
        [-11.4400,   6.8248],
        [ -9.3370,   7.9389],
        [ -9.1914,   7.7911],
        [ -4.7332,   2.4742],
        [-10.2974,   6.0985],
        [-10.2807,   7.1602],
        [ -8.9054,   7.4058],
        [ -9.0426,   7.6504],
        [ -8.8092,   7.0597],
        [ -9.3499,   7.8304],
        [ -9.6331,   7.8451],
        [ -9.5790,   7.4981],
        [ -8.6019,   6.4029],
        [ -9.4493,   7.5902],
        [ -9.1896,   7.6938],
        [ -9.2906,   7.9037],
        [ -9.3319,   7.8798],
        [ -9.7748,   7.4384],
        [ -8.7986,   7.3882],
        [ -9.0731,   7.1329],
        [ -9.5590,   7.6738],
        [ -9.4132,   7.8727],
        [ -9.3000,   7.8177],
        [ -9.4648,   7.2615],
        [ -9.0077,   7.5965],
        [ -9.0180,   7.6202],
        [-10.8951,   6.2798],
        [ -9.0281,   7.6402],
        [ -8.8489,   7.3724],
        [ -9.5223,   7.7617],
        [ -8.2288,   6.6355],
        [ -9.0785,   7.6483],
        [-10.3368,   7.6919],
        [ -9.0895,   7.6931],
        [ -9.0547,   7.3824],
        [ -9.0093,   7.5335],
        [ -9.4280,   7.6769],
        [-10.4018,   7.7832],
        [-10.0892,   7.3182],
        [ -9.5285,   8.0348],
        [ -9.7839,   8.2257],
        [ -9.0256,   7.3329],
        [ -9.6553,   8.1750],
        [ -8.8593,   7.4675],
        [ -0.2919,  -1.1055],
        [ -8.9432,   7.5560],
        [ -9.2233,   6.6211],
        [ -9.7946,   7.2130],
        [ -9.8015,   7.1048],
        [ -8.7025,   7.2860],
        [ -9.2032,   7.1973],
        [ -8.5341,   6.9646],
        [-10.9333,   7.3857],
        [ -8.9382,   7.5497],
        [ -8.4875,   6.8163],
        [ -8.5972,   7.1822],
        [ -8.6379,   7.2405],
        [ -8.9014,   7.2378],
        [ -9.8124,   7.4408],
        [ -9.2424,   7.4404],
        [ -9.1520,   7.7403],
        [ -8.8723,   7.4134],
        [ -8.8381,   7.3557]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 2.5324e-06],
        [1.7267e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0168, 0.9832], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.0833],
         [0.6180, 0.1340]],

        [[0.5373, 0.2336],
         [0.6614, 0.7942]],

        [[0.2354, 0.2090],
         [0.0164, 0.2671]],

        [[0.9171, 0.0995],
         [0.3770, 0.2708]],

        [[0.0891, 0.1246],
         [0.7737, 0.6773]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.0207567131845433
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.001829943009345947
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
Global Adjusted Rand Index: -0.0005790689491107601
Average Adjusted Rand Index: 0.0016754390265663315
9915.876623495691
new:  [-0.0005790689491107601, -0.004558957675867883, -0.0005790689491107601, -0.0005790689491107601] [0.0016754390265663315, -0.002190493387960534, 0.0016754390265663315, 0.0016754390265663315] [9768.08914118292, 9769.775893265904, 9768.105185269558, 9768.090940050048]
prior:  [0.0, 0.000857612811303036, 0.000857612811303036, 0.0] [0.0, 0.0011386784805371775, 0.0011386784805371775, 0.0] [9773.022070837165, 9770.935697971707, 9770.935644608744, nan]
-----------------------------------------------------------------------------------------
This iteration is 9
True Objective function: Loss = -10020.17750736485
Iteration 0: Loss = -22136.23100616669
Iteration 10: Loss = -9908.00187787485
Iteration 20: Loss = -9907.70222969734
Iteration 30: Loss = -9905.334884069147
Iteration 40: Loss = -9904.597533141892
Iteration 50: Loss = -9904.480700621898
Iteration 60: Loss = -9904.454179491071
Iteration 70: Loss = -9904.44577514808
Iteration 80: Loss = -9904.44242778354
Iteration 90: Loss = -9904.440907021239
Iteration 100: Loss = -9904.439806438982
Iteration 110: Loss = -9904.43800695427
Iteration 120: Loss = -9904.430572151823
Iteration 130: Loss = -9904.375707573128
Iteration 140: Loss = -9903.845096846855
Iteration 150: Loss = -9903.52263577698
Iteration 160: Loss = -9903.5183180437
Iteration 170: Loss = -9903.513702954353
Iteration 180: Loss = -9903.392200855533
Iteration 190: Loss = -9902.98620427659
Iteration 200: Loss = -9902.830141515775
Iteration 210: Loss = -9902.755235456214
Iteration 220: Loss = -9902.707425716802
Iteration 230: Loss = -9902.676164039385
Iteration 240: Loss = -9902.65539951868
Iteration 250: Loss = -9902.64140563946
Iteration 260: Loss = -9902.631791276006
Iteration 270: Loss = -9902.625167295888
Iteration 280: Loss = -9902.620564227314
Iteration 290: Loss = -9902.617326746493
Iteration 300: Loss = -9902.615107095906
Iteration 310: Loss = -9902.613502691323
Iteration 320: Loss = -9902.61235109908
Iteration 330: Loss = -9902.611542006656
Iteration 340: Loss = -9902.610957235618
Iteration 350: Loss = -9902.610536553017
Iteration 360: Loss = -9902.610272797008
Iteration 370: Loss = -9902.610074869403
Iteration 380: Loss = -9902.609898065977
Iteration 390: Loss = -9902.609778547718
Iteration 400: Loss = -9902.609726446406
Iteration 410: Loss = -9902.609684495857
Iteration 420: Loss = -9902.60961011986
Iteration 430: Loss = -9902.6095722212
Iteration 440: Loss = -9902.609579215783
1
Iteration 450: Loss = -9902.609565405426
Iteration 460: Loss = -9902.609546265565
Iteration 470: Loss = -9902.609548995733
1
Iteration 480: Loss = -9902.609571951223
2
Iteration 490: Loss = -9902.609554633851
3
Stopping early at iteration 489 due to no improvement.
pi: tensor([[0.1556, 0.8444],
        [0.0998, 0.9002]], dtype=torch.float64)
alpha: tensor([0.1074, 0.8926])
beta: tensor([[[0.0767, 0.1102],
         [0.4088, 0.1444]],

        [[0.9192, 0.0890],
         [0.9576, 0.1131]],

        [[0.9017, 0.0980],
         [0.3038, 0.6139]],

        [[0.9880, 0.0967],
         [0.6533, 0.5570]],

        [[0.9511, 0.1355],
         [0.8254, 0.1979]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.001684040076915292
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.001296339264509808
Average Adjusted Rand Index: 0.0010063406390739664
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22442.478660881232
Iteration 100: Loss = -9912.658466936615
Iteration 200: Loss = -9910.461737226206
Iteration 300: Loss = -9909.512937021076
Iteration 400: Loss = -9909.03497388931
Iteration 500: Loss = -9908.737110737511
Iteration 600: Loss = -9908.52983168199
Iteration 700: Loss = -9908.377954465044
Iteration 800: Loss = -9908.262388035715
Iteration 900: Loss = -9908.171769817804
Iteration 1000: Loss = -9908.099260157818
Iteration 1100: Loss = -9908.040760973738
Iteration 1200: Loss = -9907.994061514768
Iteration 1300: Loss = -9907.957786112413
Iteration 1400: Loss = -9907.930643130592
Iteration 1500: Loss = -9907.91106820265
Iteration 1600: Loss = -9907.897130467234
Iteration 1700: Loss = -9907.886957372915
Iteration 1800: Loss = -9907.879095660073
Iteration 1900: Loss = -9907.872633630843
Iteration 2000: Loss = -9907.867057016732
Iteration 2100: Loss = -9907.86209874178
Iteration 2200: Loss = -9907.857643625877
Iteration 2300: Loss = -9907.85369124543
Iteration 2400: Loss = -9907.850096336071
Iteration 2500: Loss = -9907.846907150823
Iteration 2600: Loss = -9907.843987505355
Iteration 2700: Loss = -9907.841348405618
Iteration 2800: Loss = -9907.838930587328
Iteration 2900: Loss = -9907.836749845446
Iteration 3000: Loss = -9907.834706282105
Iteration 3100: Loss = -9907.83283545681
Iteration 3200: Loss = -9907.831056203551
Iteration 3300: Loss = -9907.829354329167
Iteration 3400: Loss = -9907.827733831347
Iteration 3500: Loss = -9907.826158072716
Iteration 3600: Loss = -9907.824614904262
Iteration 3700: Loss = -9907.823108736598
Iteration 3800: Loss = -9907.821545571862
Iteration 3900: Loss = -9907.819909189875
Iteration 4000: Loss = -9907.818210209627
Iteration 4100: Loss = -9907.816281701243
Iteration 4200: Loss = -9907.814033772926
Iteration 4300: Loss = -9907.811385431442
Iteration 4400: Loss = -9907.808135246716
Iteration 4500: Loss = -9907.804078325085
Iteration 4600: Loss = -9907.798713499897
Iteration 4700: Loss = -9907.79066247314
Iteration 4800: Loss = -9907.773145073214
Iteration 4900: Loss = -9907.438408069558
Iteration 5000: Loss = -9907.165983884664
Iteration 5100: Loss = -9907.108232882245
Iteration 5200: Loss = -9907.07313089226
Iteration 5300: Loss = -9907.043439132196
Iteration 5400: Loss = -9907.0115863022
Iteration 5500: Loss = -9906.980270641707
Iteration 5600: Loss = -9906.94957269338
Iteration 5700: Loss = -9906.917506870428
Iteration 5800: Loss = -9906.883702192146
Iteration 5900: Loss = -9906.848196975156
Iteration 6000: Loss = -9906.811236508338
Iteration 6100: Loss = -9906.774527835434
Iteration 6200: Loss = -9906.735859073524
Iteration 6300: Loss = -9906.700860696978
Iteration 6400: Loss = -9906.698537140242
Iteration 6500: Loss = -9906.662680495663
Iteration 6600: Loss = -9906.65507413919
Iteration 6700: Loss = -9906.649840772392
Iteration 6800: Loss = -9906.6451990458
Iteration 6900: Loss = -9906.64047400858
Iteration 7000: Loss = -9906.643400636156
1
Iteration 7100: Loss = -9906.630014631164
Iteration 7200: Loss = -9906.703115715409
1
Iteration 7300: Loss = -9906.610261078717
Iteration 7400: Loss = -9906.596673645428
Iteration 7500: Loss = -9906.581230021195
Iteration 7600: Loss = -9906.554299116686
Iteration 7700: Loss = -9906.504029953845
Iteration 7800: Loss = -9906.414071752099
Iteration 7900: Loss = -9906.30131091954
Iteration 8000: Loss = -9906.265212674127
Iteration 8100: Loss = -9906.248171742876
Iteration 8200: Loss = -9906.23910357739
Iteration 8300: Loss = -9906.233988542623
Iteration 8400: Loss = -9906.246325856779
1
Iteration 8500: Loss = -9906.227017245099
Iteration 8600: Loss = -9906.23951200795
1
Iteration 8700: Loss = -9906.22340101327
Iteration 8800: Loss = -9906.223326231162
Iteration 8900: Loss = -9906.221199727026
Iteration 9000: Loss = -9906.220317663448
Iteration 9100: Loss = -9906.222094751525
1
Iteration 9200: Loss = -9906.218965986925
Iteration 9300: Loss = -9906.218487985432
Iteration 9400: Loss = -9906.218151146275
Iteration 9500: Loss = -9906.217613482713
Iteration 9600: Loss = -9906.217652450494
1
Iteration 9700: Loss = -9906.21749949264
Iteration 9800: Loss = -9906.216785890942
Iteration 9900: Loss = -9906.219106752005
1
Iteration 10000: Loss = -9906.2170210262
2
Iteration 10100: Loss = -9906.21660547132
Iteration 10200: Loss = -9906.226874048623
1
Iteration 10300: Loss = -9906.216489550758
Iteration 10400: Loss = -9906.234915359038
1
Iteration 10500: Loss = -9906.215354676224
Iteration 10600: Loss = -9906.215186428502
Iteration 10700: Loss = -9906.21541042254
1
Iteration 10800: Loss = -9906.21516171904
Iteration 10900: Loss = -9906.217212748465
1
Iteration 11000: Loss = -9906.21475704945
Iteration 11100: Loss = -9906.214690553397
Iteration 11200: Loss = -9906.214580089598
Iteration 11300: Loss = -9906.214593842387
1
Iteration 11400: Loss = -9906.214386665564
Iteration 11500: Loss = -9906.214793181178
1
Iteration 11600: Loss = -9906.215400177172
2
Iteration 11700: Loss = -9906.216344281556
3
Iteration 11800: Loss = -9906.21417318494
Iteration 11900: Loss = -9906.214276087554
1
Iteration 12000: Loss = -9906.216187747028
2
Iteration 12100: Loss = -9906.21396080961
Iteration 12200: Loss = -9906.213994122669
1
Iteration 12300: Loss = -9906.217458471034
2
Iteration 12400: Loss = -9906.21473653919
3
Iteration 12500: Loss = -9906.221100261564
4
Iteration 12600: Loss = -9906.248592643407
5
Iteration 12700: Loss = -9906.217193402628
6
Iteration 12800: Loss = -9906.213921610388
Iteration 12900: Loss = -9906.213590604531
Iteration 13000: Loss = -9906.21345446041
Iteration 13100: Loss = -9906.198919233124
Iteration 13200: Loss = -9906.17228775956
Iteration 13300: Loss = -9906.316216941072
1
Iteration 13400: Loss = -9906.16316161187
Iteration 13500: Loss = -9906.153965069068
Iteration 13600: Loss = -9906.141223575512
Iteration 13700: Loss = -9906.139193264578
Iteration 13800: Loss = -9906.134920882529
Iteration 13900: Loss = -9906.12499650801
Iteration 14000: Loss = -9905.769957192553
Iteration 14100: Loss = -9905.683498282133
Iteration 14200: Loss = -9905.58110668746
Iteration 14300: Loss = -9905.369452743904
Iteration 14400: Loss = -9905.101776800648
Iteration 14500: Loss = -9905.007757942554
Iteration 14600: Loss = -9904.9325208905
Iteration 14700: Loss = -9904.861130341367
Iteration 14800: Loss = -9904.753045597312
Iteration 14900: Loss = -9904.628558737371
Iteration 15000: Loss = -9904.488180030121
Iteration 15100: Loss = -9904.354299534238
Iteration 15200: Loss = -9904.235733601075
Iteration 15300: Loss = -9904.089098901939
Iteration 15400: Loss = -9904.033903878748
Iteration 15500: Loss = -9903.928053206682
Iteration 15600: Loss = -9903.753405862013
Iteration 15700: Loss = -9903.544457717699
Iteration 15800: Loss = -9903.362103211628
Iteration 15900: Loss = -9903.361671604589
Iteration 16000: Loss = -9903.360473271247
Iteration 16100: Loss = -9903.360365522478
Iteration 16200: Loss = -9903.385796530865
1
Iteration 16300: Loss = -9903.360256737835
Iteration 16400: Loss = -9903.363993839743
1
Iteration 16500: Loss = -9903.35987044598
Iteration 16600: Loss = -9903.37730218091
1
Iteration 16700: Loss = -9903.359747518545
Iteration 16800: Loss = -9903.359941372424
1
Iteration 16900: Loss = -9903.359609453342
Iteration 17000: Loss = -9903.36188090108
1
Iteration 17100: Loss = -9903.361053904351
2
Iteration 17200: Loss = -9903.366605963005
3
Iteration 17300: Loss = -9903.39685270646
4
Iteration 17400: Loss = -9903.377367086077
5
Iteration 17500: Loss = -9903.360191817288
6
Iteration 17600: Loss = -9903.360737484078
7
Iteration 17700: Loss = -9903.41104161438
8
Iteration 17800: Loss = -9903.390279535097
9
Iteration 17900: Loss = -9903.361085381368
10
Stopping early at iteration 17900 due to no improvement.
tensor([[ 3.3389, -7.9541],
        [ 3.2637, -7.8789],
        [ 3.2829, -7.8982],
        [ 3.2531, -7.8683],
        [ 3.3649, -7.9801],
        [ 3.3336, -7.9489],
        [ 3.2935, -7.9087],
        [ 3.3287, -7.9440],
        [ 3.0405, -7.6557],
        [ 3.3659, -7.9811],
        [ 3.2959, -7.9111],
        [ 3.3494, -7.9646],
        [ 3.2763, -7.8915],
        [ 3.3144, -7.9296],
        [ 3.3006, -7.9158],
        [ 3.3197, -7.9349],
        [ 3.2751, -7.8903],
        [ 3.3321, -7.9473],
        [ 3.3508, -7.9660],
        [ 3.3023, -7.9175],
        [ 3.3683, -7.9835],
        [ 3.1932, -7.8085],
        [ 3.1234, -7.7387],
        [ 3.3002, -7.9155],
        [ 3.2392, -7.8545],
        [ 3.2479, -7.8631],
        [ 3.3244, -7.9396],
        [ 3.2835, -7.8987],
        [ 3.2536, -7.8689],
        [ 2.8707, -7.4859],
        [ 3.3201, -7.9353],
        [ 3.3212, -7.9365],
        [ 3.3464, -7.9616],
        [ 3.0373, -7.6525],
        [ 3.3060, -7.9212],
        [ 3.2634, -7.8786],
        [ 3.2938, -7.9090],
        [ 3.3274, -7.9426],
        [ 3.3229, -7.9381],
        [ 3.0804, -7.6956],
        [ 3.2649, -7.8801],
        [ 3.3307, -7.9459],
        [ 3.3742, -7.9894],
        [ 3.2935, -7.9088],
        [ 3.3056, -7.9208],
        [ 3.1586, -7.7738],
        [ 3.3159, -7.9311],
        [ 3.3055, -7.9208],
        [ 3.3211, -7.9363],
        [ 3.1316, -7.7468],
        [ 3.2769, -7.8921],
        [ 3.1546, -7.7698],
        [ 3.1218, -7.7370],
        [ 3.3404, -7.9556],
        [ 3.2830, -7.8982],
        [ 2.9652, -7.5805],
        [ 3.3024, -7.9176],
        [ 3.2621, -7.8774],
        [ 3.3249, -7.9402],
        [ 3.3309, -7.9461],
        [ 3.3119, -7.9271],
        [ 3.0256, -7.6408],
        [ 3.3538, -7.9690],
        [ 3.0632, -7.6784],
        [ 3.3407, -7.9559],
        [ 3.3064, -7.9216],
        [ 3.3029, -7.9181],
        [ 3.3200, -7.9352],
        [ 3.3216, -7.9368],
        [ 3.3308, -7.9460],
        [ 3.2845, -7.8997],
        [ 3.0929, -7.7081],
        [ 3.3504, -7.9656],
        [ 3.2669, -7.8821],
        [ 3.3320, -7.9472],
        [ 3.3048, -7.9200],
        [ 3.3123, -7.9276],
        [ 3.3865, -8.0017],
        [ 3.3485, -7.9637],
        [ 3.3148, -7.9300],
        [ 3.2631, -7.8783],
        [ 3.3875, -8.0027],
        [ 3.2330, -7.8482],
        [ 3.2610, -7.8762],
        [ 3.3195, -7.9347],
        [ 3.2290, -7.8442],
        [ 3.3118, -7.9270],
        [ 3.3571, -7.9723],
        [ 3.3238, -7.9390],
        [ 3.3318, -7.9470],
        [ 3.2735, -7.8887],
        [ 3.2750, -7.8903],
        [ 3.3382, -7.9534],
        [ 3.1615, -7.7767],
        [ 3.3010, -7.9162],
        [ 3.2765, -7.8917],
        [ 3.2478, -7.8630],
        [ 3.3144, -7.9296],
        [ 3.3763, -7.9915],
        [ 3.2010, -7.8162]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0596, 0.9404],
        [0.1518, 0.8482]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 1.4503e-05], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1333, 0.1310],
         [0.4088, 0.1423]],

        [[0.9192, 0.0806],
         [0.9576, 0.1131]],

        [[0.9017, 0.1365],
         [0.3038, 0.6139]],

        [[0.9880, 0.1043],
         [0.6533, 0.5570]],

        [[0.9511, 0.1557],
         [0.8254, 0.1979]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -5.603047611785861e-05
Average Adjusted Rand Index: -0.00015850936499207452
Iteration 0: Loss = -15220.4675893907
Iteration 10: Loss = -9903.458347212876
Iteration 20: Loss = -9903.202343733617
Iteration 30: Loss = -9903.055977366486
Iteration 40: Loss = -9902.92855515768
Iteration 50: Loss = -9902.814098769653
Iteration 60: Loss = -9902.713945707206
Iteration 70: Loss = -9902.628711926684
Iteration 80: Loss = -9902.558777079055
Iteration 90: Loss = -9902.504965800857
Iteration 100: Loss = -9902.469011222822
Iteration 110: Loss = -9902.45195487138
Iteration 120: Loss = -9902.452436529491
1
Iteration 130: Loss = -9902.466165373247
2
Iteration 140: Loss = -9902.487240584558
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.1874, 0.8126],
        [0.1237, 0.8763]], dtype=torch.float64)
alpha: tensor([0.1344, 0.8656])
beta: tensor([[[0.0810, 0.1128],
         [0.8843, 0.1461]],

        [[0.9236, 0.0940],
         [0.5331, 0.6850]],

        [[0.1886, 0.1038],
         [0.4539, 0.1167]],

        [[0.5960, 0.0993],
         [0.0805, 0.6976]],

        [[0.2644, 0.1334],
         [0.5441, 0.8881]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.007389132627638759
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.003226781515726774
Average Adjusted Rand Index: 0.0014106950214070842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15220.082117017722
Iteration 100: Loss = -9942.3791327972
Iteration 200: Loss = -9909.488619787715
Iteration 300: Loss = -9908.027205902625
Iteration 400: Loss = -9907.581376391461
Iteration 500: Loss = -9907.373136944969
Iteration 600: Loss = -9907.201679967255
Iteration 700: Loss = -9907.083831524802
Iteration 800: Loss = -9906.93343499049
Iteration 900: Loss = -9906.681271105552
Iteration 1000: Loss = -9906.388303993508
Iteration 1100: Loss = -9906.138629818006
Iteration 1200: Loss = -9905.768905961759
Iteration 1300: Loss = -9905.427271631392
Iteration 1400: Loss = -9905.183891637611
Iteration 1500: Loss = -9904.457475803256
Iteration 1600: Loss = -9903.949192988604
Iteration 1700: Loss = -9903.736638216138
Iteration 1800: Loss = -9903.447719326085
Iteration 1900: Loss = -9903.26854160577
Iteration 2000: Loss = -9903.191480423018
Iteration 2100: Loss = -9903.094672161842
Iteration 2200: Loss = -9902.157125508673
Iteration 2300: Loss = -9902.005518579965
Iteration 2400: Loss = -9901.703480891125
Iteration 2500: Loss = -9901.333888431644
Iteration 2600: Loss = -9901.227166060056
Iteration 2700: Loss = -9901.11120004726
Iteration 2800: Loss = -9900.565404196062
Iteration 2900: Loss = -9900.500283898133
Iteration 3000: Loss = -9900.488166992633
Iteration 3100: Loss = -9900.480386422505
Iteration 3200: Loss = -9900.474402607326
Iteration 3300: Loss = -9900.469590048606
Iteration 3400: Loss = -9900.465674954463
Iteration 3500: Loss = -9900.462365658606
Iteration 3600: Loss = -9900.45944204695
Iteration 3700: Loss = -9900.45629029708
Iteration 3800: Loss = -9900.410315943853
Iteration 3900: Loss = -9900.475377130564
1
Iteration 4000: Loss = -9899.176257635745
Iteration 4100: Loss = -9899.171574571577
Iteration 4200: Loss = -9899.142809703924
Iteration 4300: Loss = -9899.140733853077
Iteration 4400: Loss = -9899.140150361893
Iteration 4500: Loss = -9899.138275860229
Iteration 4600: Loss = -9899.15396690556
1
Iteration 4700: Loss = -9899.136433817319
Iteration 4800: Loss = -9899.135675279773
Iteration 4900: Loss = -9899.135008116238
Iteration 5000: Loss = -9899.134364686008
Iteration 5100: Loss = -9899.164610907272
1
Iteration 5200: Loss = -9899.133205112146
Iteration 5300: Loss = -9899.132554333379
Iteration 5400: Loss = -9899.13471848517
1
Iteration 5500: Loss = -9898.960306624209
Iteration 5600: Loss = -9898.849524432351
Iteration 5700: Loss = -9898.592231451803
Iteration 5800: Loss = -9898.498310558083
Iteration 5900: Loss = -9898.497740637049
Iteration 6000: Loss = -9898.500193453297
1
Iteration 6100: Loss = -9898.496852026601
Iteration 6200: Loss = -9898.49616264441
Iteration 6300: Loss = -9898.44217573695
Iteration 6400: Loss = -9898.441687886107
Iteration 6500: Loss = -9898.445494854403
1
Iteration 6600: Loss = -9898.362102759089
Iteration 6700: Loss = -9898.306096338758
Iteration 6800: Loss = -9898.304432253904
Iteration 6900: Loss = -9898.304083844529
Iteration 7000: Loss = -9898.300881697112
Iteration 7100: Loss = -9898.299470332337
Iteration 7200: Loss = -9898.297627443271
Iteration 7300: Loss = -9898.252836994749
Iteration 7400: Loss = -9897.117266924943
Iteration 7500: Loss = -9897.04956909749
Iteration 7600: Loss = -9897.086503687044
1
Iteration 7700: Loss = -9897.011442145524
Iteration 7800: Loss = -9897.01015399668
Iteration 7900: Loss = -9897.00960577194
Iteration 8000: Loss = -9897.009413974423
Iteration 8100: Loss = -9897.009737503018
1
Iteration 8200: Loss = -9897.009155137066
Iteration 8300: Loss = -9897.097944986894
1
Iteration 8400: Loss = -9896.992213877249
Iteration 8500: Loss = -9895.088523402857
Iteration 8600: Loss = -9895.088163165045
Iteration 8700: Loss = -9895.088045773593
Iteration 8800: Loss = -9895.094249811438
1
Iteration 8900: Loss = -9895.087755321574
Iteration 9000: Loss = -9895.08776144584
1
Iteration 9100: Loss = -9895.16172664902
2
Iteration 9200: Loss = -9895.094012159905
3
Iteration 9300: Loss = -9895.084591864326
Iteration 9400: Loss = -9895.07288287029
Iteration 9500: Loss = -9895.138443224841
1
Iteration 9600: Loss = -9895.018568890924
Iteration 9700: Loss = -9895.018660455902
1
Iteration 9800: Loss = -9895.019799088373
2
Iteration 9900: Loss = -9895.02065504765
3
Iteration 10000: Loss = -9895.025111990133
4
Iteration 10100: Loss = -9895.021416968953
5
Iteration 10200: Loss = -9895.025718928942
6
Iteration 10300: Loss = -9895.019106437903
7
Iteration 10400: Loss = -9895.019925263636
8
Iteration 10500: Loss = -9894.952633802428
Iteration 10600: Loss = -9894.921321002234
Iteration 10700: Loss = -9894.92787889817
1
Iteration 10800: Loss = -9894.922778247892
2
Iteration 10900: Loss = -9890.461892044637
Iteration 11000: Loss = -9888.711057944056
Iteration 11100: Loss = -9888.68540382389
Iteration 11200: Loss = -9886.008330685618
Iteration 11300: Loss = -9882.734878594094
Iteration 11400: Loss = -9882.155454775157
Iteration 11500: Loss = -9882.025165401617
Iteration 11600: Loss = -9881.648459338718
Iteration 11700: Loss = -9881.522688580322
Iteration 11800: Loss = -9881.52016138172
Iteration 11900: Loss = -9881.518099546855
Iteration 12000: Loss = -9881.496872029535
Iteration 12100: Loss = -9881.438283516398
Iteration 12200: Loss = -9881.434664667111
Iteration 12300: Loss = -9881.434347031613
Iteration 12400: Loss = -9881.417925682954
Iteration 12500: Loss = -9881.481214545374
1
Iteration 12600: Loss = -9881.414477225035
Iteration 12700: Loss = -9881.403660875047
Iteration 12800: Loss = -9881.432535269809
1
Iteration 12900: Loss = -9881.438103006873
2
Iteration 13000: Loss = -9881.345186983703
Iteration 13100: Loss = -9881.391534965727
1
Iteration 13200: Loss = -9881.587903806061
2
Iteration 13300: Loss = -9881.340612861924
Iteration 13400: Loss = -9881.34506263403
1
Iteration 13500: Loss = -9881.340347816913
Iteration 13600: Loss = -9881.340091870328
Iteration 13700: Loss = -9881.422649879358
1
Iteration 13800: Loss = -9881.330985007919
Iteration 13900: Loss = -9881.322204784194
Iteration 14000: Loss = -9881.319281233642
Iteration 14100: Loss = -9881.319188771691
Iteration 14200: Loss = -9881.322039702118
1
Iteration 14300: Loss = -9881.319119637956
Iteration 14400: Loss = -9881.32260064421
1
Iteration 14500: Loss = -9881.291746330462
Iteration 14600: Loss = -9881.286273649148
Iteration 14700: Loss = -9881.28617894909
Iteration 14800: Loss = -9881.306451441495
1
Iteration 14900: Loss = -9881.286115799468
Iteration 15000: Loss = -9881.287925759178
1
Iteration 15100: Loss = -9881.282910221647
Iteration 15200: Loss = -9881.283462118594
1
Iteration 15300: Loss = -9881.286232362678
2
Iteration 15400: Loss = -9881.300297306374
3
Iteration 15500: Loss = -9881.285405781915
4
Iteration 15600: Loss = -9881.062375263497
Iteration 15700: Loss = -9881.050675683298
Iteration 15800: Loss = -9881.06525225176
1
Iteration 15900: Loss = -9881.20012292603
2
Iteration 16000: Loss = -9881.048295684875
Iteration 16100: Loss = -9881.048647936228
1
Iteration 16200: Loss = -9881.050111936584
2
Iteration 16300: Loss = -9881.048366502835
3
Iteration 16400: Loss = -9881.066264143774
4
Iteration 16500: Loss = -9881.169328525924
5
Iteration 16600: Loss = -9881.067911435184
6
Iteration 16700: Loss = -9881.048797849438
7
Iteration 16800: Loss = -9881.073979278359
8
Iteration 16900: Loss = -9881.059065177713
9
Iteration 17000: Loss = -9881.058607014409
10
Stopping early at iteration 17000 due to no improvement.
tensor([[ 0.2251, -2.9764],
        [-0.2927, -1.1172],
        [ 0.8725, -3.2577],
        [ 0.2099, -2.4077],
        [-0.4000, -1.2492],
        [ 0.7741, -2.2699],
        [-2.2333,  0.4883],
        [ 1.8291, -3.3312],
        [ 0.6025, -2.3412],
        [ 0.9738, -2.4460],
        [ 0.6761, -3.1956],
        [ 1.4413, -2.8461],
        [ 1.4053, -3.8704],
        [-0.8784, -2.0773],
        [ 0.5403, -2.7024],
        [-0.3850, -1.8618],
        [-0.2184, -1.2569],
        [-1.4440, -0.3421],
        [ 0.4064, -2.1256],
        [ 0.2661, -2.8630],
        [-1.9040,  0.4163],
        [ 0.4018, -1.8939],
        [ 0.4729, -3.6582],
        [ 0.4280, -2.1598],
        [-0.0643, -1.3438],
        [ 1.4840, -3.0925],
        [ 1.6123, -3.1633],
        [-1.4256, -1.4366],
        [-0.2979, -2.0847],
        [ 1.1008, -2.7334],
        [-2.6866,  0.7086],
        [-1.4385, -2.2670],
        [-1.8192,  0.4309],
        [ 1.2116, -3.4166],
        [-1.1857, -0.5230],
        [ 0.5365, -2.9919],
        [-1.8134,  0.4202],
        [-0.0067, -1.4793],
        [ 0.3628, -4.9780],
        [ 0.4784, -3.7857],
        [ 0.6663, -2.7544],
        [ 0.1702, -1.6250],
        [ 0.8871, -3.2136],
        [-0.4071, -1.1980],
        [-0.0996, -1.3533],
        [ 0.0121, -3.7701],
        [-1.6107, -0.7407],
        [-2.8810, -0.4668],
        [-0.0132, -1.3744],
        [-0.5883, -0.8838],
        [-2.8098,  0.8633],
        [ 0.4919, -1.9183],
        [-0.1872, -1.5088],
        [-2.6667,  1.1054],
        [ 0.7166, -4.0287],
        [-0.0976, -1.2960],
        [-1.3032, -0.2254],
        [ 1.1882, -2.7926],
        [-2.2460,  0.8027],
        [-2.2262, -2.0232],
        [ 1.5659, -3.2689],
        [ 0.6240, -2.2646],
        [-2.7240,  1.1673],
        [ 0.4798, -4.3843],
        [ 1.3649, -4.1756],
        [ 1.8104, -3.2018],
        [ 0.0612, -4.4615],
        [ 0.1192, -2.0012],
        [ 1.6868, -3.1238],
        [-3.3797,  1.8406],
        [-0.6257, -1.4203],
        [ 1.5310, -3.1897],
        [-0.2765, -2.1578],
        [-0.2088, -1.7894],
        [ 0.3172, -2.0486],
        [-1.3292, -0.9137],
        [-0.2215, -1.3516],
        [-3.1722,  0.1187],
        [-0.8502, -1.6689],
        [ 0.7045, -2.4264],
        [-2.1930,  0.7625],
        [-1.9789,  0.5863],
        [ 0.4577, -2.3944],
        [-0.0455, -2.1368],
        [ 0.9069, -3.3481],
        [-1.6251, -1.1407],
        [-2.7219,  1.0528],
        [ 0.7865, -2.2975],
        [ 1.8166, -3.2548],
        [-1.9976, -0.5176],
        [ 1.6347, -4.0497],
        [-0.3142, -1.0825],
        [-0.1339, -3.8846],
        [ 0.8873, -2.6210],
        [-1.1887, -1.3455],
        [ 0.5298, -2.0484],
        [ 1.0809, -3.1117],
        [-1.4029, -1.1261],
        [-0.3866, -1.3467],
        [ 0.4100, -2.1190]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8179, 0.1821],
        [0.0801, 0.9199]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7157, 0.2843], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1393, 0.1158],
         [0.8843, 0.2065]],

        [[0.9236, 0.1062],
         [0.5331, 0.6850]],

        [[0.1886, 0.1067],
         [0.4539, 0.1167]],

        [[0.5960, 0.0871],
         [0.0805, 0.6976]],

        [[0.2644, 0.1142],
         [0.5441, 0.8881]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 65
Adjusted Rand Index: 0.08236841963735249
time is 1
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 85
Adjusted Rand Index: 0.48505024049347056
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 83
Adjusted Rand Index: 0.42985207965687533
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7025982975809663
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7026203884610964
Global Adjusted Rand Index: 0.4451291011121901
Average Adjusted Rand Index: 0.48049788516595215
Iteration 0: Loss = -58983.109448000236
Iteration 10: Loss = -9908.01193516954
Iteration 20: Loss = -9908.01193516954
1
Iteration 30: Loss = -9908.01193516954
2
Iteration 40: Loss = -9908.011935169561
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[2.6193e-21, 1.0000e+00],
        [1.3885e-15, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.2930e-15, 1.0000e+00])
beta: tensor([[[0.1081, 0.0953],
         [0.2422, 0.1361]],

        [[0.0680, 0.0619],
         [0.9984, 0.7786]],

        [[0.9533, 0.1894],
         [0.5695, 0.2361]],

        [[0.1444, 0.0810],
         [0.3324, 0.2078]],

        [[0.0927, 0.1935],
         [0.4991, 0.9865]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -58982.836480868136
Iteration 100: Loss = -9973.756439980505
Iteration 200: Loss = -9922.323008494728
Iteration 300: Loss = -9909.965881792961
Iteration 400: Loss = -9908.8702874424
Iteration 500: Loss = -9908.507213925262
Iteration 600: Loss = -9908.296914002762
Iteration 700: Loss = -9908.15700573913
Iteration 800: Loss = -9908.056800717333
Iteration 900: Loss = -9907.980803871611
Iteration 1000: Loss = -9907.920657526169
Iteration 1100: Loss = -9907.871154519575
Iteration 1200: Loss = -9907.828782399629
Iteration 1300: Loss = -9907.791231215006
Iteration 1400: Loss = -9907.756274970463
Iteration 1500: Loss = -9907.722075539714
Iteration 1600: Loss = -9907.686772026445
Iteration 1700: Loss = -9907.648206518734
Iteration 1800: Loss = -9907.603166969531
Iteration 1900: Loss = -9907.550165116434
Iteration 2000: Loss = -9907.48554257543
Iteration 2100: Loss = -9907.407715187244
Iteration 2200: Loss = -9907.316882541027
Iteration 2300: Loss = -9907.217495285413
Iteration 2400: Loss = -9907.116716112685
Iteration 2500: Loss = -9907.021538014622
Iteration 2600: Loss = -9906.930441983226
Iteration 2700: Loss = -9906.839220410655
Iteration 2800: Loss = -9906.742724579084
Iteration 2900: Loss = -9906.635196899304
Iteration 3000: Loss = -9906.510830002648
Iteration 3100: Loss = -9906.363271559325
Iteration 3200: Loss = -9906.185779070682
Iteration 3300: Loss = -9905.972718046636
Iteration 3400: Loss = -9905.7228467597
Iteration 3500: Loss = -9905.444729707195
Iteration 3600: Loss = -9905.161517056438
Iteration 3700: Loss = -9904.906550507401
Iteration 3800: Loss = -9904.705741468268
Iteration 3900: Loss = -9904.565410003139
Iteration 4000: Loss = -9904.477535294342
Iteration 4100: Loss = -9904.427354020081
Iteration 4200: Loss = -9904.398774503255
Iteration 4300: Loss = -9904.380299854109
Iteration 4400: Loss = -9904.367248078315
Iteration 4500: Loss = -9904.357879515128
Iteration 4600: Loss = -9904.3498815725
Iteration 4700: Loss = -9904.342376723758
Iteration 4800: Loss = -9904.335326225324
Iteration 4900: Loss = -9904.328847000566
Iteration 5000: Loss = -9904.322810100923
Iteration 5100: Loss = -9904.316798848027
Iteration 5200: Loss = -9904.310375588919
Iteration 5300: Loss = -9904.303228569022
Iteration 5400: Loss = -9904.294694080763
Iteration 5500: Loss = -9904.284080939033
Iteration 5600: Loss = -9904.269997941941
Iteration 5700: Loss = -9904.249562015526
Iteration 5800: Loss = -9904.215124648035
Iteration 5900: Loss = -9904.136351489527
Iteration 6000: Loss = -9903.6576813522
Iteration 6100: Loss = -9902.80661515977
Iteration 6200: Loss = -9902.754328576435
Iteration 6300: Loss = -9902.629961484718
Iteration 6400: Loss = -9902.050297600847
Iteration 6500: Loss = -9901.352048916066
Iteration 6600: Loss = -9892.954550971528
Iteration 6700: Loss = -9884.718210159157
Iteration 6800: Loss = -9882.806110276533
Iteration 6900: Loss = -9882.754868135613
Iteration 7000: Loss = -9882.748419566202
Iteration 7100: Loss = -9882.429887809467
Iteration 7200: Loss = -9882.430565810313
1
Iteration 7300: Loss = -9882.402729687457
Iteration 7400: Loss = -9882.40190183302
Iteration 7500: Loss = -9882.40262803848
1
Iteration 7600: Loss = -9882.399888958686
Iteration 7700: Loss = -9882.323627385584
Iteration 7800: Loss = -9882.33923386406
1
Iteration 7900: Loss = -9882.312173058377
Iteration 8000: Loss = -9882.351616225284
1
Iteration 8100: Loss = -9882.308115201871
Iteration 8200: Loss = -9882.32443830934
1
Iteration 8300: Loss = -9882.306919224908
Iteration 8400: Loss = -9881.71765647255
Iteration 8500: Loss = -9881.559966300963
Iteration 8600: Loss = -9881.555677648614
Iteration 8700: Loss = -9881.554747291479
Iteration 8800: Loss = -9881.55455466435
Iteration 8900: Loss = -9881.554952476014
1
Iteration 9000: Loss = -9881.554619869597
2
Iteration 9100: Loss = -9881.566241966277
3
Iteration 9200: Loss = -9881.56680649981
4
Iteration 9300: Loss = -9881.554685014158
5
Iteration 9400: Loss = -9881.563375161928
6
Iteration 9500: Loss = -9881.523711952508
Iteration 9600: Loss = -9881.511989314335
Iteration 9700: Loss = -9881.511578973783
Iteration 9800: Loss = -9881.511500849509
Iteration 9900: Loss = -9881.512494600762
1
Iteration 10000: Loss = -9881.511425236316
Iteration 10100: Loss = -9881.522439486896
1
Iteration 10200: Loss = -9881.513173374115
2
Iteration 10300: Loss = -9881.515275556008
3
Iteration 10400: Loss = -9881.51143874901
4
Iteration 10500: Loss = -9881.503393062334
Iteration 10600: Loss = -9881.555034314484
1
Iteration 10700: Loss = -9881.503319308677
Iteration 10800: Loss = -9881.512755207337
1
Iteration 10900: Loss = -9881.503179313604
Iteration 11000: Loss = -9881.503065881492
Iteration 11100: Loss = -9881.503595860806
1
Iteration 11200: Loss = -9881.503035010435
Iteration 11300: Loss = -9881.503198494167
1
Iteration 11400: Loss = -9881.502994021288
Iteration 11500: Loss = -9881.503220447652
1
Iteration 11600: Loss = -9881.5030082424
2
Iteration 11700: Loss = -9881.502912021211
Iteration 11800: Loss = -9881.597929353997
1
Iteration 11900: Loss = -9881.502819912355
Iteration 12000: Loss = -9881.593730296709
1
Iteration 12100: Loss = -9881.502665678707
Iteration 12200: Loss = -9881.515863083057
1
Iteration 12300: Loss = -9881.502646771567
Iteration 12400: Loss = -9881.50262896823
Iteration 12500: Loss = -9881.503032666547
1
Iteration 12600: Loss = -9881.502598996967
Iteration 12700: Loss = -9881.62704686372
1
Iteration 12800: Loss = -9881.502611522135
2
Iteration 12900: Loss = -9881.502610335094
3
Iteration 13000: Loss = -9881.503655806042
4
Iteration 13100: Loss = -9881.502632906599
5
Iteration 13200: Loss = -9881.757360280184
6
Iteration 13300: Loss = -9881.5026081769
7
Iteration 13400: Loss = -9881.502576237015
Iteration 13500: Loss = -9881.502923127031
1
Iteration 13600: Loss = -9881.502428038926
Iteration 13700: Loss = -9881.509749439772
1
Iteration 13800: Loss = -9881.502429829183
2
Iteration 13900: Loss = -9881.50240095181
Iteration 14000: Loss = -9881.502657213488
1
Iteration 14100: Loss = -9881.5024108447
2
Iteration 14200: Loss = -9881.503513252917
3
Iteration 14300: Loss = -9881.50241689213
4
Iteration 14400: Loss = -9881.578235388866
5
Iteration 14500: Loss = -9881.502406653593
6
Iteration 14600: Loss = -9881.502453702598
7
Iteration 14700: Loss = -9881.502512809198
8
Iteration 14800: Loss = -9881.502437184196
9
Iteration 14900: Loss = -9881.596569997277
10
Stopping early at iteration 14900 due to no improvement.
tensor([[ 0.5109, -1.8984],
        [-1.8316,  0.4442],
        [-0.5499, -3.0644],
        [ 0.6733, -2.4889],
        [-1.9770,  0.4813],
        [-1.5729,  0.0271],
        [-2.8791,  0.5639],
        [ 1.6616, -3.1475],
        [ 1.4048, -2.8958],
        [-1.1372, -0.4725],
        [ 0.9058, -2.2964],
        [ 0.9924, -2.8840],
        [ 2.7639, -4.1906],
        [-2.3475,  0.3432],
        [ 0.6812, -2.1111],
        [-0.7744, -0.7492],
        [-1.6195,  0.2112],
        [-0.1610, -1.2691],
        [-2.0021,  0.5992],
        [ 0.8036, -2.1927],
        [-3.1377,  1.7420],
        [-2.3497, -2.2655],
        [ 2.2903, -3.6824],
        [ 0.5478, -1.9589],
        [ 0.6267, -2.3224],
        [ 2.7745, -5.1133],
        [ 1.6543, -3.1327],
        [-0.9483, -0.4573],
        [-0.4710, -1.0998],
        [ 0.6161, -3.5395],
        [-4.0606,  2.6743],
        [-0.5983, -1.6400],
        [-3.4875,  1.4532],
        [ 1.2134, -3.3022],
        [-1.7066, -1.1488],
        [-1.4703, -0.0626],
        [-2.6856, -0.9219],
        [ 0.4569, -2.2970],
        [ 2.1900, -3.8880],
        [ 0.5974, -3.6348],
        [ 1.3572, -3.4853],
        [-2.5201,  1.0941],
        [ 0.2889, -2.0304],
        [-1.0692, -1.0847],
        [-2.4978,  0.1793],
        [ 1.1937, -3.4949],
        [-1.7894,  0.3315],
        [-4.1072,  2.6924],
        [-0.7052, -1.0052],
        [-2.4099,  0.9561],
        [-3.4359,  1.8025],
        [-1.1423, -0.3315],
        [-0.5373, -1.2986],
        [-3.9336,  2.4103],
        [ 2.6795, -4.4800],
        [-1.8141, -1.4461],
        [-1.2476, -0.1517],
        [ 1.8550, -3.2603],
        [-2.8739,  0.7823],
        [-2.4073,  0.7071],
        [ 1.7759, -3.1624],
        [ 0.0698, -3.3619],
        [-4.9233,  3.1943],
        [ 1.8530, -3.3784],
        [ 2.8855, -4.2829],
        [ 2.6187, -4.3806],
        [ 1.8336, -3.5074],
        [-1.2449, -0.5641],
        [ 1.5097, -2.9317],
        [-3.8910,  2.4813],
        [-1.6260, -0.0621],
        [ 1.7830, -3.1803],
        [-2.4206,  0.4687],
        [ 0.9328, -2.3833],
        [-2.5717,  1.1811],
        [ 0.2082, -1.7359],
        [-3.8438, -0.7715],
        [-2.8542,  0.4197],
        [ 0.0913, -3.3069],
        [-0.4948, -1.4573],
        [-2.6063,  0.8099],
        [-3.6333,  0.9398],
        [ 1.5808, -3.3536],
        [ 1.2899, -2.7928],
        [ 0.5128, -2.4055],
        [-1.2638, -0.4800],
        [-2.1174,  0.6141],
        [ 0.8986, -2.5915],
        [ 1.2898, -3.5941],
        [-2.7414,  1.3544],
        [ 0.5972, -2.4717],
        [-1.4490,  0.0613],
        [ 0.7640, -2.2163],
        [ 0.7978, -2.8573],
        [-3.3356,  0.8448],
        [-0.3683, -1.6884],
        [ 1.1267, -2.6105],
        [-2.2452, -0.0061],
        [ 0.2924, -2.8735],
        [-1.1133, -1.2241]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8083, 0.1917],
        [0.1369, 0.8631]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5689, 0.4311], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1458, 0.1000],
         [0.2422, 0.2044]],

        [[0.0680, 0.1000],
         [0.9984, 0.7786]],

        [[0.9533, 0.1051],
         [0.5695, 0.2361]],

        [[0.1444, 0.0844],
         [0.3324, 0.2078]],

        [[0.0927, 0.1108],
         [0.4991, 0.9865]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 78
Adjusted Rand Index: 0.30666666666666664
time is 1
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 85
Adjusted Rand Index: 0.48484848484848486
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 82
Adjusted Rand Index: 0.4035802815618291
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 90
Adjusted Rand Index: 0.6363408394869687
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 90
Adjusted Rand Index: 0.636356453281232
Global Adjusted Rand Index: 0.4889761123256437
Average Adjusted Rand Index: 0.4935585451690363
Iteration 0: Loss = -13127.102474723062
Iteration 10: Loss = -9905.484117564518
Iteration 20: Loss = -9905.307486029895
Iteration 30: Loss = -9905.22249631518
Iteration 40: Loss = -9905.120016883438
Iteration 50: Loss = -9904.95642082861
Iteration 60: Loss = -9904.741042976575
Iteration 70: Loss = -9904.517772896525
Iteration 80: Loss = -9904.309933374469
Iteration 90: Loss = -9904.11761466182
Iteration 100: Loss = -9903.9363435816
Iteration 110: Loss = -9903.763042356532
Iteration 120: Loss = -9903.596458994076
Iteration 130: Loss = -9903.43601133674
Iteration 140: Loss = -9903.282311010531
Iteration 150: Loss = -9903.136734488742
Iteration 160: Loss = -9903.001292558556
Iteration 170: Loss = -9902.878297336812
Iteration 180: Loss = -9902.769450224172
Iteration 190: Loss = -9902.675515299523
Iteration 200: Loss = -9902.596843739206
Iteration 210: Loss = -9902.533614813496
Iteration 220: Loss = -9902.48724557018
Iteration 230: Loss = -9902.459358239561
Iteration 240: Loss = -9902.450201865026
Iteration 250: Loss = -9902.457025825872
1
Iteration 260: Loss = -9902.47464140553
2
Iteration 270: Loss = -9902.497150079302
3
Stopping early at iteration 269 due to no improvement.
pi: tensor([[0.1833, 0.8167],
        [0.1206, 0.8794]], dtype=torch.float64)
alpha: tensor([0.1309, 0.8691])
beta: tensor([[[0.0804, 0.1125],
         [0.6986, 0.1459]],

        [[0.0479, 0.0933],
         [0.5971, 0.5489]],

        [[0.2571, 0.1031],
         [0.4078, 0.9187]],

        [[0.1519, 0.0990],
         [0.7520, 0.9388]],

        [[0.3598, 0.1335],
         [0.0431, 0.6500]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.007389132627638759
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0027765295166374973
Average Adjusted Rand Index: 0.001402936275741824
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -13126.752195550265
Iteration 100: Loss = -9906.169405027376
Iteration 200: Loss = -9905.063454239893
Iteration 300: Loss = -9904.731403349295
Iteration 400: Loss = -9904.588521606998
Iteration 500: Loss = -9904.520669961907
Iteration 600: Loss = -9904.481493044012
Iteration 700: Loss = -9904.454710414668
Iteration 800: Loss = -9904.435293813725
Iteration 900: Loss = -9904.420276859908
Iteration 1000: Loss = -9904.408249940298
Iteration 1100: Loss = -9904.398387341875
Iteration 1200: Loss = -9904.390072973254
Iteration 1300: Loss = -9904.382931187029
Iteration 1400: Loss = -9904.37668011078
Iteration 1500: Loss = -9904.37108621298
Iteration 1600: Loss = -9904.36782095649
Iteration 1700: Loss = -9904.361242066107
Iteration 1800: Loss = -9904.356717347691
Iteration 1900: Loss = -9904.605694712667
1
Iteration 2000: Loss = -9904.347888004297
Iteration 2100: Loss = -9904.343361243853
Iteration 2200: Loss = -9904.338545999659
Iteration 2300: Loss = -9904.333218073707
Iteration 2400: Loss = -9904.327161641975
Iteration 2500: Loss = -9904.320134605074
Iteration 2600: Loss = -9904.311792420065
Iteration 2700: Loss = -9904.30200762021
Iteration 2800: Loss = -9904.29046986544
Iteration 2900: Loss = -9904.278284274378
Iteration 3000: Loss = -9904.266799845982
Iteration 3100: Loss = -9904.253143370523
Iteration 3200: Loss = -9904.237460834163
Iteration 3300: Loss = -9904.214942377812
Iteration 3400: Loss = -9904.177159302631
Iteration 3500: Loss = -9904.041732686923
Iteration 3600: Loss = -9902.831350003238
Iteration 3700: Loss = -9902.814947770388
Iteration 3800: Loss = -9902.79911525764
Iteration 3900: Loss = -9902.665201634247
Iteration 4000: Loss = -9901.696043937422
Iteration 4100: Loss = -9899.659803625627
Iteration 4200: Loss = -9886.393563132102
Iteration 4300: Loss = -9884.04911231638
Iteration 4400: Loss = -9881.719881051587
Iteration 4500: Loss = -9881.6519740559
Iteration 4600: Loss = -9881.638770885573
Iteration 4700: Loss = -9881.603960900218
Iteration 4800: Loss = -9881.574253573153
Iteration 4900: Loss = -9881.56526323516
Iteration 5000: Loss = -9881.563893821443
Iteration 5100: Loss = -9881.596719359102
1
Iteration 5200: Loss = -9881.563082768873
Iteration 5300: Loss = -9881.563116918127
1
Iteration 5400: Loss = -9881.561304873925
Iteration 5500: Loss = -9881.558461666047
Iteration 5600: Loss = -9881.627574969523
1
Iteration 5700: Loss = -9881.558227783633
Iteration 5800: Loss = -9881.60949927547
1
Iteration 5900: Loss = -9881.558162827943
Iteration 6000: Loss = -9881.558086626603
Iteration 6100: Loss = -9881.558101136125
1
Iteration 6200: Loss = -9881.557912945555
Iteration 6300: Loss = -9881.557770173553
Iteration 6400: Loss = -9881.563028356126
1
Iteration 6500: Loss = -9881.520496161336
Iteration 6600: Loss = -9881.524033850705
1
Iteration 6700: Loss = -9881.519329128254
Iteration 6800: Loss = -9881.517336618492
Iteration 6900: Loss = -9881.505979940146
Iteration 7000: Loss = -9881.505947988799
Iteration 7100: Loss = -9881.506883541173
1
Iteration 7200: Loss = -9881.505908107123
Iteration 7300: Loss = -9881.50602640633
1
Iteration 7400: Loss = -9881.50585456946
Iteration 7500: Loss = -9881.508172632086
1
Iteration 7600: Loss = -9881.505274364581
Iteration 7700: Loss = -9881.52479110284
1
Iteration 7800: Loss = -9881.544635157923
2
Iteration 7900: Loss = -9881.506179922268
3
Iteration 8000: Loss = -9881.50273287541
Iteration 8100: Loss = -9881.503957075254
1
Iteration 8200: Loss = -9881.508321179961
2
Iteration 8300: Loss = -9881.502653391895
Iteration 8400: Loss = -9881.502912590018
1
Iteration 8500: Loss = -9881.502813256133
2
Iteration 8600: Loss = -9881.502703013612
3
Iteration 8700: Loss = -9881.508733883971
4
Iteration 8800: Loss = -9881.503170643842
5
Iteration 8900: Loss = -9881.712998682895
6
Iteration 9000: Loss = -9881.502507672152
Iteration 9100: Loss = -9881.50273030928
1
Iteration 9200: Loss = -9881.502481374928
Iteration 9300: Loss = -9881.504893857278
1
Iteration 9400: Loss = -9881.502505181275
2
Iteration 9500: Loss = -9881.50381284603
3
Iteration 9600: Loss = -9881.520617700531
4
Iteration 9700: Loss = -9881.50251080045
5
Iteration 9800: Loss = -9881.503196368845
6
Iteration 9900: Loss = -9881.502486564668
7
Iteration 10000: Loss = -9881.502524774734
8
Iteration 10100: Loss = -9881.507299400766
9
Iteration 10200: Loss = -9881.50251678036
10
Stopping early at iteration 10200 due to no improvement.
tensor([[-1.9464e+00,  4.8558e-01],
        [-2.5991e-01, -2.5340e+00],
        [-2.0326e+00,  4.7558e-01],
        [-2.3386e+00,  8.6622e-01],
        [ 1.8736e-01, -2.2658e+00],
        [-1.9911e-02, -1.6191e+00],
        [ 9.7626e-01, -2.4572e+00],
        [-4.0487e+00,  7.7673e-01],
        [-2.9818e+00,  1.3317e+00],
        [-3.7021e-01, -1.0295e+00],
        [-2.3184e+00,  9.0784e-01],
        [-2.8725e+00,  1.0096e+00],
        [-4.2399e+00,  2.7023e+00],
        [ 4.6002e-01, -2.2244e+00],
        [-2.1768e+00,  6.2880e-01],
        [-7.0549e-01, -6.9795e-01],
        [-2.9059e-01, -2.1133e+00],
        [-1.3371e+00, -1.9122e-01],
        [ 6.2006e-01, -2.0067e+00],
        [-2.3113e+00,  7.0381e-01],
        [ 1.4332e+00, -3.4552e+00],
        [-7.0831e-01, -7.9524e-01],
        [-5.2907e+00,  6.7552e-01],
        [-2.2442e+00,  2.6358e-01],
        [-2.2011e+00,  7.8464e-01],
        [-4.9517e+00,  2.9222e+00],
        [-3.3888e+00,  1.3908e+00],
        [-5.0285e-01, -9.8070e-01],
        [-1.2079e+00, -5.7462e-01],
        [-2.9054e+00,  1.2688e+00],
        [ 2.4029e+00, -4.3189e+00],
        [-1.2386e+00, -1.6622e-01],
        [ 1.4827e+00, -3.4753e+00],
        [-2.9604e+00,  1.5734e+00],
        [-6.1022e-01, -1.1523e+00],
        [-5.3954e-01, -1.9471e+00],
        [-1.6828e-01, -1.9274e+00],
        [-2.7993e+00,  3.7317e-04],
        [-4.5062e+00,  1.5656e+00],
        [-2.8883e+00,  1.3755e+00],
        [-3.2754e+00,  1.5903e+00],
        [ 9.6054e-01, -2.6666e+00],
        [-1.9086e+00,  4.1654e-01],
        [-1.3862e+00, -1.3451e+00],
        [ 1.1262e-02, -2.6629e+00],
        [-3.1642e+00,  1.5411e+00],
        [ 3.5779e-01, -1.7553e+00],
        [ 1.8269e+00, -4.9652e+00],
        [-8.5481e-01, -5.4861e-01],
        [ 9.8221e-01, -2.3855e+00],
        [ 1.7799e+00, -3.4595e+00],
        [-2.8979e-01, -1.0975e+00],
        [-1.0775e+00, -3.1645e-01],
        [ 2.3974e+00, -3.9398e+00],
        [-4.3132e+00,  2.8365e+00],
        [-1.3858e+00, -1.7524e+00],
        [-2.3641e-01, -1.3175e+00],
        [-3.2578e+00,  1.8642e+00],
        [ 4.3772e-01, -3.2057e+00],
        [ 6.3470e-01, -2.4869e+00],
        [-3.6768e+00,  1.2650e+00],
        [-2.9227e+00,  5.4430e-01],
        [ 3.2431e+00, -4.8646e+00],
        [-3.3125e+00,  1.9225e+00],
        [-4.6551e+00,  2.5025e+00],
        [-4.2292e+00,  2.7597e+00],
        [-3.5850e+00,  1.7567e+00],
        [-3.9479e-01, -1.0796e+00],
        [-3.4524e+00,  1.0082e+00],
        [ 2.1491e+00, -4.2137e+00],
        [-3.5017e-02, -1.5861e+00],
        [-3.4390e+00,  1.5346e+00],
        [ 7.5649e-01, -2.1428e+00],
        [-3.2713e+00,  8.9500e-02],
        [ 9.9195e-01, -2.7741e+00],
        [-1.7168e+00,  2.7964e-01],
        [ 6.5495e-01, -2.4125e+00],
        [ 9.4447e-01, -2.3368e+00],
        [-2.5399e+00,  9.0731e-01],
        [-1.2088e+00, -2.0814e-01],
        [ 1.0101e+00, -2.4007e+00],
        [ 1.6066e+00, -2.9976e+00],
        [-3.1699e+00,  1.7694e+00],
        [-2.8550e+00,  1.2539e+00],
        [-2.3595e+00,  5.8931e-01],
        [-3.7457e-01, -1.1519e+00],
        [ 3.4696e-01, -2.3844e+00],
        [-2.4972e+00,  1.0362e+00],
        [-4.7711e+00,  1.5592e-01],
        [ 1.3078e+00, -2.7954e+00],
        [-3.0472e+00,  3.8919e-02],
        [ 4.1959e-02, -1.4510e+00],
        [-3.4649e+00, -4.5524e-01],
        [-2.8570e+00,  8.3489e-01],
        [ 1.2739e+00, -2.9043e+00],
        [-1.5102e+00, -1.6786e-01],
        [-2.5984e+00,  1.1539e+00],
        [-1.4945e-01, -2.3817e+00],
        [-2.3091e+00,  9.1114e-01],
        [-1.0010e+00, -9.0654e-01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8661, 0.1339],
        [0.1906, 0.8094]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4354, 0.5646], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.1005],
         [0.6986, 0.1458]],

        [[0.0479, 0.1003],
         [0.5971, 0.5489]],

        [[0.2571, 0.1059],
         [0.4078, 0.9187]],

        [[0.1519, 0.0846],
         [0.7520, 0.9388]],

        [[0.3598, 0.1114],
         [0.0431, 0.6500]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 23
Adjusted Rand Index: 0.28446500302572675
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 15
Adjusted Rand Index: 0.48484848484848486
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 18
Adjusted Rand Index: 0.4035802815618291
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 10
Adjusted Rand Index: 0.6363408394869687
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 10
Adjusted Rand Index: 0.636356453281232
Global Adjusted Rand Index: 0.4833809787750444
Average Adjusted Rand Index: 0.4891182124408483
Iteration 0: Loss = -21020.02124770036
Iteration 10: Loss = -9905.428187488424
Iteration 20: Loss = -9905.253914605915
Iteration 30: Loss = -9905.08980171627
Iteration 40: Loss = -9904.885847202331
Iteration 50: Loss = -9904.66091187569
Iteration 60: Loss = -9904.444164657936
Iteration 70: Loss = -9904.242912016707
Iteration 80: Loss = -9904.054528051376
Iteration 90: Loss = -9903.87586907216
Iteration 100: Loss = -9903.704772102215
Iteration 110: Loss = -9903.540156472463
Iteration 120: Loss = -9903.38196567462
Iteration 130: Loss = -9903.23078931683
Iteration 140: Loss = -9903.08851454575
Iteration 150: Loss = -9902.95714892228
Iteration 160: Loss = -9902.838927509154
Iteration 170: Loss = -9902.735173949408
Iteration 180: Loss = -9902.646535875223
Iteration 190: Loss = -9902.57311662934
Iteration 200: Loss = -9902.515564024598
Iteration 210: Loss = -9902.475408988434
Iteration 220: Loss = -9902.454136971437
Iteration 230: Loss = -9902.45098102831
Iteration 240: Loss = -9902.462200525784
1
Iteration 250: Loss = -9902.482164120393
2
Iteration 260: Loss = -9902.50524233027
3
Stopping early at iteration 259 due to no improvement.
pi: tensor([[0.1804, 0.8196],
        [0.1184, 0.8816]], dtype=torch.float64)
alpha: tensor([0.1284, 0.8716])
beta: tensor([[[0.0800, 0.1123],
         [0.9921, 0.1458]],

        [[0.2423, 0.0929],
         [0.2841, 0.7033]],

        [[0.5001, 0.1026],
         [0.1936, 0.4220]],

        [[0.5747, 0.0987],
         [0.8995, 0.6723]],

        [[0.6607, 0.1336],
         [0.1869, 0.7611]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.007389132627638759
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0027765295166374973
Average Adjusted Rand Index: 0.001402936275741824
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21019.859303262245
Iteration 100: Loss = -9999.939365332688
Iteration 200: Loss = -9938.964825162218
Iteration 300: Loss = -9913.426826637824
Iteration 400: Loss = -9909.010152230407
Iteration 500: Loss = -9907.9178583503
Iteration 600: Loss = -9907.390424096151
Iteration 700: Loss = -9907.079155247156
Iteration 800: Loss = -9906.875677020571
Iteration 900: Loss = -9906.731456925176
Iteration 1000: Loss = -9906.625363245199
Iteration 1100: Loss = -9906.54622564754
Iteration 1200: Loss = -9906.483416421848
Iteration 1300: Loss = -9906.43178300518
Iteration 1400: Loss = -9906.388075852226
Iteration 1500: Loss = -9906.35033929817
Iteration 1600: Loss = -9906.316803363368
Iteration 1700: Loss = -9906.286436481456
Iteration 1800: Loss = -9906.25894495191
Iteration 1900: Loss = -9906.233292955887
Iteration 2000: Loss = -9906.207345293085
Iteration 2100: Loss = -9906.181960399324
Iteration 2200: Loss = -9906.15659981668
Iteration 2300: Loss = -9906.133768888907
Iteration 2400: Loss = -9906.113119342002
Iteration 2500: Loss = -9906.102041077093
Iteration 2600: Loss = -9906.069808936441
Iteration 2700: Loss = -9906.042654512341
Iteration 2800: Loss = -9906.013014602871
Iteration 2900: Loss = -9905.973563029283
Iteration 3000: Loss = -9905.92171505322
Iteration 3100: Loss = -9905.859126778842
Iteration 3200: Loss = -9905.786193841566
Iteration 3300: Loss = -9905.691794768087
Iteration 3400: Loss = -9905.5783739705
Iteration 3500: Loss = -9905.479711469814
Iteration 3600: Loss = -9905.370480247848
Iteration 3700: Loss = -9905.27489006557
Iteration 3800: Loss = -9905.178286662449
Iteration 3900: Loss = -9905.080999036325
Iteration 4000: Loss = -9904.98099747591
Iteration 4100: Loss = -9904.861389036101
Iteration 4200: Loss = -9904.663654891912
Iteration 4300: Loss = -9904.569043402154
Iteration 4400: Loss = -9904.446051110548
Iteration 4500: Loss = -9904.345686366898
Iteration 4600: Loss = -9904.238722635931
Iteration 4700: Loss = -9904.110931337507
Iteration 4800: Loss = -9904.015263280893
Iteration 4900: Loss = -9903.884903559301
Iteration 5000: Loss = -9903.791317665273
Iteration 5100: Loss = -9903.738596796804
Iteration 5200: Loss = -9903.676205648988
Iteration 5300: Loss = -9903.604147501488
Iteration 5400: Loss = -9903.561293483614
Iteration 5500: Loss = -9903.541888324131
Iteration 5600: Loss = -9903.5323807794
Iteration 5700: Loss = -9903.503494068802
Iteration 5800: Loss = -9903.399935049996
Iteration 5900: Loss = -9903.460079258632
1
Iteration 6000: Loss = -9903.385107825643
Iteration 6100: Loss = -9903.381290084115
Iteration 6200: Loss = -9903.375647499643
Iteration 6300: Loss = -9903.385991114337
1
Iteration 6400: Loss = -9903.371462382667
Iteration 6500: Loss = -9903.369853927199
Iteration 6600: Loss = -9903.445900842531
1
Iteration 6700: Loss = -9903.367296582357
Iteration 6800: Loss = -9903.366373367184
Iteration 6900: Loss = -9903.365194286227
Iteration 7000: Loss = -9903.36366674334
Iteration 7100: Loss = -9903.36307421504
Iteration 7200: Loss = -9903.367739473888
1
Iteration 7300: Loss = -9903.36241538782
Iteration 7400: Loss = -9903.362207427686
Iteration 7500: Loss = -9903.361927983338
Iteration 7600: Loss = -9903.361610078522
Iteration 7700: Loss = -9903.732766049501
1
Iteration 7800: Loss = -9903.36121644761
Iteration 7900: Loss = -9903.361052610731
Iteration 8000: Loss = -9903.361456244565
1
Iteration 8100: Loss = -9903.360873546373
Iteration 8200: Loss = -9903.360791292154
Iteration 8300: Loss = -9903.464384112744
1
Iteration 8400: Loss = -9903.360641760228
Iteration 8500: Loss = -9903.360535643798
Iteration 8600: Loss = -9903.361065097468
1
Iteration 8700: Loss = -9903.360435607316
Iteration 8800: Loss = -9903.698852348298
1
Iteration 8900: Loss = -9903.36031719707
Iteration 9000: Loss = -9903.360220342278
Iteration 9100: Loss = -9903.36795047447
1
Iteration 9200: Loss = -9903.360138027809
Iteration 9300: Loss = -9903.360067701866
Iteration 9400: Loss = -9903.360064335016
Iteration 9500: Loss = -9903.360316013344
1
Iteration 9600: Loss = -9903.359997572743
Iteration 9700: Loss = -9903.484146172299
1
Iteration 9800: Loss = -9903.359990987892
Iteration 9900: Loss = -9903.359980060357
Iteration 10000: Loss = -9903.36437032757
1
Iteration 10100: Loss = -9903.35994336719
Iteration 10200: Loss = -9903.361569196064
1
Iteration 10300: Loss = -9903.36096433708
2
Iteration 10400: Loss = -9903.359900398853
Iteration 10500: Loss = -9903.362115698545
1
Iteration 10600: Loss = -9903.362170058506
2
Iteration 10700: Loss = -9903.359932271605
3
Iteration 10800: Loss = -9903.360043737042
4
Iteration 10900: Loss = -9903.359841902346
Iteration 11000: Loss = -9903.376976179563
1
Iteration 11100: Loss = -9903.359765870235
Iteration 11200: Loss = -9903.360189739811
1
Iteration 11300: Loss = -9903.53449828559
2
Iteration 11400: Loss = -9903.359750770062
Iteration 11500: Loss = -9903.36012098928
1
Iteration 11600: Loss = -9903.359702525073
Iteration 11700: Loss = -9903.359695657116
Iteration 11800: Loss = -9903.359727553629
1
Iteration 11900: Loss = -9903.360163285037
2
Iteration 12000: Loss = -9903.35977281983
3
Iteration 12100: Loss = -9903.360334731578
4
Iteration 12200: Loss = -9903.359874107786
5
Iteration 12300: Loss = -9903.376750691144
6
Iteration 12400: Loss = -9903.36440338287
7
Iteration 12500: Loss = -9903.359866106228
8
Iteration 12600: Loss = -9903.361963180576
9
Iteration 12700: Loss = -9903.36376797675
10
Stopping early at iteration 12700 due to no improvement.
tensor([[-7.1128,  4.6007],
        [-6.5446,  5.1382],
        [-6.5711,  5.1664],
        [-6.6180,  5.0004],
        [-7.6801,  6.2349],
        [-7.2103,  4.5526],
        [-6.6266,  5.0816],
        [-6.7319,  4.9906],
        [-6.3832,  4.9820],
        [-6.7454,  4.9693],
        [-6.6022,  5.1088],
        [-6.8132,  4.8281],
        [-6.8964,  4.8238],
        [-6.5943,  5.1533],
        [-6.5489,  5.1043],
        [-6.5725,  5.0919],
        [-6.8675,  4.8618],
        [-6.6499,  5.0641],
        [-6.5635,  5.1491],
        [-9.1316,  5.7231],
        [-6.5594,  5.1655],
        [-6.5250,  5.0029],
        [-6.4768,  5.0018],
        [-6.7223,  4.9675],
        [-6.4859,  5.0983],
        [-6.5339,  5.1442],
        [-6.8287,  4.8765],
        [-6.5702,  5.1186],
        [-7.1059,  4.6611],
        [-6.2221,  4.8086],
        [-6.9151,  4.7629],
        [-6.5724,  5.1817],
        [-6.9023,  4.8371],
        [-6.5838,  4.7175],
        [-8.1762,  3.5610],
        [-8.2988,  6.9122],
        [-6.5710,  5.1838],
        [-6.6347,  5.0887],
        [-6.9572,  4.7437],
        [-7.6092,  4.3030],
        [-7.6300,  5.8182],
        [-6.6683,  5.0190],
        [-6.6139,  5.0947],
        [-7.7692,  3.9206],
        [-6.6016,  5.0682],
        [-6.5037,  5.0423],
        [-6.6817,  5.0705],
        [-6.7473,  4.9051],
        [-6.6699,  4.9917],
        [-6.4351,  4.8622],
        [-6.5784,  5.1767],
        [-6.5829,  4.9538],
        [-6.4674,  4.9158],
        [-6.5671,  5.1703],
        [-6.5450,  5.1587],
        [-6.2819,  4.7622],
        [-6.6035,  5.2107],
        [-8.1194,  3.6140],
        [-7.3409,  4.3828],
        [-6.6729,  5.0559],
        [-7.0429,  4.7046],
        [-6.3921,  4.7897],
        [-8.1697,  3.5545],
        [-6.8395,  4.3759],
        [-6.7071,  5.0049],
        [-6.7196,  4.9858],
        [-6.6296,  5.1336],
        [-7.0178,  4.7372],
        [-6.6099,  5.0936],
        [-6.5737,  5.1547],
        [-6.8158,  4.9481],
        [-7.0952,  4.2797],
        [-6.5654,  5.1783],
        [-6.8791,  4.8687],
        [-6.6398,  5.0955],
        [-7.1820,  4.5745],
        [-6.8516,  4.9008],
        [-6.6295,  5.0665],
        [-6.5753,  5.0521],
        [-6.5547,  5.1680],
        [-6.7414,  4.9566],
        [-6.6418,  5.0864],
        [-6.8503,  4.8963],
        [-7.6655,  6.2552],
        [-6.7153,  5.0109],
        [-6.8754,  4.8093],
        [-6.6566,  5.0956],
        [-6.5988,  5.0200],
        [-6.7691,  4.9441],
        [-6.6292,  5.0834],
        [-7.4985,  4.1614],
        [-6.6697,  5.0245],
        [-7.5248,  4.0938],
        [-6.3645,  4.9768],
        [-6.8800,  4.8337],
        [-6.6453,  5.0539],
        [-6.7093,  5.1377],
        [-6.6471,  5.0758],
        [-6.9934,  4.6195],
        [-7.0946,  4.6181]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8461, 0.1539],
        [0.9402, 0.0598]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([8.4316e-06, 9.9999e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1425, 0.1336],
         [0.9921, 0.1330]],

        [[0.2423, 0.0803],
         [0.2841, 0.7033]],

        [[0.5001, 0.1364],
         [0.1936, 0.4220]],

        [[0.5747, 0.1045],
         [0.8995, 0.6723]],

        [[0.6607, 0.1553],
         [0.1869, 0.7611]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -5.603047611785861e-05
Average Adjusted Rand Index: -0.00015850936499207452
10020.17750736485
new:  [0.4451291011121901, 0.4889761123256437, 0.4833809787750444, -5.603047611785861e-05] [0.48049788516595215, 0.4935585451690363, 0.4891182124408483, -0.00015850936499207452] [9881.058607014409, 9881.596569997277, 9881.50251678036, 9903.36376797675]
prior:  [0.003226781515726774, 0.0, 0.0027765295166374973, 0.0027765295166374973] [0.0014106950214070842, 0.0, 0.001402936275741824, 0.001402936275741824] [9902.487240584558, 9908.011935169561, 9902.497150079302, 9902.50524233027]
-----------------------------------------------------------------------------------------
This iteration is 10
True Objective function: Loss = -10006.830704534359
Iteration 0: Loss = -29616.876208482197
Iteration 10: Loss = -9895.09256673192
Iteration 20: Loss = -9895.000065052354
Iteration 30: Loss = -9894.765596998368
Iteration 40: Loss = -9894.471256650304
Iteration 50: Loss = -9894.293693449263
Iteration 60: Loss = -9894.248021318108
Iteration 70: Loss = -9894.235192072132
Iteration 80: Loss = -9894.229014524351
Iteration 90: Loss = -9894.225218246529
Iteration 100: Loss = -9894.222557778701
Iteration 110: Loss = -9894.220585333922
Iteration 120: Loss = -9894.219079586497
Iteration 130: Loss = -9894.217918101751
Iteration 140: Loss = -9894.21696962909
Iteration 150: Loss = -9894.216197263584
Iteration 160: Loss = -9894.215556029354
Iteration 170: Loss = -9894.215024524316
Iteration 180: Loss = -9894.214561964449
Iteration 190: Loss = -9894.214192991216
Iteration 200: Loss = -9894.213871793985
Iteration 210: Loss = -9894.21358146819
Iteration 220: Loss = -9894.213288271774
Iteration 230: Loss = -9894.213117705658
Iteration 240: Loss = -9894.212927977927
Iteration 250: Loss = -9894.212750819854
Iteration 260: Loss = -9894.212603972184
Iteration 270: Loss = -9894.212456076977
Iteration 280: Loss = -9894.212354990574
Iteration 290: Loss = -9894.212235270248
Iteration 300: Loss = -9894.212156596304
Iteration 310: Loss = -9894.212083540602
Iteration 320: Loss = -9894.211981528733
Iteration 330: Loss = -9894.211924262076
Iteration 340: Loss = -9894.21190994231
Iteration 350: Loss = -9894.211792747994
Iteration 360: Loss = -9894.211801878831
1
Iteration 370: Loss = -9894.211743433234
Iteration 380: Loss = -9894.211695696446
Iteration 390: Loss = -9894.211676708148
Iteration 400: Loss = -9894.211632194232
Iteration 410: Loss = -9894.211633333203
1
Iteration 420: Loss = -9894.211558667776
Iteration 430: Loss = -9894.211566586415
1
Iteration 440: Loss = -9894.211535029543
Iteration 450: Loss = -9894.211502299075
Iteration 460: Loss = -9894.211537587762
1
Iteration 470: Loss = -9894.211485585349
Iteration 480: Loss = -9894.21146845656
Iteration 490: Loss = -9894.211472397663
1
Iteration 500: Loss = -9894.2114900465
2
Iteration 510: Loss = -9894.21145260398
Iteration 520: Loss = -9894.211447649526
Iteration 530: Loss = -9894.211449375729
1
Iteration 540: Loss = -9894.21139201737
Iteration 550: Loss = -9894.211400311418
1
Iteration 560: Loss = -9894.211382218738
Iteration 570: Loss = -9894.211392105468
1
Iteration 580: Loss = -9894.211410352927
2
Iteration 590: Loss = -9894.211406961484
3
Stopping early at iteration 589 due to no improvement.
pi: tensor([[1.2808e-10, 1.0000e+00],
        [1.6491e-02, 9.8351e-01]], dtype=torch.float64)
alpha: tensor([0.0169, 0.9831])
beta: tensor([[[0.1872, 0.2015],
         [0.8833, 0.1352]],

        [[0.5966, 0.0928],
         [0.1435, 0.5930]],

        [[0.4484, 0.0846],
         [0.2324, 0.1776]],

        [[0.6881, 0.1821],
         [0.5416, 0.7452]],

        [[0.0667, 0.1776],
         [0.5715, 0.2923]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29380.572731770142
Iteration 100: Loss = -9897.493451204327
Iteration 200: Loss = -9896.018923597789
Iteration 300: Loss = -9895.547178849107
Iteration 400: Loss = -9895.266994246447
Iteration 500: Loss = -9894.888767333328
Iteration 600: Loss = -9894.593196053998
Iteration 700: Loss = -9894.446607419479
Iteration 800: Loss = -9894.371037363617
Iteration 900: Loss = -9894.309911349244
Iteration 1000: Loss = -9894.225022295434
Iteration 1100: Loss = -9894.07118785063
Iteration 1200: Loss = -9893.835029268897
Iteration 1300: Loss = -9893.595199622318
Iteration 1400: Loss = -9893.450273339782
Iteration 1500: Loss = -9893.366844300468
Iteration 1600: Loss = -9893.309937224389
Iteration 1700: Loss = -9893.267353202822
Iteration 1800: Loss = -9893.233612896895
Iteration 1900: Loss = -9893.205512655582
Iteration 2000: Loss = -9893.180936596471
Iteration 2100: Loss = -9893.1581842491
Iteration 2200: Loss = -9893.13548459495
Iteration 2300: Loss = -9893.1107629829
Iteration 2400: Loss = -9893.081996760882
Iteration 2500: Loss = -9893.050630705673
Iteration 2600: Loss = -9893.022787788883
Iteration 2700: Loss = -9893.000743150002
Iteration 2800: Loss = -9892.982166906426
Iteration 2900: Loss = -9892.966042651942
Iteration 3000: Loss = -9892.952060914378
Iteration 3100: Loss = -9892.940196301217
Iteration 3200: Loss = -9892.929999642496
Iteration 3300: Loss = -9892.92136281123
Iteration 3400: Loss = -9892.914141033389
Iteration 3500: Loss = -9892.90801191066
Iteration 3600: Loss = -9892.9029810743
Iteration 3700: Loss = -9892.898785257597
Iteration 3800: Loss = -9892.895247693375
Iteration 3900: Loss = -9892.892257577287
Iteration 4000: Loss = -9892.889717989705
Iteration 4100: Loss = -9892.887460869586
Iteration 4200: Loss = -9892.885503965146
Iteration 4300: Loss = -9892.88357347862
Iteration 4400: Loss = -9892.881869161261
Iteration 4500: Loss = -9892.880292644477
Iteration 4600: Loss = -9892.878895950756
Iteration 4700: Loss = -9892.877558042168
Iteration 4800: Loss = -9892.876451819047
Iteration 4900: Loss = -9892.875452235643
Iteration 5000: Loss = -9892.874438518422
Iteration 5100: Loss = -9892.873549395326
Iteration 5200: Loss = -9892.872779960078
Iteration 5300: Loss = -9892.871918197538
Iteration 5400: Loss = -9892.871201091504
Iteration 5500: Loss = -9892.870476060574
Iteration 5600: Loss = -9892.869943192823
Iteration 5700: Loss = -9892.869305651442
Iteration 5800: Loss = -9892.871116326098
1
Iteration 5900: Loss = -9892.868315813233
Iteration 6000: Loss = -9892.869476984608
1
Iteration 6100: Loss = -9892.86734336986
Iteration 6200: Loss = -9892.866875376249
Iteration 6300: Loss = -9892.866526702051
Iteration 6400: Loss = -9892.866238349365
Iteration 6500: Loss = -9892.865973724905
Iteration 6600: Loss = -9892.867206819752
1
Iteration 6700: Loss = -9892.86516491155
Iteration 6800: Loss = -9892.865609108605
1
Iteration 6900: Loss = -9892.86450220373
Iteration 7000: Loss = -9892.864201431783
Iteration 7100: Loss = -9892.863928376297
Iteration 7200: Loss = -9892.863689698193
Iteration 7300: Loss = -9892.863495366615
Iteration 7400: Loss = -9892.863377735046
Iteration 7500: Loss = -9892.86307670826
Iteration 7600: Loss = -9892.86306087606
Iteration 7700: Loss = -9892.863103774192
1
Iteration 7800: Loss = -9892.86575958634
2
Iteration 7900: Loss = -9892.862981563607
Iteration 8000: Loss = -9893.00804954412
1
Iteration 8100: Loss = -9892.862002283422
Iteration 8200: Loss = -9892.863152164115
1
Iteration 8300: Loss = -9892.862220916633
2
Iteration 8400: Loss = -9892.86169700947
Iteration 8500: Loss = -9892.953223398725
1
Iteration 8600: Loss = -9892.861422278884
Iteration 8700: Loss = -9892.864759767554
1
Iteration 8800: Loss = -9892.863367496127
2
Iteration 8900: Loss = -9892.861179225343
Iteration 9000: Loss = -9892.864117715475
1
Iteration 9100: Loss = -9892.860977337847
Iteration 9200: Loss = -9892.8611646306
1
Iteration 9300: Loss = -9892.860821398062
Iteration 9400: Loss = -9892.860847792726
1
Iteration 9500: Loss = -9892.861136096686
2
Iteration 9600: Loss = -9892.860622721484
Iteration 9700: Loss = -9892.860627818696
1
Iteration 9800: Loss = -9892.860500974444
Iteration 9900: Loss = -9892.861544810206
1
Iteration 10000: Loss = -9892.860895919668
2
Iteration 10100: Loss = -9892.861260290892
3
Iteration 10200: Loss = -9892.87241073278
4
Iteration 10300: Loss = -9892.860311861015
Iteration 10400: Loss = -9892.867203550482
1
Iteration 10500: Loss = -9892.868916306807
2
Iteration 10600: Loss = -9892.870016550994
3
Iteration 10700: Loss = -9892.861681931645
4
Iteration 10800: Loss = -9892.860048965375
Iteration 10900: Loss = -9892.863119001659
1
Iteration 11000: Loss = -9892.916754568902
2
Iteration 11100: Loss = -9892.859886536682
Iteration 11200: Loss = -9892.860995899011
1
Iteration 11300: Loss = -9892.865254820332
2
Iteration 11400: Loss = -9892.86228180685
3
Iteration 11500: Loss = -9892.859822731702
Iteration 11600: Loss = -9892.860537112158
1
Iteration 11700: Loss = -9892.85981386279
Iteration 11800: Loss = -9892.86070543145
1
Iteration 11900: Loss = -9892.861323749476
2
Iteration 12000: Loss = -9892.859748150408
Iteration 12100: Loss = -9892.864366059488
1
Iteration 12200: Loss = -9892.860214432725
2
Iteration 12300: Loss = -9892.85976781821
3
Iteration 12400: Loss = -9892.874997858316
4
Iteration 12500: Loss = -9892.859628840284
Iteration 12600: Loss = -9892.859622967035
Iteration 12700: Loss = -9892.878951902092
1
Iteration 12800: Loss = -9892.859573258576
Iteration 12900: Loss = -9892.914572561896
1
Iteration 13000: Loss = -9892.8595641163
Iteration 13100: Loss = -9892.861981567163
1
Iteration 13200: Loss = -9893.001775771223
2
Iteration 13300: Loss = -9892.86171540582
3
Iteration 13400: Loss = -9892.885592342194
4
Iteration 13500: Loss = -9892.860967305873
5
Iteration 13600: Loss = -9892.859717026628
6
Iteration 13700: Loss = -9892.860767110707
7
Iteration 13800: Loss = -9892.859738690144
8
Iteration 13900: Loss = -9892.859569449505
9
Iteration 14000: Loss = -9892.859985517653
10
Stopping early at iteration 14000 due to no improvement.
tensor([[-6.5776,  1.9624],
        [-4.7801,  0.1649],
        [-4.9326,  0.3174],
        [-6.3277,  1.7125],
        [-3.9497, -0.6656],
        [-9.0577,  4.4425],
        [-6.6097,  1.9945],
        [-5.9024,  1.2872],
        [-6.5286,  1.9134],
        [-4.1363, -0.4789],
        [-7.8462,  3.2310],
        [-5.2352,  0.6200],
        [-4.9781,  0.3629],
        [-5.8498,  1.2346],
        [-7.6987,  3.0835],
        [-7.9411,  3.3259],
        [-7.7501,  3.1349],
        [-7.9066,  3.2914],
        [-6.6857,  2.0705],
        [-5.6266,  1.0114],
        [-6.5748,  1.9596],
        [-6.9421,  2.3269],
        [-4.5360, -0.0792],
        [-5.8935,  1.2783],
        [-6.3140,  1.6988],
        [-6.9333,  2.3181],
        [-6.8134,  2.1982],
        [-3.5557, -1.0595],
        [-4.2560, -0.3592],
        [-9.0108,  4.3956],
        [-6.5330,  1.9178],
        [-5.2619,  0.6467],
        [-8.0642,  3.4489],
        [-4.1305, -0.4847],
        [-5.2270,  0.6118],
        [-4.6360,  0.0208],
        [-4.9597,  0.3444],
        [-6.7991,  2.1839],
        [-6.3861,  1.7708],
        [-4.3333, -0.2819],
        [-4.4294, -0.1858],
        [-3.7293, -0.8859],
        [-3.1532, -1.4620],
        [-5.2462,  0.6310],
        [-5.2017,  0.5864],
        [-5.2753,  0.6601],
        [-3.9260, -0.6892],
        [-4.9071,  0.2918],
        [-5.1846,  0.5694],
        [-5.1290,  0.5138],
        [-7.9831,  3.3679],
        [-4.1333, -0.4819],
        [-6.8370,  2.2218],
        [-6.1087,  1.4935],
        [-6.2688,  1.6535],
        [-1.9220, -2.6932],
        [-5.3837,  0.7685],
        [-7.3805,  2.7653],
        [-7.4833,  2.8681],
        [-5.8673,  1.2521],
        [-7.1216,  2.5064],
        [-9.4849,  4.8697],
        [-5.4113,  0.7961],
        [-5.9814,  1.3662],
        [-5.8753,  1.2601],
        [-8.7567,  4.1415],
        [-5.2541,  0.6388],
        [-7.9098,  3.2946],
        [-7.0485,  2.4333],
        [-6.3191,  1.7039],
        [-5.9214,  1.3062],
        [-5.2834,  0.6681],
        [-6.7151,  2.0999],
        [-5.8819,  1.2667],
        [-6.6548,  2.0396],
        [-3.8891, -0.7262],
        [-5.3282,  0.7130],
        [-5.4928,  0.8775],
        [-7.1992,  2.5840],
        [-7.0756,  2.4604],
        [-5.9199,  1.3047],
        [-7.0939,  2.4786],
        [-5.7222,  1.1070],
        [-2.1875, -2.4277],
        [-4.8635,  0.2483],
        [-7.0289,  2.4136],
        [-4.9877,  0.3724],
        [-6.3558,  1.7406],
        [-6.4633,  1.8480],
        [-7.0112,  2.3960],
        [-7.0387,  2.4235],
        [-5.2290,  0.6137],
        [-9.5190,  4.9037],
        [-5.3343,  0.7191],
        [-5.2135,  0.5983],
        [-5.2076,  0.5924],
        [-6.7397,  2.1245],
        [-8.9002,  4.2850],
        [-5.5656,  0.9504],
        [-7.5760,  2.9608]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9997e-01, 3.0468e-05],
        [1.1461e-02, 9.8854e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0190, 0.9810], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1955, 0.1765],
         [0.8833, 0.1357]],

        [[0.5966, 0.0767],
         [0.1435, 0.5930]],

        [[0.4484, 0.1610],
         [0.2324, 0.1776]],

        [[0.6881, 0.1549],
         [0.5416, 0.7452]],

        [[0.0667, 0.1767],
         [0.5715, 0.2923]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: -0.000568071846293413
Average Adjusted Rand Index: -0.0025822689111018996
Iteration 0: Loss = -17492.15651533765
Iteration 10: Loss = -9895.147063747243
Iteration 20: Loss = -9895.147116603886
1
Iteration 30: Loss = -9895.147022623483
Iteration 40: Loss = -9895.14695669861
Iteration 50: Loss = -9895.146765384925
Iteration 60: Loss = -9895.145869005979
Iteration 70: Loss = -9895.140082443746
Iteration 80: Loss = -9895.103342344783
Iteration 90: Loss = -9894.925684841946
Iteration 100: Loss = -9894.577178152796
Iteration 110: Loss = -9894.365592471944
Iteration 120: Loss = -9894.288209361514
Iteration 130: Loss = -9894.257255181075
Iteration 140: Loss = -9894.242178918348
Iteration 150: Loss = -9894.23372343679
Iteration 160: Loss = -9894.228455718989
Iteration 170: Loss = -9894.224849392074
Iteration 180: Loss = -9894.222314387827
Iteration 190: Loss = -9894.220415882497
Iteration 200: Loss = -9894.218978681803
Iteration 210: Loss = -9894.217803103726
Iteration 220: Loss = -9894.216884902144
Iteration 230: Loss = -9894.216115598949
Iteration 240: Loss = -9894.215485303786
Iteration 250: Loss = -9894.214967619135
Iteration 260: Loss = -9894.214525412248
Iteration 270: Loss = -9894.21415713509
Iteration 280: Loss = -9894.213819663139
Iteration 290: Loss = -9894.21355829629
Iteration 300: Loss = -9894.213319421437
Iteration 310: Loss = -9894.213068550907
Iteration 320: Loss = -9894.212885847006
Iteration 330: Loss = -9894.212733711718
Iteration 340: Loss = -9894.212576901275
Iteration 350: Loss = -9894.212470291224
Iteration 360: Loss = -9894.21237292348
Iteration 370: Loss = -9894.212249962788
Iteration 380: Loss = -9894.212135471307
Iteration 390: Loss = -9894.212039445662
Iteration 400: Loss = -9894.211988857422
Iteration 410: Loss = -9894.21193010146
Iteration 420: Loss = -9894.21189671893
Iteration 430: Loss = -9894.211840218128
Iteration 440: Loss = -9894.211773454834
Iteration 450: Loss = -9894.211754446349
Iteration 460: Loss = -9894.211734733894
Iteration 470: Loss = -9894.21166860792
Iteration 480: Loss = -9894.211652551885
Iteration 490: Loss = -9894.211585426314
Iteration 500: Loss = -9894.211570078804
Iteration 510: Loss = -9894.211586655712
1
Iteration 520: Loss = -9894.211560078713
Iteration 530: Loss = -9894.211515961728
Iteration 540: Loss = -9894.211503693883
Iteration 550: Loss = -9894.211477884242
Iteration 560: Loss = -9894.211480054537
1
Iteration 570: Loss = -9894.211459112199
Iteration 580: Loss = -9894.211428997553
Iteration 590: Loss = -9894.211467896277
1
Iteration 600: Loss = -9894.211415078778
Iteration 610: Loss = -9894.21140659393
Iteration 620: Loss = -9894.21142966074
1
Iteration 630: Loss = -9894.211428300967
2
Iteration 640: Loss = -9894.211439453547
3
Stopping early at iteration 639 due to no improvement.
pi: tensor([[5.1354e-13, 1.0000e+00],
        [1.6489e-02, 9.8351e-01]], dtype=torch.float64)
alpha: tensor([0.0169, 0.9831])
beta: tensor([[[0.1872, 0.2015],
         [0.7759, 0.1352]],

        [[0.6266, 0.0928],
         [0.3626, 0.0232]],

        [[0.0302, 0.0846],
         [0.5107, 0.8087]],

        [[0.1285, 0.1821],
         [0.5645, 0.6553]],

        [[0.5734, 0.1776],
         [0.3025, 0.6640]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17492.33991702427
Iteration 100: Loss = -9923.478202465869
Iteration 200: Loss = -9901.085098709593
Iteration 300: Loss = -9896.220348389914
Iteration 400: Loss = -9895.026758519223
Iteration 500: Loss = -9894.544461914234
Iteration 600: Loss = -9894.312907939464
Iteration 700: Loss = -9894.168992781972
Iteration 800: Loss = -9894.060705160899
Iteration 900: Loss = -9893.969213780274
Iteration 1000: Loss = -9893.884473819997
Iteration 1100: Loss = -9893.810825117731
Iteration 1200: Loss = -9893.754030761997
Iteration 1300: Loss = -9893.710396347302
Iteration 1400: Loss = -9893.677489494914
Iteration 1500: Loss = -9893.654439427884
Iteration 1600: Loss = -9893.636583212612
Iteration 1700: Loss = -9893.621939573264
Iteration 1800: Loss = -9893.601082333364
Iteration 1900: Loss = -9893.58519295037
Iteration 2000: Loss = -9893.57679417962
Iteration 2100: Loss = -9893.57051664114
Iteration 2200: Loss = -9893.564084920095
Iteration 2300: Loss = -9893.55896260582
Iteration 2400: Loss = -9893.554220977147
Iteration 2500: Loss = -9893.554004092477
Iteration 2600: Loss = -9893.545495273014
Iteration 2700: Loss = -9893.541295817437
Iteration 2800: Loss = -9893.5466308051
1
Iteration 2900: Loss = -9893.530623366067
Iteration 3000: Loss = -9893.525283686758
Iteration 3100: Loss = -9893.520072673302
Iteration 3200: Loss = -9893.513954678714
Iteration 3300: Loss = -9893.505021603616
Iteration 3400: Loss = -9893.49276227507
Iteration 3500: Loss = -9893.477678198427
Iteration 3600: Loss = -9893.444010517733
Iteration 3700: Loss = -9893.39666409335
Iteration 3800: Loss = -9893.329451133355
Iteration 3900: Loss = -9893.270233667845
Iteration 4000: Loss = -9893.201772043238
Iteration 4100: Loss = -9893.170240751426
Iteration 4200: Loss = -9893.154327825085
Iteration 4300: Loss = -9893.146258011415
Iteration 4400: Loss = -9893.14231567063
Iteration 4500: Loss = -9893.140007094573
Iteration 4600: Loss = -9893.139060680165
Iteration 4700: Loss = -9893.136763924389
Iteration 4800: Loss = -9893.113078142658
Iteration 4900: Loss = -9893.118681249498
1
Iteration 5000: Loss = -9893.090450644564
Iteration 5100: Loss = -9893.08547993665
Iteration 5200: Loss = -9893.048265474401
Iteration 5300: Loss = -9893.041980959373
Iteration 5400: Loss = -9892.946340302595
Iteration 5500: Loss = -9892.968979081475
1
Iteration 5600: Loss = -9892.9432672642
Iteration 5700: Loss = -9892.942430877749
Iteration 5800: Loss = -9892.946836465018
1
Iteration 5900: Loss = -9892.940138140511
Iteration 6000: Loss = -9893.043673155044
1
Iteration 6100: Loss = -9892.940032892275
Iteration 6200: Loss = -9892.94003085766
Iteration 6300: Loss = -9892.940053976125
1
Iteration 6400: Loss = -9892.939900420834
Iteration 6500: Loss = -9893.006451035533
1
Iteration 6600: Loss = -9892.939796094126
Iteration 6700: Loss = -9892.939722127257
Iteration 6800: Loss = -9892.978128053432
1
Iteration 6900: Loss = -9892.890612635336
Iteration 7000: Loss = -9892.886289600965
Iteration 7100: Loss = -9892.885691894842
Iteration 7200: Loss = -9892.88606092089
1
Iteration 7300: Loss = -9892.88561734536
Iteration 7400: Loss = -9892.886094957219
1
Iteration 7500: Loss = -9892.88547101008
Iteration 7600: Loss = -9893.069859545378
1
Iteration 7700: Loss = -9892.88530398272
Iteration 7800: Loss = -9892.885224549393
Iteration 7900: Loss = -9892.894780754843
1
Iteration 8000: Loss = -9892.885208693742
Iteration 8100: Loss = -9892.885193690458
Iteration 8200: Loss = -9892.89169325159
1
Iteration 8300: Loss = -9892.885136090743
Iteration 8400: Loss = -9892.885152634382
1
Iteration 8500: Loss = -9892.888360714278
2
Iteration 8600: Loss = -9892.885074373875
Iteration 8700: Loss = -9892.885096082176
1
Iteration 8800: Loss = -9892.88524089057
2
Iteration 8900: Loss = -9892.885085951777
3
Iteration 9000: Loss = -9893.02456118541
4
Iteration 9100: Loss = -9892.885081357594
5
Iteration 9200: Loss = -9892.885010847065
Iteration 9300: Loss = -9892.890164824885
1
Iteration 9400: Loss = -9892.88501732065
2
Iteration 9500: Loss = -9892.884975699493
Iteration 9600: Loss = -9892.995981118309
1
Iteration 9700: Loss = -9892.88495742425
Iteration 9800: Loss = -9893.004860845396
1
Iteration 9900: Loss = -9892.884918230631
Iteration 10000: Loss = -9892.897285058674
1
Iteration 10100: Loss = -9892.884887590397
Iteration 10200: Loss = -9892.884886308582
Iteration 10300: Loss = -9892.893816489339
1
Iteration 10400: Loss = -9892.976656465082
2
Iteration 10500: Loss = -9892.884838283318
Iteration 10600: Loss = -9892.886341125992
1
Iteration 10700: Loss = -9892.885028102377
2
Iteration 10800: Loss = -9892.947537178019
3
Iteration 10900: Loss = -9892.886835114983
4
Iteration 11000: Loss = -9892.884757977197
Iteration 11100: Loss = -9892.884923948457
1
Iteration 11200: Loss = -9892.900349808024
2
Iteration 11300: Loss = -9892.884858752162
3
Iteration 11400: Loss = -9892.908179301241
4
Iteration 11500: Loss = -9892.884749902072
Iteration 11600: Loss = -9892.884797520574
1
Iteration 11700: Loss = -9892.885575928509
2
Iteration 11800: Loss = -9892.88468706692
Iteration 11900: Loss = -9892.884708328285
1
Iteration 12000: Loss = -9892.889572836026
2
Iteration 12100: Loss = -9892.911734179272
3
Iteration 12200: Loss = -9892.884647783427
Iteration 12300: Loss = -9892.884724612106
1
Iteration 12400: Loss = -9892.885149495587
2
Iteration 12500: Loss = -9892.884800545755
3
Iteration 12600: Loss = -9892.9009718318
4
Iteration 12700: Loss = -9892.884609967927
Iteration 12800: Loss = -9892.885897592052
1
Iteration 12900: Loss = -9892.885689570103
2
Iteration 13000: Loss = -9892.884602265945
Iteration 13100: Loss = -9892.884789753429
1
Iteration 13200: Loss = -9892.884555111075
Iteration 13300: Loss = -9892.884590918253
1
Iteration 13400: Loss = -9892.88478316871
2
Iteration 13500: Loss = -9892.888393109717
3
Iteration 13600: Loss = -9892.890404693577
4
Iteration 13700: Loss = -9892.915860048399
5
Iteration 13800: Loss = -9892.88471857719
6
Iteration 13900: Loss = -9892.884585723741
7
Iteration 14000: Loss = -9892.928715272039
8
Iteration 14100: Loss = -9892.885125538254
9
Iteration 14200: Loss = -9892.884543543469
Iteration 14300: Loss = -9892.896755114594
1
Iteration 14400: Loss = -9892.884497171546
Iteration 14500: Loss = -9892.884615177782
1
Iteration 14600: Loss = -9892.884478643655
Iteration 14700: Loss = -9892.884547415617
1
Iteration 14800: Loss = -9892.885814625914
2
Iteration 14900: Loss = -9892.892624526148
3
Iteration 15000: Loss = -9892.884501830777
4
Iteration 15100: Loss = -9892.887226622523
5
Iteration 15200: Loss = -9892.884451562868
Iteration 15300: Loss = -9892.942610603708
1
Iteration 15400: Loss = -9892.884814762008
2
Iteration 15500: Loss = -9892.884472545464
3
Iteration 15600: Loss = -9892.909197892564
4
Iteration 15700: Loss = -9892.884468537999
5
Iteration 15800: Loss = -9892.930933641119
6
Iteration 15900: Loss = -9892.884505348906
7
Iteration 16000: Loss = -9892.884813464525
8
Iteration 16100: Loss = -9892.890433943488
9
Iteration 16200: Loss = -9892.884466847163
10
Stopping early at iteration 16200 due to no improvement.
tensor([[-4.3991,  2.5715],
        [-5.4217,  1.4773],
        [-3.9488,  2.0343],
        [-4.3816,  1.8002],
        [-2.9127,  1.0486],
        [-6.6702,  2.0550],
        [-3.9719,  2.5777],
        [-4.2515,  2.3490],
        [-4.4413,  2.5135],
        [-2.7085,  1.2899],
        [-3.9349,  2.5175],
        [-3.4904,  1.2166],
        [-4.3882,  2.8009],
        [-3.1444,  0.8011],
        [-4.1243,  2.7342],
        [-5.2662,  3.5457],
        [-5.7223,  3.6891],
        [-4.7386,  2.6683],
        [-4.6950,  2.3108],
        [-3.7133,  2.1185],
        [-3.9208,  2.3654],
        [-4.5512,  2.4396],
        [-3.4130,  1.0966],
        [-4.5673,  2.0542],
        [-4.2394,  2.7662],
        [-2.2041,  0.7039],
        [-3.1124,  1.2663],
        [-2.3864,  0.9641],
        [-2.9384, -0.0968],
        [-3.1829,  1.7942],
        [-4.4551,  2.5026],
        [-2.9764,  1.5552],
        [-4.3351,  2.5707],
        [-3.9800,  1.5243],
        [-2.9968,  1.5819],
        [-3.9340,  1.6167],
        [-4.0022,  1.7969],
        [-5.1591,  0.6721],
        [-4.0193,  2.4582],
        [-3.9765,  2.4669],
        [-3.8180,  2.3194],
        [-4.9717,  2.2192],
        [-3.4128,  1.9813],
        [-4.3154,  1.7710],
        [-3.3769,  1.9862],
        [-3.9080,  2.5068],
        [-4.2902,  2.1845],
        [-3.6893,  1.2912],
        [-3.2995,  1.4081],
        [-3.9159,  1.3405],
        [-4.8266,  2.9785],
        [-3.2757,  1.6348],
        [-2.6056, -0.8924],
        [-4.0110,  1.2325],
        [-4.5038,  2.6901],
        [-2.4957,  0.6438],
        [-4.7251,  3.2915],
        [-3.4265,  2.0354],
        [-4.2305,  2.6769],
        [-4.6654,  3.1747],
        [-4.5731,  0.8966],
        [-4.5907,  3.1514],
        [-3.8062,  1.9103],
        [-4.8034,  3.0153],
        [-2.4543,  0.7624],
        [-3.4448,  1.9034],
        [-1.8464,  0.4553],
        [-3.8458,  1.8539],
        [-5.8359,  1.5874],
        [-5.0143,  2.1095],
        [-2.8732,  1.4868],
        [-4.3490,  1.9293],
        [-3.8471,  2.3223],
        [-5.7520,  2.3240],
        [-3.3233,  1.4728],
        [-4.4989,  1.0860],
        [-3.8243,  1.6631],
        [-2.1705, -0.3624],
        [-6.2827,  2.9311],
        [-3.0689,  1.6474],
        [-2.3973, -0.4013],
        [-4.5366,  2.6985],
        [-5.1987,  3.7634],
        [-3.4819,  1.6874],
        [-4.5829,  2.9733],
        [-4.4014,  2.8351],
        [-4.0747,  1.7462],
        [-2.2499,  0.8619],
        [-3.9672,  2.5741],
        [-3.6284,  1.6807],
        [-4.3121,  0.1067],
        [-4.7336,  2.4835],
        [-5.3909,  3.8394],
        [-4.3883,  1.0466],
        [-5.5726,  0.9574],
        [-4.4565,  2.5955],
        [-4.2760,  1.5217],
        [-3.2277,  1.8348],
        [-3.0929,  1.5456],
        [-5.5447,  4.0729]], dtype=torch.float64, requires_grad=True)
pi: tensor([[3.4812e-04, 9.9965e-01],
        [1.0320e-01, 8.9680e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0110, 0.9890], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1043, 0.2028],
         [0.7759, 0.1400]],

        [[0.6266, 0.1072],
         [0.3626, 0.0232]],

        [[0.0302, 0.1191],
         [0.5107, 0.8087]],

        [[0.1285, 0.1076],
         [0.5645, 0.6553]],

        [[0.5734, 0.1484],
         [0.3025, 0.6640]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -27600.959913991348
Iteration 10: Loss = -9895.146866723411
Iteration 20: Loss = -9895.146436224706
Iteration 30: Loss = -9895.142517147795
Iteration 40: Loss = -9895.103764926673
Iteration 50: Loss = -9894.74929064026
Iteration 60: Loss = -9894.003123010647
Iteration 70: Loss = -9893.67921138973
Iteration 80: Loss = -9893.574651908957
Iteration 90: Loss = -9893.532178812533
Iteration 100: Loss = -9893.5042138663
Iteration 110: Loss = -9893.487526738276
Iteration 120: Loss = -9893.489874150715
1
Iteration 130: Loss = -9893.500665393069
2
Iteration 140: Loss = -9893.511316528153
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[9.4317e-01, 5.6829e-02],
        [1.0000e+00, 4.6131e-10]], dtype=torch.float64)
alpha: tensor([0.9472, 0.0528])
beta: /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|         | 11/100 [7:52:56<64:46:41, 2620.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|        | 12/100 [8:34:30<63:07:01, 2582.06s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|        | 13/100 [9:20:44<63:48:07, 2640.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|        | 14/100 [10:13:52<67:01:29, 2805.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|        | 15/100 [11:09:48<70:09:39, 2971.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|        | 16/100 [11:57:10<68:25:28, 2932.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|        | 17/100 [12:33:42<62:28:35, 2709.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|        | 18/100 [13:22:33<63:14:01, 2776.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|        | 19/100 [14:13:25<64:19:42, 2859.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|        | 20/100 [14:43:35<56:32:01, 2544.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
tensor([[[0.1358, 0.1777],
         [0.4201, 0.1411]],

        [[0.7132, 0.1035],
         [0.6381, 0.0511]],

        [[0.7848, 0.1311],
         [0.0011, 0.1420]],

        [[0.2506, 0.1050],
         [0.8525, 0.0109]],

        [[0.7732, 0.1645],
         [0.3409, 0.9733]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27601.17694225231
Iteration 100: Loss = -9908.446495061396
Iteration 200: Loss = -9895.716430008279
Iteration 300: Loss = -9894.902156368968
Iteration 400: Loss = -9894.440145839006
Iteration 500: Loss = -9894.023829044963
Iteration 600: Loss = -9893.901886900587
Iteration 700: Loss = -9893.834704435234
Iteration 800: Loss = -9893.78402773304
Iteration 900: Loss = -9893.741174173842
Iteration 1000: Loss = -9893.704949562236
Iteration 1100: Loss = -9893.67471532038
Iteration 1200: Loss = -9893.648095841916
Iteration 1300: Loss = -9893.623676372627
Iteration 1400: Loss = -9893.600544020634
Iteration 1500: Loss = -9893.57870655909
Iteration 1600: Loss = -9893.55836925276
Iteration 1700: Loss = -9893.540178082838
Iteration 1800: Loss = -9893.52449164866
Iteration 1900: Loss = -9893.511084965954
Iteration 2000: Loss = -9893.499460378713
Iteration 2100: Loss = -9893.489091731039
Iteration 2200: Loss = -9893.47961950653
Iteration 2300: Loss = -9893.470818885035
Iteration 2400: Loss = -9893.462553854377
Iteration 2500: Loss = -9893.454626595081
Iteration 2600: Loss = -9893.446870408772
Iteration 2700: Loss = -9893.439171468428
Iteration 2800: Loss = -9893.431123493727
Iteration 2900: Loss = -9893.422436058912
Iteration 3000: Loss = -9893.412445449792
Iteration 3100: Loss = -9893.40004742525
Iteration 3200: Loss = -9893.382011352049
Iteration 3300: Loss = -9893.33079015527
Iteration 3400: Loss = -9893.291252572455
Iteration 3500: Loss = -9893.234714857274
Iteration 3600: Loss = -9893.153655342585
Iteration 3700: Loss = -9893.052144751782
Iteration 3800: Loss = -9892.963319496093
Iteration 3900: Loss = -9892.912637103815
Iteration 4000: Loss = -9892.89541076132
Iteration 4100: Loss = -9892.89130557676
Iteration 4200: Loss = -9892.888666280955
Iteration 4300: Loss = -9892.887772441487
Iteration 4400: Loss = -9892.887290080984
Iteration 4500: Loss = -9892.886958159366
Iteration 4600: Loss = -9892.887467083172
1
Iteration 4700: Loss = -9892.8872368555
2
Iteration 4800: Loss = -9892.886535299536
Iteration 4900: Loss = -9892.886513788448
Iteration 5000: Loss = -9892.8870425392
1
Iteration 5100: Loss = -9892.886413582424
Iteration 5200: Loss = -9892.88684741311
1
Iteration 5300: Loss = -9892.88623735106
Iteration 5400: Loss = -9892.886501878436
1
Iteration 5500: Loss = -9892.886182742071
Iteration 5600: Loss = -9892.889311044471
1
Iteration 5700: Loss = -9892.886186959937
2
Iteration 5800: Loss = -9892.886044957639
Iteration 5900: Loss = -9892.88612178424
1
Iteration 6000: Loss = -9892.885950157017
Iteration 6100: Loss = -9892.885926115985
Iteration 6200: Loss = -9892.885883358964
Iteration 6300: Loss = -9892.885888291019
1
Iteration 6400: Loss = -9892.885800320193
Iteration 6500: Loss = -9892.885780264438
Iteration 6600: Loss = -9892.885767133781
Iteration 6700: Loss = -9892.885682244392
Iteration 6800: Loss = -9892.885671129176
Iteration 6900: Loss = -9892.885976340065
1
Iteration 7000: Loss = -9892.885619365707
Iteration 7100: Loss = -9892.885597717857
Iteration 7200: Loss = -9892.885541790809
Iteration 7300: Loss = -9892.885503627209
Iteration 7400: Loss = -9892.885843961962
1
Iteration 7500: Loss = -9892.885464675033
Iteration 7600: Loss = -9892.885411384876
Iteration 7700: Loss = -9892.88544521109
1
Iteration 7800: Loss = -9892.88534587399
Iteration 7900: Loss = -9892.885419517494
1
Iteration 8000: Loss = -9892.885283968144
Iteration 8100: Loss = -9892.892917381987
1
Iteration 8200: Loss = -9892.887812005187
2
Iteration 8300: Loss = -9892.88524856226
Iteration 8400: Loss = -9892.885176750748
Iteration 8500: Loss = -9892.885347057885
1
Iteration 8600: Loss = -9892.885137070516
Iteration 8700: Loss = -9892.889013750286
1
Iteration 8800: Loss = -9892.885346339133
2
Iteration 8900: Loss = -9892.94491523487
3
Iteration 9000: Loss = -9892.885053910566
Iteration 9100: Loss = -9892.885114061066
1
Iteration 9200: Loss = -9892.897884719023
2
Iteration 9300: Loss = -9892.884974637982
Iteration 9400: Loss = -9892.884973308435
Iteration 9500: Loss = -9892.884931400895
Iteration 9600: Loss = -9892.885072618923
1
Iteration 9700: Loss = -9892.884871786646
Iteration 9800: Loss = -9892.88508744532
1
Iteration 9900: Loss = -9892.884845719735
Iteration 10000: Loss = -9892.885566589015
1
Iteration 10100: Loss = -9892.884808932338
Iteration 10200: Loss = -9892.966952903978
1
Iteration 10300: Loss = -9892.88479087778
Iteration 10400: Loss = -9892.993835261066
1
Iteration 10500: Loss = -9892.884755179195
Iteration 10600: Loss = -9892.896265805644
1
Iteration 10700: Loss = -9892.893588000377
2
Iteration 10800: Loss = -9892.885150238317
3
Iteration 10900: Loss = -9892.884797435972
4
Iteration 11000: Loss = -9892.889112197248
5
Iteration 11100: Loss = -9892.90709950689
6
Iteration 11200: Loss = -9892.884690757488
Iteration 11300: Loss = -9892.897576133339
1
Iteration 11400: Loss = -9892.886001337532
2
Iteration 11500: Loss = -9892.884701788445
3
Iteration 11600: Loss = -9892.892333369591
4
Iteration 11700: Loss = -9892.891161395166
5
Iteration 11800: Loss = -9892.88612500417
6
Iteration 11900: Loss = -9892.885090167294
7
Iteration 12000: Loss = -9892.910713563459
8
Iteration 12100: Loss = -9892.896924046816
9
Iteration 12200: Loss = -9893.068095346765
10
Stopping early at iteration 12200 due to no improvement.
tensor([[ 2.7153, -4.2564],
        [ 1.8658, -5.0282],
        [ 2.2086, -3.7728],
        [ 2.3771, -3.8054],
        [ 0.3816, -3.5657],
        [ 2.6504, -6.0759],
        [ 2.5712, -3.9793],
        [ 2.5138, -4.0858],
        [ 1.8985, -5.0599],
        [ 1.1558, -2.8347],
        [ 2.4609, -3.9932],
        [ 1.3605, -3.3424],
        [ 2.8343, -4.3547],
        [ 1.2202, -2.7148],
        [ 2.6747, -4.1853],
        [ 3.3789, -5.4332],
        [ 3.7302, -5.6823],
        [ 2.9589, -4.4488],
        [ 2.7998, -4.2061],
        [ 1.6269, -4.2035],
        [ 2.0736, -4.2132],
        [ 2.7657, -4.2256],
        [ 1.4689, -3.0374],
        [ 2.3215, -4.2994],
        [ 2.8066, -4.2000],
        [ 0.7317, -2.1609],
        [ 1.4256, -2.9453],
        [ 0.9165, -2.4190],
        [ 0.5557, -2.2696],
        [ 1.6956, -3.2805],
        [ 2.0057, -4.9528],
        [ 0.6491, -3.8656],
        [ 2.7344, -4.1731],
        [ 2.0555, -3.4454],
        [ 1.4591, -3.1155],
        [ 2.0624, -3.4850],
        [ 1.1495, -4.6476],
        [ 1.6118, -4.2175],
        [ 2.5259, -3.9532],
        [ 2.5158, -3.9241],
        [ 2.1480, -3.9863],
        [ 2.5041, -4.6831],
        [ 1.6467, -3.7423],
        [ 2.2754, -3.8111],
        [ 1.8356, -3.5273],
        [ 1.9396, -4.4726],
        [ 2.5410, -3.9315],
        [ 1.7343, -3.2423],
        [ 1.5413, -3.1571],
        [ 1.6641, -3.5906],
        [ 3.0705, -4.7357],
        [ 1.7270, -3.1794],
        [ 0.1330, -1.5656],
        [ 1.5350, -3.7050],
        [ 2.2542, -4.9390],
        [-0.0576, -3.1773],
        [ 3.2719, -4.7451],
        [ 0.9834, -4.4771],
        [ 2.7612, -4.1475],
        [ 2.5888, -5.2531],
        [ 1.6555, -3.8114],
        [ 2.4761, -5.2669],
        [ 1.5528, -4.1635],
        [ 3.1835, -4.6359],
        [ 0.9071, -2.2944],
        [ 1.9773, -3.3712],
        [-0.1020, -2.3885],
        [ 1.8740, -3.8262],
        [ 2.4588, -4.9653],
        [ 1.8182, -5.3064],
        [ 0.7912, -3.5613],
        [ 0.8256, -5.4408],
        [ 2.3288, -3.8398],
        [ 3.1590, -4.9175],
        [ 1.1596, -3.6340],
        [ 1.4513, -4.1254],
        [ 0.7585, -4.7101],
        [ 0.0997, -1.6933],
        [ 3.9121, -5.3002],
        [ 1.5418, -3.1743],
        [-0.2009, -2.1814],
        [ 2.4029, -4.8319],
        [ 3.7859, -5.1742],
        [ 1.5609, -3.5947],
        [ 2.7875, -4.7664],
        [ 2.8787, -4.3583],
        [ 2.0820, -3.7383],
        [ 0.8527, -2.2454],
        [ 2.5673, -3.9742],
        [ 1.9452, -3.3634],
        [ 1.4404, -2.9699],
        [ 2.9124, -4.3026],
        [ 2.3080, -6.9232],
        [ 1.9741, -3.4576],
        [ 0.9477, -5.5630],
        [ 2.6033, -4.4472],
        [ 0.8019, -4.9788],
        [ 1.3257, -3.7344],
        [ 1.5324, -3.1003],
        [ 4.1082, -5.5077]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8962, 0.1038],
        [0.9986, 0.0014]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9893, 0.0107], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1411, 0.2027],
         [0.4201, 0.1042]],

        [[0.7132, 0.1063],
         [0.6381, 0.0511]],

        [[0.7848, 0.1181],
         [0.0011, 0.1420]],

        [[0.2506, 0.1068],
         [0.8525, 0.0109]],

        [[0.7732, 0.1472],
         [0.3409, 0.9733]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -25411.66166089473
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.4806,    nan]],

        [[0.2634,    nan],
         [0.4411, 0.3879]],

        [[0.1153,    nan],
         [0.9189, 0.0311]],

        [[0.9935,    nan],
         [0.1216, 0.5348]],

        [[0.2192,    nan],
         [0.4774, 0.9517]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25412.27185338505
Iteration 100: Loss = -9900.959038647872
Iteration 200: Loss = -9898.393046438428
Iteration 300: Loss = -9897.2996178317
Iteration 400: Loss = -9896.645744229103
Iteration 500: Loss = -9896.222600439238
Iteration 600: Loss = -9895.934120445314
Iteration 700: Loss = -9895.72801221416
Iteration 800: Loss = -9895.575545068432
Iteration 900: Loss = -9895.458818572657
Iteration 1000: Loss = -9895.36683435819
Iteration 1100: Loss = -9895.292683800355
Iteration 1200: Loss = -9895.23160392393
Iteration 1300: Loss = -9895.180198924307
Iteration 1400: Loss = -9895.13591060764
Iteration 1500: Loss = -9895.096788132088
Iteration 1600: Loss = -9895.060849825782
Iteration 1700: Loss = -9895.025914701144
Iteration 1800: Loss = -9894.988774582052
Iteration 1900: Loss = -9894.94357044678
Iteration 2000: Loss = -9894.878604182622
Iteration 2100: Loss = -9894.781527212848
Iteration 2200: Loss = -9894.678297452741
Iteration 2300: Loss = -9894.610163431174
Iteration 2400: Loss = -9894.559537231737
Iteration 2500: Loss = -9894.512053694501
Iteration 2600: Loss = -9894.464911231646
Iteration 2700: Loss = -9894.416921633348
Iteration 2800: Loss = -9894.367318344908
Iteration 2900: Loss = -9894.315179137353
Iteration 3000: Loss = -9894.260906607155
Iteration 3100: Loss = -9894.20607810405
Iteration 3200: Loss = -9894.150844195123
Iteration 3300: Loss = -9894.095583870581
Iteration 3400: Loss = -9894.040916525582
Iteration 3500: Loss = -9893.987323514068
Iteration 3600: Loss = -9893.93482461133
Iteration 3700: Loss = -9893.882789153857
Iteration 3800: Loss = -9893.83071969803
Iteration 3900: Loss = -9893.77878464649
Iteration 4000: Loss = -9893.728513241187
Iteration 4100: Loss = -9893.681791006806
Iteration 4200: Loss = -9893.640113631394
Iteration 4300: Loss = -9893.603860171837
Iteration 4400: Loss = -9893.572750285257
Iteration 4500: Loss = -9893.54578571332
Iteration 4600: Loss = -9893.522079619508
Iteration 4700: Loss = -9893.50081858533
Iteration 4800: Loss = -9893.481603597007
Iteration 4900: Loss = -9893.463816056485
Iteration 5000: Loss = -9893.447116315325
Iteration 5100: Loss = -9893.430714432834
Iteration 5200: Loss = -9893.413435208171
Iteration 5300: Loss = -9893.393161826354
Iteration 5400: Loss = -9893.36633847755
Iteration 5500: Loss = -9893.326418915707
Iteration 5600: Loss = -9893.263974989355
Iteration 5700: Loss = -9893.172335397496
Iteration 5800: Loss = -9893.067835045125
Iteration 5900: Loss = -9892.983966800159
Iteration 6000: Loss = -9892.938133191285
Iteration 6100: Loss = -9892.915622742941
Iteration 6200: Loss = -9892.905346744725
Iteration 6300: Loss = -9892.894370807644
Iteration 6400: Loss = -9892.89093427093
Iteration 6500: Loss = -9892.897206426394
1
Iteration 6600: Loss = -9892.888457639925
Iteration 6700: Loss = -9892.888468519168
1
Iteration 6800: Loss = -9892.88756464274
Iteration 6900: Loss = -9892.891883957107
1
Iteration 7000: Loss = -9892.88715553634
Iteration 7100: Loss = -9892.887030725919
Iteration 7200: Loss = -9892.886935274551
Iteration 7300: Loss = -9892.886849273667
Iteration 7400: Loss = -9892.886734630985
Iteration 7500: Loss = -9892.887270988873
1
Iteration 7600: Loss = -9892.886601686287
Iteration 7700: Loss = -9892.886574784336
Iteration 7800: Loss = -9892.892237774264
1
Iteration 7900: Loss = -9892.886457312981
Iteration 8000: Loss = -9892.886399949843
Iteration 8100: Loss = -9892.944357795603
1
Iteration 8200: Loss = -9892.886357539111
Iteration 8300: Loss = -9892.88629320294
Iteration 8400: Loss = -9892.88631170168
1
Iteration 8500: Loss = -9892.88628248718
Iteration 8600: Loss = -9892.88619962796
Iteration 8700: Loss = -9892.886177263466
Iteration 8800: Loss = -9892.886395358777
1
Iteration 8900: Loss = -9892.88610646648
Iteration 9000: Loss = -9892.886049660043
Iteration 9100: Loss = -9892.887602866449
1
Iteration 9200: Loss = -9892.885977318718
Iteration 9300: Loss = -9892.885959062156
Iteration 9400: Loss = -9892.887770717824
1
Iteration 9500: Loss = -9892.885880173899
Iteration 9600: Loss = -9892.885842835576
Iteration 9700: Loss = -9893.06714349308
1
Iteration 9800: Loss = -9892.885743888588
Iteration 9900: Loss = -9892.885740309132
Iteration 10000: Loss = -9892.887988932926
1
Iteration 10100: Loss = -9892.885705105888
Iteration 10200: Loss = -9892.885622991567
Iteration 10300: Loss = -9892.921132783547
1
Iteration 10400: Loss = -9892.885547632064
Iteration 10500: Loss = -9892.885528432036
Iteration 10600: Loss = -9892.88550093072
Iteration 10700: Loss = -9892.88584259395
1
Iteration 10800: Loss = -9892.885449977512
Iteration 10900: Loss = -9892.88539889082
Iteration 11000: Loss = -9892.885408003409
1
Iteration 11100: Loss = -9892.885336596151
Iteration 11200: Loss = -9892.88531339494
Iteration 11300: Loss = -9892.898517481317
1
Iteration 11400: Loss = -9892.885262244059
Iteration 11500: Loss = -9892.885234595082
Iteration 11600: Loss = -9892.885182826845
Iteration 11700: Loss = -9892.88522668803
1
Iteration 11800: Loss = -9892.88513568415
Iteration 11900: Loss = -9892.885125752415
Iteration 12000: Loss = -9892.8910742721
1
Iteration 12100: Loss = -9892.885063692162
Iteration 12200: Loss = -9892.885041510499
Iteration 12300: Loss = -9892.885463671762
1
Iteration 12400: Loss = -9892.88500649367
Iteration 12500: Loss = -9892.884976282025
Iteration 12600: Loss = -9892.903052151918
1
Iteration 12700: Loss = -9892.88494208213
Iteration 12800: Loss = -9892.884920234546
Iteration 12900: Loss = -9892.972260170229
1
Iteration 13000: Loss = -9892.884899492485
Iteration 13100: Loss = -9892.889216814623
1
Iteration 13200: Loss = -9892.884876012433
Iteration 13300: Loss = -9892.884800810854
Iteration 13400: Loss = -9892.885182021042
1
Iteration 13500: Loss = -9892.884805004254
2
Iteration 13600: Loss = -9892.885624516637
3
Iteration 13700: Loss = -9892.884763983791
Iteration 13800: Loss = -9892.884769236922
1
Iteration 13900: Loss = -9892.891716704087
2
Iteration 14000: Loss = -9892.888493459155
3
Iteration 14100: Loss = -9892.884721603688
Iteration 14200: Loss = -9892.884917445408
1
Iteration 14300: Loss = -9892.885369560105
2
Iteration 14400: Loss = -9892.884657647699
Iteration 14500: Loss = -9892.896934830254
1
Iteration 14600: Loss = -9892.884855080136
2
Iteration 14700: Loss = -9892.8846325559
Iteration 14800: Loss = -9892.894449333464
1
Iteration 14900: Loss = -9892.884684914452
2
Iteration 15000: Loss = -9892.884724504614
3
Iteration 15100: Loss = -9893.142068156154
4
Iteration 15200: Loss = -9892.88457414401
Iteration 15300: Loss = -9892.956533545586
1
Iteration 15400: Loss = -9892.884584913085
2
Iteration 15500: Loss = -9892.911056196479
3
Iteration 15600: Loss = -9892.884591202004
4
Iteration 15700: Loss = -9892.902908848982
5
Iteration 15800: Loss = -9892.884554402102
Iteration 15900: Loss = -9892.910327881405
1
Iteration 16000: Loss = -9892.884707536286
2
Iteration 16100: Loss = -9892.959535656264
3
Iteration 16200: Loss = -9892.884555049186
4
Iteration 16300: Loss = -9892.907972765885
5
Iteration 16400: Loss = -9892.884504269326
Iteration 16500: Loss = -9892.88470476756
1
Iteration 16600: Loss = -9892.884505848851
2
Iteration 16700: Loss = -9892.885026873399
3
Iteration 16800: Loss = -9892.884601351043
4
Iteration 16900: Loss = -9893.098687705062
5
Iteration 17000: Loss = -9892.884529483734
6
Iteration 17100: Loss = -9892.885313548182
7
Iteration 17200: Loss = -9893.008353523046
8
Iteration 17300: Loss = -9892.884984412936
9
Iteration 17400: Loss = -9892.884563036047
10
Stopping early at iteration 17400 due to no improvement.
tensor([[ 2.7926, -4.1790],
        [ 2.7360, -4.1624],
        [ 1.7998, -4.1830],
        [ 2.3897, -3.7925],
        [ 0.6137, -3.3485],
        [ 3.6681, -5.0588],
        [ 1.9293, -4.6213],
        [ 2.5699, -4.0309],
        [ 2.7037, -4.2516],
        [ 0.6082, -3.3909],
        [ 0.9192, -5.5344],
        [ 1.3023, -3.4053],
        [ 2.4784, -4.7109],
        [ 0.4986, -3.4477],
        [ 2.7285, -4.1307],
        [ 3.4866, -5.3243],
        [ 3.7995, -5.6120],
        [ 3.0042, -4.4036],
        [ 2.6782, -4.3280],
        [ 2.2196, -3.6124],
        [ 2.2541, -4.0326],
        [ 2.7574, -4.2339],
        [ 0.9904, -3.5202],
        [ 2.0864, -4.5355],
        [ 2.2019, -4.8045],
        [-0.0223, -2.9310],
        [ 1.4392, -2.9405],
        [-0.3964, -3.7474],
        [ 0.5039, -2.3382],
        [ 1.7506, -3.2271],
        [ 2.0674, -4.8908],
        [ 0.6037, -3.9290],
        [ 2.3078, -4.5986],
        [ 2.0584, -3.4456],
        [ 1.5623, -3.0172],
        [ 2.0792, -3.4711],
        [ 2.1098, -3.6892],
        [ 1.6970, -4.1344],
        [ 2.2323, -4.2458],
        [ 2.4686, -3.9743],
        [ 1.8559, -4.2823],
        [ 2.9014, -4.2889],
        [ 1.7991, -3.5954],
        [ 2.2973, -3.7893],
        [ 1.6645, -3.6998],
        [ 2.4446, -3.9700],
        [ 2.4442, -4.0305],
        [ 1.6932, -3.2877],
        [ 1.1513, -3.5570],
        [ 1.8622, -3.3946],
        [ 2.9246, -4.8811],
        [ 0.9624, -3.9488],
        [-0.1027, -1.8167],
        [ 1.7646, -3.4792],
        [ 2.9008, -4.2937],
        [ 0.3324, -2.8070],
        [ 2.5984, -5.4194],
        [ 2.0196, -3.4428],
        [ 1.1465, -5.7618],
        [ 3.0938, -4.7483],
        [ 1.7818, -3.6885],
        [ 2.9996, -4.7418],
        [ 1.7853, -3.9319],
        [ 2.8525, -4.9668],
        [ 0.8431, -2.3744],
        [ 1.9483, -3.4006],
        [ 0.2367, -2.0657],
        [ 1.3426, -4.3577],
        [ 2.9378, -4.4862],
        [ 2.7963, -4.3276],
        [ 1.2589, -3.1018],
        [ 2.4362, -3.8420],
        [ 2.2131, -3.9565],
        [ 3.3403, -4.7349],
        [ 0.0911, -4.7064],
        [ 1.7438, -3.8413],
        [ 1.9802, -3.5073],
        [-0.0284, -1.8372],
        [ 3.8918, -5.3203],
        [ 1.2229, -3.4947],
        [ 0.3041, -1.6925],
        [ 2.9244, -4.3111],
        [ 3.3728, -5.5893],
        [ 1.7622, -3.4066],
        [ 2.7813, -4.7756],
        [ 1.3112, -5.9264],
        [ 2.1799, -3.6411],
        [ 0.4898, -2.6228],
        [ 2.2301, -4.3112],
        [ 1.8551, -3.4543],
        [ 1.3090, -3.1107],
        [ 2.9144, -4.3024],
        [ 3.8661, -5.3653],
        [ 1.4898, -3.9459],
        [ 2.2798, -4.2499],
        [ 2.3534, -4.6986],
        [ 2.0637, -3.7344],
        [ 1.8244, -3.2396],
        [ 0.0122, -4.6274],
        [ 4.1138, -5.5044]], dtype=torch.float64, requires_grad=True)
pi: tensor([[8.9680e-01, 1.0320e-01],
        [9.9944e-01, 5.6468e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9890, 0.0110], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1399, 0.2027],
         [0.4806, 0.1043]],

        [[0.2634, 0.1072],
         [0.4411, 0.3879]],

        [[0.1153, 0.1192],
         [0.9189, 0.0311]],

        [[0.9935, 0.1077],
         [0.1216, 0.5348]],

        [[0.2192, 0.1485],
         [0.4774, 0.9517]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -25835.42970930102
Iteration 10: Loss = -9895.146950589055
Iteration 20: Loss = -9895.147043247916
1
Iteration 30: Loss = -9895.146947916433
Iteration 40: Loss = -9895.146852229735
Iteration 50: Loss = -9895.146407230419
Iteration 60: Loss = -9895.143717029678
Iteration 70: Loss = -9895.125752066433
Iteration 80: Loss = -9895.02340261617
Iteration 90: Loss = -9894.713333877042
Iteration 100: Loss = -9894.42562031739
Iteration 110: Loss = -9894.309967791845
Iteration 120: Loss = -9894.266557133966
Iteration 130: Loss = -9894.247015032975
Iteration 140: Loss = -9894.236547453638
Iteration 150: Loss = -9894.230267551595
Iteration 160: Loss = -9894.226119140892
Iteration 170: Loss = -9894.22322928982
Iteration 180: Loss = -9894.221104297472
Iteration 190: Loss = -9894.21950161954
Iteration 200: Loss = -9894.218230503713
Iteration 210: Loss = -9894.217216733277
Iteration 220: Loss = -9894.21641860279
Iteration 230: Loss = -9894.215724020642
Iteration 240: Loss = -9894.215172457974
Iteration 250: Loss = -9894.214689841157
Iteration 260: Loss = -9894.214284844382
Iteration 270: Loss = -9894.213987040905
Iteration 280: Loss = -9894.213616851124
Iteration 290: Loss = -9894.213411819492
Iteration 300: Loss = -9894.213149609272
Iteration 310: Loss = -9894.21296577209
Iteration 320: Loss = -9894.212782253144
Iteration 330: Loss = -9894.21265360027
Iteration 340: Loss = -9894.212504262001
Iteration 350: Loss = -9894.212417036832
Iteration 360: Loss = -9894.21228580033
Iteration 370: Loss = -9894.212189047608
Iteration 380: Loss = -9894.212108719465
Iteration 390: Loss = -9894.21203550084
Iteration 400: Loss = -9894.211983507856
Iteration 410: Loss = -9894.211911478606
Iteration 420: Loss = -9894.211848222942
Iteration 430: Loss = -9894.211788164523
Iteration 440: Loss = -9894.211758847547
Iteration 450: Loss = -9894.211697489445
Iteration 460: Loss = -9894.211658856379
Iteration 470: Loss = -9894.211636723046
Iteration 480: Loss = -9894.211627952785
Iteration 490: Loss = -9894.211571344402
Iteration 500: Loss = -9894.211570147416
Iteration 510: Loss = -9894.211535354854
Iteration 520: Loss = -9894.211490520214
Iteration 530: Loss = -9894.211522665664
1
Iteration 540: Loss = -9894.211493661749
2
Iteration 550: Loss = -9894.211481583263
Iteration 560: Loss = -9894.211446965524
Iteration 570: Loss = -9894.21145288768
1
Iteration 580: Loss = -9894.211442849024
Iteration 590: Loss = -9894.211428572467
Iteration 600: Loss = -9894.211424831981
Iteration 610: Loss = -9894.21144335625
1
Iteration 620: Loss = -9894.211424242747
Iteration 630: Loss = -9894.211396307282
Iteration 640: Loss = -9894.211393814816
Iteration 650: Loss = -9894.211416109862
1
Iteration 660: Loss = -9894.211374702807
Iteration 670: Loss = -9894.211398820586
1
Iteration 680: Loss = -9894.211405812592
2
Iteration 690: Loss = -9894.211379814009
3
Stopping early at iteration 689 due to no improvement.
pi: tensor([[1.6199e-13, 1.0000e+00],
        [1.6491e-02, 9.8351e-01]], dtype=torch.float64)
alpha: tensor([0.0169, 0.9831])
beta: tensor([[[0.1872, 0.2015],
         [0.0525, 0.1352]],

        [[0.1053, 0.0928],
         [0.7651, 0.6888]],

        [[0.0188, 0.0846],
         [0.8015, 0.1598]],

        [[0.3148, 0.1821],
         [0.6556, 0.0331]],

        [[0.0163, 0.1776],
         [0.8295, 0.9215]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25835.553226493987
Iteration 100: Loss = -9911.188095110978
Iteration 200: Loss = -9901.143032696431
Iteration 300: Loss = -9898.329652918685
Iteration 400: Loss = -9897.055105842886
Iteration 500: Loss = -9896.349894656267
Iteration 600: Loss = -9895.910656266149
Iteration 700: Loss = -9895.617276597834
Iteration 800: Loss = -9895.410116999165
Iteration 900: Loss = -9895.254139873377
Iteration 1000: Loss = -9895.132637350454
Iteration 1100: Loss = -9895.035117905654
Iteration 1200: Loss = -9894.954625979011
Iteration 1300: Loss = -9894.886317281913
Iteration 1400: Loss = -9894.825689261637
Iteration 1500: Loss = -9894.770039391366
Iteration 1600: Loss = -9894.719016973957
Iteration 1700: Loss = -9894.669501681425
Iteration 1800: Loss = -9894.618832314382
Iteration 1900: Loss = -9894.590041518379
Iteration 2000: Loss = -9894.498367593154
Iteration 2100: Loss = -9894.417299472758
Iteration 2200: Loss = -9894.31309658213
Iteration 2300: Loss = -9894.192976510358
Iteration 2400: Loss = -9894.077487908537
Iteration 2500: Loss = -9893.974345966548
Iteration 2600: Loss = -9893.884392717928
Iteration 2700: Loss = -9893.781690925152
Iteration 2800: Loss = -9893.681483927921
Iteration 2900: Loss = -9893.565741538814
Iteration 3000: Loss = -9893.428457985301
Iteration 3100: Loss = -9893.283416052032
Iteration 3200: Loss = -9893.160293955922
Iteration 3300: Loss = -9893.068704363715
Iteration 3400: Loss = -9893.014491099158
Iteration 3500: Loss = -9892.983858988286
Iteration 3600: Loss = -9892.965875450573
Iteration 3700: Loss = -9892.954470870452
Iteration 3800: Loss = -9892.946724672956
Iteration 3900: Loss = -9892.941415355615
Iteration 4000: Loss = -9892.937317762066
Iteration 4100: Loss = -9892.937756823065
1
Iteration 4200: Loss = -9892.93048716212
Iteration 4300: Loss = -9892.927203998892
Iteration 4400: Loss = -9892.924431532965
Iteration 4500: Loss = -9892.921579411966
Iteration 4600: Loss = -9892.918923766556
Iteration 4700: Loss = -9892.916316738056
Iteration 4800: Loss = -9892.914935114564
Iteration 4900: Loss = -9892.911072873416
Iteration 5000: Loss = -9892.908232696627
Iteration 5100: Loss = -9892.92978694822
1
Iteration 5200: Loss = -9892.90233260353
Iteration 5300: Loss = -9892.900171802767
Iteration 5400: Loss = -9892.898511524587
Iteration 5500: Loss = -9892.897053827262
Iteration 5600: Loss = -9892.895689720435
Iteration 5700: Loss = -9892.894531146992
Iteration 5800: Loss = -9892.895098375511
1
Iteration 5900: Loss = -9892.892526931833
Iteration 6000: Loss = -9892.891629491147
Iteration 6100: Loss = -9892.892308316555
1
Iteration 6200: Loss = -9892.890137848819
Iteration 6300: Loss = -9892.88953614157
Iteration 6400: Loss = -9893.021825048656
1
Iteration 6500: Loss = -9892.888520037413
Iteration 6600: Loss = -9892.888110643145
Iteration 6700: Loss = -9892.88772817883
Iteration 6800: Loss = -9892.887461902357
Iteration 6900: Loss = -9892.887160680442
Iteration 7000: Loss = -9892.886953738704
Iteration 7100: Loss = -9892.88723904764
1
Iteration 7200: Loss = -9892.886595523341
Iteration 7300: Loss = -9892.886471126
Iteration 7400: Loss = -9892.88646203344
Iteration 7500: Loss = -9892.886230553006
Iteration 7600: Loss = -9892.886140530762
Iteration 7700: Loss = -9892.934035793967
1
Iteration 7800: Loss = -9892.885991136383
Iteration 7900: Loss = -9892.885907588381
Iteration 8000: Loss = -9893.099372132627
1
Iteration 8100: Loss = -9892.885827039412
Iteration 8200: Loss = -9892.885766845297
Iteration 8300: Loss = -9893.33363253716
1
Iteration 8400: Loss = -9892.885686642487
Iteration 8500: Loss = -9892.885611762926
Iteration 8600: Loss = -9892.91691826401
1
Iteration 8700: Loss = -9892.885547727343
Iteration 8800: Loss = -9892.885510983318
Iteration 8900: Loss = -9892.973545398358
1
Iteration 9000: Loss = -9892.885470553716
Iteration 9100: Loss = -9892.885386562974
Iteration 9200: Loss = -9892.89765306243
1
Iteration 9300: Loss = -9892.885373269808
Iteration 9400: Loss = -9892.885315602052
Iteration 9500: Loss = -9892.885506513294
1
Iteration 9600: Loss = -9892.885284417289
Iteration 9700: Loss = -9892.885223557558
Iteration 9800: Loss = -9892.885187533071
Iteration 9900: Loss = -9892.885509822523
1
Iteration 10000: Loss = -9892.885152509236
Iteration 10100: Loss = -9892.885123893177
Iteration 10200: Loss = -9892.886837996
1
Iteration 10300: Loss = -9892.885060234514
Iteration 10400: Loss = -9892.885045836281
Iteration 10500: Loss = -9892.89855778235
1
Iteration 10600: Loss = -9892.885012302164
Iteration 10700: Loss = -9892.884993215175
Iteration 10800: Loss = -9892.949591457962
1
Iteration 10900: Loss = -9892.884915203607
Iteration 11000: Loss = -9892.884946694614
1
Iteration 11100: Loss = -9892.889814646587
2
Iteration 11200: Loss = -9892.88483978044
Iteration 11300: Loss = -9893.046364604299
1
Iteration 11400: Loss = -9892.88483887555
Iteration 11500: Loss = -9892.884822467176
Iteration 11600: Loss = -9892.884852683548
1
Iteration 11700: Loss = -9892.884766170755
Iteration 11800: Loss = -9893.00923991262
1
Iteration 11900: Loss = -9892.884725884953
Iteration 12000: Loss = -9892.884737958673
1
Iteration 12100: Loss = -9892.888560867086
2
Iteration 12200: Loss = -9892.885240377827
3
Iteration 12300: Loss = -9892.88485880776
4
Iteration 12400: Loss = -9892.90773290418
5
Iteration 12500: Loss = -9892.88816043988
6
Iteration 12600: Loss = -9892.884685445648
Iteration 12700: Loss = -9892.898596108622
1
Iteration 12800: Loss = -9892.884690082075
2
Iteration 12900: Loss = -9892.88463368516
Iteration 13000: Loss = -9892.890278423842
1
Iteration 13100: Loss = -9892.884633357988
Iteration 13200: Loss = -9892.884728181083
1
Iteration 13300: Loss = -9893.013717923946
2
Iteration 13400: Loss = -9892.884617109748
Iteration 13500: Loss = -9892.885214651158
1
Iteration 13600: Loss = -9892.884581114104
Iteration 13700: Loss = -9892.885243982382
1
Iteration 13800: Loss = -9892.884695871338
2
Iteration 13900: Loss = -9892.884566096976
Iteration 14000: Loss = -9892.88457727025
1
Iteration 14100: Loss = -9892.884772314603
2
Iteration 14200: Loss = -9892.913700083975
3
Iteration 14300: Loss = -9892.884504052712
Iteration 14400: Loss = -9892.88915781873
1
Iteration 14500: Loss = -9892.884536876463
2
Iteration 14600: Loss = -9892.884747790495
3
Iteration 14700: Loss = -9892.885103677107
4
Iteration 14800: Loss = -9893.027039234195
5
Iteration 14900: Loss = -9892.884500675274
Iteration 15000: Loss = -9892.887146968833
1
Iteration 15100: Loss = -9892.884485385466
Iteration 15200: Loss = -9892.885105921345
1
Iteration 15300: Loss = -9892.884475587349
Iteration 15400: Loss = -9892.887019465476
1
Iteration 15500: Loss = -9892.885777105694
2
Iteration 15600: Loss = -9892.885427259782
3
Iteration 15700: Loss = -9892.885877931274
4
Iteration 15800: Loss = -9892.886943917421
5
Iteration 15900: Loss = -9892.885174673376
6
Iteration 16000: Loss = -9892.884461736143
Iteration 16100: Loss = -9892.884775252272
1
Iteration 16200: Loss = -9892.893491813213
2
Iteration 16300: Loss = -9892.88444637203
Iteration 16400: Loss = -9892.885409618588
1
Iteration 16500: Loss = -9892.888070092617
2
Iteration 16600: Loss = -9892.884441104678
Iteration 16700: Loss = -9892.904149043814
1
Iteration 16800: Loss = -9892.884451287333
2
Iteration 16900: Loss = -9892.885162935832
3
Iteration 17000: Loss = -9892.884412884692
Iteration 17100: Loss = -9892.884595010604
1
Iteration 17200: Loss = -9892.884485061437
2
Iteration 17300: Loss = -9892.884483600681
3
Iteration 17400: Loss = -9892.884666381822
4
Iteration 17500: Loss = -9892.884472931923
5
Iteration 17600: Loss = -9892.887777863418
6
Iteration 17700: Loss = -9892.884452169521
7
Iteration 17800: Loss = -9892.884452256554
8
Iteration 17900: Loss = -9892.884541190158
9
Iteration 18000: Loss = -9892.884419333957
10
Stopping early at iteration 18000 due to no improvement.
tensor([[-5.7930,  1.1778],
        [-4.2358,  2.6634],
        [-3.7722,  2.2108],
        [-4.0971,  2.0848],
        [-3.2648,  0.6965],
        [-5.4242,  3.3009],
        [-4.2566,  2.2931],
        [-4.0035,  2.5975],
        [-4.2489,  2.7059],
        [-2.9556,  1.0428],
        [-3.9201,  2.5324],
        [-3.7759,  0.9311],
        [-4.3339,  2.8556],
        [-2.6660,  1.2795],
        [-4.6738,  2.1850],
        [-5.1594,  3.6522],
        [-5.9979,  3.4144],
        [-4.4370,  2.9701],
        [-4.2104,  2.7956],
        [-3.8487,  1.9833],
        [-3.8974,  2.3889],
        [-5.0824,  1.9088],
        [-3.5327,  0.9768],
        [-4.3259,  2.2961],
        [-4.2002,  2.8055],
        [-2.4949,  0.4130],
        [-3.2591,  1.1196],
        [-3.6839, -0.3329],
        [-2.9767, -0.1350],
        [-3.1816,  1.7953],
        [-4.7766,  2.1814],
        [-2.9836,  1.5480],
        [-4.2365,  2.6695],
        [-4.0302,  1.4743],
        [-3.1673,  1.4113],
        [-3.4793,  2.0713],
        [-3.9655,  1.8335],
        [-4.0723,  1.7589],
        [-4.2811,  2.1964],
        [-5.4355,  1.0082],
        [-3.8039,  2.3341],
        [-4.6706,  2.5210],
        [-3.6804,  1.7139],
        [-3.8709,  2.2155],
        [-3.6707,  1.6922],
        [-4.7483,  1.6671],
        [-4.2631,  2.2121],
        [-3.1856,  1.7949],
        [-3.1662,  1.5415],
        [-4.0763,  1.1803],
        [-5.6809,  2.1244],
        [-3.1485,  1.7622],
        [-1.5636,  0.1496],
        [-3.5629,  1.6802],
        [-5.6431,  1.5512],
        [-2.2633,  0.8770],
        [-4.9193,  3.0985],
        [-3.4247,  2.0371],
        [-5.1625,  1.7450],
        [-5.5148,  2.3270],
        [-3.4300,  2.0394],
        [-4.8321,  2.9090],
        [-3.6207,  2.0960],
        [-4.7345,  3.0846],
        [-2.8085,  0.4083],
        [-3.4351,  1.9129],
        [-3.4238, -1.1220],
        [-3.9901,  1.7093],
        [-4.8362,  2.5869],
        [-4.2598,  2.8638],
        [-2.9174,  1.4425],
        [-3.8356,  2.4430],
        [-3.9847,  2.1847],
        [-4.8330,  3.2430],
        [-3.1693,  1.6266],
        [-4.0120,  1.5731],
        [-3.5248,  1.9629],
        [-1.7663,  0.0419],
        [-5.2994,  3.9131],
        [-3.0627,  1.6533],
        [-2.5978, -0.6017],
        [-4.5439,  2.6915],
        [-5.4181,  3.5450],
        [-3.2789,  1.8916],
        [-5.0084,  2.5486],
        [-4.3370,  2.8998],
        [-3.6478,  2.1730],
        [-2.7415,  0.3703],
        [-4.6654,  1.8761],
        [-3.6747,  1.6341],
        [-2.9073,  1.5113],
        [-4.8986,  2.3188],
        [-5.4692,  3.7605],
        [-3.4107,  2.0241],
        [-5.4130,  1.1168],
        [-5.5930,  1.4595],
        [-3.5952,  2.2024],
        [-3.2770,  1.7853],
        [-3.6628,  0.9754],
        [-5.7392,  3.8783]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.8164e-04, 9.9982e-01],
        [1.0325e-01, 8.9675e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0110, 0.9890], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1043, 0.2028],
         [0.0525, 0.1399]],

        [[0.1053, 0.1072],
         [0.7651, 0.6888]],

        [[0.0188, 0.1191],
         [0.8015, 0.1598]],

        [[0.3148, 0.1077],
         [0.6556, 0.0331]],

        [[0.0163, 0.1484],
         [0.8295, 0.9215]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
10006.830704534359
new:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [9892.884466847163, 9893.068095346765, 9892.884563036047, 9892.884419333957]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [9894.211439453547, 9893.511316528153, nan, 9894.211379814009]
-----------------------------------------------------------------------------------------
This iteration is 11
True Objective function: Loss = -9887.677794698497
Iteration 0: Loss = -12626.999959867218
Iteration 10: Loss = -9780.044043019105
Iteration 20: Loss = -9776.86909130297
Iteration 30: Loss = -9776.511753016348
Iteration 40: Loss = -9776.44265156999
Iteration 50: Loss = -9776.415325442715
Iteration 60: Loss = -9776.384569352287
Iteration 70: Loss = -9776.376454871353
Iteration 80: Loss = -9776.377799060823
1
Iteration 90: Loss = -9776.378914337894
2
Iteration 100: Loss = -9776.379408167948
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.0217, 0.9783],
        [0.0298, 0.9702]], dtype=torch.float64)
alpha: tensor([0.0316, 0.9684])
beta: tensor([[[0.1175, 0.0673],
         [0.2613, 0.1322]],

        [[0.9666, 0.1054],
         [0.8764, 0.5364]],

        [[0.7105, 0.1955],
         [0.2230, 0.4176]],

        [[0.2969, 0.2007],
         [0.0642, 0.6090]],

        [[0.7272, 0.2198],
         [0.6335, 0.6005]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0012104823609284674
Average Adjusted Rand Index: -0.001677317058922544
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -12900.820040409382
Iteration 100: Loss = -9782.52169240801
Iteration 200: Loss = -9780.585603184038
Iteration 300: Loss = -9777.578766999437
Iteration 400: Loss = -9775.883932653871
Iteration 500: Loss = -9775.650853777175
Iteration 600: Loss = -9775.548990409497
Iteration 700: Loss = -9775.480610410259
Iteration 800: Loss = -9775.429865710768
Iteration 900: Loss = -9775.393305729405
Iteration 1000: Loss = -9775.365916464152
Iteration 1100: Loss = -9775.344186495226
Iteration 1200: Loss = -9775.326395675098
Iteration 1300: Loss = -9775.311253359625
Iteration 1400: Loss = -9775.297453680385
Iteration 1500: Loss = -9775.28275235619
Iteration 1600: Loss = -9775.255859119225
Iteration 1700: Loss = -9775.162625976925
Iteration 1800: Loss = -9775.124964896286
Iteration 1900: Loss = -9775.118462616114
Iteration 2000: Loss = -9775.114260308472
Iteration 2100: Loss = -9775.111081336343
Iteration 2200: Loss = -9775.108561725063
Iteration 2300: Loss = -9775.106406447314
Iteration 2400: Loss = -9775.104514296107
Iteration 2500: Loss = -9775.102871972409
Iteration 2600: Loss = -9775.101542580373
Iteration 2700: Loss = -9775.100510435646
Iteration 2800: Loss = -9775.099649205842
Iteration 2900: Loss = -9775.098863274052
Iteration 3000: Loss = -9775.098210064885
Iteration 3100: Loss = -9775.097578171511
Iteration 3200: Loss = -9775.097011286987
Iteration 3300: Loss = -9775.096520476995
Iteration 3400: Loss = -9775.096033543223
Iteration 3500: Loss = -9775.095580696448
Iteration 3600: Loss = -9775.095175258151
Iteration 3700: Loss = -9775.094769072906
Iteration 3800: Loss = -9775.094425775884
Iteration 3900: Loss = -9775.094076921436
Iteration 4000: Loss = -9775.093756616985
Iteration 4100: Loss = -9775.093493785756
Iteration 4200: Loss = -9775.093195490515
Iteration 4300: Loss = -9775.092907255428
Iteration 4400: Loss = -9775.092672115441
Iteration 4500: Loss = -9775.092407751828
Iteration 4600: Loss = -9775.092216015806
Iteration 4700: Loss = -9775.092006791203
Iteration 4800: Loss = -9775.091784720844
Iteration 4900: Loss = -9775.093293419173
1
Iteration 5000: Loss = -9775.091899654162
2
Iteration 5100: Loss = -9775.092140336013
3
Iteration 5200: Loss = -9775.091167934233
Iteration 5300: Loss = -9775.090930771305
Iteration 5400: Loss = -9775.121843809906
1
Iteration 5500: Loss = -9775.09062648687
Iteration 5600: Loss = -9775.121259996822
1
Iteration 5700: Loss = -9775.090360044962
Iteration 5800: Loss = -9775.09026049301
Iteration 5900: Loss = -9775.091020686938
1
Iteration 6000: Loss = -9775.090011310594
Iteration 6100: Loss = -9775.089931984252
Iteration 6200: Loss = -9775.093765789234
1
Iteration 6300: Loss = -9775.08971452664
Iteration 6400: Loss = -9775.089650329377
Iteration 6500: Loss = -9775.089565111912
Iteration 6600: Loss = -9775.123862472254
1
Iteration 6700: Loss = -9775.089411712162
Iteration 6800: Loss = -9775.089350608343
Iteration 6900: Loss = -9775.159463131378
1
Iteration 7000: Loss = -9775.089203267005
Iteration 7100: Loss = -9775.089141206825
Iteration 7200: Loss = -9775.089111179175
Iteration 7300: Loss = -9775.089172992428
1
Iteration 7400: Loss = -9775.08902138978
Iteration 7500: Loss = -9775.088930804084
Iteration 7600: Loss = -9775.09924338359
1
Iteration 7700: Loss = -9775.08888482076
Iteration 7800: Loss = -9775.088823215745
Iteration 7900: Loss = -9775.08879104174
Iteration 8000: Loss = -9775.08887613634
1
Iteration 8100: Loss = -9775.088702432009
Iteration 8200: Loss = -9775.088697894791
Iteration 8300: Loss = -9775.088652177898
Iteration 8400: Loss = -9775.08871061191
1
Iteration 8500: Loss = -9775.0885996824
Iteration 8600: Loss = -9775.088560913971
Iteration 8700: Loss = -9775.137218075504
1
Iteration 8800: Loss = -9775.088535604229
Iteration 8900: Loss = -9775.088511616745
Iteration 9000: Loss = -9775.08849762977
Iteration 9100: Loss = -9775.088447058779
Iteration 9200: Loss = -9775.088439452698
Iteration 9300: Loss = -9775.088432056464
Iteration 9400: Loss = -9775.088661643564
1
Iteration 9500: Loss = -9775.088410551381
Iteration 9600: Loss = -9775.088397324842
Iteration 9700: Loss = -9775.088366064789
Iteration 9800: Loss = -9775.08850961118
1
Iteration 9900: Loss = -9775.088293634182
Iteration 10000: Loss = -9775.088296582326
1
Iteration 10100: Loss = -9775.088339123908
2
Iteration 10200: Loss = -9775.089349590107
3
Iteration 10300: Loss = -9775.08829061746
Iteration 10400: Loss = -9775.088288862313
Iteration 10500: Loss = -9775.090485204704
1
Iteration 10600: Loss = -9775.08826980455
Iteration 10700: Loss = -9775.088357317396
1
Iteration 10800: Loss = -9775.089656537055
2
Iteration 10900: Loss = -9775.088208844192
Iteration 11000: Loss = -9775.088397381778
1
Iteration 11100: Loss = -9775.088235319752
2
Iteration 11200: Loss = -9775.089821180407
3
Iteration 11300: Loss = -9775.088217663788
4
Iteration 11400: Loss = -9775.088224553907
5
Iteration 11500: Loss = -9775.090924602928
6
Iteration 11600: Loss = -9775.088198451622
Iteration 11700: Loss = -9775.105181546096
1
Iteration 11800: Loss = -9775.088185107945
Iteration 11900: Loss = -9775.089225365226
1
Iteration 12000: Loss = -9775.088167360487
Iteration 12100: Loss = -9775.090575915818
1
Iteration 12200: Loss = -9775.088253305614
2
Iteration 12300: Loss = -9775.088141963834
Iteration 12400: Loss = -9775.090602819768
1
Iteration 12500: Loss = -9775.088149766898
2
Iteration 12600: Loss = -9775.088176317204
3
Iteration 12700: Loss = -9775.08819707601
4
Iteration 12800: Loss = -9775.088167130858
5
Iteration 12900: Loss = -9775.153506880466
6
Iteration 13000: Loss = -9775.08813947899
Iteration 13100: Loss = -9775.092999229655
1
Iteration 13200: Loss = -9775.088308212238
2
Iteration 13300: Loss = -9775.088414759079
3
Iteration 13400: Loss = -9775.088188264092
4
Iteration 13500: Loss = -9775.088141506176
5
Iteration 13600: Loss = -9775.088241338803
6
Iteration 13700: Loss = -9775.088153145412
7
Iteration 13800: Loss = -9775.089599867379
8
Iteration 13900: Loss = -9775.088113273843
Iteration 14000: Loss = -9775.088218957602
1
Iteration 14100: Loss = -9775.088221064638
2
Iteration 14200: Loss = -9775.088861996275
3
Iteration 14300: Loss = -9775.088136676164
4
Iteration 14400: Loss = -9775.08886386046
5
Iteration 14500: Loss = -9775.08811357307
6
Iteration 14600: Loss = -9775.089296707987
7
Iteration 14700: Loss = -9775.181014544509
8
Iteration 14800: Loss = -9775.088130412285
9
Iteration 14900: Loss = -9775.09622662321
10
Stopping early at iteration 14900 due to no improvement.
tensor([[-10.1939,   5.5787],
        [-10.4020,   5.7868],
        [ -7.8432,   3.2280],
        [-10.0701,   5.4549],
        [-10.5643,   5.9491],
        [ -6.6136,   1.9984],
        [ -7.0843,   2.4690],
        [-10.1139,   5.4987],
        [ -7.5485,   2.9332],
        [ -9.7754,   5.1602],
        [ -9.5751,   4.9599],
        [ -8.6568,   4.0416],
        [ -8.1829,   3.5677],
        [ -5.8614,   1.2462],
        [-10.3612,   5.7460],
        [-10.5222,   5.9070],
        [ -6.9472,   2.3320],
        [-10.7490,   6.1337],
        [-10.4998,   5.8846],
        [ -9.5645,   4.9493],
        [-10.7659,   6.1507],
        [ -9.9858,   5.3706],
        [ -9.4436,   4.8284],
        [ -5.8087,   1.1935],
        [ -9.1940,   4.5788],
        [-10.5488,   5.9336],
        [ -9.3405,   4.7253],
        [ -9.2183,   4.6031],
        [-10.4201,   5.8048],
        [-10.0788,   5.4635],
        [ -7.9950,   3.3798],
        [-10.1164,   5.5012],
        [ -9.3529,   4.7377],
        [ -8.1080,   3.4928],
        [ -9.4616,   4.8464],
        [ -7.2557,   2.6405],
        [-10.4153,   5.8001],
        [-10.3932,   5.7780],
        [ -8.2402,   3.6250],
        [ -9.8401,   5.2249],
        [ -7.3228,   2.7075],
        [-10.6602,   6.0450],
        [ -8.8083,   4.1931],
        [ -8.0236,   3.4084],
        [ -8.6998,   4.0845],
        [ -9.3201,   4.7049],
        [-10.1540,   5.5388],
        [-10.7639,   6.1487],
        [ -9.2705,   4.6553],
        [ -7.8049,   3.1896],
        [ -5.2714,   0.6562],
        [ -7.3818,   2.7666],
        [ -8.0063,   3.3911],
        [ -8.4948,   3.8796],
        [ -6.0630,   1.4478],
        [ -9.8435,   5.2283],
        [ -9.7646,   5.1494],
        [-10.6482,   6.0330],
        [ -8.9335,   4.3183],
        [ -9.1499,   4.5347],
        [ -5.6734,   1.0582],
        [ -2.2534,  -2.3618],
        [ -5.9527,   1.3375],
        [ -8.2358,   3.6206],
        [-10.1039,   5.4887],
        [ -9.9066,   5.2914],
        [-10.3747,   5.7595],
        [ -7.2163,   2.6011],
        [ -8.1196,   3.5044],
        [-10.3333,   5.7181],
        [-10.1864,   5.5712],
        [ -7.2881,   2.6728],
        [ -9.4040,   4.7887],
        [ -8.1129,   3.4977],
        [ -9.7960,   5.1808],
        [ -6.4864,   1.8712],
        [-10.7300,   6.1148],
        [ -8.8811,   4.2659],
        [ -7.9155,   3.3003],
        [ -7.0370,   2.4218],
        [ -6.4372,   1.8220],
        [-10.6260,   6.0108],
        [ -6.6043,   1.9891],
        [-10.4549,   5.8397],
        [ -6.9364,   2.3212],
        [-10.8546,   6.2394],
        [ -8.4546,   3.8394],
        [ -9.4232,   4.8080],
        [ -7.7273,   3.1121],
        [ -9.2485,   4.6333],
        [-10.6656,   6.0504],
        [-10.0184,   5.4031],
        [ -5.8652,   1.2500],
        [ -9.8825,   5.2673],
        [ -9.9910,   5.3758],
        [ -6.1322,   1.5169],
        [ -8.9683,   4.3531],
        [ -9.2782,   4.6630],
        [ -7.9874,   3.3721],
        [ -8.8141,   4.1988]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.6926e-06],
        [7.1880e-03, 9.9281e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0054, 0.9946], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0673, 0.1012],
         [0.2613, 0.1336]],

        [[0.9666, 0.0617],
         [0.8764, 0.5364]],

        [[0.7105, 0.2319],
         [0.2230, 0.4176]],

        [[0.2969, 0.1203],
         [0.0642, 0.6090]],

        [[0.7272, 0.2157],
         [0.6335, 0.6005]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 38
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
Global Adjusted Rand Index: 0.0014877377823435802
Average Adjusted Rand Index: -0.0005880702676060405
Iteration 0: Loss = -20430.882514985256
Iteration 10: Loss = -9777.974212190871
Iteration 20: Loss = -9777.924581116586
Iteration 30: Loss = -9777.920192837857
Iteration 40: Loss = -9777.922509333228
1
Iteration 50: Loss = -9777.924436828524
2
Iteration 60: Loss = -9777.924527501224
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7919, 0.2081],
        [0.7961, 0.2039]], dtype=torch.float64)
alpha: tensor([0.7950, 0.2050])
beta: tensor([[[0.1232, 0.1392],
         [0.3611, 0.1737]],

        [[0.2125, 0.1365],
         [0.2176, 0.7611]],

        [[0.1703, 0.1546],
         [0.5382, 0.1837]],

        [[0.5783, 0.1500],
         [0.1355, 0.0502]],

        [[0.8764, 0.1571],
         [0.1117, 0.5494]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0010162821372197116
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.001684040076915292
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.00485601903559462
Global Adjusted Rand Index: 0.007711719344541677
Average Adjusted Rand Index: 0.0015112682499459247
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20430.712733882072
Iteration 100: Loss = -9834.981137531453
Iteration 200: Loss = -9790.571815188676
Iteration 300: Loss = -9782.201314215808
Iteration 400: Loss = -9780.57451157946
Iteration 500: Loss = -9779.676130796712
Iteration 600: Loss = -9779.18468606795
Iteration 700: Loss = -9778.856003793806
Iteration 800: Loss = -9778.57479019088
Iteration 900: Loss = -9778.349880750526
Iteration 1000: Loss = -9778.19214232749
Iteration 1100: Loss = -9778.086734456047
Iteration 1200: Loss = -9778.016592779453
Iteration 1300: Loss = -9777.969388686495
Iteration 1400: Loss = -9777.936115293836
Iteration 1500: Loss = -9777.91197898333
Iteration 1600: Loss = -9777.897080096795
Iteration 1700: Loss = -9777.863276174668
Iteration 1800: Loss = -9777.832312621244
Iteration 1900: Loss = -9777.824957181863
Iteration 2000: Loss = -9777.818804190101
Iteration 2100: Loss = -9777.81445611797
Iteration 2200: Loss = -9777.829946013986
1
Iteration 2300: Loss = -9777.807867550597
Iteration 2400: Loss = -9777.805143264413
Iteration 2500: Loss = -9777.80205162951
Iteration 2600: Loss = -9777.794874284085
Iteration 2700: Loss = -9777.767933282557
Iteration 2800: Loss = -9777.763587047539
Iteration 2900: Loss = -9777.761698789715
Iteration 3000: Loss = -9777.74686515452
Iteration 3100: Loss = -9777.745668517533
Iteration 3200: Loss = -9777.765454983757
1
Iteration 3300: Loss = -9777.743872977217
Iteration 3400: Loss = -9777.743107214446
Iteration 3500: Loss = -9777.742415041106
Iteration 3600: Loss = -9777.743634292234
1
Iteration 3700: Loss = -9777.741121126955
Iteration 3800: Loss = -9777.740516250047
Iteration 3900: Loss = -9777.740433203971
Iteration 4000: Loss = -9777.738868356231
Iteration 4100: Loss = -9777.72334772475
Iteration 4200: Loss = -9777.726238710924
1
Iteration 4300: Loss = -9777.71448530658
Iteration 4400: Loss = -9777.71402032241
Iteration 4500: Loss = -9777.713951000722
Iteration 4600: Loss = -9777.71341412949
Iteration 4700: Loss = -9777.713045926193
Iteration 4800: Loss = -9777.712753947499
Iteration 4900: Loss = -9777.712484355978
Iteration 5000: Loss = -9777.712247028468
Iteration 5100: Loss = -9777.712013639735
Iteration 5200: Loss = -9777.713681901181
1
Iteration 5300: Loss = -9777.711538460788
Iteration 5400: Loss = -9777.711300281544
Iteration 5500: Loss = -9777.711176637304
Iteration 5600: Loss = -9777.710830639095
Iteration 5700: Loss = -9777.7105343602
Iteration 5800: Loss = -9777.724083297853
1
Iteration 5900: Loss = -9777.688014343586
Iteration 6000: Loss = -9777.687456783782
Iteration 6100: Loss = -9777.687228795648
Iteration 6200: Loss = -9777.686990990776
Iteration 6300: Loss = -9777.686731320728
Iteration 6400: Loss = -9777.686498601111
Iteration 6500: Loss = -9777.69784469542
1
Iteration 6600: Loss = -9777.685925244816
Iteration 6700: Loss = -9777.685604049146
Iteration 6800: Loss = -9777.688133590216
1
Iteration 6900: Loss = -9777.684770140411
Iteration 7000: Loss = -9777.67697744047
Iteration 7100: Loss = -9777.66225479275
Iteration 7200: Loss = -9777.660307284701
Iteration 7300: Loss = -9777.658942871383
Iteration 7400: Loss = -9778.04485685496
1
Iteration 7500: Loss = -9777.616520567495
Iteration 7600: Loss = -9777.56159592976
Iteration 7700: Loss = -9776.19198678344
Iteration 7800: Loss = -9775.679698006834
Iteration 7900: Loss = -9774.918716699387
Iteration 8000: Loss = -9774.1379825297
Iteration 8100: Loss = -9774.086976046055
Iteration 8200: Loss = -9774.075007941592
Iteration 8300: Loss = -9774.063135997543
Iteration 8400: Loss = -9773.878409260175
Iteration 8500: Loss = -9773.871540822814
Iteration 8600: Loss = -9773.870032307217
Iteration 8700: Loss = -9773.86770800297
Iteration 8800: Loss = -9773.8649477503
Iteration 8900: Loss = -9773.864120346438
Iteration 9000: Loss = -9773.8636841579
Iteration 9100: Loss = -9773.863032803241
Iteration 9200: Loss = -9773.862571092663
Iteration 9300: Loss = -9773.865709672697
1
Iteration 9400: Loss = -9773.861976217735
Iteration 9500: Loss = -9773.861752269011
Iteration 9600: Loss = -9773.863246929466
1
Iteration 9700: Loss = -9773.861345676496
Iteration 9800: Loss = -9773.861121056747
Iteration 9900: Loss = -9773.934316987448
1
Iteration 10000: Loss = -9773.860758385872
Iteration 10100: Loss = -9773.860498594197
Iteration 10200: Loss = -9773.862840703629
1
Iteration 10300: Loss = -9773.940090595199
2
Iteration 10400: Loss = -9773.859221874642
Iteration 10500: Loss = -9773.859243965768
1
Iteration 10600: Loss = -9773.856747578224
Iteration 10700: Loss = -9773.89669148088
1
Iteration 10800: Loss = -9773.856450293863
Iteration 10900: Loss = -9773.856203474645
Iteration 11000: Loss = -9773.857658471978
1
Iteration 11100: Loss = -9773.856045780798
Iteration 11200: Loss = -9773.855647178996
Iteration 11300: Loss = -9773.855581649112
Iteration 11400: Loss = -9773.855316065701
Iteration 11500: Loss = -9773.856991672286
1
Iteration 11600: Loss = -9773.85405603061
Iteration 11700: Loss = -9773.853126264503
Iteration 11800: Loss = -9773.853115555563
Iteration 11900: Loss = -9773.853022130503
Iteration 12000: Loss = -9773.852895123897
Iteration 12100: Loss = -9773.854477010093
1
Iteration 12200: Loss = -9773.852857561675
Iteration 12300: Loss = -9773.852774344705
Iteration 12400: Loss = -9773.85284708804
1
Iteration 12500: Loss = -9773.852871190436
2
Iteration 12600: Loss = -9773.8527661912
Iteration 12700: Loss = -9773.96168654123
1
Iteration 12800: Loss = -9773.852617138142
Iteration 12900: Loss = -9773.888211209784
1
Iteration 13000: Loss = -9773.852375367152
Iteration 13100: Loss = -9773.873710927786
1
Iteration 13200: Loss = -9773.852280498844
Iteration 13300: Loss = -9773.852212282285
Iteration 13400: Loss = -9773.852287867534
1
Iteration 13500: Loss = -9773.848991008153
Iteration 13600: Loss = -9773.848925388866
Iteration 13700: Loss = -9773.864095309047
1
Iteration 13800: Loss = -9773.84881422187
Iteration 13900: Loss = -9773.849812580629
1
Iteration 14000: Loss = -9773.854977308047
2
Iteration 14100: Loss = -9773.84863688011
Iteration 14200: Loss = -9773.849079088819
1
Iteration 14300: Loss = -9774.052710726151
2
Iteration 14400: Loss = -9773.848528101205
Iteration 14500: Loss = -9773.910810335125
1
Iteration 14600: Loss = -9773.848469339433
Iteration 14700: Loss = -9773.863157603568
1
Iteration 14800: Loss = -9773.848419221764
Iteration 14900: Loss = -9773.86721579862
1
Iteration 15000: Loss = -9773.848344488768
Iteration 15100: Loss = -9773.84996759431
1
Iteration 15200: Loss = -9773.848316054336
Iteration 15300: Loss = -9773.848887840126
1
Iteration 15400: Loss = -9773.848311486276
Iteration 15500: Loss = -9773.85016107145
1
Iteration 15600: Loss = -9773.84827683089
Iteration 15700: Loss = -9773.984044321674
1
Iteration 15800: Loss = -9773.848271908795
Iteration 15900: Loss = -9773.855612891934
1
Iteration 16000: Loss = -9773.848399610411
2
Iteration 16100: Loss = -9773.84836019759
3
Iteration 16200: Loss = -9773.862294977553
4
Iteration 16300: Loss = -9773.848208835283
Iteration 16400: Loss = -9773.851250107648
1
Iteration 16500: Loss = -9773.84812615476
Iteration 16600: Loss = -9773.848959293962
1
Iteration 16700: Loss = -9773.848108839431
Iteration 16800: Loss = -9773.84937046068
1
Iteration 16900: Loss = -9773.848126880765
2
Iteration 17000: Loss = -9773.848961145746
3
Iteration 17100: Loss = -9773.848110006718
4
Iteration 17200: Loss = -9773.868124128343
5
Iteration 17300: Loss = -9773.848097906883
Iteration 17400: Loss = -9773.848080070447
Iteration 17500: Loss = -9773.84846667263
1
Iteration 17600: Loss = -9773.848055738108
Iteration 17700: Loss = -9773.929750232446
1
Iteration 17800: Loss = -9773.8480174672
Iteration 17900: Loss = -9773.847995883898
Iteration 18000: Loss = -9773.848950456908
1
Iteration 18100: Loss = -9773.848011823538
2
Iteration 18200: Loss = -9773.849228471352
3
Iteration 18300: Loss = -9773.848051256255
4
Iteration 18400: Loss = -9773.847999212185
5
Iteration 18500: Loss = -9773.862993561908
6
Iteration 18600: Loss = -9773.848020624078
7
Iteration 18700: Loss = -9773.848003295874
8
Iteration 18800: Loss = -9773.848128830668
9
Iteration 18900: Loss = -9773.84790570179
Iteration 19000: Loss = -9773.855799680472
1
Iteration 19100: Loss = -9773.847959171862
2
Iteration 19200: Loss = -9773.847981167053
3
Iteration 19300: Loss = -9773.847925260754
4
Iteration 19400: Loss = -9773.848521113574
5
Iteration 19500: Loss = -9773.847866183396
Iteration 19600: Loss = -9773.847864898376
Iteration 19700: Loss = -9773.847918092826
1
Iteration 19800: Loss = -9773.847862850018
Iteration 19900: Loss = -9773.848178760518
1
tensor([[ 2.0075, -4.1264],
        [ 0.7546, -2.1740],
        [ 0.7901, -3.7928],
        [ 1.4751, -2.8626],
        [ 1.5898, -3.8552],
        [ 0.3794, -2.0481],
        [-0.7165, -2.5972],
        [ 1.2383, -2.7695],
        [ 0.4093, -3.0821],
        [ 0.2506, -4.2431],
        [ 0.6813, -2.2394],
        [-1.1005, -0.7641],
        [-0.6172, -1.9278],
        [-0.2578, -1.2666],
        [ 0.2918, -2.7968],
        [ 1.8195, -3.2180],
        [ 0.5374, -2.5385],
        [ 2.5150, -4.0584],
        [ 1.1651, -2.8646],
        [ 0.2162, -1.6172],
        [ 1.5869, -4.6318],
        [ 0.0421, -2.1077],
        [ 0.7374, -2.2019],
        [-0.5129, -1.4977],
        [-1.0100, -2.4929],
        [ 0.9813, -2.9642],
        [-0.6740, -0.7204],
        [ 1.2812, -2.7387],
        [ 0.1809, -3.4860],
        [ 0.5883, -2.7506],
        [ 1.9798, -3.5728],
        [ 2.0949, -3.6670],
        [-0.1440, -1.2439],
        [ 0.1252, -1.8062],
        [ 1.9107, -3.4453],
        [-1.4019, -0.7048],
        [ 0.9969, -3.0597],
        [ 1.2094, -2.6072],
        [ 0.0452, -1.8959],
        [ 2.4117, -3.8319],
        [ 2.2460, -3.9526],
        [ 1.7955, -3.1828],
        [-0.9005, -0.6812],
        [ 0.8457, -2.2493],
        [ 2.5505, -4.5705],
        [-0.8302, -0.7655],
        [ 1.5104, -4.8372],
        [ 1.2414, -4.9056],
        [ 0.8512, -2.8977],
        [ 2.0931, -3.4831],
        [ 0.5157, -2.5461],
        [-0.8289, -1.1275],
        [-1.2079, -0.2318],
        [ 0.3853, -1.8869],
        [ 0.9666, -2.4639],
        [ 2.2623, -3.6541],
        [ 1.4491, -2.8459],
        [ 1.7850, -3.3562],
        [ 0.4811, -1.8697],
        [ 1.7958, -3.2391],
        [-0.8040, -0.8877],
        [-3.4567,  0.0677],
        [-1.5000, -2.4694],
        [-1.0003, -0.3914],
        [-0.1868, -1.2026],
        [ 1.6695, -3.1305],
        [ 0.5919, -2.0129],
        [-1.1644, -0.3224],
        [-0.1621, -1.9030],
        [ 1.2161, -3.4058],
        [ 1.9922, -3.4126],
        [ 1.4146, -2.8033],
        [-0.9866, -0.4603],
        [ 0.9129, -2.3353],
        [ 0.6151, -2.3477],
        [ 2.2437, -3.7530],
        [ 2.0009, -3.7270],
        [ 2.0248, -3.4422],
        [ 0.6226, -2.0264],
        [ 0.0241, -2.6382],
        [ 0.6389, -2.9351],
        [ 2.4697, -3.8919],
        [-0.9121, -0.7662],
        [ 0.9409, -2.3416],
        [-0.2761, -2.5082],
        [ 2.3939, -4.2748],
        [ 0.2140, -2.2479],
        [ 0.9006, -2.6067],
        [ 1.5220, -4.2491],
        [ 2.4047, -4.2353],
        [ 1.4781, -3.8083],
        [-0.0223, -1.4449],
        [-0.1128, -2.0138],
        [ 1.7124, -3.5451],
        [ 2.5473, -4.8395],
        [-0.1084, -1.3483],
        [-0.1945, -1.8787],
        [ 0.0198, -1.4168],
        [ 0.3955, -2.9865],
        [ 1.6183, -3.0170]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.0969e-07],
        [6.5959e-01, 3.4041e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8706, 0.1294], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1379, 0.0830],
         [0.3611, 0.0760]],

        [[0.2125, 0.0839],
         [0.2176, 0.7611]],

        [[0.1703, 0.2300],
         [0.5382, 0.1837]],

        [[0.5783, 0.0917],
         [0.1355, 0.0502]],

        [[0.8764, 0.1808],
         [0.1117, 0.5494]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.01596629338064087
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.006384526905169038
Average Adjusted Rand Index: -0.00569697302392939
Iteration 0: Loss = -25590.754003329796
Iteration 10: Loss = -9780.466276887666
Iteration 20: Loss = -9780.45061537925
Iteration 30: Loss = -9779.335902311836
Iteration 40: Loss = -9776.796339311382
Iteration 50: Loss = -9776.498860446081
Iteration 60: Loss = -9776.413193391532
Iteration 70: Loss = -9776.386131727613
Iteration 80: Loss = -9776.381400207418
Iteration 90: Loss = -9776.380252483694
Iteration 100: Loss = -9776.379907280267
Iteration 110: Loss = -9776.379790152709
Iteration 120: Loss = -9776.379731765126
Iteration 130: Loss = -9776.379701209617
Iteration 140: Loss = -9776.37972636486
1
Iteration 150: Loss = -9776.379691914366
Iteration 160: Loss = -9776.379713807179
1
Iteration 170: Loss = -9776.379687049235
Iteration 180: Loss = -9776.379693465262
1
Iteration 190: Loss = -9776.379732017691
2
Iteration 200: Loss = -9776.379721656427
3
Stopping early at iteration 199 due to no improvement.
pi: tensor([[0.0217, 0.9783],
        [0.0298, 0.9702]], dtype=torch.float64)
alpha: tensor([0.0316, 0.9684])
beta: tensor([[[0.1175, 0.0673],
         [0.2020, 0.1322]],

        [[0.8466, 0.1054],
         [0.2337, 0.5833]],

        [[0.3673, 0.1955],
         [0.1975, 0.2704]],

        [[0.6160, 0.2007],
         [0.4709, 0.5094]],

        [[0.4483, 0.2198],
         [0.5298, 0.6225]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0012104823609284674
Average Adjusted Rand Index: -0.001677317058922544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25590.377936116958
Iteration 100: Loss = -9798.30419840057
Iteration 200: Loss = -9785.918546450826
Iteration 300: Loss = -9783.047817150302
Iteration 400: Loss = -9781.897753994124
Iteration 500: Loss = -9781.310879887002
Iteration 600: Loss = -9780.961793770395
Iteration 700: Loss = -9780.731389740327
Iteration 800: Loss = -9780.563643434603
Iteration 900: Loss = -9780.42570857148
Iteration 1000: Loss = -9780.305906851843
Iteration 1100: Loss = -9780.201957284406
Iteration 1200: Loss = -9780.111545898
Iteration 1300: Loss = -9780.028020030963
Iteration 1400: Loss = -9779.950005584937
Iteration 1500: Loss = -9779.889118058994
Iteration 1600: Loss = -9779.83223175283
Iteration 1700: Loss = -9779.776555993163
Iteration 1800: Loss = -9779.717507434649
Iteration 1900: Loss = -9779.650437462366
Iteration 2000: Loss = -9779.565401651987
Iteration 2100: Loss = -9779.427002847193
Iteration 2200: Loss = -9779.063061742756
Iteration 2300: Loss = -9778.307352812562
Iteration 2400: Loss = -9778.11459684549
Iteration 2500: Loss = -9777.991632968116
Iteration 2600: Loss = -9777.891285132519
Iteration 2700: Loss = -9777.805694477654
Iteration 2800: Loss = -9777.731329866734
Iteration 2900: Loss = -9777.665106613988
Iteration 3000: Loss = -9777.60348772691
Iteration 3100: Loss = -9777.545665027817
Iteration 3200: Loss = -9777.495561717395
Iteration 3300: Loss = -9777.456344438255
Iteration 3400: Loss = -9777.424671644847
Iteration 3500: Loss = -9777.398512890133
Iteration 3600: Loss = -9777.385590390919
Iteration 3700: Loss = -9777.356317640746
Iteration 3800: Loss = -9777.341286445693
Iteration 3900: Loss = -9777.330393617018
Iteration 4000: Loss = -9777.3220289813
Iteration 4100: Loss = -9777.315732167463
Iteration 4200: Loss = -9777.311084683823
Iteration 4300: Loss = -9777.307576088919
Iteration 4400: Loss = -9777.317041286537
1
Iteration 4500: Loss = -9777.30327144352
Iteration 4600: Loss = -9777.301806641897
Iteration 4700: Loss = -9777.300797843538
Iteration 4800: Loss = -9777.299815633443
Iteration 4900: Loss = -9777.299065034375
Iteration 5000: Loss = -9777.298554275114
Iteration 5100: Loss = -9777.297829293535
Iteration 5200: Loss = -9777.297291931765
Iteration 5300: Loss = -9777.297482761684
1
Iteration 5400: Loss = -9777.296338369502
Iteration 5500: Loss = -9777.295890136897
Iteration 5600: Loss = -9777.295457123915
Iteration 5700: Loss = -9777.294981722034
Iteration 5800: Loss = -9777.298123196948
1
Iteration 5900: Loss = -9777.293961578282
Iteration 6000: Loss = -9777.293403320744
Iteration 6100: Loss = -9777.292685405122
Iteration 6200: Loss = -9777.29189909734
Iteration 6300: Loss = -9777.290980050011
Iteration 6400: Loss = -9777.29616285376
1
Iteration 6500: Loss = -9777.287311193004
Iteration 6600: Loss = -9777.288278163442
1
Iteration 6700: Loss = -9777.275617344225
Iteration 6800: Loss = -9777.24715792213
Iteration 6900: Loss = -9776.334677070545
Iteration 7000: Loss = -9775.825971990153
Iteration 7100: Loss = -9775.729754568012
Iteration 7200: Loss = -9775.567361466403
Iteration 7300: Loss = -9775.434761822175
Iteration 7400: Loss = -9774.569064391448
Iteration 7500: Loss = -9773.918624322474
Iteration 7600: Loss = -9773.851614329682
Iteration 7700: Loss = -9773.81885297266
Iteration 7800: Loss = -9773.805816796512
Iteration 7900: Loss = -9773.806031854729
1
Iteration 8000: Loss = -9773.78896984312
Iteration 8100: Loss = -9773.7988466645
1
Iteration 8200: Loss = -9773.773889946888
Iteration 8300: Loss = -9773.771910807634
Iteration 8400: Loss = -9773.771944198887
1
Iteration 8500: Loss = -9773.767805498708
Iteration 8600: Loss = -9773.769658161831
1
Iteration 8700: Loss = -9773.766376454543
Iteration 8800: Loss = -9773.766015241798
Iteration 8900: Loss = -9773.77005028104
1
Iteration 9000: Loss = -9773.764818875756
Iteration 9100: Loss = -9773.763832357365
Iteration 9200: Loss = -9773.763272459644
Iteration 9300: Loss = -9773.762014959211
Iteration 9400: Loss = -9773.76132877975
Iteration 9500: Loss = -9773.761553010681
1
Iteration 9600: Loss = -9773.762603955678
2
Iteration 9700: Loss = -9773.760604661396
Iteration 9800: Loss = -9773.762189302815
1
Iteration 9900: Loss = -9773.760410881185
Iteration 10000: Loss = -9773.75891966822
Iteration 10100: Loss = -9773.76327538858
1
Iteration 10200: Loss = -9773.758048005635
Iteration 10300: Loss = -9773.757616082548
Iteration 10400: Loss = -9773.754435360743
Iteration 10500: Loss = -9773.754140616289
Iteration 10600: Loss = -9773.80605652128
1
Iteration 10700: Loss = -9773.753751677836
Iteration 10800: Loss = -9773.753672489198
Iteration 10900: Loss = -9773.759231147042
1
Iteration 11000: Loss = -9773.752961893066
Iteration 11100: Loss = -9773.752175393145
Iteration 11200: Loss = -9773.750433415187
Iteration 11300: Loss = -9773.749854110716
Iteration 11400: Loss = -9773.749807928023
Iteration 11500: Loss = -9773.749586580887
Iteration 11600: Loss = -9773.750108143015
1
Iteration 11700: Loss = -9773.749328001193
Iteration 11800: Loss = -9773.792027458469
1
Iteration 11900: Loss = -9773.749158576271
Iteration 12000: Loss = -9773.749256471518
1
Iteration 12100: Loss = -9773.749351198734
2
Iteration 12200: Loss = -9773.749109904107
Iteration 12300: Loss = -9773.74913351873
1
Iteration 12400: Loss = -9773.748996910614
Iteration 12500: Loss = -9773.748929035599
Iteration 12600: Loss = -9773.749221772257
1
Iteration 12700: Loss = -9773.748857356431
Iteration 12800: Loss = -9773.751219631971
1
Iteration 12900: Loss = -9773.84496071274
2
Iteration 13000: Loss = -9773.752194421615
3
Iteration 13100: Loss = -9773.748783835561
Iteration 13200: Loss = -9773.750174416879
1
Iteration 13300: Loss = -9773.750252575535
2
Iteration 13400: Loss = -9773.748741090974
Iteration 13500: Loss = -9773.95791280559
1
Iteration 13600: Loss = -9773.748379505058
Iteration 13700: Loss = -9773.801766821261
1
Iteration 13800: Loss = -9773.748379537203
2
Iteration 13900: Loss = -9773.748338573912
Iteration 14000: Loss = -9773.748361786693
1
Iteration 14100: Loss = -9773.831118374907
2
Iteration 14200: Loss = -9773.748261674344
Iteration 14300: Loss = -9774.056553513848
1
Iteration 14400: Loss = -9773.748266876628
2
Iteration 14500: Loss = -9773.755152633856
3
Iteration 14600: Loss = -9773.748283257704
4
Iteration 14700: Loss = -9773.749307385568
5
Iteration 14800: Loss = -9773.748251941466
Iteration 14900: Loss = -9773.748249421791
Iteration 15000: Loss = -9773.74823961576
Iteration 15100: Loss = -9773.748419106909
1
Iteration 15200: Loss = -9773.748267462972
2
Iteration 15300: Loss = -9773.75578162391
3
Iteration 15400: Loss = -9773.748266580355
4
Iteration 15500: Loss = -9773.75774868125
5
Iteration 15600: Loss = -9773.748278716768
6
Iteration 15700: Loss = -9773.748638693178
7
Iteration 15800: Loss = -9773.748292809385
8
Iteration 15900: Loss = -9773.748334633658
9
Iteration 16000: Loss = -9773.748316585848
10
Stopping early at iteration 16000 due to no improvement.
tensor([[-3.8325,  2.4412],
        [-2.2436,  0.8119],
        [-3.0669,  1.6261],
        [-4.1506,  0.2990],
        [-4.4477,  1.1282],
        [-1.9902,  0.4398],
        [-1.6570,  0.2707],
        [-2.9587,  1.1979],
        [-2.7008,  0.9209],
        [-3.7744,  0.8317],
        [-2.2499,  0.8421],
        [-1.3795, -1.5767],
        [-1.5669, -0.1794],
        [-1.5397, -0.3855],
        [-2.3912,  0.8455],
        [-4.1229,  1.0523],
        [-2.2924,  0.9053],
        [-4.0463,  2.6564],
        [-2.9930,  1.1505],
        [-2.2534, -0.3043],
        [-4.6549,  1.7001],
        [-1.8340,  0.4086],
        [-2.9301,  0.1274],
        [-1.2747, -0.1532],
        [-1.9036, -0.3102],
        [-2.7154,  1.3286],
        [-0.7991, -0.6464],
        [-2.8210,  1.3581],
        [-2.6262,  1.1340],
        [-2.4476,  1.0361],
        [-4.2880,  1.3833],
        [-3.7260,  2.1607],
        [-1.3055, -0.0808],
        [-2.8046, -0.7912],
        [-3.5825,  1.9085],
        [-0.5046, -1.0922],
        [-3.0491,  1.0884],
        [-3.1913,  0.7515],
        [-2.2259, -0.2021],
        [-4.0419,  2.3431],
        [-3.9938,  2.4983],
        [-3.2523,  1.8626],
        [-1.4751, -1.5798],
        [-2.4849,  0.7478],
        [-4.3898,  2.8480],
        [-1.1409, -1.0949],
        [-4.0625,  2.4235],
        [-4.3131,  1.9024],
        [-2.8336,  0.9887],
        [-3.6437,  2.0855],
        [-2.7723,  0.3183],
        [-1.2165, -0.6399],
        [-0.2629, -1.1392],
        [-1.8977,  0.4629],
        [-2.6889,  0.7363],
        [-4.3696,  1.6637],
        [-3.1442,  1.2738],
        [-3.9527,  1.2961],
        [-1.9328,  0.5130],
        [-3.6484,  1.5364],
        [-0.8452, -0.6767],
        [-0.3952, -2.3252],
        [-1.9799, -0.8020],
        [-0.9262, -1.3637],
        [-1.3961, -0.2028],
        [-3.1872,  1.7852],
        [-2.6148,  0.1407],
        [-0.4135, -1.0696],
        [-2.0567, -0.2291],
        [-3.1729,  1.5810],
        [-4.2211,  1.2777],
        [-3.0847,  1.2486],
        [-0.6580, -1.0886],
        [-2.5615,  0.8347],
        [-2.2267,  0.8246],
        [-3.9835,  2.0803],
        [-3.7087,  2.1419],
        [-3.5481,  2.0419],
        [-2.0810,  0.6640],
        [-3.2182, -0.4825],
        [-2.6786,  0.9971],
        [-3.9258,  2.5346],
        [-1.0345, -1.1294],
        [-2.6327,  0.7728],
        [-3.0514, -0.7063],
        [-4.4709,  2.3277],
        [-2.0320,  0.5181],
        [-2.5214,  1.0674],
        [-3.6461,  2.2347],
        [-4.1423,  2.5733],
        [-3.4101,  1.9951],
        [-1.6446, -0.0941],
        [-1.8240,  0.3222],
        [-4.9681,  0.4299],
        [-4.5531,  2.9617],
        [-2.6808, -1.5155],
        [-2.1423, -0.3464],
        [-1.7208, -0.2063],
        [-2.4584,  1.0677],
        [-3.1672,  1.5712]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3638, 0.6362],
        [0.0071, 0.9929]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1203, 0.8797], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0755, 0.0823],
         [0.2020, 0.1370]],

        [[0.8466, 0.0871],
         [0.2337, 0.5833]],

        [[0.3673, 0.2190],
         [0.1975, 0.2704]],

        [[0.6160, 0.1171],
         [0.4709, 0.5094]],

        [[0.4483, 0.2255],
         [0.5298, 0.6225]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.01669750032904028
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 38
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.005350265809674365
Average Adjusted Rand Index: -0.005843214413609273
Iteration 0: Loss = -18572.287019370786
Iteration 10: Loss = -9777.77484941061
Iteration 20: Loss = -9776.403027800256
Iteration 30: Loss = -9776.382162534905
Iteration 40: Loss = -9776.379966395616
Iteration 50: Loss = -9776.37972110324
Iteration 60: Loss = -9776.379677748388
Iteration 70: Loss = -9776.379668396976
Iteration 80: Loss = -9776.379704103669
1
Iteration 90: Loss = -9776.379698005172
2
Iteration 100: Loss = -9776.379705056983
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.9702, 0.0298],
        [0.9783, 0.0217]], dtype=torch.float64)
alpha: tensor([0.9684, 0.0316])
beta: tensor([[[0.1322, 0.0673],
         [0.9491, 0.1175]],

        [[0.6351, 0.1054],
         [0.8955, 0.9529]],

        [[0.6212, 0.1955],
         [0.9158, 0.0025]],

        [[0.5392, 0.2007],
         [0.0065, 0.4324]],

        [[0.5768, 0.2198],
         [0.3438, 0.5869]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0012104823609284674
Average Adjusted Rand Index: -0.001677317058922544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18571.84568984484
Iteration 100: Loss = -9797.678437380628
Iteration 200: Loss = -9783.185396807557
Iteration 300: Loss = -9779.656973781628
Iteration 400: Loss = -9778.002381722972
Iteration 500: Loss = -9777.255070046112
Iteration 600: Loss = -9776.918506777438
Iteration 700: Loss = -9776.711778285473
Iteration 800: Loss = -9776.551915366008
Iteration 900: Loss = -9776.40773673492
Iteration 1000: Loss = -9776.277916512823
Iteration 1100: Loss = -9776.167279809428
Iteration 1200: Loss = -9776.071818325716
Iteration 1300: Loss = -9775.985566033834
Iteration 1400: Loss = -9775.90413795029
Iteration 1500: Loss = -9775.824040629968
Iteration 1600: Loss = -9775.741871901768
Iteration 1700: Loss = -9775.65197169141
Iteration 1800: Loss = -9775.563636749828
Iteration 1900: Loss = -9775.479021268002
Iteration 2000: Loss = -9775.423426612266
Iteration 2100: Loss = -9775.398681810486
Iteration 2200: Loss = -9775.377798372227
Iteration 2300: Loss = -9775.358321792673
Iteration 2400: Loss = -9775.328659857943
Iteration 2500: Loss = -9775.237058182747
Iteration 2600: Loss = -9774.54940873887
Iteration 2700: Loss = -9774.138372090973
Iteration 2800: Loss = -9774.083002757214
Iteration 2900: Loss = -9774.056984884059
Iteration 3000: Loss = -9774.041876729889
Iteration 3100: Loss = -9774.007990087195
Iteration 3200: Loss = -9773.836155089291
Iteration 3300: Loss = -9773.8239627699
Iteration 3400: Loss = -9773.816275655623
Iteration 3500: Loss = -9773.803376773643
Iteration 3600: Loss = -9773.79288278497
Iteration 3700: Loss = -9773.789410884909
Iteration 3800: Loss = -9773.787002464123
Iteration 3900: Loss = -9773.785839629594
Iteration 4000: Loss = -9773.7847627786
Iteration 4100: Loss = -9773.783607033221
Iteration 4200: Loss = -9773.782221029638
Iteration 4300: Loss = -9773.780666660667
Iteration 4400: Loss = -9773.779133088314
Iteration 4500: Loss = -9773.777013882449
Iteration 4600: Loss = -9773.773860748011
Iteration 4700: Loss = -9773.769199021795
Iteration 4800: Loss = -9773.75875866363
Iteration 4900: Loss = -9773.757764394473
Iteration 5000: Loss = -9773.75734068
Iteration 5100: Loss = -9773.756983321946
Iteration 5200: Loss = -9773.756555752721
Iteration 5300: Loss = -9773.755912945599
Iteration 5400: Loss = -9773.755159743647
Iteration 5500: Loss = -9773.75459582428
Iteration 5600: Loss = -9773.753988902581
Iteration 5700: Loss = -9773.753254955562
Iteration 5800: Loss = -9773.752607868288
Iteration 5900: Loss = -9773.752247495999
Iteration 6000: Loss = -9773.751997388694
Iteration 6100: Loss = -9773.751755990865
Iteration 6200: Loss = -9773.751568696505
Iteration 6300: Loss = -9773.75139202718
Iteration 6400: Loss = -9773.751321686415
Iteration 6500: Loss = -9773.751271861132
Iteration 6600: Loss = -9773.751159068606
Iteration 6700: Loss = -9773.751131292798
Iteration 6800: Loss = -9773.751015616375
Iteration 6900: Loss = -9773.750958524724
Iteration 7000: Loss = -9773.750895867171
Iteration 7100: Loss = -9773.751003193298
1
Iteration 7200: Loss = -9773.761915369918
2
Iteration 7300: Loss = -9773.750687854501
Iteration 7400: Loss = -9773.750736720878
1
Iteration 7500: Loss = -9773.750528866969
Iteration 7600: Loss = -9773.776787765102
1
Iteration 7700: Loss = -9773.748836384219
Iteration 7800: Loss = -9773.802312402917
1
Iteration 7900: Loss = -9773.74835408996
Iteration 8000: Loss = -9773.748325688104
Iteration 8100: Loss = -9773.748369272012
1
Iteration 8200: Loss = -9773.74833877639
2
Iteration 8300: Loss = -9773.748796829084
3
Iteration 8400: Loss = -9773.74830838524
Iteration 8500: Loss = -9773.748299613675
Iteration 8600: Loss = -9773.76316476798
1
Iteration 8700: Loss = -9773.74828155555
Iteration 8800: Loss = -9773.748272699588
Iteration 8900: Loss = -9773.748241130175
Iteration 9000: Loss = -9773.748389379118
1
Iteration 9100: Loss = -9773.748259244574
2
Iteration 9200: Loss = -9773.748240933957
Iteration 9300: Loss = -9773.748762494615
1
Iteration 9400: Loss = -9773.74820932602
Iteration 9500: Loss = -9773.748248604355
1
Iteration 9600: Loss = -9773.749394670205
2
Iteration 9700: Loss = -9773.748221298629
3
Iteration 9800: Loss = -9773.74825788464
4
Iteration 9900: Loss = -9773.748268791607
5
Iteration 10000: Loss = -9773.74822525874
6
Iteration 10100: Loss = -9774.155906879201
7
Iteration 10200: Loss = -9773.74824053263
8
Iteration 10300: Loss = -9773.748222010698
9
Iteration 10400: Loss = -9773.766005339512
10
Stopping early at iteration 10400 due to no improvement.
tensor([[ 2.4468, -3.8334],
        [-0.0269, -3.0967],
        [ 1.3155, -3.3830],
        [ 0.9420, -3.5162],
        [ 1.4927, -4.0891],
        [-0.0477, -2.4886],
        [ 0.2772, -1.6635],
        [ 1.0244, -3.1364],
        [ 0.8056, -2.8265],
        [ 1.6020, -3.0137],
        [ 0.7478, -2.3571],
        [-0.9548, -0.7676],
        [-1.6082, -3.0070],
        [-0.6197, -1.7863],
        [ 0.8911, -2.3502],
        [ 1.0788, -4.0932],
        [ 0.7954, -2.4130],
        [ 2.6568, -4.0534],
        [ 1.3782, -2.7731],
        [-0.1984, -2.1585],
        [ 1.0474, -5.3196],
        [ 0.4336, -1.8199],
        [ 0.6728, -2.3980],
        [-0.1696, -1.3034],
        [ 0.1040, -1.5016],
        [ 1.3269, -2.7289],
        [-0.7942, -0.9585],
        [ 1.3903, -2.7914],
        [ 1.0814, -2.6853],
        [ 0.5117, -2.9867],
        [ 1.9551, -3.7193],
        [ 2.2242, -3.6701],
        [-0.6009, -1.8389],
        [ 0.3043, -1.7217],
        [ 1.3443, -4.1559],
        [-1.0510, -0.4746],
        [ 1.3679, -2.7764],
        [ 1.2824, -2.6687],
        [ 0.1530, -1.8836],
        [ 2.4376, -3.9426],
        [ 1.9731, -4.5117],
        [ 1.8673, -3.2538],
        [-1.1971, -1.1024],
        [ 0.8150, -2.4282],
        [ 2.7859, -4.4691],
        [-0.6675, -0.7234],
        [ 2.5270, -3.9670],
        [ 2.0313, -4.1818],
        [-0.3903, -4.2249],
        [ 2.1439, -3.5871],
        [ 0.8412, -2.2535],
        [-0.4004, -0.9894],
        [-1.2617, -0.3942],
        [ 0.3382, -2.0355],
        [ 0.1597, -3.2795],
        [ 2.0289, -4.0123],
        [ 1.1495, -3.2706],
        [ 1.7698, -3.4834],
        [ 0.5068, -1.9510],
        [ 0.2858, -4.9010],
        [-0.6522, -0.8318],
        [-2.2970, -0.3720],
        [-0.2369, -1.4237],
        [-1.1111, -0.6796],
        [-0.2943, -1.4979],
        [ 1.6489, -3.3180],
        [ 0.5606, -2.2086],
        [-1.0501, -0.4029],
        [ 0.1944, -1.6462],
        [ 1.5342, -3.2231],
        [ 1.3978, -4.1085],
        [ 0.8278, -3.5107],
        [-0.9930, -0.5727],
        [ 0.1673, -3.2412],
        [ 0.4701, -2.5956],
        [ 2.3232, -3.7477],
        [ 2.2138, -3.6457],
        [ 2.0590, -3.5309],
        [ 0.4930, -2.2628],
        [ 0.1554, -2.5943],
        [ 1.0663, -2.6225],
        [ 2.3458, -4.1190],
        [-0.7546, -0.6702],
        [ 0.7517, -2.6583],
        [ 0.3599, -1.9987],
        [ 2.2900, -4.5128],
        [ 0.5683, -1.9919],
        [ 0.2764, -3.3263],
        [ 2.0439, -3.8396],
        [ 2.6665, -4.0649],
        [ 1.9177, -3.4872],
        [ 0.0803, -1.4841],
        [-0.3047, -2.4598],
        [ 1.9896, -3.4057],
        [ 3.0407, -4.4863],
        [-0.8675, -2.0435],
        [ 0.2054, -1.6021],
        [-0.6438, -2.1701],
        [ 0.5852, -2.9531],
        [ 1.5606, -3.1822]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9929, 0.0071],
        [0.6399, 0.3601]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8786, 0.1214], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1376, 0.0826],
         [0.9491, 0.0755]],

        [[0.6351, 0.0871],
         [0.8955, 0.9529]],

        [[0.6212, 0.2190],
         [0.9158, 0.0025]],

        [[0.5392, 0.1171],
         [0.0065, 0.4324]],

        [[0.5768, 0.2257],
         [0.3438, 0.5869]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.01669750032904028
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.005350265809674365
Average Adjusted Rand Index: -0.005843214413609273
Iteration 0: Loss = -22302.975698371538
Iteration 10: Loss = -9780.466362660081
Iteration 20: Loss = -9780.46636786728
1
Iteration 30: Loss = -9780.466428643007
2
Iteration 40: Loss = -9780.465577974901
Iteration 50: Loss = -9780.384573915651
Iteration 60: Loss = -9777.955282585244
Iteration 70: Loss = -9776.626355825387
Iteration 80: Loss = -9776.47156338416
Iteration 90: Loss = -9776.430748193343
Iteration 100: Loss = -9776.416020017336
Iteration 110: Loss = -9776.400268352842
Iteration 120: Loss = -9776.377192084385
Iteration 130: Loss = -9776.375735453967
Iteration 140: Loss = -9776.377832555572
1
Iteration 150: Loss = -9776.378936898634
2
Iteration 160: Loss = -9776.3794329091
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[0.9702, 0.0298],
        [0.9783, 0.0217]], dtype=torch.float64)
alpha: tensor([0.9684, 0.0316])
beta: tensor([[[0.1322, 0.0673],
         [0.3125, 0.1175]],

        [[0.5102, 0.1054],
         [0.7183, 0.7764]],

        [[0.4179, 0.1955],
         [0.4833, 0.7488]],

        [[0.0173, 0.2007],
         [0.3691, 0.3885]],

        [[0.0279, 0.2198],
         [0.0309, 0.5141]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0012104823609284674
Average Adjusted Rand Index: -0.001677317058922544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22304.00350823016
Iteration 100: Loss = -9788.296913552063
Iteration 200: Loss = -9781.394383933632
Iteration 300: Loss = -9780.625022224267
Iteration 400: Loss = -9780.232443982432
Iteration 500: Loss = -9779.912383700601
Iteration 600: Loss = -9779.487220880466
Iteration 700: Loss = -9778.842095057511
Iteration 800: Loss = -9778.390929016656
Iteration 900: Loss = -9778.137071580899
Iteration 1000: Loss = -9777.96412230063
Iteration 1100: Loss = -9777.82845956241
Iteration 1200: Loss = -9777.724751866686
Iteration 1300: Loss = -9777.642986680448
Iteration 1400: Loss = -9777.576511594449
Iteration 1500: Loss = -9777.522144446624
Iteration 1600: Loss = -9777.477933976199
Iteration 1700: Loss = -9777.44137263871
Iteration 1800: Loss = -9777.409274672653
Iteration 1900: Loss = -9777.378352055961
Iteration 2000: Loss = -9777.344302917123
Iteration 2100: Loss = -9777.298476886428
Iteration 2200: Loss = -9777.21731369055
Iteration 2300: Loss = -9777.008527181346
Iteration 2400: Loss = -9776.485114990977
Iteration 2500: Loss = -9776.107778400761
Iteration 2600: Loss = -9775.969505673822
Iteration 2700: Loss = -9775.910209851345
Iteration 2800: Loss = -9775.871232675363
Iteration 2900: Loss = -9775.839998847918
Iteration 3000: Loss = -9775.8118844991
Iteration 3100: Loss = -9775.784706385042
Iteration 3200: Loss = -9775.757159841003
Iteration 3300: Loss = -9775.72823938418
Iteration 3400: Loss = -9775.697061653314
Iteration 3500: Loss = -9775.662713294078
Iteration 3600: Loss = -9775.627126755013
Iteration 3700: Loss = -9775.579552032554
Iteration 3800: Loss = -9775.531676258252
Iteration 3900: Loss = -9775.445945553858
Iteration 4000: Loss = -9775.550384951344
1
Iteration 4100: Loss = -9774.749941815024
Iteration 4200: Loss = -9774.101108763256
Iteration 4300: Loss = -9773.957083194466
Iteration 4400: Loss = -9773.888926015296
Iteration 4500: Loss = -9773.860674957838
Iteration 4600: Loss = -9773.846260653772
Iteration 4700: Loss = -9773.821301401322
Iteration 4800: Loss = -9773.815274065033
Iteration 4900: Loss = -9773.811059430513
Iteration 5000: Loss = -9773.80716374927
Iteration 5100: Loss = -9773.802360673331
Iteration 5200: Loss = -9773.798413914883
Iteration 5300: Loss = -9773.792675117902
Iteration 5400: Loss = -9773.782702515655
Iteration 5500: Loss = -9773.781163132217
Iteration 5600: Loss = -9773.779757649563
Iteration 5700: Loss = -9773.778446126667
Iteration 5800: Loss = -9773.774753788717
Iteration 5900: Loss = -9773.772793422311
Iteration 6000: Loss = -9773.771525913651
Iteration 6100: Loss = -9773.767811269006
Iteration 6200: Loss = -9773.766883354192
Iteration 6300: Loss = -9773.78819755878
1
Iteration 6400: Loss = -9773.763762948214
Iteration 6500: Loss = -9773.761587599349
Iteration 6600: Loss = -9773.760466075531
Iteration 6700: Loss = -9773.759821111682
Iteration 6800: Loss = -9773.75913863954
Iteration 6900: Loss = -9773.758587259645
Iteration 7000: Loss = -9773.757907126721
Iteration 7100: Loss = -9773.756029755346
Iteration 7200: Loss = -9773.754000927454
Iteration 7300: Loss = -9773.795497567584
1
Iteration 7400: Loss = -9773.752508728176
Iteration 7500: Loss = -9773.752213221278
Iteration 7600: Loss = -9773.75202439559
Iteration 7700: Loss = -9773.751869874353
Iteration 7800: Loss = -9773.751673591263
Iteration 7900: Loss = -9773.751463315908
Iteration 8000: Loss = -9773.752095763291
1
Iteration 8100: Loss = -9773.750974457793
Iteration 8200: Loss = -9773.750708706
Iteration 8300: Loss = -9773.751775963108
1
Iteration 8400: Loss = -9773.750157856519
Iteration 8500: Loss = -9773.74995177732
Iteration 8600: Loss = -9773.811885761514
1
Iteration 8700: Loss = -9773.749678689695
Iteration 8800: Loss = -9773.749608831813
Iteration 8900: Loss = -9773.853875019813
1
Iteration 9000: Loss = -9773.749495439512
Iteration 9100: Loss = -9773.749439884688
Iteration 9200: Loss = -9773.795078544792
1
Iteration 9300: Loss = -9773.749249896515
Iteration 9400: Loss = -9773.749169301407
Iteration 9500: Loss = -9773.749075715654
Iteration 9600: Loss = -9773.749426120617
1
Iteration 9700: Loss = -9773.748927108729
Iteration 9800: Loss = -9773.748874937084
Iteration 9900: Loss = -9773.751216044913
1
Iteration 10000: Loss = -9773.748493471981
Iteration 10100: Loss = -9773.748506484966
1
Iteration 10200: Loss = -9773.748504448191
2
Iteration 10300: Loss = -9773.748512177825
3
Iteration 10400: Loss = -9773.748453043096
Iteration 10500: Loss = -9773.748460270584
1
Iteration 10600: Loss = -9773.748909409092
2
Iteration 10700: Loss = -9773.748415781502
Iteration 10800: Loss = -9773.748431734626
1
Iteration 10900: Loss = -9773.748614962837
2
Iteration 11000: Loss = -9773.748431160187
3
Iteration 11100: Loss = -9773.748390396711
Iteration 11200: Loss = -9773.748448484112
1
Iteration 11300: Loss = -9773.748356477934
Iteration 11400: Loss = -9773.776402049749
1
Iteration 11500: Loss = -9773.748357933433
2
Iteration 11600: Loss = -9773.748341580662
Iteration 11700: Loss = -9773.748811369762
1
Iteration 11800: Loss = -9773.748295343677
Iteration 11900: Loss = -9773.748287511153
Iteration 12000: Loss = -9773.749523904833
1
Iteration 12100: Loss = -9773.748266067141
Iteration 12200: Loss = -9773.74827569815
1
Iteration 12300: Loss = -9773.748305617455
2
Iteration 12400: Loss = -9773.748220903826
Iteration 12500: Loss = -9773.748708740093
1
Iteration 12600: Loss = -9773.751894744446
2
Iteration 12700: Loss = -9773.750413628517
3
Iteration 12800: Loss = -9773.748250383878
4
Iteration 12900: Loss = -9773.750351279177
5
Iteration 13000: Loss = -9773.748257800837
6
Iteration 13100: Loss = -9773.748241619536
7
Iteration 13200: Loss = -9773.748343415959
8
Iteration 13300: Loss = -9773.748243096805
9
Iteration 13400: Loss = -9773.748648578241
10
Stopping early at iteration 13400 due to no improvement.
tensor([[ 2.0323, -4.2412],
        [ 0.8184, -2.2378],
        [ 1.1029, -3.5901],
        [ 1.3853, -3.0645],
        [ 1.9867, -3.5894],
        [ 0.5004, -1.9303],
        [ 0.2596, -1.6692],
        [ 1.1369, -3.0197],
        [ 0.3526, -3.2694],
        [ 1.2922, -3.3138],
        [-0.2270, -3.3199],
        [-1.7280, -1.5327],
        [-0.3734, -1.7623],
        [-0.1291, -1.2846],
        [ 0.9232, -2.3141],
        [ 1.2578, -3.9172],
        [ 0.7856, -2.4127],
        [ 2.5854, -4.1171],
        [ 0.3852, -3.7583],
        [ 0.1249, -1.8255],
        [ 1.6630, -4.6919],
        [-0.1656, -2.4094],
        [ 0.5914, -2.4668],
        [-0.2324, -1.3552],
        [ 0.0745, -1.5203],
        [ 1.2792, -2.7651],
        [-0.6177, -0.7721],
        [ 1.3750, -2.8041],
        [ 0.4444, -3.3161],
        [ 0.9390, -2.5451],
        [ 2.1365, -3.5346],
        [ 2.0362, -3.8504],
        [-0.0977, -1.3238],
        [-0.3440, -2.3586],
        [ 2.0254, -3.4660],
        [-1.3130, -0.7271],
        [ 0.9346, -3.2030],
        [ 0.5061, -3.4369],
        [ 0.2442, -1.7807],
        [ 2.4674, -3.9175],
        [ 2.4329, -4.0586],
        [ 1.3571, -3.7575],
        [-0.8905, -0.7876],
        [ 0.7777, -2.4555],
        [ 2.7606, -4.4781],
        [-0.6729, -0.7207],
        [ 2.3083, -4.1771],
        [ 2.0069, -4.2085],
        [ 0.9286, -2.8939],
        [ 2.1280, -3.6012],
        [ 0.7524, -2.3387],
        [-1.0158, -1.5938],
        [-1.6119, -0.7375],
        [ 0.4847, -1.8769],
        [ 0.5022, -2.9232],
        [ 2.2864, -3.7468],
        [ 1.5048, -2.9131],
        [ 1.9263, -3.3226],
        [ 0.4832, -1.9634],
        [ 1.8927, -3.2918],
        [-0.7179, -0.8881],
        [-2.5019, -0.5733],
        [-0.4503, -1.6296],
        [-0.9394, -0.5040],
        [-0.2272, -1.4223],
        [ 1.0223, -3.9496],
        [ 0.6808, -2.0755],
        [-1.1286, -0.4742],
        [ 0.0570, -1.7719],
        [ 1.4451, -3.3089],
        [ 1.6743, -3.8244],
        [ 1.4598, -2.8739],
        [-1.1631, -0.7344],
        [ 1.0052, -2.3915],
        [ 0.5289, -2.5230],
        [ 2.1102, -3.9536],
        [ 1.8665, -3.9840],
        [ 0.9514, -4.6386],
        [ 0.4871, -2.2586],
        [-0.8235, -3.5600],
        [ 0.7755, -2.9007],
        [ 1.4479, -5.0123],
        [-1.0473, -0.9542],
        [ 0.6773, -2.7286],
        [-0.1900, -2.5360],
        [ 1.9435, -4.8560],
        [ 0.1038, -2.4473],
        [ 0.8802, -2.7090],
        [ 2.0702, -3.8104],
        [ 2.0835, -4.6322],
        [ 2.0093, -3.3957],
        [-1.2979, -2.8500],
        [-0.5304, -2.6772],
        [ 1.9693, -3.4289],
        [ 2.8244, -4.6895],
        [-0.1864, -1.3527],
        [ 0.1645, -1.6328],
        [-0.3260, -1.8418],
        [ 1.0689, -2.4577],
        [ 1.5671, -3.1714]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9929, 0.0071],
        [0.6364, 0.3636]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8796, 0.1204], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1371, 0.0823],
         [0.3125, 0.0755]],

        [[0.5102, 0.0872],
         [0.7183, 0.7764]],

        [[0.4179, 0.2189],
         [0.4833, 0.7488]],

        [[0.0173, 0.1171],
         [0.3691, 0.3885]],

        [[0.0279, 0.2256],
         [0.0309, 0.5141]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.01669750032904028
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.005350265809674365
Average Adjusted Rand Index: -0.005843214413609273
9887.677794698497
new:  [-0.006384526905169038, -0.005350265809674365, -0.005350265809674365, -0.005350265809674365] [-0.00569697302392939, -0.005843214413609273, -0.005843214413609273, -0.005843214413609273] [9773.847856950533, 9773.748316585848, 9773.766005339512, 9773.748648578241]
prior:  [0.007711719344541677, 0.0012104823609284674, 0.0012104823609284674, 0.0012104823609284674] [0.0015112682499459247, -0.001677317058922544, -0.001677317058922544, -0.001677317058922544] [9777.924527501224, 9776.379721656427, 9776.379705056983, 9776.3794329091]
-----------------------------------------------------------------------------------------
This iteration is 12
True Objective function: Loss = -9987.588496510527
Iteration 0: Loss = -38821.610217090594
Iteration 10: Loss = -9871.210405626123
Iteration 20: Loss = -9871.21034906102
Iteration 30: Loss = -9871.20939566002
Iteration 40: Loss = -9871.198563701168
Iteration 50: Loss = -9871.077699300291
Iteration 60: Loss = -9870.510846526571
Iteration 70: Loss = -9870.030883394751
Iteration 80: Loss = -9869.875994708747
Iteration 90: Loss = -9869.828527864887
Iteration 100: Loss = -9869.81213916881
Iteration 110: Loss = -9869.805768078873
Iteration 120: Loss = -9869.803147083017
Iteration 130: Loss = -9869.802007449538
Iteration 140: Loss = -9869.801499066889
Iteration 150: Loss = -9869.801308527161
Iteration 160: Loss = -9869.801170179126
Iteration 170: Loss = -9869.80115609019
Iteration 180: Loss = -9869.80111574896
Iteration 190: Loss = -9869.80113792763
1
Iteration 200: Loss = -9869.801136008471
2
Iteration 210: Loss = -9869.801111662659
Iteration 220: Loss = -9869.801116500674
1
Iteration 230: Loss = -9869.801121855504
2
Iteration 240: Loss = -9869.801139450135
3
Stopping early at iteration 239 due to no improvement.
pi: tensor([[9.7657e-01, 2.3430e-02],
        [1.0000e+00, 9.3598e-31]], dtype=torch.float64)
alpha: tensor([0.9767, 0.0233])
beta: tensor([[[0.1344, 0.1975],
         [0.2419, 0.1912]],

        [[0.8547, 0.0741],
         [0.2905, 0.0457]],

        [[0.6377, 0.1641],
         [0.8509, 0.5594]],

        [[0.2369, 0.1959],
         [0.0332, 0.8618]],

        [[0.9670, 0.1294],
         [0.2557, 0.3727]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38417.40437520995
Iteration 100: Loss = -9874.546317240101
Iteration 200: Loss = -9872.809782466236
Iteration 300: Loss = -9872.263164590295
Iteration 400: Loss = -9871.93110182506
Iteration 500: Loss = -9871.712658911829
Iteration 600: Loss = -9871.558048830424
Iteration 700: Loss = -9871.44074884555
Iteration 800: Loss = -9871.34519864223
Iteration 900: Loss = -9871.262625350017
Iteration 1000: Loss = -9871.192620120704
Iteration 1100: Loss = -9871.141444905128
Iteration 1200: Loss = -9871.107864387357
Iteration 1300: Loss = -9871.08179064433
Iteration 1400: Loss = -9871.054732381255
Iteration 1500: Loss = -9871.015417406703
Iteration 1600: Loss = -9870.916341249142
Iteration 1700: Loss = -9870.644463822948
Iteration 1800: Loss = -9870.461942769618
Iteration 1900: Loss = -9870.369826309938
Iteration 2000: Loss = -9870.3107980677
Iteration 2100: Loss = -9870.269774464963
Iteration 2200: Loss = -9870.23943288031
Iteration 2300: Loss = -9870.216190183877
Iteration 2400: Loss = -9870.19766248537
Iteration 2500: Loss = -9870.182259912577
Iteration 2600: Loss = -9870.168903298698
Iteration 2700: Loss = -9870.156503697905
Iteration 2800: Loss = -9870.143568900361
Iteration 2900: Loss = -9870.124904495995
Iteration 3000: Loss = -9870.051666681158
Iteration 3100: Loss = -9869.683691376551
Iteration 3200: Loss = -9868.887925230336
Iteration 3300: Loss = -9868.336932520633
Iteration 3400: Loss = -9867.789679118494
Iteration 3500: Loss = -9867.619511328996
Iteration 3600: Loss = -9867.477263592262
Iteration 3700: Loss = -9867.385599904032
Iteration 3800: Loss = -9867.327830144906
Iteration 3900: Loss = -9867.302462178304
Iteration 4000: Loss = -9867.284047565261
Iteration 4100: Loss = -9867.268309651401
Iteration 4200: Loss = -9867.25555569643
Iteration 4300: Loss = -9867.247801076088
Iteration 4400: Loss = -9867.24392844506
Iteration 4500: Loss = -9867.24182253459
Iteration 4600: Loss = -9867.240458128756
Iteration 4700: Loss = -9867.239438178647
Iteration 4800: Loss = -9867.238651665468
Iteration 4900: Loss = -9867.238051650947
Iteration 5000: Loss = -9867.237559665173
Iteration 5100: Loss = -9867.237105167484
Iteration 5200: Loss = -9867.23672819806
Iteration 5300: Loss = -9867.236381579207
Iteration 5400: Loss = -9867.236130544698
Iteration 5500: Loss = -9867.235851808464
Iteration 5600: Loss = -9867.235629017425
Iteration 5700: Loss = -9867.235405368256
Iteration 5800: Loss = -9867.235238375511
Iteration 5900: Loss = -9867.235016961726
Iteration 6000: Loss = -9867.234865056373
Iteration 6100: Loss = -9867.234679847694
Iteration 6200: Loss = -9867.234564656617
Iteration 6300: Loss = -9867.234432155157
Iteration 6400: Loss = -9867.234327641474
Iteration 6500: Loss = -9867.234194680343
Iteration 6600: Loss = -9867.23409160115
Iteration 6700: Loss = -9867.233987518392
Iteration 6800: Loss = -9867.233851255622
Iteration 6900: Loss = -9867.233794398315
Iteration 7000: Loss = -9867.23371756208
Iteration 7100: Loss = -9867.233604675876
Iteration 7200: Loss = -9867.233568160322
Iteration 7300: Loss = -9867.233489558028
Iteration 7400: Loss = -9867.233387225027
Iteration 7500: Loss = -9867.233383901734
Iteration 7600: Loss = -9867.233272606712
Iteration 7700: Loss = -9867.233263309143
Iteration 7800: Loss = -9867.233185877405
Iteration 7900: Loss = -9867.233137029833
Iteration 8000: Loss = -9867.233111345162
Iteration 8100: Loss = -9867.233175925616
1
Iteration 8200: Loss = -9867.232993004005
Iteration 8300: Loss = -9867.23294295699
Iteration 8400: Loss = -9867.239674749455
1
Iteration 8500: Loss = -9867.242177348873
2
Iteration 8600: Loss = -9867.236115523152
3
Iteration 8700: Loss = -9867.232817833474
Iteration 8800: Loss = -9867.232800047756
Iteration 8900: Loss = -9867.232746267244
Iteration 9000: Loss = -9867.232812660028
1
Iteration 9100: Loss = -9867.232672502038
Iteration 9200: Loss = -9867.234064161627
1
Iteration 9300: Loss = -9867.232618450604
Iteration 9400: Loss = -9867.232572212466
Iteration 9500: Loss = -9867.23266162075
1
Iteration 9600: Loss = -9867.232484454687
Iteration 9700: Loss = -9867.23241835407
Iteration 9800: Loss = -9867.232431725468
1
Iteration 9900: Loss = -9867.232328298047
Iteration 10000: Loss = -9867.232276217144
Iteration 10100: Loss = -9867.232517882398
1
Iteration 10200: Loss = -9867.232203530592
Iteration 10300: Loss = -9867.232162838922
Iteration 10400: Loss = -9867.232337110772
1
Iteration 10500: Loss = -9867.232115280227
Iteration 10600: Loss = -9867.232087294531
Iteration 10700: Loss = -9867.232823899983
1
Iteration 10800: Loss = -9867.232059291193
Iteration 10900: Loss = -9867.232051117158
Iteration 11000: Loss = -9867.234874071506
1
Iteration 11100: Loss = -9867.231967287664
Iteration 11200: Loss = -9867.231982350242
1
Iteration 11300: Loss = -9867.38773806111
2
Iteration 11400: Loss = -9867.231893504764
Iteration 11500: Loss = -9867.231844486761
Iteration 11600: Loss = -9867.236581954307
1
Iteration 11700: Loss = -9867.23165164475
Iteration 11800: Loss = -9867.231557997939
Iteration 11900: Loss = -9867.231363156365
Iteration 12000: Loss = -9867.231192755633
Iteration 12100: Loss = -9867.047051422089
Iteration 12200: Loss = -9866.958059363413
Iteration 12300: Loss = -9866.417972024821
Iteration 12400: Loss = -9866.405669661302
Iteration 12500: Loss = -9866.403612304368
Iteration 12600: Loss = -9866.423954366082
1
Iteration 12700: Loss = -9866.402230827807
Iteration 12800: Loss = -9866.401894741903
Iteration 12900: Loss = -9866.403215502687
1
Iteration 13000: Loss = -9866.401454863368
Iteration 13100: Loss = -9866.401348991598
Iteration 13200: Loss = -9866.401276163897
Iteration 13300: Loss = -9866.401158452549
Iteration 13400: Loss = -9866.410317443931
1
Iteration 13500: Loss = -9866.401004839163
Iteration 13600: Loss = -9866.400986784278
Iteration 13700: Loss = -9866.401026587153
1
Iteration 13800: Loss = -9866.406682083363
2
Iteration 13900: Loss = -9866.402448045084
3
Iteration 14000: Loss = -9866.400805694013
Iteration 14100: Loss = -9866.40122476326
1
Iteration 14200: Loss = -9866.400732646674
Iteration 14300: Loss = -9866.400931860753
1
Iteration 14400: Loss = -9866.400707231516
Iteration 14500: Loss = -9866.4008168284
1
Iteration 14600: Loss = -9866.400829834907
2
Iteration 14700: Loss = -9866.410256679968
3
Iteration 14800: Loss = -9866.400637988885
Iteration 14900: Loss = -9866.400698796375
1
Iteration 15000: Loss = -9866.400627661582
Iteration 15100: Loss = -9866.40062958628
1
Iteration 15200: Loss = -9866.401008099421
2
Iteration 15300: Loss = -9866.401135078962
3
Iteration 15400: Loss = -9866.531132440614
4
Iteration 15500: Loss = -9866.400748104861
5
Iteration 15600: Loss = -9866.400715596577
6
Iteration 15700: Loss = -9866.47137124884
7
Iteration 15800: Loss = -9866.400538643651
Iteration 15900: Loss = -9866.405247171842
1
Iteration 16000: Loss = -9866.400533419024
Iteration 16100: Loss = -9866.40060158748
1
Iteration 16200: Loss = -9866.40051677124
Iteration 16300: Loss = -9866.401918705995
1
Iteration 16400: Loss = -9866.400486370467
Iteration 16500: Loss = -9866.401676557574
1
Iteration 16600: Loss = -9866.400551832292
2
Iteration 16700: Loss = -9866.401427689427
3
Iteration 16800: Loss = -9866.400477298499
Iteration 16900: Loss = -9866.411584654958
1
Iteration 17000: Loss = -9866.400504944904
2
Iteration 17100: Loss = -9866.400459881388
Iteration 17200: Loss = -9866.400498332529
1
Iteration 17300: Loss = -9866.400486454237
2
Iteration 17400: Loss = -9866.40228775025
3
Iteration 17500: Loss = -9866.400473362388
4
Iteration 17600: Loss = -9866.44709242112
5
Iteration 17700: Loss = -9866.40049041057
6
Iteration 17800: Loss = -9866.516700122196
7
Iteration 17900: Loss = -9866.400496049078
8
Iteration 18000: Loss = -9866.41899796965
9
Iteration 18100: Loss = -9866.400472913281
10
Stopping early at iteration 18100 due to no improvement.
tensor([[-7.8182e-02, -4.5370e+00],
        [-1.5386e-01, -4.4614e+00],
        [-2.6875e-01, -4.3465e+00],
        [ 3.2276e-01, -4.9380e+00],
        [ 6.9217e-01, -5.3074e+00],
        [-4.9051e+00,  2.8988e-01],
        [-3.8914e+00, -7.2387e-01],
        [ 4.3834e-01, -5.0536e+00],
        [-8.7893e-01, -3.7363e+00],
        [ 4.3725e-01, -5.0525e+00],
        [ 1.4381e+00, -6.0534e+00],
        [-2.1552e+00, -2.4600e+00],
        [-5.6025e-01, -4.0550e+00],
        [-3.0993e+00, -1.5160e+00],
        [ 3.0722e-01, -4.9224e+00],
        [ 6.8543e-01, -5.3007e+00],
        [-9.6939e-01, -3.6458e+00],
        [-2.7501e+00, -1.8651e+00],
        [-8.1871e-02, -4.5333e+00],
        [-4.6786e+00,  6.3382e-02],
        [-6.2269e+00,  1.6117e+00],
        [ 5.4502e-01, -5.1602e+00],
        [ 4.1257e-01, -5.0278e+00],
        [ 1.0032e+00, -5.6184e+00],
        [-4.7484e+00,  1.3315e-01],
        [-1.3768e+00, -3.2384e+00],
        [-5.9594e+00,  1.3442e+00],
        [-7.6179e-01, -3.8534e+00],
        [ 8.5030e-01, -5.4655e+00],
        [-3.6669e+00, -9.4834e-01],
        [-5.7652e-01, -4.0387e+00],
        [-8.9525e-01, -3.7200e+00],
        [-1.7295e-01, -4.4423e+00],
        [ 1.1771e+00, -5.7923e+00],
        [-1.0260e+00, -3.5892e+00],
        [ 1.9117e+00, -6.5269e+00],
        [ 2.2285e-01, -4.8381e+00],
        [ 4.2435e-03, -4.6195e+00],
        [-2.8757e-01, -4.3276e+00],
        [ 3.2095e-01, -4.9362e+00],
        [-3.9125e+00, -7.0273e-01],
        [ 9.6093e-01, -5.5762e+00],
        [-1.1163e+00, -3.4989e+00],
        [ 6.1428e-01, -5.2295e+00],
        [-5.0657e-01, -4.1087e+00],
        [-1.0552e+00, -3.5600e+00],
        [-4.1960e+00, -4.1918e-01],
        [-1.4093e-01, -4.4743e+00],
        [-1.5618e+00, -3.0535e+00],
        [ 1.9562e+00, -6.5714e+00],
        [-1.9135e+00, -2.7017e+00],
        [-3.8563e+00, -7.5895e-01],
        [ 2.0596e-03, -4.6173e+00],
        [ 2.8655e-01, -4.9018e+00],
        [-3.2996e+00, -1.3156e+00],
        [-5.2598e+00,  6.4460e-01],
        [-9.5866e-01, -3.6566e+00],
        [ 1.5835e+00, -6.1988e+00],
        [-1.1619e-02, -4.6036e+00],
        [-5.4333e+00,  8.1812e-01],
        [-3.8706e+00, -7.4466e-01],
        [ 4.6256e-01, -5.0778e+00],
        [ 1.2893e+00, -5.9046e+00],
        [-1.5657e-01, -4.4586e+00],
        [ 6.6886e-01, -5.2841e+00],
        [-7.6739e-01, -3.8478e+00],
        [ 1.4900e+00, -6.1052e+00],
        [-8.7747e-01, -3.7377e+00],
        [-2.2032e+00, -2.4120e+00],
        [-2.6666e-01, -4.3486e+00],
        [ 1.1296e+00, -5.7448e+00],
        [-1.8141e-01, -4.4338e+00],
        [ 1.6677e+00, -6.2829e+00],
        [-8.7399e-02, -4.5278e+00],
        [ 1.4227e+00, -6.0379e+00],
        [-1.2899e+00, -3.3254e+00],
        [-6.3343e-01, -3.9818e+00],
        [ 2.7278e-01, -4.8880e+00],
        [-2.8053e+00, -1.8099e+00],
        [-1.1901e+00, -3.4251e+00],
        [-1.2484e+00, -3.3668e+00],
        [ 4.8728e-01, -5.1025e+00],
        [ 1.1026e+00, -5.7178e+00],
        [-5.3248e-01, -4.0827e+00],
        [ 2.5759e-02, -4.6410e+00],
        [-3.5377e+00, -1.0775e+00],
        [-4.7727e+00,  1.5747e-01],
        [-4.7600e+00,  1.4481e-01],
        [-1.3298e+00, -3.2854e+00],
        [-4.1600e-01, -4.1992e+00],
        [ 1.0813e+00, -5.6965e+00],
        [ 5.2398e-02, -4.6676e+00],
        [-7.8710e-01, -3.8281e+00],
        [-4.2434e-01, -4.1909e+00],
        [-4.4206e+00, -1.9463e-01],
        [ 4.9750e-01, -5.1127e+00],
        [ 1.0889e+00, -5.7041e+00],
        [-2.6413e+00, -1.9739e+00],
        [-4.2437e+00, -3.7155e-01],
        [-5.3158e+00,  7.0061e-01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.8147e-01, 1.8526e-02],
        [1.0000e+00, 6.8297e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7452, 0.2548], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1347, 0.1186],
         [0.2419, 0.3442]],

        [[0.8547, 0.0664],
         [0.2905, 0.0457]],

        [[0.6377, 0.1739],
         [0.8509, 0.5594]],

        [[0.2369, 0.2054],
         [0.0332, 0.8618]],

        [[0.9670, 0.1423],
         [0.2557, 0.3727]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.11977908180876769
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.009622902507968966
Average Adjusted Rand Index: 0.02465156936943581
Iteration 0: Loss = -38075.664834786316
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6043,    nan]],

        [[0.2210,    nan],
         [0.4085, 0.2750]],

        [[0.4850,    nan],
         [0.6827, 0.8587]],

        [[0.7636,    nan],
         [0.5058, 0.3544]],

        [[0.1426,    nan],
         [0.5672, 0.4660]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38074.93875077928
Iteration 100: Loss = -9900.722827429992
Iteration 200: Loss = -9883.368582374473
Iteration 300: Loss = -9876.385138301137
Iteration 400: Loss = -9874.39254631762
Iteration 500: Loss = -9873.368508360445
Iteration 600: Loss = -9872.753718910877
Iteration 700: Loss = -9872.348844086519
Iteration 800: Loss = -9872.061027556936
Iteration 900: Loss = -9871.840699876255
Iteration 1000: Loss = -9871.6565603389
Iteration 1100: Loss = -9871.47934005802
Iteration 1200: Loss = -9871.269568274616
Iteration 1300: Loss = -9871.043063437093
Iteration 1400: Loss = -9870.869871434435
Iteration 1500: Loss = -9870.726794226357
Iteration 1600: Loss = -9870.605413883557
Iteration 1700: Loss = -9870.498788534453
Iteration 1800: Loss = -9870.393932800833
Iteration 1900: Loss = -9870.221682508403
Iteration 2000: Loss = -9869.672751732318
Iteration 2100: Loss = -9869.26895023862
Iteration 2200: Loss = -9869.187208026451
Iteration 2300: Loss = -9869.142924827227
Iteration 2400: Loss = -9869.108749568437
Iteration 2500: Loss = -9869.080956078738
Iteration 2600: Loss = -9869.057504060294
Iteration 2700: Loss = -9869.03740274246
Iteration 2800: Loss = -9869.019970395524
Iteration 2900: Loss = -9869.00474238343
Iteration 3000: Loss = -9868.991413047745
Iteration 3100: Loss = -9868.979745942886
Iteration 3200: Loss = -9868.96946886934
Iteration 3300: Loss = -9868.960578417335
Iteration 3400: Loss = -9868.95275486651
Iteration 3500: Loss = -9868.9460257243
Iteration 3600: Loss = -9868.940148421863
Iteration 3700: Loss = -9868.934982332356
Iteration 3800: Loss = -9868.930419387727
Iteration 3900: Loss = -9868.926438193583
Iteration 4000: Loss = -9868.922849912002
Iteration 4100: Loss = -9868.919682553205
Iteration 4200: Loss = -9868.91682134486
Iteration 4300: Loss = -9868.914234488278
Iteration 4400: Loss = -9868.911905564864
Iteration 4500: Loss = -9868.90981448656
Iteration 4600: Loss = -9868.90786381599
Iteration 4700: Loss = -9868.906094033355
Iteration 4800: Loss = -9868.904473400678
Iteration 4900: Loss = -9868.902990636274
Iteration 5000: Loss = -9868.901602408865
Iteration 5100: Loss = -9868.900336928818
Iteration 5200: Loss = -9868.899148604873
Iteration 5300: Loss = -9868.898068286971
Iteration 5400: Loss = -9868.89701210798
Iteration 5500: Loss = -9868.896047413355
Iteration 5600: Loss = -9868.895150968578
Iteration 5700: Loss = -9868.894311455837
Iteration 5800: Loss = -9868.893479707198
Iteration 5900: Loss = -9868.892729924717
Iteration 6000: Loss = -9868.891994887874
Iteration 6100: Loss = -9868.89132763754
Iteration 6200: Loss = -9868.890675374301
Iteration 6300: Loss = -9868.89006195978
Iteration 6400: Loss = -9868.889467881723
Iteration 6500: Loss = -9868.888889296915
Iteration 6600: Loss = -9868.888320606538
Iteration 6700: Loss = -9868.887848695362
Iteration 6800: Loss = -9868.887378331583
Iteration 6900: Loss = -9868.886885353182
Iteration 7000: Loss = -9868.886447076518
Iteration 7100: Loss = -9868.885989792803
Iteration 7200: Loss = -9868.8856444074
Iteration 7300: Loss = -9868.88521961548
Iteration 7400: Loss = -9868.884831740052
Iteration 7500: Loss = -9868.884526242193
Iteration 7600: Loss = -9868.884122703228
Iteration 7700: Loss = -9868.883807912616
Iteration 7800: Loss = -9868.883513858833
Iteration 7900: Loss = -9868.883158849088
Iteration 8000: Loss = -9868.882821134524
Iteration 8100: Loss = -9868.882577900076
Iteration 8200: Loss = -9868.882242388872
Iteration 8300: Loss = -9868.8824349664
1
Iteration 8400: Loss = -9868.881545919445
Iteration 8500: Loss = -9868.881166882993
Iteration 8600: Loss = -9868.88075039832
Iteration 8700: Loss = -9868.88025536206
Iteration 8800: Loss = -9868.879577744696
Iteration 8900: Loss = -9868.878659525186
Iteration 9000: Loss = -9868.878017662475
Iteration 9100: Loss = -9868.875007500186
Iteration 9200: Loss = -9868.870966020966
Iteration 9300: Loss = -9868.882762465762
1
Iteration 9400: Loss = -9868.85437644301
Iteration 9500: Loss = -9868.8469119728
Iteration 9600: Loss = -9868.842892241708
Iteration 9700: Loss = -9868.85903785968
1
Iteration 9800: Loss = -9868.83930343709
Iteration 9900: Loss = -9868.838366573138
Iteration 10000: Loss = -9868.837641336911
Iteration 10100: Loss = -9868.837115990562
Iteration 10200: Loss = -9868.83662812566
Iteration 10300: Loss = -9868.8362238689
Iteration 10400: Loss = -9868.841342602875
1
Iteration 10500: Loss = -9868.835631869246
Iteration 10600: Loss = -9868.835337978146
Iteration 10700: Loss = -9868.84083513663
1
Iteration 10800: Loss = -9868.834983661949
Iteration 10900: Loss = -9868.834802831401
Iteration 11000: Loss = -9868.834621515205
Iteration 11100: Loss = -9868.834519893413
Iteration 11200: Loss = -9868.834362726333
Iteration 11300: Loss = -9868.83425031475
Iteration 11400: Loss = -9868.838393149008
1
Iteration 11500: Loss = -9868.834055927493
Iteration 11600: Loss = -9868.833987017073
Iteration 11700: Loss = -9868.833879195889
Iteration 11800: Loss = -9868.834401072916
1
Iteration 11900: Loss = -9868.833727975745
Iteration 12000: Loss = -9868.833665968701
Iteration 12100: Loss = -9868.842826394495
1
Iteration 12200: Loss = -9868.833556492908
Iteration 12300: Loss = -9868.83365310926
1
Iteration 12400: Loss = -9868.83695123672
2
Iteration 12500: Loss = -9868.833417983366
Iteration 12600: Loss = -9868.83337507734
Iteration 12700: Loss = -9868.833483010329
1
Iteration 12800: Loss = -9868.87557162229
2
Iteration 12900: Loss = -9868.928186636022
3
Iteration 13000: Loss = -9868.833323335371
Iteration 13100: Loss = -9868.83324242632
Iteration 13200: Loss = -9868.91581692009
1
Iteration 13300: Loss = -9868.833125561188
Iteration 13400: Loss = -9868.835553648045
1
Iteration 13500: Loss = -9868.833134833712
2
Iteration 13600: Loss = -9868.833056675167
Iteration 13700: Loss = -9868.836549927859
1
Iteration 13800: Loss = -9868.833019426233
Iteration 13900: Loss = -9868.832964569721
Iteration 14000: Loss = -9868.835744623848
1
Iteration 14100: Loss = -9868.832980481695
2
Iteration 14200: Loss = -9868.832912387707
Iteration 14300: Loss = -9868.83448041847
1
Iteration 14400: Loss = -9868.83289239114
Iteration 14500: Loss = -9868.832900444431
1
Iteration 14600: Loss = -9868.833072014311
2
Iteration 14700: Loss = -9868.832921343848
3
Iteration 14800: Loss = -9868.838170928564
4
Iteration 14900: Loss = -9868.83288587772
Iteration 15000: Loss = -9868.83328873946
1
Iteration 15100: Loss = -9868.83281782732
Iteration 15200: Loss = -9868.83316379772
1
Iteration 15300: Loss = -9868.832804281568
Iteration 15400: Loss = -9868.847731927779
1
Iteration 15500: Loss = -9868.832800209213
Iteration 15600: Loss = -9868.832786724226
Iteration 15700: Loss = -9868.834538999994
1
Iteration 15800: Loss = -9868.832737941704
Iteration 15900: Loss = -9868.832754664463
1
Iteration 16000: Loss = -9868.857347190493
2
Iteration 16100: Loss = -9868.832728485855
Iteration 16200: Loss = -9868.83273674497
1
Iteration 16300: Loss = -9868.83273113961
2
Iteration 16400: Loss = -9868.833461262257
3
Iteration 16500: Loss = -9868.83272801665
Iteration 16600: Loss = -9868.832716216984
Iteration 16700: Loss = -9868.832893992092
1
Iteration 16800: Loss = -9868.837987166708
2
Iteration 16900: Loss = -9868.882660894571
3
Iteration 17000: Loss = -9868.832719131298
4
Iteration 17100: Loss = -9868.848606651594
5
Iteration 17200: Loss = -9868.83268818699
Iteration 17300: Loss = -9868.839011199078
1
Iteration 17400: Loss = -9868.877349628625
2
Iteration 17500: Loss = -9868.832688650957
3
Iteration 17600: Loss = -9868.833135001738
4
Iteration 17700: Loss = -9868.832702449967
5
Iteration 17800: Loss = -9868.832819355075
6
Iteration 17900: Loss = -9868.832695964626
7
Iteration 18000: Loss = -9868.832887512111
8
Iteration 18100: Loss = -9868.832649843533
Iteration 18200: Loss = -9868.839451733356
1
Iteration 18300: Loss = -9868.832658114641
2
Iteration 18400: Loss = -9868.832635417893
Iteration 18500: Loss = -9868.832737228733
1
Iteration 18600: Loss = -9868.832636422567
2
Iteration 18700: Loss = -9868.8326546398
3
Iteration 18800: Loss = -9868.832962413486
4
Iteration 18900: Loss = -9868.832656633227
5
Iteration 19000: Loss = -9868.83265351186
6
Iteration 19100: Loss = -9868.838407300704
7
Iteration 19200: Loss = -9868.832654458776
8
Iteration 19300: Loss = -9868.832653970181
9
Iteration 19400: Loss = -9868.833308263185
10
Stopping early at iteration 19400 due to no improvement.
tensor([[-3.5762,  2.1047],
        [-5.9721,  3.3799],
        [-4.8688,  3.4222],
        [-6.0109,  4.2022],
        [-6.4094,  3.4471],
        [-3.3504,  1.2220],
        [-4.8151,  3.4134],
        [-3.7659,  2.3487],
        [-4.7533,  3.3584],
        [-6.3914,  4.9663],
        [-6.8218,  3.9952],
        [-2.0275,  0.5844],
        [-4.8674,  3.3050],
        [-4.1165,  2.6913],
        [-6.1871,  3.5678],
        [-5.4682,  3.0773],
        [-4.9219,  1.3963],
        [-4.9953,  3.1227],
        [-6.1360,  4.7415],
        [ 1.3981, -3.1042],
        [ 2.0391, -3.4426],
        [-4.5418,  3.1217],
        [-5.1774,  1.8204],
        [-5.4707,  4.0712],
        [-3.7169,  2.1441],
        [-2.7104,  0.9789],
        [-4.4927,  2.0640],
        [-3.6330,  1.6400],
        [-5.5878,  4.1484],
        [ 1.2282, -3.4374],
        [-5.8704,  4.4495],
        [-5.9132,  2.7997],
        [-6.5054,  4.7183],
        [-6.1196,  4.2695],
        [-5.2270,  2.6889],
        [-7.1985,  4.3575],
        [-5.3406,  2.4973],
        [-4.8899,  2.5442],
        [-4.7985,  3.3356],
        [-5.3872,  3.2526],
        [-1.3201, -0.2057],
        [-5.9784,  4.5918],
        [-4.3487,  2.4416],
        [-5.8449,  3.9810],
        [-5.2452,  3.3043],
        [-4.1757,  2.7883],
        [-0.2626, -1.1488],
        [-5.7216,  2.4739],
        [-3.5066,  2.0591],
        [-6.3702,  4.9532],
        [-4.0355,  2.4134],
        [-3.0063,  0.5716],
        [-3.7411,  2.3542],
        [-5.6780,  3.9945],
        [-2.9848,  0.4251],
        [ 0.8661, -2.2839],
        [-5.0819,  3.6238],
        [-5.8221,  4.3160],
        [-4.1511,  2.6037],
        [ 0.8680, -2.2631],
        [-3.0937,  1.6569],
        [-5.5514,  3.9136],
        [-6.5719,  3.9333],
        [-5.7976,  4.2112],
        [-5.9429,  4.4926],
        [-3.5401,  2.1430],
        [-4.7345,  3.3440],
        [-5.4808,  1.8006],
        [-3.6061,  1.1539],
        [-4.8059,  3.0521],
        [-6.5896,  4.1406],
        [-3.4006,  1.5049],
        [-5.8986,  4.4951],
        [-4.5802,  3.1368],
        [-6.9171,  5.3249],
        [-5.0955,  3.3704],
        [-4.0111,  2.6194],
        [-6.2820,  4.5140],
        [-6.8242,  3.2275],
        [-3.3606,  1.9446],
        [-5.4582,  4.0270],
        [-4.0061,  2.6087],
        [-7.1251,  5.6117],
        [-6.6095,  1.9943],
        [-5.2003,  3.4717],
        [-3.1400,  1.4396],
        [ 0.4490, -2.0059],
        [-2.4665,  0.9145],
        [-5.1465,  3.1883],
        [-4.4918,  2.9594],
        [-5.8053,  2.6970],
        [-6.0237,  4.2804],
        [-4.8721,  3.4847],
        [-4.0429,  2.6564],
        [-3.2878,  1.1737],
        [-5.4296,  3.7185],
        [-6.3284,  3.3175],
        [-4.9487,  3.0629],
        [-0.9321, -0.7163],
        [-2.8967,  0.5909]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.2749e-06, 1.0000e+00],
        [1.8332e-03, 9.9817e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0755, 0.9245], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.7193, 0.1644],
         [0.6043, 0.1352]],

        [[0.2210, 0.0670],
         [0.4085, 0.2750]],

        [[0.4850, 0.1744],
         [0.6827, 0.8587]],

        [[0.7636, 0.1063],
         [0.5058, 0.3544]],

        [[0.1426, 0.1272],
         [0.5672, 0.4660]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.013015020241383225
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000673421209670367
Average Adjusted Rand Index: -0.0026030040482766447
Iteration 0: Loss = -19605.51371051139
Iteration 10: Loss = -9870.303722120008
Iteration 20: Loss = -9870.236481966987
Iteration 30: Loss = -9870.19274812589
Iteration 40: Loss = -9870.152870625183
Iteration 50: Loss = -9870.121678134214
Iteration 60: Loss = -9870.102086241921
Iteration 70: Loss = -9870.092425441
Iteration 80: Loss = -9870.088992393647
Iteration 90: Loss = -9870.089063735644
1
Iteration 100: Loss = -9870.091087620161
2
Iteration 110: Loss = -9870.094737364492
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.9347, 0.0653],
        [0.9578, 0.0422]], dtype=torch.float64)
alpha: tensor([0.9350, 0.0650])
beta: tensor([[[0.1317, 0.1726],
         [0.3491, 0.1944]],

        [[0.9638, 0.1436],
         [0.9482, 0.8730]],

        [[0.2688, 0.1613],
         [0.5967, 0.9034]],

        [[0.3548, 0.1727],
         [0.1741, 0.9442]],

        [[0.4417, 0.1430],
         [0.9264, 0.3710]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0006957530076822728
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19604.836631083635
Iteration 100: Loss = -9917.287850276805
Iteration 200: Loss = -9874.24553511839
Iteration 300: Loss = -9871.684185594433
Iteration 400: Loss = -9870.979209190196
Iteration 500: Loss = -9870.647421826818
Iteration 600: Loss = -9870.45133581291
Iteration 700: Loss = -9870.313550935034
Iteration 800: Loss = -9870.188228982783
Iteration 900: Loss = -9870.124469103692
Iteration 1000: Loss = -9870.07658299567
Iteration 1100: Loss = -9870.037046408876
Iteration 1200: Loss = -9870.003896104909
Iteration 1300: Loss = -9869.973499781103
Iteration 1400: Loss = -9869.889013893073
Iteration 1500: Loss = -9869.852460844726
Iteration 1600: Loss = -9869.823919564613
Iteration 1700: Loss = -9869.801471647359
Iteration 1800: Loss = -9869.783567833107
Iteration 1900: Loss = -9869.76886674899
Iteration 2000: Loss = -9869.756420830827
Iteration 2100: Loss = -9869.7456273792
Iteration 2200: Loss = -9869.736123247441
Iteration 2300: Loss = -9869.727490101282
Iteration 2400: Loss = -9869.719325939963
Iteration 2500: Loss = -9869.71143565201
Iteration 2600: Loss = -9869.703639490263
Iteration 2700: Loss = -9869.695729344574
Iteration 2800: Loss = -9869.687525568179
Iteration 2900: Loss = -9869.679001368524
Iteration 3000: Loss = -9869.669950843105
Iteration 3100: Loss = -9869.660498908119
Iteration 3200: Loss = -9869.650806156827
Iteration 3300: Loss = -9869.641130174054
Iteration 3400: Loss = -9869.631925869122
Iteration 3500: Loss = -9869.712976117324
1
Iteration 3600: Loss = -9869.61634606055
Iteration 3700: Loss = -9869.610753119301
Iteration 3800: Loss = -9869.606985513139
Iteration 3900: Loss = -9869.60453976368
Iteration 4000: Loss = -9869.60287447319
Iteration 4100: Loss = -9869.601667799636
Iteration 4200: Loss = -9869.608519546904
1
Iteration 4300: Loss = -9869.600043989767
Iteration 4400: Loss = -9869.59939653137
Iteration 4500: Loss = -9869.598906053403
Iteration 4600: Loss = -9869.598318130507
Iteration 4700: Loss = -9869.597778981515
Iteration 4800: Loss = -9869.597341927696
Iteration 4900: Loss = -9869.5969417291
Iteration 5000: Loss = -9869.596510476558
Iteration 5100: Loss = -9869.596137927412
Iteration 5200: Loss = -9869.598195625455
1
Iteration 5300: Loss = -9869.595427589626
Iteration 5400: Loss = -9869.59512219908
Iteration 5500: Loss = -9869.596383128857
1
Iteration 5600: Loss = -9869.594538217232
Iteration 5700: Loss = -9869.59425141435
Iteration 5800: Loss = -9869.595312327223
1
Iteration 5900: Loss = -9869.593721427782
Iteration 6000: Loss = -9869.5935471207
Iteration 6100: Loss = -9869.607666086451
1
Iteration 6200: Loss = -9869.593096838096
Iteration 6300: Loss = -9869.592845264337
Iteration 6400: Loss = -9869.600773720304
1
Iteration 6500: Loss = -9869.5924804215
Iteration 6600: Loss = -9869.592308513958
Iteration 6700: Loss = -9869.592840233174
1
Iteration 6800: Loss = -9869.591967392
Iteration 6900: Loss = -9869.630026697741
1
Iteration 7000: Loss = -9869.591745691623
Iteration 7100: Loss = -9869.591572399175
Iteration 7200: Loss = -9869.607275470695
1
Iteration 7300: Loss = -9869.591307631521
Iteration 7400: Loss = -9869.591198933342
Iteration 7500: Loss = -9869.59853826431
1
Iteration 7600: Loss = -9869.590951533788
Iteration 7700: Loss = -9869.590873244855
Iteration 7800: Loss = -9869.606389428562
1
Iteration 7900: Loss = -9869.68901489997
2
Iteration 8000: Loss = -9869.595348918056
3
Iteration 8100: Loss = -9869.590537984823
Iteration 8200: Loss = -9869.590484280741
Iteration 8300: Loss = -9869.611498623402
1
Iteration 8400: Loss = -9869.590289974662
Iteration 8500: Loss = -9869.599037255179
1
Iteration 8600: Loss = -9869.590137964224
Iteration 8700: Loss = -9869.59006888285
Iteration 8800: Loss = -9869.590152988902
1
Iteration 8900: Loss = -9869.589920829927
Iteration 9000: Loss = -9869.595345933774
1
Iteration 9100: Loss = -9869.589823770326
Iteration 9200: Loss = -9869.592561702788
1
Iteration 9300: Loss = -9869.589728355
Iteration 9400: Loss = -9869.589652306124
Iteration 9500: Loss = -9869.587593314545
Iteration 9600: Loss = -9869.382938173168
Iteration 9700: Loss = -9869.382161616979
Iteration 9800: Loss = -9869.382128797592
Iteration 9900: Loss = -9869.383985293716
1
Iteration 10000: Loss = -9869.381987075156
Iteration 10100: Loss = -9869.410637553183
1
Iteration 10200: Loss = -9869.381862735632
Iteration 10300: Loss = -9869.38240654777
1
Iteration 10400: Loss = -9869.518243287224
2
Iteration 10500: Loss = -9869.390512641601
3
Iteration 10600: Loss = -9869.4121697645
4
Iteration 10700: Loss = -9869.381630682088
Iteration 10800: Loss = -9869.419252322474
1
Iteration 10900: Loss = -9869.383458544165
2
Iteration 11000: Loss = -9869.381579110937
Iteration 11100: Loss = -9869.404121707841
1
Iteration 11200: Loss = -9869.381512390422
Iteration 11300: Loss = -9869.427179027763
1
Iteration 11400: Loss = -9869.381399917733
Iteration 11500: Loss = -9869.382299926883
1
Iteration 11600: Loss = -9869.381347265207
Iteration 11700: Loss = -9869.3812414551
Iteration 11800: Loss = -9869.382558043693
1
Iteration 11900: Loss = -9869.381252211433
2
Iteration 12000: Loss = -9869.381261625225
3
Iteration 12100: Loss = -9869.381321610312
4
Iteration 12200: Loss = -9869.382312152797
5
Iteration 12300: Loss = -9869.381265086424
6
Iteration 12400: Loss = -9869.381532310828
7
Iteration 12500: Loss = -9869.477530370612
8
Iteration 12600: Loss = -9869.386162135721
9
Iteration 12700: Loss = -9869.388748118823
10
Stopping early at iteration 12700 due to no improvement.
tensor([[ 2.3387e-01, -1.8407e+00],
        [ 1.2386e+00, -2.6825e+00],
        [ 4.0004e-01, -2.9748e+00],
        [-8.5969e-01, -3.7555e+00],
        [ 1.2776e+00, -3.2367e+00],
        [-3.1491e-01, -1.3061e+00],
        [-1.1021e-01, -2.6915e+00],
        [ 2.8196e-01, -1.8043e+00],
        [ 8.6970e-01, -2.2564e+00],
        [ 1.3694e+00, -2.8510e+00],
        [ 7.4096e-01, -2.2478e+00],
        [ 2.9236e-01, -1.7485e+00],
        [ 4.1855e-01, -2.1228e+00],
        [ 8.2113e-01, -2.5780e+00],
        [ 6.6752e+00, -1.0861e+01],
        [-9.7493e-02, -3.0302e+00],
        [ 3.9309e-01, -2.4654e+00],
        [ 2.6293e-01, -2.3214e+00],
        [ 7.1734e-01, -2.4552e+00],
        [-2.7447e-01, -1.1654e+00],
        [-8.8498e-01, -7.1886e-01],
        [-3.8645e-02, -1.9995e+00],
        [ 8.8626e-03, -3.6668e+00],
        [ 1.2390e+00, -2.7140e+00],
        [ 1.3976e-01, -1.6763e+00],
        [ 1.2240e-01, -1.6962e+00],
        [-2.2355e-01, -1.7010e+00],
        [ 6.0875e-01, -2.2664e+00],
        [ 1.1401e+00, -2.6125e+00],
        [ 3.1258e-02, -1.6580e+00],
        [-1.0364e-01, -2.4892e+00],
        [ 2.7525e-01, -2.0976e+00],
        [ 1.0193e+00, -2.4378e+00],
        [ 4.2001e-01, -1.9615e+00],
        [ 1.6088e+00, -3.0530e+00],
        [ 1.0717e+00, -2.6647e+00],
        [ 1.4962e+00, -3.3160e+00],
        [-1.1550e+00, -3.4602e+00],
        [-1.5785e-01, -3.6010e+00],
        [ 8.7904e-01, -2.2833e+00],
        [-6.1920e-01, -8.0519e-01],
        [ 6.0412e-01, -2.2987e+00],
        [ 3.5296e-01, -1.7522e+00],
        [ 2.1428e-01, -2.6890e+00],
        [ 7.4365e-01, -2.1794e+00],
        [ 8.9653e-01, -2.7209e+00],
        [ 1.0407e-01, -1.6845e+00],
        [ 6.8710e-01, -2.4576e+00],
        [ 3.1992e-01, -1.7563e+00],
        [ 8.0472e-01, -2.6695e+00],
        [ 8.3402e-01, -2.5345e+00],
        [ 3.2099e-01, -1.9691e+00],
        [-1.3964e-01, -2.7881e+00],
        [ 1.4104e+00, -2.8070e+00],
        [ 4.2701e-01, -1.8616e+00],
        [-8.1669e-01, -6.6435e-01],
        [ 5.7931e-01, -2.8648e+00],
        [-1.3612e-01, -2.2513e+00],
        [-5.6772e-01, -2.6620e+00],
        [-5.3589e-01, -1.7502e+00],
        [-1.4327e+00, -3.1825e+00],
        [-3.0558e-01, -4.3096e+00],
        [ 6.3324e-01, -2.0249e+00],
        [ 1.3395e-01, -1.9790e+00],
        [ 8.8401e-01, -2.3064e+00],
        [ 1.3883e-01, -2.2206e+00],
        [ 3.5557e-01, -2.0358e+00],
        [ 5.4983e-03, -2.0761e+00],
        [ 1.7215e-01, -1.5804e+00],
        [ 1.3568e-01, -2.2222e+00],
        [ 1.2736e+00, -2.7056e+00],
        [-6.0393e-02, -1.3316e+00],
        [ 2.4108e-01, -2.1555e+00],
        [ 5.8380e-01, -1.9974e+00],
        [ 1.2814e+00, -3.2564e+00],
        [-7.6348e-03, -3.3576e+00],
        [ 4.5106e-01, -2.2110e+00],
        [ 6.7137e-01, -2.2552e+00],
        [ 2.0761e-01, -1.8585e+00],
        [ 7.3026e-01, -2.1348e+00],
        [ 1.7206e-02, -2.0756e+00],
        [ 4.1485e-01, -2.7446e+00],
        [ 1.8146e+00, -3.2466e+00],
        [ 8.1066e-01, -2.6427e+00],
        [ 1.2832e-01, -2.2894e+00],
        [-2.8763e-01, -1.9751e+00],
        [-1.3691e-01, -1.3350e+00],
        [ 3.7052e-02, -1.4264e+00],
        [-4.3356e-01, -3.0624e+00],
        [ 1.0249e+00, -2.4457e+00],
        [ 7.9166e-01, -2.1780e+00],
        [ 3.4662e-01, -2.2744e+00],
        [ 9.5337e-01, -2.4484e+00],
        [ 3.0397e-01, -1.7432e+00],
        [-1.1959e-01, -2.1877e+00],
        [ 5.4246e-01, -3.1716e+00],
        [-3.7895e-01, -1.9663e+00],
        [ 1.0049e+00, -2.4031e+00],
        [-3.4191e-01, -1.5182e+00],
        [-5.9309e-01, -1.5502e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.7456e-01, 2.5442e-02],
        [9.9997e-01, 2.5756e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9057, 0.0943], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1350, 0.1680],
         [0.3491, 0.2232]],

        [[0.9638, 0.0712],
         [0.9482, 0.8730]],

        [[0.2688, 0.1687],
         [0.5967, 0.9034]],

        [[0.3548, 0.1967],
         [0.1741, 0.9442]],

        [[0.4417, 0.1366],
         [0.9264, 0.3710]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.847940178382811e-07
Average Adjusted Rand Index: -0.0008563678154033607
Iteration 0: Loss = -33980.395624576864
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6285,    nan]],

        [[0.1018,    nan],
         [0.8774, 0.5020]],

        [[0.2116,    nan],
         [0.9962, 0.5502]],

        [[0.0774,    nan],
         [0.0556, 0.8350]],

        [[0.6533,    nan],
         [0.5158, 0.9800]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33980.74662750786
Iteration 100: Loss = -9911.601293635344
Iteration 200: Loss = -9886.474993551301
Iteration 300: Loss = -9878.55069603942
Iteration 400: Loss = -9875.684323901538
Iteration 500: Loss = -9874.146903828001
Iteration 600: Loss = -9873.310153784534
Iteration 700: Loss = -9872.785951107055
Iteration 800: Loss = -9872.402719307307
Iteration 900: Loss = -9872.084686452221
Iteration 1000: Loss = -9871.8164026774
Iteration 1100: Loss = -9871.57771636812
Iteration 1200: Loss = -9871.373882943028
Iteration 1300: Loss = -9871.208460080197
Iteration 1400: Loss = -9871.064402003143
Iteration 1500: Loss = -9870.935428124692
Iteration 1600: Loss = -9870.820165905345
Iteration 1700: Loss = -9870.711607940611
Iteration 1800: Loss = -9870.602897517425
Iteration 1900: Loss = -9870.461325479315
Iteration 2000: Loss = -9870.269147617124
Iteration 2100: Loss = -9870.18454483524
Iteration 2200: Loss = -9870.062379773663
Iteration 2300: Loss = -9869.673077138119
Iteration 2400: Loss = -9869.269533374043
Iteration 2500: Loss = -9869.15026694141
Iteration 2600: Loss = -9869.09363881556
Iteration 2700: Loss = -9869.049930824589
Iteration 2800: Loss = -9869.011867294405
Iteration 2900: Loss = -9868.983289026392
Iteration 3000: Loss = -9868.950216516912
Iteration 3100: Loss = -9868.925027291963
Iteration 3200: Loss = -9868.905045686179
Iteration 3300: Loss = -9868.888661543275
Iteration 3400: Loss = -9868.874026483325
Iteration 3500: Loss = -9868.861363675258
Iteration 3600: Loss = -9868.870341756041
1
Iteration 3700: Loss = -9868.838476042763
Iteration 3800: Loss = -9868.827650615256
Iteration 3900: Loss = -9868.816861794927
Iteration 4000: Loss = -9868.805850378387
Iteration 4100: Loss = -9868.793970891276
Iteration 4200: Loss = -9868.781079743
Iteration 4300: Loss = -9868.772570610063
Iteration 4400: Loss = -9868.753126256108
Iteration 4500: Loss = -9868.739689764689
Iteration 4600: Loss = -9868.72770721833
Iteration 4700: Loss = -9868.717274695904
Iteration 4800: Loss = -9868.708373000502
Iteration 4900: Loss = -9868.700956285711
Iteration 5000: Loss = -9868.699991307853
Iteration 5100: Loss = -9868.68919536318
Iteration 5200: Loss = -9868.684645761517
Iteration 5300: Loss = -9868.850916310701
1
Iteration 5400: Loss = -9868.677384040635
Iteration 5500: Loss = -9868.674418956845
Iteration 5600: Loss = -9868.671818700193
Iteration 5700: Loss = -9868.67062892507
Iteration 5800: Loss = -9868.667520533201
Iteration 5900: Loss = -9868.665730449788
Iteration 6000: Loss = -9868.681530106687
1
Iteration 6100: Loss = -9868.662585268396
Iteration 6200: Loss = -9868.66127855488
Iteration 6300: Loss = -9868.681847517013
1
Iteration 6400: Loss = -9868.658966947674
Iteration 6500: Loss = -9868.657927859042
Iteration 6600: Loss = -9868.6569783978
Iteration 6700: Loss = -9868.675937730442
1
Iteration 6800: Loss = -9868.65531728956
Iteration 6900: Loss = -9868.654576893534
Iteration 7000: Loss = -9868.6798214815
1
Iteration 7100: Loss = -9868.653215957615
Iteration 7200: Loss = -9868.652583339663
Iteration 7300: Loss = -9868.652027624068
Iteration 7400: Loss = -9868.65152009243
Iteration 7500: Loss = -9868.65098200355
Iteration 7600: Loss = -9868.650506200185
Iteration 7700: Loss = -9868.650117314892
Iteration 7800: Loss = -9868.649764341462
Iteration 7900: Loss = -9868.649269622565
Iteration 8000: Loss = -9868.648889201437
Iteration 8100: Loss = -9868.65834256142
1
Iteration 8200: Loss = -9868.648228766906
Iteration 8300: Loss = -9868.647921880287
Iteration 8400: Loss = -9868.647590591354
Iteration 8500: Loss = -9868.647462989446
Iteration 8600: Loss = -9868.647080603825
Iteration 8700: Loss = -9868.646817144663
Iteration 8800: Loss = -9868.646561074696
Iteration 8900: Loss = -9868.64643323472
Iteration 9000: Loss = -9868.646092190349
Iteration 9100: Loss = -9868.645899546835
Iteration 9200: Loss = -9868.87271264486
1
Iteration 9300: Loss = -9868.6455275256
Iteration 9400: Loss = -9868.645367841842
Iteration 9500: Loss = -9868.645171118833
Iteration 9600: Loss = -9868.689935083305
1
Iteration 9700: Loss = -9868.644851452324
Iteration 9800: Loss = -9868.64470434439
Iteration 9900: Loss = -9868.644567856643
Iteration 10000: Loss = -9868.649366758224
1
Iteration 10100: Loss = -9868.644364643107
Iteration 10200: Loss = -9868.644180287212
Iteration 10300: Loss = -9868.644083735679
Iteration 10400: Loss = -9868.934072579144
1
Iteration 10500: Loss = -9868.643848160122
Iteration 10600: Loss = -9868.643755937064
Iteration 10700: Loss = -9868.643664676025
Iteration 10800: Loss = -9868.64803172842
1
Iteration 10900: Loss = -9868.64347349798
Iteration 11000: Loss = -9868.643371622164
Iteration 11100: Loss = -9868.643321723534
Iteration 11200: Loss = -9868.647841229322
1
Iteration 11300: Loss = -9868.643200175873
Iteration 11400: Loss = -9868.643081074106
Iteration 11500: Loss = -9868.643027166996
Iteration 11600: Loss = -9868.645570272398
1
Iteration 11700: Loss = -9868.642919657666
Iteration 11800: Loss = -9868.64285747044
Iteration 11900: Loss = -9868.642767158624
Iteration 12000: Loss = -9868.642728458135
Iteration 12100: Loss = -9868.642678333934
Iteration 12200: Loss = -9868.642648777753
Iteration 12300: Loss = -9868.717571633737
1
Iteration 12400: Loss = -9868.64255734367
Iteration 12500: Loss = -9868.642514569274
Iteration 12600: Loss = -9868.642455837678
Iteration 12700: Loss = -9868.642681656122
1
Iteration 12800: Loss = -9868.642370560048
Iteration 12900: Loss = -9868.642328910604
Iteration 13000: Loss = -9868.69502302512
1
Iteration 13100: Loss = -9868.642299026875
Iteration 13200: Loss = -9868.642278408173
Iteration 13300: Loss = -9868.645940411257
1
Iteration 13400: Loss = -9868.64220122387
Iteration 13500: Loss = -9868.642187891806
Iteration 13600: Loss = -9868.642326653455
1
Iteration 13700: Loss = -9868.642116773724
Iteration 13800: Loss = -9868.642634448139
1
Iteration 13900: Loss = -9868.642073833942
Iteration 14000: Loss = -9868.642013353117
Iteration 14100: Loss = -9868.642730775524
1
Iteration 14200: Loss = -9868.651127624824
2
Iteration 14300: Loss = -9868.642050079025
3
Iteration 14400: Loss = -9868.64203449679
4
Iteration 14500: Loss = -9868.66068583431
5
Iteration 14600: Loss = -9868.641916179002
Iteration 14700: Loss = -9868.662973912802
1
Iteration 14800: Loss = -9868.641868560233
Iteration 14900: Loss = -9868.692085177743
1
Iteration 15000: Loss = -9868.643449653098
2
Iteration 15100: Loss = -9868.645976485393
3
Iteration 15200: Loss = -9868.659423562398
4
Iteration 15300: Loss = -9868.643642745494
5
Iteration 15400: Loss = -9868.641820859348
Iteration 15500: Loss = -9868.65107416228
1
Iteration 15600: Loss = -9868.641800931022
Iteration 15700: Loss = -9868.65978819888
1
Iteration 15800: Loss = -9868.64177844863
Iteration 15900: Loss = -9868.641816761396
1
Iteration 16000: Loss = -9868.641950382054
2
Iteration 16100: Loss = -9868.641776821049
Iteration 16200: Loss = -9868.642017633973
1
Iteration 16300: Loss = -9868.641778625493
2
Iteration 16400: Loss = -9868.641743045991
Iteration 16500: Loss = -9868.64172576496
Iteration 16600: Loss = -9868.642228900047
1
Iteration 16700: Loss = -9868.641726160764
2
Iteration 16800: Loss = -9868.641718619578
Iteration 16900: Loss = -9868.699783877102
1
Iteration 17000: Loss = -9868.641705732545
Iteration 17100: Loss = -9868.641680542714
Iteration 17200: Loss = -9868.641700062417
1
Iteration 17300: Loss = -9868.64189857353
2
Iteration 17400: Loss = -9868.641683127136
3
Iteration 17500: Loss = -9868.641679568524
Iteration 17600: Loss = -9868.70852897592
1
Iteration 17700: Loss = -9868.64164487634
Iteration 17800: Loss = -9868.641692365916
1
Iteration 17900: Loss = -9868.641758271862
2
Iteration 18000: Loss = -9868.641667765569
3
Iteration 18100: Loss = -9868.65179389954
4
Iteration 18200: Loss = -9868.641664311093
5
Iteration 18300: Loss = -9868.641676087387
6
Iteration 18400: Loss = -9868.64166137315
7
Iteration 18500: Loss = -9868.641793602626
8
Iteration 18600: Loss = -9868.641647790695
9
Iteration 18700: Loss = -9868.643827710595
10
Stopping early at iteration 18700 due to no improvement.
tensor([[-4.0109,  1.4869],
        [-5.2500,  3.8581],
        [-5.2417,  2.6944],
        [-6.4890,  3.3114],
        [-5.5415,  4.0389],
        [-3.0493,  1.2865],
        [-4.6285,  3.2242],
        [-4.1080,  1.7703],
        [-4.9324,  2.8304],
        [-6.3607,  4.5642],
        [-5.9190,  4.5327],
        [-2.1368,  0.2903],
        [-4.8578,  3.0662],
        [-4.4162,  2.2167],
        [-5.4474,  3.9226],
        [-4.9491,  3.3389],
        [-3.7535,  2.3672],
        [-4.5911,  3.1344],
        [-5.9430,  4.5567],
        [ 1.5469, -2.9970],
        [ 1.8282, -3.6467],
        [-5.5226,  1.8698],
        [-5.7167,  1.1015],
        [-6.1068,  3.1793],
        [-3.6207,  1.9983],
        [-2.4071,  1.0033],
        [-3.9850,  2.1608],
        [-3.2976,  1.7670],
        [-5.4190,  4.0159],
        [ 1.2505, -3.3065],
        [-5.6724,  4.2861],
        [-4.9667,  3.4521],
        [-6.3012,  4.5497],
        [-6.2038,  3.8261],
        [-4.9351,  2.8507],
        [-6.3012,  4.9148],
        [-4.5386,  3.1443],
        [-5.8270,  1.2524],
        [-5.2056,  2.6546],
        [-4.8735,  3.4674],
        [-2.3687, -1.4631],
        [-6.0816,  4.1441],
        [-4.3061,  2.1880],
        [-5.5268,  3.9296],
        [-4.9548,  3.3271],
        [-4.2855,  2.4988],
        [-0.2717, -1.1561],
        [-5.4133,  2.4554],
        [-3.3956,  1.9669],
        [-6.9259,  4.0547],
        [-4.1408,  2.0928],
        [-2.5985,  0.8325],
        [-5.2647,  0.6495],
        [-5.6459,  3.7709],
        [-2.6977,  0.5901],
        [ 0.8848, -2.3052],
        [-5.1351,  3.2977],
        [-5.9186,  3.8552],
        [-3.9305,  2.5179],
        [ 0.8723, -2.2603],
        [-3.4535,  1.0129],
        [-5.4246,  3.7878],
        [-5.7663,  4.3688],
        [-5.5973,  4.0192],
        [-5.6957,  4.2998],
        [-4.0606,  1.4086],
        [-4.6051,  3.2179],
        [-4.2801,  2.6551],
        [-3.3526,  1.1429],
        [-4.7266,  2.8354],
        [-7.0995,  3.2700],
        [-3.4755,  1.2112],
        [-5.7534,  4.2877],
        [-4.5080,  2.8837],
        [-6.7725,  5.1517],
        [-5.1903,  2.9371],
        [-4.0341,  2.3942],
        [-5.9089,  4.5194],
        [-5.8463,  3.8241],
        [-3.2561,  1.8450],
        [-5.3084,  3.7307],
        [-4.6155,  1.8181],
        [-7.4523,  4.9306],
        [-4.9685,  3.3304],
        [-4.8890,  3.4917],
        [-3.0553,  1.2178],
        [ 0.2876, -2.1014],
        [-3.8416, -0.7736],
        [-5.0050,  3.0456],
        [-4.7730,  2.4826],
        [-4.9248,  3.3091],
        [-5.8355,  4.0610],
        [-6.3226,  1.7074],
        [-4.4857,  1.8928],
        [-2.8639,  1.3504],
        [-5.6037,  3.2657],
        [-5.3366,  3.9387],
        [-4.5705,  3.1359],
        [-0.7648, -0.6660],
        [-2.3195,  0.9168]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.7347e-06, 1.0000e+00],
        [4.5415e-03, 9.9546e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0766, 0.9234], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.7024, 0.1645],
         [0.6285, 0.1354]],

        [[0.1018, 0.0669],
         [0.8774, 0.5020]],

        [[0.2116, 0.1755],
         [0.9962, 0.5502]],

        [[0.0774, 0.2287],
         [0.0556, 0.8350]],

        [[0.6533, 0.1314],
         [0.5158, 0.9800]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.013015020241383225
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000673421209670367
Average Adjusted Rand Index: -0.0026030040482766447
Iteration 0: Loss = -21654.762495670853
Iteration 10: Loss = -9871.209898762512
Iteration 20: Loss = -9871.205735184942
Iteration 30: Loss = -9871.153260236611
Iteration 40: Loss = -9870.764653598406
Iteration 50: Loss = -9870.153010414577
Iteration 60: Loss = -9869.912115644824
Iteration 70: Loss = -9869.840089002995
Iteration 80: Loss = -9869.81632154766
Iteration 90: Loss = -9869.807470378548
Iteration 100: Loss = -9869.80386777546
Iteration 110: Loss = -9869.80233463342
Iteration 120: Loss = -9869.801679055146
Iteration 130: Loss = -9869.801356763064
Iteration 140: Loss = -9869.801229689156
Iteration 150: Loss = -9869.801163347762
Iteration 160: Loss = -9869.80111569605
Iteration 170: Loss = -9869.801098609678
Iteration 180: Loss = -9869.801129484676
1
Iteration 190: Loss = -9869.801092840456
Iteration 200: Loss = -9869.80110024715
1
Iteration 210: Loss = -9869.80111492908
2
Iteration 220: Loss = -9869.801124329562
3
Stopping early at iteration 219 due to no improvement.
pi: tensor([[2.4634e-14, 1.0000e+00],
        [2.3430e-02, 9.7657e-01]], dtype=torch.float64)
alpha: tensor([0.0233, 0.9767])
beta: tensor([[[0.1912, 0.1975],
         [0.6600, 0.1344]],

        [[0.8120, 0.0741],
         [0.3219, 0.3064]],

        [[0.6738, 0.1641],
         [0.7326, 0.3394]],

        [[0.5413, 0.1959],
         [0.7086, 0.4266]],

        [[0.6813, 0.1294],
         [0.5754, 0.6323]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21654.35790010275
Iteration 100: Loss = -9880.7089714555
Iteration 200: Loss = -9875.497046608074
Iteration 300: Loss = -9873.525931622904
Iteration 400: Loss = -9872.592414219163
Iteration 500: Loss = -9872.07501518333
Iteration 600: Loss = -9871.752296723602
Iteration 700: Loss = -9871.533071688016
Iteration 800: Loss = -9871.375435394071
Iteration 900: Loss = -9871.254111834169
Iteration 1000: Loss = -9871.155058221122
Iteration 1100: Loss = -9871.067871421874
Iteration 1200: Loss = -9870.982748474335
Iteration 1300: Loss = -9870.892816107995
Iteration 1400: Loss = -9870.797680349484
Iteration 1500: Loss = -9870.686115367384
Iteration 1600: Loss = -9870.58027370847
Iteration 1700: Loss = -9870.456942903395
Iteration 1800: Loss = -9870.284860866293
Iteration 1900: Loss = -9870.11001504904
Iteration 2000: Loss = -9869.971882804823
Iteration 2100: Loss = -9869.867237070643
Iteration 2200: Loss = -9869.78461623642
Iteration 2300: Loss = -9869.717886080478
Iteration 2400: Loss = -9869.664916428697
Iteration 2500: Loss = -9869.623986791237
Iteration 2600: Loss = -9869.592428125967
Iteration 2700: Loss = -9869.567870664261
Iteration 2800: Loss = -9869.548303071711
Iteration 2900: Loss = -9869.532256428249
Iteration 3000: Loss = -9869.51892816116
Iteration 3100: Loss = -9869.507426293778
Iteration 3200: Loss = -9869.497284989186
Iteration 3300: Loss = -9869.488306018184
Iteration 3400: Loss = -9869.480246276555
Iteration 3500: Loss = -9869.473027108897
Iteration 3600: Loss = -9869.575277717322
1
Iteration 3700: Loss = -9869.46052531157
Iteration 3800: Loss = -9869.45518535822
Iteration 3900: Loss = -9869.450330604415
Iteration 4000: Loss = -9869.445879041281
Iteration 4100: Loss = -9869.441948180685
Iteration 4200: Loss = -9869.43829848889
Iteration 4300: Loss = -9869.440805169848
1
Iteration 4400: Loss = -9869.431500180171
Iteration 4500: Loss = -9869.428275372815
Iteration 4600: Loss = -9869.425197059209
Iteration 4700: Loss = -9869.422282019987
Iteration 4800: Loss = -9869.489774109736
1
Iteration 4900: Loss = -9869.414527562123
Iteration 5000: Loss = -9869.409766460767
Iteration 5100: Loss = -9869.404604334803
Iteration 5200: Loss = -9869.398282798955
Iteration 5300: Loss = -9869.391396021701
Iteration 5400: Loss = -9869.425280533738
1
Iteration 5500: Loss = -9869.38302242057
Iteration 5600: Loss = -9869.365115182209
Iteration 5700: Loss = -9869.353287132777
Iteration 5800: Loss = -9869.342908085244
Iteration 5900: Loss = -9869.336341766113
Iteration 6000: Loss = -9869.332556553354
Iteration 6100: Loss = -9869.341587000894
1
Iteration 6200: Loss = -9869.32769091679
Iteration 6300: Loss = -9869.32695767555
Iteration 6400: Loss = -9869.326818920259
Iteration 6500: Loss = -9869.331805868118
1
Iteration 6600: Loss = -9869.326165475606
Iteration 6700: Loss = -9869.329868058157
1
Iteration 6800: Loss = -9869.326128307091
Iteration 6900: Loss = -9869.327436358952
1
Iteration 7000: Loss = -9869.35672878638
2
Iteration 7100: Loss = -9869.32534444921
Iteration 7200: Loss = -9869.325358934615
1
Iteration 7300: Loss = -9869.32503485193
Iteration 7400: Loss = -9869.32590121427
1
Iteration 7500: Loss = -9869.324787084475
Iteration 7600: Loss = -9869.324784041706
Iteration 7700: Loss = -9869.363769976368
1
Iteration 7800: Loss = -9869.340407753898
2
Iteration 7900: Loss = -9869.459616464148
3
Iteration 8000: Loss = -9869.333186499178
4
Iteration 8100: Loss = -9869.345702031995
5
Iteration 8200: Loss = -9869.324052137414
Iteration 8300: Loss = -9869.323931762874
Iteration 8400: Loss = -9869.324312024119
1
Iteration 8500: Loss = -9869.33536826325
2
Iteration 8600: Loss = -9869.373797334818
3
Iteration 8700: Loss = -9869.323613981422
Iteration 8800: Loss = -9869.388094415363
1
Iteration 8900: Loss = -9869.32503008604
2
Iteration 9000: Loss = -9869.324858930097
3
Iteration 9100: Loss = -9869.325692336324
4
Iteration 9200: Loss = -9869.349992221167
5
Iteration 9300: Loss = -9869.324428110742
6
Iteration 9400: Loss = -9869.32964338947
7
Iteration 9500: Loss = -9869.32316086977
Iteration 9600: Loss = -9869.323659897098
1
Iteration 9700: Loss = -9869.323113273113
Iteration 9800: Loss = -9869.323021439435
Iteration 9900: Loss = -9869.327001860565
1
Iteration 10000: Loss = -9869.322937869116
Iteration 10100: Loss = -9869.322998183468
1
Iteration 10200: Loss = -9869.324640998353
2
Iteration 10300: Loss = -9869.325255755984
3
Iteration 10400: Loss = -9869.325176166767
4
Iteration 10500: Loss = -9869.32393213032
5
Iteration 10600: Loss = -9869.323083759915
6
Iteration 10700: Loss = -9869.322792989633
Iteration 10800: Loss = -9869.323228967576
1
Iteration 10900: Loss = -9869.400488098405
2
Iteration 11000: Loss = -9869.392385037545
3
Iteration 11100: Loss = -9869.324146178707
4
Iteration 11200: Loss = -9869.33024438419
5
Iteration 11300: Loss = -9869.32311954368
6
Iteration 11400: Loss = -9869.34019206545
7
Iteration 11500: Loss = -9869.323912023565
8
Iteration 11600: Loss = -9869.322527183176
Iteration 11700: Loss = -9869.332939420941
1
Iteration 11800: Loss = -9869.355747844284
2
Iteration 11900: Loss = -9869.323694912202
3
Iteration 12000: Loss = -9869.332317833107
4
Iteration 12100: Loss = -9869.323104480338
5
Iteration 12200: Loss = -9869.323729884627
6
Iteration 12300: Loss = -9869.327867866536
7
Iteration 12400: Loss = -9869.322462218224
Iteration 12500: Loss = -9869.346427539484
1
Iteration 12600: Loss = -9869.322562891337
2
Iteration 12700: Loss = -9869.336844713842
3
Iteration 12800: Loss = -9869.322567863426
4
Iteration 12900: Loss = -9869.323486390747
5
Iteration 13000: Loss = -9869.32350286433
6
Iteration 13100: Loss = -9869.322430167898
Iteration 13200: Loss = -9869.324333922556
1
Iteration 13300: Loss = -9869.322349555692
Iteration 13400: Loss = -9869.322365979362
1
Iteration 13500: Loss = -9869.322288309997
Iteration 13600: Loss = -9869.334359849929
1
Iteration 13700: Loss = -9869.322363071948
2
Iteration 13800: Loss = -9869.324998782395
3
Iteration 13900: Loss = -9869.358853698126
4
Iteration 14000: Loss = -9869.322497851766
5
Iteration 14100: Loss = -9869.325092402076
6
Iteration 14200: Loss = -9869.56590753793
7
Iteration 14300: Loss = -9869.322708461104
8
Iteration 14400: Loss = -9869.424419223804
9
Iteration 14500: Loss = -9869.329390522205
10
Stopping early at iteration 14500 due to no improvement.
tensor([[-4.0168,  2.4636],
        [-6.4213,  3.2739],
        [-5.1023,  3.7067],
        [-4.5512,  3.0905],
        [-6.1872,  4.2597],
        [-3.2759,  1.1470],
        [-4.3375,  2.9510],
        [-4.0075,  2.2328],
        [-5.3205,  2.9000],
        [-5.8760,  4.3716],
        [-4.7323,  3.1832],
        [-4.0859,  2.6514],
        [-4.4704,  2.9683],
        [-5.0354,  3.6405],
        [-3.8457,  1.8106],
        [-4.5630,  3.0807],
        [-4.8624,  3.1717],
        [-4.5184,  2.5112],
        [-5.2112,  2.8643],
        [-3.6139,  1.1061],
        [-1.9447,  0.5481],
        [-4.0408,  2.3263],
        [-5.6783,  3.5068],
        [-5.7225,  4.0939],
        [-3.5500,  2.1246],
        [-3.7793,  2.3692],
        [-3.4745,  2.0570],
        [-4.6114,  3.2190],
        [-5.7085,  3.5676],
        [-3.9452,  2.0629],
        [-4.5209,  2.2339],
        [-4.3965,  2.4185],
        [-5.1144,  3.7103],
        [-4.7484,  2.2721],
        [-5.7634,  4.2159],
        [-5.1716,  3.7844],
        [-6.5742,  4.5391],
        [-4.2160,  2.7126],
        [-5.3006,  3.0258],
        [-4.9760,  3.5855],
        [-2.2580,  0.8372],
        [-4.4685,  3.0821],
        [-3.9462,  2.2891],
        [-5.0185,  2.6141],
        [-4.6504,  2.8521],
        [-5.3108,  3.7801],
        [-3.5475,  2.1604],
        [-5.3778,  2.8465],
        [-3.9713,  2.4643],
        [-6.0105,  2.5230],
        [-5.2166,  3.6121],
        [-4.1790,  2.7912],
        [-4.3968,  2.9931],
        [-5.5741,  4.1821],
        [-4.1284,  2.6247],
        [-2.1478,  0.6803],
        [-5.6370,  2.8512],
        [-3.6909,  2.2829],
        [-3.8239,  2.3761],
        [-3.9405,  1.3621],
        [-3.6689,  2.1782],
        [-5.3722,  3.9857],
        [-4.2503,  2.8494],
        [-4.0907,  2.1984],
        [-5.7784,  2.5560],
        [-4.9924,  1.4639],
        [-4.3967,  2.8356],
        [-3.7470,  2.2943],
        [-3.7622,  2.1251],
        [-4.5025,  2.2739],
        [-5.6644,  3.8074],
        [-3.3605,  1.5646],
        [-4.3611,  2.2428],
        [-4.3426,  2.7241],
        [-6.0358,  4.5863],
        [-5.8624,  2.7708],
        [-5.1798,  2.2259],
        [-4.5811,  3.1553],
        [-3.8360,  2.4480],
        [-4.8651,  3.2054],
        [-3.9207,  2.5317],
        [-5.1984,  2.9371],
        [-6.3663,  4.9611],
        [-5.2412,  3.6796],
        [-4.2252,  2.7106],
        [-3.6685,  2.2252],
        [-3.3787,  1.6898],
        [-3.7598,  1.7618],
        [-4.2812,  2.8739],
        [-5.0582,  3.6081],
        [-5.3036,  2.4375],
        [-4.4900,  3.0610],
        [-5.2583,  3.5864],
        [-3.8573,  2.4143],
        [-4.0990,  2.7009],
        [-5.1702,  3.7836],
        [-3.5142,  1.7018],
        [-4.8229,  3.4341],
        [-3.0906,  1.6540],
        [-3.7065,  1.1283]], dtype=torch.float64, requires_grad=True)
pi: tensor([[5.5524e-05, 9.9994e-01],
        [4.9350e-01, 5.0650e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0032, 0.9968], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1189, 0.2099],
         [0.6600, 0.1435]],

        [[0.8120, 0.1319],
         [0.3219, 0.3064]],

        [[0.6738, 0.1347],
         [0.7326, 0.3394]],

        [[0.5413, 0.1313],
         [0.7086, 0.4266]],

        [[0.6813, 0.1285],
         [0.5754, 0.6323]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.022626262626262626
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0017599824001759982
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
Global Adjusted Rand Index: 0.004897565848027641
Average Adjusted Rand Index: 0.004410943747447095
9987.588496510527
new:  [0.000673421209670367, 3.847940178382811e-07, 0.000673421209670367, 0.004897565848027641] [-0.0026030040482766447, -0.0008563678154033607, -0.0026030040482766447, 0.004410943747447095] [9868.833308263185, 9869.388748118823, 9868.643827710595, 9869.329390522205]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0006957530076822728, 0.0, 0.0] [nan, 9870.094737364492, nan, 9869.801124329562]
-----------------------------------------------------------------------------------------
This iteration is 13
True Objective function: Loss = -10087.563434266836
Iteration 0: Loss = -18489.31911011522
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.3580,    nan]],

        [[0.1902,    nan],
         [0.3271, 0.8688]],

        [[0.9648,    nan],
         [0.7406, 0.0262]],

        [[0.7752,    nan],
         [0.8310, 0.2377]],

        [[0.1039,    nan],
         [0.9764, 0.4765]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19453.817822445406
Iteration 100: Loss = -9980.08528685664
Iteration 200: Loss = -9978.716166584452
Iteration 300: Loss = -9978.187748683973
Iteration 400: Loss = -9977.92133436335
Iteration 500: Loss = -9977.767266288647
Iteration 600: Loss = -9977.669366655195
Iteration 700: Loss = -9977.602750519847
Iteration 800: Loss = -9977.55522412281
Iteration 900: Loss = -9977.520116361302
Iteration 1000: Loss = -9977.493483977136
Iteration 1100: Loss = -9977.472951128124
Iteration 1200: Loss = -9977.456969122826
Iteration 1300: Loss = -9977.444248423306
Iteration 1400: Loss = -9977.43412031593
Iteration 1500: Loss = -9977.425906934468
Iteration 1600: Loss = -9977.419109335495
Iteration 1700: Loss = -9977.413501991428
Iteration 1800: Loss = -9977.408745059833
Iteration 1900: Loss = -9977.404695084902
Iteration 2000: Loss = -9977.401191207418
Iteration 2100: Loss = -9977.398087556106
Iteration 2200: Loss = -9977.395386335724
Iteration 2300: Loss = -9977.392962704236
Iteration 2400: Loss = -9977.39076516692
Iteration 2500: Loss = -9977.388807474332
Iteration 2600: Loss = -9977.38699786294
Iteration 2700: Loss = -9977.385416444818
Iteration 2800: Loss = -9977.383938227365
Iteration 2900: Loss = -9977.382565935579
Iteration 3000: Loss = -9977.381302437781
Iteration 3100: Loss = -9977.38013774557
Iteration 3200: Loss = -9977.379038892042
Iteration 3300: Loss = -9977.378021163102
Iteration 3400: Loss = -9977.377050200355
Iteration 3500: Loss = -9977.376147004139
Iteration 3600: Loss = -9977.37531711212
Iteration 3700: Loss = -9977.37455707954
Iteration 3800: Loss = -9977.373796321328
Iteration 3900: Loss = -9977.373054834394
Iteration 4000: Loss = -9977.372431642847
Iteration 4100: Loss = -9977.371765814214
Iteration 4200: Loss = -9977.371176476343
Iteration 4300: Loss = -9977.370594827844
Iteration 4400: Loss = -9977.370056463145
Iteration 4500: Loss = -9977.369537746765
Iteration 4600: Loss = -9977.369017322704
Iteration 4700: Loss = -9977.36855470816
Iteration 4800: Loss = -9977.368136718678
Iteration 4900: Loss = -9977.367678431427
Iteration 5000: Loss = -9977.367270207174
Iteration 5100: Loss = -9977.366845072247
Iteration 5200: Loss = -9977.366501947628
Iteration 5300: Loss = -9977.366099496065
Iteration 5400: Loss = -9977.36571344018
Iteration 5500: Loss = -9977.365404626818
Iteration 5600: Loss = -9977.365128746853
Iteration 5700: Loss = -9977.364827956135
Iteration 5800: Loss = -9977.364483008723
Iteration 5900: Loss = -9977.36415218771
Iteration 6000: Loss = -9977.363851718623
Iteration 6100: Loss = -9977.363588566961
Iteration 6200: Loss = -9977.36335862057
Iteration 6300: Loss = -9977.36316978389
Iteration 6400: Loss = -9977.36287287043
Iteration 6500: Loss = -9977.362604863869
Iteration 6600: Loss = -9977.362348834862
Iteration 6700: Loss = -9977.362006463843
Iteration 6800: Loss = -9977.361709462793
Iteration 6900: Loss = -9977.361367926758
Iteration 7000: Loss = -9977.36102538407
Iteration 7100: Loss = -9977.360607225131
Iteration 7200: Loss = -9977.360125031479
Iteration 7300: Loss = -9977.359421797799
Iteration 7400: Loss = -9977.358370747887
Iteration 7500: Loss = -9977.356866357852
Iteration 7600: Loss = -9977.354914061561
Iteration 7700: Loss = -9977.352921303238
Iteration 7800: Loss = -9977.35122544691
Iteration 7900: Loss = -9977.349744015948
Iteration 8000: Loss = -9977.348489476615
Iteration 8100: Loss = -9977.347489311156
Iteration 8200: Loss = -9977.346750285831
Iteration 8300: Loss = -9977.346166180587
Iteration 8400: Loss = -9977.345624923728
Iteration 8500: Loss = -9977.345574952378
Iteration 8600: Loss = -9977.345222782815
Iteration 8700: Loss = -9977.488475915814
1
Iteration 8800: Loss = -9977.344064756611
Iteration 8900: Loss = -9977.358684399325
1
Iteration 9000: Loss = -9977.343620620619
Iteration 9100: Loss = -9977.371532075465
1
Iteration 9200: Loss = -9977.343267199132
Iteration 9300: Loss = -9977.343090437833
Iteration 9400: Loss = -9977.344395767215
1
Iteration 9500: Loss = -9977.342821170569
Iteration 9600: Loss = -9977.342767961283
Iteration 9700: Loss = -9977.347304252842
1
Iteration 9800: Loss = -9977.342644101116
Iteration 9900: Loss = -9977.342501353902
Iteration 10000: Loss = -9977.353283070546
1
Iteration 10100: Loss = -9977.342399888117
Iteration 10200: Loss = -9977.34232946079
Iteration 10300: Loss = -9977.34229293812
Iteration 10400: Loss = -9977.394247667407
1
Iteration 10500: Loss = -9977.34217274259
Iteration 10600: Loss = -9977.342123989069
Iteration 10700: Loss = -9977.342056015937
Iteration 10800: Loss = -9977.342245513777
1
Iteration 10900: Loss = -9977.34202030829
Iteration 11000: Loss = -9977.341929819131
Iteration 11100: Loss = -9977.343149609573
1
Iteration 11200: Loss = -9977.34185504728
Iteration 11300: Loss = -9977.34184707387
Iteration 11400: Loss = -9977.341974995643
1
Iteration 11500: Loss = -9977.341789625583
Iteration 11600: Loss = -9977.341724628786
Iteration 11700: Loss = -9977.34171226653
Iteration 11800: Loss = -9977.34240166211
1
Iteration 11900: Loss = -9977.341632171618
Iteration 12000: Loss = -9977.34158788394
Iteration 12100: Loss = -9977.361027255654
1
Iteration 12200: Loss = -9977.341533652272
Iteration 12300: Loss = -9977.341519407179
Iteration 12400: Loss = -9977.341472597023
Iteration 12500: Loss = -9977.341502216057
1
Iteration 12600: Loss = -9977.341426130604
Iteration 12700: Loss = -9977.341423573498
Iteration 12800: Loss = -9977.353476197193
1
Iteration 12900: Loss = -9977.341385309708
Iteration 13000: Loss = -9977.34133449561
Iteration 13100: Loss = -9977.341356396155
1
Iteration 13200: Loss = -9977.341918840319
2
Iteration 13300: Loss = -9977.341304986721
Iteration 13400: Loss = -9977.341310175427
1
Iteration 13500: Loss = -9977.341306888438
2
Iteration 13600: Loss = -9977.341767157908
3
Iteration 13700: Loss = -9977.341283834552
Iteration 13800: Loss = -9977.341276788018
Iteration 13900: Loss = -9977.359187508484
1
Iteration 14000: Loss = -9977.341241502034
Iteration 14100: Loss = -9977.34124250618
1
Iteration 14200: Loss = -9977.34123705911
Iteration 14300: Loss = -9977.341287416108
1
Iteration 14400: Loss = -9977.341210979006
Iteration 14500: Loss = -9977.341222345396
1
Iteration 14600: Loss = -9977.596332550025
2
Iteration 14700: Loss = -9977.341187272828
Iteration 14800: Loss = -9977.341201420737
1
Iteration 14900: Loss = -9977.34131381224
2
Iteration 15000: Loss = -9977.341808033982
3
Iteration 15100: Loss = -9977.341190573954
4
Iteration 15200: Loss = -9977.365553627855
5
Iteration 15300: Loss = -9977.341172535878
Iteration 15400: Loss = -9977.341191761916
1
Iteration 15500: Loss = -9977.34124715382
2
Iteration 15600: Loss = -9977.341168432238
Iteration 15700: Loss = -9977.4860220941
1
Iteration 15800: Loss = -9977.341170151509
2
Iteration 15900: Loss = -9977.341159603557
Iteration 16000: Loss = -9977.341213081687
1
Iteration 16100: Loss = -9977.341155898783
Iteration 16200: Loss = -9977.341150110737
Iteration 16300: Loss = -9977.34752915726
1
Iteration 16400: Loss = -9977.341141146937
Iteration 16500: Loss = -9977.341118783152
Iteration 16600: Loss = -9977.362454317365
1
Iteration 16700: Loss = -9977.34112350345
2
Iteration 16800: Loss = -9977.341101795248
Iteration 16900: Loss = -9977.350288870854
1
Iteration 17000: Loss = -9977.341130256225
2
Iteration 17100: Loss = -9977.341126449193
3
Iteration 17200: Loss = -9977.342209988336
4
Iteration 17300: Loss = -9977.34113131611
5
Iteration 17400: Loss = -9977.341122152093
6
Iteration 17500: Loss = -9977.341277782885
7
Iteration 17600: Loss = -9977.341116363392
8
Iteration 17700: Loss = -9977.341175594423
9
Iteration 17800: Loss = -9977.341143121736
10
Stopping early at iteration 17800 due to no improvement.
tensor([[ 0.0114, -4.6266],
        [-0.0366, -4.5786],
        [ 0.0589, -4.6741],
        [-0.4142, -4.2010],
        [ 0.1953, -4.8105],
        [-0.1662, -4.4490],
        [ 0.1680, -4.7832],
        [-0.4366, -4.1787],
        [-0.0278, -4.5874],
        [-0.4864, -4.1288],
        [ 0.2003, -4.8155],
        [-0.1728, -4.4424],
        [-0.6499, -3.9653],
        [ 0.1837, -4.7989],
        [ 0.4865, -5.1017],
        [-0.6497, -3.9656],
        [ 0.1425, -4.7578],
        [ 0.1722, -4.7875],
        [-0.0427, -4.5725],
        [ 0.0699, -4.6851],
        [ 0.4130, -5.0283],
        [-0.2742, -4.3410],
        [ 0.3141, -4.9293],
        [ 0.6209, -5.2361],
        [-0.1298, -4.4854],
        [ 0.6173, -5.2325],
        [ 0.5334, -5.1486],
        [ 0.3005, -4.9157],
        [-0.1477, -4.4675],
        [ 0.4147, -5.0299],
        [ 0.1702, -4.7854],
        [ 0.1926, -4.8078],
        [ 0.7458, -5.3610],
        [-0.1908, -4.4244],
        [ 0.7242, -5.3394],
        [ 0.8205, -5.4357],
        [-0.2713, -4.3440],
        [ 0.1783, -4.7935],
        [ 0.5345, -5.1497],
        [-0.3759, -4.2394],
        [ 0.6219, -5.2372],
        [ 0.4303, -5.0455],
        [-0.1333, -4.4820],
        [-1.0431, -3.5722],
        [ 0.5335, -5.1487],
        [-0.1993, -4.4159],
        [ 0.0506, -4.6658],
        [ 0.1207, -4.7360],
        [-0.3738, -4.2415],
        [-0.5833, -4.0319],
        [-0.1320, -4.4832],
        [ 0.5346, -5.1498],
        [ 0.6870, -5.3022],
        [ 0.0606, -4.6758],
        [ 0.5943, -5.2095],
        [ 0.2869, -4.9021],
        [ 0.6300, -5.2453],
        [-0.7185, -3.8967],
        [ 0.1741, -4.7894],
        [ 0.4209, -5.0362],
        [-0.9368, -3.6785],
        [ 0.8452, -5.4604],
        [-0.2674, -4.3478],
        [-0.3938, -4.2214],
        [ 1.3086, -5.9239],
        [ 0.1294, -4.7446],
        [ 0.3935, -5.0087],
        [ 0.1542, -4.7694],
        [ 0.1562, -4.7714],
        [-0.1420, -4.4732],
        [ 0.4881, -5.1033],
        [ 0.5153, -5.1306],
        [-0.1896, -4.4256],
        [-0.2204, -4.3948],
        [ 0.0857, -4.7009],
        [-0.1498, -4.4654],
        [ 0.0565, -4.6717],
        [ 0.6415, -5.2568],
        [ 0.1513, -4.7665],
        [-0.4802, -4.1351],
        [ 0.5912, -5.2064],
        [ 0.2969, -4.9121],
        [-0.1279, -4.4873],
        [ 0.3185, -4.9337],
        [-0.2398, -4.3754],
        [-0.1047, -4.5105],
        [ 0.6905, -5.3057],
        [-1.0756, -3.5396],
        [ 0.1652, -4.7804],
        [ 0.4175, -5.0327],
        [ 0.2917, -4.9070],
        [ 0.5593, -5.1746],
        [ 0.5203, -5.1355],
        [-0.4452, -4.1700],
        [ 0.2392, -4.8544],
        [-0.1906, -4.4247],
        [ 0.0661, -4.6813],
        [ 0.5256, -5.1409],
        [ 0.6523, -5.2676],
        [-0.0894, -4.5258]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 7.3681e-08],
        [7.2094e-01, 2.7906e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9883, 0.0117], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1389, 0.1682],
         [0.3580, 0.1918]],

        [[0.1902, 0.0962],
         [0.3271, 0.8688]],

        [[0.9648, 0.1496],
         [0.7406, 0.0262]],

        [[0.7752, 0.1475],
         [0.8310, 0.2377]],

        [[0.1039, 0.7067],
         [0.9764, 0.4765]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -19490.423955617487
Iteration 10: Loss = -9975.680090833826
Iteration 20: Loss = -9975.57317442271
Iteration 30: Loss = -9975.538423437194
Iteration 40: Loss = -9975.509312107748
Iteration 50: Loss = -9975.480369099827
Iteration 60: Loss = -9975.450563425347
Iteration 70: Loss = -9975.419408251046
Iteration 80: Loss = -9975.386724875369
Iteration 90: Loss = -9975.352324694466
Iteration 100: Loss = -9975.315939075612
Iteration 110: Loss = -9975.277268235362
Iteration 120: Loss = -9975.235938222842
Iteration 130: Loss = -9975.19158562016
Iteration 140: Loss = -9975.14362624016
Iteration 150: Loss = -9975.091456832975
Iteration 160: Loss = -9975.034253973283
Iteration 170: Loss = -9974.971166454405
Iteration 180: Loss = -9974.901244390778
Iteration 190: Loss = -9974.823407308224
Iteration 200: Loss = -9974.736980871698
Iteration 210: Loss = -9974.642300245665
Iteration 220: Loss = -9974.542035973302
Iteration 230: Loss = -9974.44237896748
Iteration 240: Loss = -9974.353329665482
Iteration 250: Loss = -9974.284563413294
Iteration 260: Loss = -9974.239869261242
Iteration 270: Loss = -9974.215755591731
Iteration 280: Loss = -9974.205398640383
Iteration 290: Loss = -9974.202356501675
Iteration 300: Loss = -9974.202564587962
1
Iteration 310: Loss = -9974.20385144307
2
Iteration 320: Loss = -9974.205233711275
3
Stopping early at iteration 319 due to no improvement.
pi: tensor([[0.0883, 0.9117],
        [0.0811, 0.9189]], dtype=torch.float64)
alpha: tensor([0.0820, 0.9180])
beta: tensor([[[0.0808, 0.1090],
         [0.2418, 0.1430]],

        [[0.1916, 0.1099],
         [0.4957, 0.0589]],

        [[0.7206, 0.1360],
         [0.8216, 0.2138]],

        [[0.8476, 0.1155],
         [0.7816, 0.0638]],

        [[0.9446, 0.0846],
         [0.2529, 0.9494]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.004682108332720131
Global Adjusted Rand Index: -0.00038523117239545255
Average Adjusted Rand Index: 0.0003653940237608727
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19490.4117535486
Iteration 100: Loss = -10113.807399382398
Iteration 200: Loss = -10020.236334461488
Iteration 300: Loss = -9984.658893926253
Iteration 400: Loss = -9979.55427738841
Iteration 500: Loss = -9978.532447303456
Iteration 600: Loss = -9978.071465277053
Iteration 700: Loss = -9977.764431460466
Iteration 800: Loss = -9977.532688519077
Iteration 900: Loss = -9977.36167379788
Iteration 1000: Loss = -9977.224675273088
Iteration 1100: Loss = -9977.107177870783
Iteration 1200: Loss = -9977.00525115876
Iteration 1300: Loss = -9976.921379754327
Iteration 1400: Loss = -9976.853053492112
Iteration 1500: Loss = -9976.795873614084
Iteration 1600: Loss = -9976.749998465266
Iteration 1700: Loss = -9976.713452794476
Iteration 1800: Loss = -9976.681861034538
Iteration 1900: Loss = -9976.63644673192
Iteration 2000: Loss = -9976.611882484229
Iteration 2100: Loss = -9976.589451578338
Iteration 2200: Loss = -9976.571166015881
Iteration 2300: Loss = -9976.55636226187
Iteration 2400: Loss = -9976.542915597325
Iteration 2500: Loss = -9976.523847111326
Iteration 2600: Loss = -9976.500182093776
Iteration 2700: Loss = -9976.487880617447
Iteration 2800: Loss = -9976.47753960703
Iteration 2900: Loss = -9976.461802322181
Iteration 3000: Loss = -9976.430448716432
Iteration 3100: Loss = -9976.409837824187
Iteration 3200: Loss = -9976.354777377926
Iteration 3300: Loss = -9976.173312627803
Iteration 3400: Loss = -9976.068185809369
Iteration 3500: Loss = -9976.055420719913
Iteration 3600: Loss = -9976.050569815681
Iteration 3700: Loss = -9976.046979249071
Iteration 3800: Loss = -9976.04392620464
Iteration 3900: Loss = -9976.041157570893
Iteration 4000: Loss = -9976.038616934467
Iteration 4100: Loss = -9976.036262919568
Iteration 4200: Loss = -9976.034038870472
Iteration 4300: Loss = -9976.030036423721
Iteration 4400: Loss = -9976.026223315328
Iteration 4500: Loss = -9976.024906346549
Iteration 4600: Loss = -9976.023826567378
Iteration 4700: Loss = -9976.022731445864
Iteration 4800: Loss = -9976.021782694257
Iteration 4900: Loss = -9976.020890094802
Iteration 5000: Loss = -9976.019700084958
Iteration 5100: Loss = -9976.08117547648
1
Iteration 5200: Loss = -9976.004979081088
Iteration 5300: Loss = -9976.004175182106
Iteration 5400: Loss = -9976.003525022885
Iteration 5500: Loss = -9976.003683036775
1
Iteration 5600: Loss = -9976.002415402254
Iteration 5700: Loss = -9976.002001785415
Iteration 5800: Loss = -9976.004658629903
1
Iteration 5900: Loss = -9976.001272041414
Iteration 6000: Loss = -9976.000974966222
Iteration 6100: Loss = -9976.000664980196
Iteration 6200: Loss = -9976.000591238595
Iteration 6300: Loss = -9976.000115455145
Iteration 6400: Loss = -9975.999835944014
Iteration 6500: Loss = -9975.99961735953
Iteration 6600: Loss = -9975.999380411822
Iteration 6700: Loss = -9975.99913011397
Iteration 6800: Loss = -9975.99891665206
Iteration 6900: Loss = -9976.003674919957
1
Iteration 7000: Loss = -9975.998510293957
Iteration 7100: Loss = -9975.99834588451
Iteration 7200: Loss = -9976.21497645635
1
Iteration 7300: Loss = -9975.998026881054
Iteration 7400: Loss = -9975.997857876118
Iteration 7500: Loss = -9975.99400302544
Iteration 7600: Loss = -9975.986520437973
Iteration 7700: Loss = -9975.986324763318
Iteration 7800: Loss = -9975.990024612141
1
Iteration 7900: Loss = -9975.990458538767
2
Iteration 8000: Loss = -9975.93617815099
Iteration 8100: Loss = -9975.90650544642
Iteration 8200: Loss = -9975.906159265982
Iteration 8300: Loss = -9975.908361766351
1
Iteration 8400: Loss = -9975.905570522958
Iteration 8500: Loss = -9975.90521935559
Iteration 8600: Loss = -9975.905286724557
1
Iteration 8700: Loss = -9975.905318662391
2
Iteration 8800: Loss = -9975.94116394689
3
Iteration 8900: Loss = -9975.900740190238
Iteration 9000: Loss = -9975.900837905541
1
Iteration 9100: Loss = -9975.900636092143
Iteration 9200: Loss = -9975.900449937259
Iteration 9300: Loss = -9975.900239330249
Iteration 9400: Loss = -9975.905204337783
1
Iteration 9500: Loss = -9975.899902682211
Iteration 9600: Loss = -9975.900322828105
1
Iteration 9700: Loss = -9975.902621309162
2
Iteration 9800: Loss = -9975.909271602432
3
Iteration 9900: Loss = -9975.898957632899
Iteration 10000: Loss = -9975.897777882672
Iteration 10100: Loss = -9975.90021106242
1
Iteration 10200: Loss = -9975.876679103629
Iteration 10300: Loss = -9975.780930624525
Iteration 10400: Loss = -9975.71576350845
Iteration 10500: Loss = -9975.677731397489
Iteration 10600: Loss = -9975.668088142345
Iteration 10700: Loss = -9975.547222219991
Iteration 10800: Loss = -9975.498336862107
Iteration 10900: Loss = -9975.37053080927
Iteration 11000: Loss = -9975.310328842039
Iteration 11100: Loss = -9975.229956180088
Iteration 11200: Loss = -9975.244680757309
1
Iteration 11300: Loss = -9975.109017032522
Iteration 11400: Loss = -9974.885488559665
Iteration 11500: Loss = -9974.85835738767
Iteration 11600: Loss = -9974.85343434965
Iteration 11700: Loss = -9974.844422804405
Iteration 11800: Loss = -9974.841813927313
Iteration 11900: Loss = -9974.834547029395
Iteration 12000: Loss = -9974.842791376193
1
Iteration 12100: Loss = -9974.82273657697
Iteration 12200: Loss = -9974.809489545083
Iteration 12300: Loss = -9974.786712825642
Iteration 12400: Loss = -9974.60158366862
Iteration 12500: Loss = -9974.404602721448
Iteration 12600: Loss = -9973.95226635057
Iteration 12700: Loss = -9973.864112172583
Iteration 12800: Loss = -9973.863908889536
Iteration 12900: Loss = -9973.863756987643
Iteration 13000: Loss = -9973.863588560384
Iteration 13100: Loss = -9973.875016070207
1
Iteration 13200: Loss = -9973.863251101924
Iteration 13300: Loss = -9973.863131327853
Iteration 13400: Loss = -9973.863101911022
Iteration 13500: Loss = -9973.840614694898
Iteration 13600: Loss = -9973.847466257244
1
Iteration 13700: Loss = -9973.838493047962
Iteration 13800: Loss = -9973.844064359906
1
Iteration 13900: Loss = -9973.890585442268
2
Iteration 14000: Loss = -9973.851759711215
3
Iteration 14100: Loss = -9973.840174106317
4
Iteration 14200: Loss = -9973.83796668976
Iteration 14300: Loss = -9973.86357961415
1
Iteration 14400: Loss = -9973.823671783832
Iteration 14500: Loss = -9973.8235096981
Iteration 14600: Loss = -9973.82358927046
1
Iteration 14700: Loss = -9973.822401432319
Iteration 14800: Loss = -9973.82185203049
Iteration 14900: Loss = -9973.91007735498
1
Iteration 15000: Loss = -9973.928158155168
2
Iteration 15100: Loss = -9973.844700118414
3
Iteration 15200: Loss = -9973.821709331396
Iteration 15300: Loss = -9973.822310365647
1
Iteration 15400: Loss = -9973.820577565037
Iteration 15500: Loss = -9973.819762455623
Iteration 15600: Loss = -9973.818846223894
Iteration 15700: Loss = -9973.819329050511
1
Iteration 15800: Loss = -9973.818718315699
Iteration 15900: Loss = -9973.827327608305
1
Iteration 16000: Loss = -9973.89185356745
2
Iteration 16100: Loss = -9973.81842030361
Iteration 16200: Loss = -9973.818547814924
1
Iteration 16300: Loss = -9973.818593714783
2
Iteration 16400: Loss = -9973.81845107445
3
Iteration 16500: Loss = -9973.886525793818
4
Iteration 16600: Loss = -9973.819027895353
5
Iteration 16700: Loss = -9973.820555004002
6
Iteration 16800: Loss = -9973.872522985042
7
Iteration 16900: Loss = -9973.893541270863
8
Iteration 17000: Loss = -9973.883247507487
9
Iteration 17100: Loss = -9973.82350729648
10
Stopping early at iteration 17100 due to no improvement.
tensor([[-0.0609, -1.3946],
        [-1.5811, -2.0562],
        [-0.5592, -1.8837],
        [-0.4787, -2.2381],
        [-0.6191, -0.9601],
        [-0.2222, -2.1337],
        [-0.0107, -1.3791],
        [-0.7053, -0.9950],
        [-1.0466, -0.3412],
        [ 0.4576, -2.0161],
        [ 0.0483, -1.4937],
        [-0.3089, -1.8765],
        [ 0.1384, -1.5277],
        [-0.6338, -1.8363],
        [-0.8331, -1.3478],
        [-1.5326, -1.1740],
        [-0.6063, -1.3627],
        [-1.4747, -2.7123],
        [ 0.0623, -1.6372],
        [-1.6989, -2.4588],
        [-0.4669, -1.3482],
        [-0.2196, -1.4468],
        [-0.0223, -1.6419],
        [-0.8309, -1.7644],
        [ 0.3973, -1.8311],
        [-0.6700, -1.6076],
        [-2.3398, -2.2754],
        [-0.2306, -1.1652],
        [-0.2084, -1.2167],
        [-0.6203, -1.4020],
        [-1.3798, -0.4336],
        [-0.4145, -0.9998],
        [-0.4292, -0.9571],
        [-1.0596, -2.6782],
        [-0.7587, -1.6059],
        [-1.4466, -0.6110],
        [ 0.2680, -1.7468],
        [-0.2452, -1.3863],
        [-0.3377, -1.4601],
        [-0.4616, -1.5717],
        [-1.1075, -1.6748],
        [ 0.0542, -1.5156],
        [-0.2798, -1.3716],
        [-0.8659, -1.0133],
        [-0.4085, -0.9779],
        [-0.5161, -0.9492],
        [ 0.0350, -1.4544],
        [-0.5411, -1.7072],
        [-1.2004, -2.3246],
        [ 0.0212, -1.4078],
        [ 0.0229, -2.0236],
        [-0.3971, -1.6792],
        [-0.3752, -1.0386],
        [-0.7103, -2.0173],
        [-0.8860, -1.4045],
        [-0.6269, -1.0141],
        [-0.1878, -1.1992],
        [-0.6260, -1.4186],
        [-0.1988, -1.2775],
        [-0.1872, -1.2232],
        [-0.4884, -2.7162],
        [-0.4468, -1.2475],
        [-0.7110, -1.5816],
        [-0.2926, -2.0828],
        [-1.2273, -0.1984],
        [-0.1915, -1.1953],
        [-0.9356, -1.5050],
        [-0.5273, -1.7607],
        [-0.7306, -1.0320],
        [ 0.1384, -1.7749],
        [-0.1659, -1.2566],
        [-0.1000, -1.3474],
        [-0.1600, -1.3762],
        [-0.5351, -1.6658],
        [-0.6466, -1.8025],
        [-0.3762, -1.4010],
        [-2.1265, -0.0668],
        [-0.2352, -1.3687],
        [-0.5085, -1.3281],
        [-0.8713, -0.8868],
        [-2.0380, -2.5772],
        [-0.0974, -1.2918],
        [ 0.3545, -1.7461],
        [ 0.0464, -1.6034],
        [-0.2903, -1.2376],
        [-0.0929, -1.5321],
        [-1.2449, -0.2195],
        [-0.8889, -1.0978],
        [ 0.0099, -1.4478],
        [-0.2071, -1.1880],
        [ 0.0430, -1.4315],
        [-1.6581, -2.4799],
        [-0.1379, -1.2557],
        [-0.2910, -2.1291],
        [-0.7164, -0.7571],
        [-0.0391, -1.6751],
        [-0.7786, -1.3479],
        [-0.4212, -1.1075],
        [-0.5971, -1.0045],
        [ 0.0634, -1.6174]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9990e-01, 1.0334e-04],
        [2.8035e-01, 7.1965e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7026, 0.2974], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1450, 0.1329],
         [0.2418, 0.1183]],

        [[0.1916, 0.1270],
         [0.4957, 0.0589]],

        [[0.7206, 0.1426],
         [0.8216, 0.2138]],

        [[0.8476, 0.1267],
         [0.7816, 0.0638]],

        [[0.9446, 0.0810],
         [0.2529, 0.9494]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0029034060286109754
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: -0.012924071082390954
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.003422492374587702
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.020424516829035635
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.003702958125914224
Global Adjusted Rand Index: -0.004642911204330692
Average Adjusted Rand Index: 0.0006556802549507461
Iteration 0: Loss = -23379.46814946832
Iteration 10: Loss = -9976.19684453149
Iteration 20: Loss = -9975.62377869749
Iteration 30: Loss = -9975.061218762781
Iteration 40: Loss = -9974.71631174138
Iteration 50: Loss = -9974.427473023325
Iteration 60: Loss = -9974.2480154327
Iteration 70: Loss = -9974.206978679405
Iteration 80: Loss = -9974.202726238886
Iteration 90: Loss = -9974.203527401098
1
Iteration 100: Loss = -9974.204908279593
2
Iteration 110: Loss = -9974.20612783846
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.0874, 0.9126],
        [0.0805, 0.9195]], dtype=torch.float64)
alpha: tensor([0.0813, 0.9187])
beta: tensor([[[0.0807, 0.1088],
         [0.0354, 0.1430]],

        [[0.2940, 0.1098],
         [0.3433, 0.8475]],

        [[0.6506, 0.1361],
         [0.9500, 0.7110]],

        [[0.7851, 0.1154],
         [0.1686, 0.1199]],

        [[0.8234, 0.0844],
         [0.0504, 0.1727]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.004682108332720131
Global Adjusted Rand Index: -0.00038523117239545255
Average Adjusted Rand Index: 0.0003653940237608727
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23379.33262969224
Iteration 100: Loss = -10037.917098057169
Iteration 200: Loss = -9994.763603568668
Iteration 300: Loss = -9984.149078828095
Iteration 400: Loss = -9979.101311788492
Iteration 500: Loss = -9977.475754542545
Iteration 600: Loss = -9976.589578178733
Iteration 700: Loss = -9976.246708404951
Iteration 800: Loss = -9976.155178140803
Iteration 900: Loss = -9976.109796698569
Iteration 1000: Loss = -9976.060412314831
Iteration 1100: Loss = -9976.005791832436
Iteration 1200: Loss = -9975.966215957907
Iteration 1300: Loss = -9975.931997092175
Iteration 1400: Loss = -9975.897297797666
Iteration 1500: Loss = -9975.860160378024
Iteration 1600: Loss = -9975.785708722931
Iteration 1700: Loss = -9975.728487528668
Iteration 1800: Loss = -9975.68090590189
Iteration 1900: Loss = -9975.617754973147
Iteration 2000: Loss = -9975.470547360972
Iteration 2100: Loss = -9974.927653603861
Iteration 2200: Loss = -9974.068087705167
Iteration 2300: Loss = -9973.838432694043
Iteration 2400: Loss = -9973.743045242036
Iteration 2500: Loss = -9973.360990941981
Iteration 2600: Loss = -9971.753099455576
Iteration 2700: Loss = -9971.634890751375
Iteration 2800: Loss = -9971.518898703205
Iteration 2900: Loss = -9971.467163121586
Iteration 3000: Loss = -9971.450478131319
Iteration 3100: Loss = -9971.19068842428
Iteration 3200: Loss = -9971.1733364077
Iteration 3300: Loss = -9971.150235137533
Iteration 3400: Loss = -9971.128338411572
Iteration 3500: Loss = -9971.11093201117
Iteration 3600: Loss = -9971.089052702886
Iteration 3700: Loss = -9971.076540462775
Iteration 3800: Loss = -9971.07277945645
Iteration 3900: Loss = -9971.068177114257
Iteration 4000: Loss = -9971.058207599854
Iteration 4100: Loss = -9971.035312087613
Iteration 4200: Loss = -9971.020272593585
Iteration 4300: Loss = -9971.010137660924
Iteration 4400: Loss = -9971.007006276179
Iteration 4500: Loss = -9971.00329806157
Iteration 4600: Loss = -9970.995564911262
Iteration 4700: Loss = -9970.967297885507
Iteration 4800: Loss = -9970.931510393397
Iteration 4900: Loss = -9970.906594815504
Iteration 5000: Loss = -9970.18299178254
Iteration 5100: Loss = -9969.744135932084
Iteration 5200: Loss = -9969.693933192824
Iteration 5300: Loss = -9969.672372327599
Iteration 5400: Loss = -9969.660328377535
Iteration 5500: Loss = -9969.65254747409
Iteration 5600: Loss = -9969.647180503296
Iteration 5700: Loss = -9969.642970032302
Iteration 5800: Loss = -9969.391554589027
Iteration 5900: Loss = -9969.378188477573
Iteration 6000: Loss = -9969.372962467476
Iteration 6100: Loss = -9969.368433155207
Iteration 6200: Loss = -9969.366152276925
Iteration 6300: Loss = -9969.367148024046
1
Iteration 6400: Loss = -9969.363230444165
Iteration 6500: Loss = -9969.362146839456
Iteration 6600: Loss = -9969.361287257057
Iteration 6700: Loss = -9969.360448534217
Iteration 6800: Loss = -9969.35974382067
Iteration 6900: Loss = -9969.359132160898
Iteration 7000: Loss = -9969.358617867678
Iteration 7100: Loss = -9969.358754338264
1
Iteration 7200: Loss = -9969.357681863325
Iteration 7300: Loss = -9969.357310481362
Iteration 7400: Loss = -9969.357692517497
1
Iteration 7500: Loss = -9969.356670559853
Iteration 7600: Loss = -9969.356639142774
Iteration 7700: Loss = -9969.356350404263
Iteration 7800: Loss = -9969.356888103015
1
Iteration 7900: Loss = -9969.36175720675
2
Iteration 8000: Loss = -9969.355508509669
Iteration 8100: Loss = -9969.355302785136
Iteration 8200: Loss = -9969.355158270053
Iteration 8300: Loss = -9969.355609989709
1
Iteration 8400: Loss = -9969.415325520928
2
Iteration 8500: Loss = -9969.31597074725
Iteration 8600: Loss = -9969.315888592844
Iteration 8700: Loss = -9969.502676575703
1
Iteration 8800: Loss = -9969.314166268136
Iteration 8900: Loss = -9969.32422786622
1
Iteration 9000: Loss = -9969.313903182981
Iteration 9100: Loss = -9969.429091896607
1
Iteration 9200: Loss = -9969.313738154287
Iteration 9300: Loss = -9969.31369978142
Iteration 9400: Loss = -9969.314122661783
1
Iteration 9500: Loss = -9969.313561171728
Iteration 9600: Loss = -9969.31364041526
1
Iteration 9700: Loss = -9969.313430623082
Iteration 9800: Loss = -9969.313318383312
Iteration 9900: Loss = -9969.313323652139
1
Iteration 10000: Loss = -9969.313410612634
2
Iteration 10100: Loss = -9969.313215186645
Iteration 10200: Loss = -9969.321036415751
1
Iteration 10300: Loss = -9969.313053062519
Iteration 10400: Loss = -9969.300124384627
Iteration 10500: Loss = -9969.713697444837
1
Iteration 10600: Loss = -9969.299366738041
Iteration 10700: Loss = -9969.285678020176
Iteration 10800: Loss = -9969.365814104212
1
Iteration 10900: Loss = -9969.285507991874
Iteration 11000: Loss = -9969.285197125131
Iteration 11100: Loss = -9969.28558737301
1
Iteration 11200: Loss = -9969.283707102271
Iteration 11300: Loss = -9969.288280175442
1
Iteration 11400: Loss = -9969.283472861067
Iteration 11500: Loss = -9969.282321237952
Iteration 11600: Loss = -9969.29125082418
1
Iteration 11700: Loss = -9969.282250900489
Iteration 11800: Loss = -9969.28221796802
Iteration 11900: Loss = -9969.352209278446
1
Iteration 12000: Loss = -9969.282171791378
Iteration 12100: Loss = -9969.282184950112
1
Iteration 12200: Loss = -9969.282260768092
2
Iteration 12300: Loss = -9969.282139322666
Iteration 12400: Loss = -9969.425389965534
1
Iteration 12500: Loss = -9969.282094033328
Iteration 12600: Loss = -9969.2820669604
Iteration 12700: Loss = -9969.285614360633
1
Iteration 12800: Loss = -9969.282037755665
Iteration 12900: Loss = -9969.282063349618
1
Iteration 13000: Loss = -9969.28212583888
2
Iteration 13100: Loss = -9969.281993445285
Iteration 13200: Loss = -9969.286018170113
1
Iteration 13300: Loss = -9969.282024302902
2
Iteration 13400: Loss = -9969.281986846208
Iteration 13500: Loss = -9969.308027153495
1
Iteration 13600: Loss = -9969.281994536728
2
Iteration 13700: Loss = -9969.281978486155
Iteration 13800: Loss = -9969.282245464647
1
Iteration 13900: Loss = -9969.281947063351
Iteration 14000: Loss = -9969.60648929724
1
Iteration 14100: Loss = -9969.28195164799
2
Iteration 14200: Loss = -9969.281954089085
3
Iteration 14300: Loss = -9969.283631995271
4
Iteration 14400: Loss = -9969.281935179293
Iteration 14500: Loss = -9969.281954094278
1
Iteration 14600: Loss = -9969.282111988681
2
Iteration 14700: Loss = -9969.281948993923
3
Iteration 14800: Loss = -9969.32243673539
4
Iteration 14900: Loss = -9969.28190174843
Iteration 15000: Loss = -9969.281889824435
Iteration 15100: Loss = -9969.282093858023
1
Iteration 15200: Loss = -9969.281921077732
2
Iteration 15300: Loss = -9969.281997477461
3
Iteration 15400: Loss = -9969.281941708023
4
Iteration 15500: Loss = -9969.281907235572
5
Iteration 15600: Loss = -9969.294461179434
6
Iteration 15700: Loss = -9969.28191584831
7
Iteration 15800: Loss = -9969.281908940535
8
Iteration 15900: Loss = -9969.27927847935
Iteration 16000: Loss = -9969.278859911235
Iteration 16100: Loss = -9969.332904306797
1
Iteration 16200: Loss = -9969.278828496528
Iteration 16300: Loss = -9969.278813402001
Iteration 16400: Loss = -9969.286976556492
1
Iteration 16500: Loss = -9969.27881263714
Iteration 16600: Loss = -9969.278742759887
Iteration 16700: Loss = -9969.278992581454
1
Iteration 16800: Loss = -9969.278755944062
2
Iteration 16900: Loss = -9969.278505814107
Iteration 17000: Loss = -9969.27782932646
Iteration 17100: Loss = -9969.269431534745
Iteration 17200: Loss = -9969.31544047421
1
Iteration 17300: Loss = -9969.264862132713
Iteration 17400: Loss = -9969.265359483737
1
Iteration 17500: Loss = -9969.264810542014
Iteration 17600: Loss = -9969.55971390604
1
Iteration 17700: Loss = -9969.264799738123
Iteration 17800: Loss = -9969.264704269272
Iteration 17900: Loss = -9969.289061226422
1
Iteration 18000: Loss = -9969.2647376936
2
Iteration 18100: Loss = -9969.258632016872
Iteration 18200: Loss = -9969.258617621424
Iteration 18300: Loss = -9969.2587047155
1
Iteration 18400: Loss = -9969.258580842548
Iteration 18500: Loss = -9969.389087978487
1
Iteration 18600: Loss = -9969.258168748802
Iteration 18700: Loss = -9969.258153061961
Iteration 18800: Loss = -9969.258134232763
Iteration 18900: Loss = -9969.258366640186
1
Iteration 19000: Loss = -9969.258191402812
2
Iteration 19100: Loss = -9969.26355021565
3
Iteration 19200: Loss = -9969.25817115764
4
Iteration 19300: Loss = -9969.258158761924
5
Iteration 19400: Loss = -9969.263852045795
6
Iteration 19500: Loss = -9969.258177301763
7
Iteration 19600: Loss = -9969.25815724018
8
Iteration 19700: Loss = -9969.25234795765
Iteration 19800: Loss = -9969.25231851932
Iteration 19900: Loss = -9969.291751793686
1
tensor([[ -9.5518,   6.1928],
        [ -9.4128,   7.4332],
        [ -9.0702,   7.4440],
        [ -7.8251,   6.3708],
        [ -9.6446,   7.8254],
        [ -1.7610,  -2.8542],
        [ -9.1233,   7.5931],
        [ -9.2793,   7.5571],
        [ -8.9242,   7.3676],
        [  1.3855,  -3.2492],
        [ -2.5935,   0.6158],
        [ -1.1859,  -0.3003],
        [ -0.8524,  -0.5342],
        [ -8.7377,   7.0847],
        [ -9.4014,   7.8480],
        [ -8.4505,   7.0136],
        [ -4.9041,   2.2673],
        [ -9.6263,   7.4476],
        [ -9.8719,   6.5673],
        [ -3.3949,  -0.1677],
        [ -8.7623,   7.2716],
        [ -9.7623,   5.1471],
        [ -2.0136,   0.1661],
        [ -9.4637,   7.8401],
        [ -0.6511,  -1.9242],
        [ -9.2504,   7.8266],
        [ -9.4525,   8.0598],
        [ -9.4103,   7.8400],
        [ -4.1040,   2.6945],
        [ -9.7073,   7.9239],
        [ -8.5283,   7.1116],
        [ -5.2800,   0.6648],
        [ -8.8695,   7.2370],
        [ -8.2538,   6.0500],
        [ -4.1180,  -0.4972],
        [ -9.7901,   8.1116],
        [  0.7949,  -2.6605],
        [ -1.8027,   0.3973],
        [ -9.0744,   7.4376],
        [ -7.9173,   6.5310],
        [-10.5680,   7.4653],
        [ -2.8141,  -0.4861],
        [ -8.2548,   6.8665],
        [ -9.4633,   5.7685],
        [ -9.6415,   8.2336],
        [ -9.7119,   6.3710],
        [ -9.4283,   6.0038],
        [ -8.6555,   7.1594],
        [ -8.8595,   6.4602],
        [ -8.4109,   6.9607],
        [ -0.1104,  -1.7198],
        [ -2.4731,   0.5308],
        [ -9.6791,   7.3281],
        [ -9.2932,   6.9648],
        [ -4.4821,   2.9776],
        [ -1.5593,   0.1697],
        [-10.2834,   7.0579],
        [ -8.7688,   7.2656],
        [ -2.6392,   1.2358],
        [ -8.8491,   7.4530],
        [ -0.0658,  -2.5467],
        [-10.4038,   6.9651],
        [ -8.4658,   6.9547],
        [  0.5918,  -2.4421],
        [-10.2670,   8.4873],
        [ -2.8392,   1.4294],
        [ -1.3434,  -0.2079],
        [ -3.2105,   1.7963],
        [ -9.5631,   7.9677],
        [ -1.5349,  -0.8436],
        [-10.1799,   6.8380],
        [ -0.1761,  -1.2544],
        [ -3.2769,   1.6486],
        [ -8.7083,   7.2790],
        [ -9.0374,   7.5411],
        [ -8.8600,   6.6569],
        [ -9.3408,   7.7331],
        [-10.2687,   6.7290],
        [ -9.1847,   7.7497],
        [ -8.5346,   7.1483],
        [ -9.4447,   7.9596],
        [ -4.4501,   0.6305],
        [  0.1584,  -1.6058],
        [  0.2979,  -1.6965],
        [ -4.1316,   2.2463],
        [ -2.8957,   1.5058],
        [-10.4995,   6.4093],
        [ -3.3512,   1.7834],
        [ -2.8890,   0.0391],
        [ -9.5644,   6.1495],
        [  0.4142,  -1.8055],
        [-10.2884,   7.5279],
        [ -8.7808,   7.3944],
        [  0.4914,  -2.2026],
        [ -9.0725,   7.4639],
        [  0.3630,  -2.0754],
        [ -9.1594,   7.6532],
        [ -2.9022,   1.5104],
        [ -9.9960,   7.4732],
        [  0.4120,  -2.2545]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.6691e-07],
        [1.7138e-03, 9.9829e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1440, 0.8560], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2661, 0.1571],
         [0.0354, 0.1301]],

        [[0.2940, 0.1452],
         [0.3433, 0.8475]],

        [[0.6506, 0.1618],
         [0.9500, 0.7110]],

        [[0.7851, 0.1565],
         [0.1686, 0.1199]],

        [[0.8234, 0.1505],
         [0.0504, 0.1727]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.017734840702916317
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.01129297049898015
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.020027579857731626
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.02586023568974811
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.02586023568974811
Global Adjusted Rand Index: 0.023033235523174662
Average Adjusted Rand Index: 0.015637984288232803
Iteration 0: Loss = -35462.520620245705
Iteration 10: Loss = -9977.55955859483
Iteration 20: Loss = -9977.281029770205
Iteration 30: Loss = -9975.712314758237
Iteration 40: Loss = -9975.51070825183
Iteration 50: Loss = -9975.445242993983
Iteration 60: Loss = -9975.423814773925
Iteration 70: Loss = -9975.417162373808
Iteration 80: Loss = -9975.414885204551
Iteration 90: Loss = -9975.414019336225
Iteration 100: Loss = -9975.413692174821
Iteration 110: Loss = -9975.413534569436
Iteration 120: Loss = -9975.413430991792
Iteration 130: Loss = -9975.413449847969
1
Iteration 140: Loss = -9975.413423949181
Iteration 150: Loss = -9975.41338494121
Iteration 160: Loss = -9975.413425233832
1
Iteration 170: Loss = -9975.41340565567
2
Iteration 180: Loss = -9975.413426803321
3
Stopping early at iteration 179 due to no improvement.
pi: tensor([[0.9724, 0.0276],
        [0.9012, 0.0988]], dtype=torch.float64)
alpha: tensor([0.9701, 0.0299])
beta: tensor([[[0.1350, 0.1670],
         [0.2676, 0.2305]],

        [[0.6631, 0.1434],
         [0.3069, 0.2820]],

        [[0.9882, 0.1759],
         [0.7687, 0.6326]],

        [[0.3525, 0.1812],
         [0.1709, 0.9699]],

        [[0.7371, 0.2340],
         [0.7931, 0.9826]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: 6.064153498089786e-05
Average Adjusted Rand Index: -0.00010178366459815819
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35461.80788482549
Iteration 100: Loss = -10011.468371694391
Iteration 200: Loss = -9992.02608744008
Iteration 300: Loss = -9980.253117330496
Iteration 400: Loss = -9978.784720864673
Iteration 500: Loss = -9978.103694249598
Iteration 600: Loss = -9977.719473535331
Iteration 700: Loss = -9977.469658432643
Iteration 800: Loss = -9977.290048352592
Iteration 900: Loss = -9977.128060570545
Iteration 1000: Loss = -9976.91520825945
Iteration 1100: Loss = -9976.670273634873
Iteration 1200: Loss = -9976.50439007853
Iteration 1300: Loss = -9976.410958734867
Iteration 1400: Loss = -9976.31857754301
Iteration 1500: Loss = -9976.254495660727
Iteration 1600: Loss = -9976.188093956842
Iteration 1700: Loss = -9976.111581705694
Iteration 1800: Loss = -9976.021167871564
Iteration 1900: Loss = -9975.808810244449
Iteration 2000: Loss = -9975.582687932165
Iteration 2100: Loss = -9975.44213301969
Iteration 2200: Loss = -9975.19955895153
Iteration 2300: Loss = -9975.040255362483
Iteration 2400: Loss = -9974.787166446735
Iteration 2500: Loss = -9974.008641807211
Iteration 2600: Loss = -9973.481467628348
Iteration 2700: Loss = -9973.038865114757
Iteration 2800: Loss = -9972.731976151574
Iteration 2900: Loss = -9972.473093139179
Iteration 3000: Loss = -9972.264517853786
Iteration 3100: Loss = -9972.089780836113
Iteration 3200: Loss = -9971.937342832669
Iteration 3300: Loss = -9971.801954467177
Iteration 3400: Loss = -9971.677581612585
Iteration 3500: Loss = -9971.56100121949
Iteration 3600: Loss = -9971.47077618253
Iteration 3700: Loss = -9971.40210355069
Iteration 3800: Loss = -9971.34314208703
Iteration 3900: Loss = -9971.290592614943
Iteration 4000: Loss = -9971.243692213042
Iteration 4100: Loss = -9971.201834146037
Iteration 4200: Loss = -9971.164535629012
Iteration 4300: Loss = -9971.131252733072
Iteration 4400: Loss = -9971.101546884083
Iteration 4500: Loss = -9971.074982777202
Iteration 4600: Loss = -9971.051202976545
Iteration 4700: Loss = -9971.029918420332
Iteration 4800: Loss = -9971.01085690634
Iteration 4900: Loss = -9970.993698259983
Iteration 5000: Loss = -9970.978281018166
Iteration 5100: Loss = -9970.964387060423
Iteration 5200: Loss = -9970.951682410045
Iteration 5300: Loss = -9970.93989255971
Iteration 5400: Loss = -9970.928024055363
Iteration 5500: Loss = -9970.912883722685
Iteration 5600: Loss = -9970.902009849842
Iteration 5700: Loss = -9970.895381968312
Iteration 5800: Loss = -9970.889555236821
Iteration 5900: Loss = -9970.884322388221
Iteration 6000: Loss = -9970.879629974459
Iteration 6100: Loss = -9970.875396683354
Iteration 6200: Loss = -9970.871585397901
Iteration 6300: Loss = -9970.868142165238
Iteration 6400: Loss = -9970.865059216234
Iteration 6500: Loss = -9970.862237694613
Iteration 6600: Loss = -9970.859727397139
Iteration 6700: Loss = -9970.857466310574
Iteration 6800: Loss = -9970.855420829132
Iteration 6900: Loss = -9970.853562592432
Iteration 7000: Loss = -9970.851934615728
Iteration 7100: Loss = -9970.850416649684
Iteration 7200: Loss = -9970.84906960525
Iteration 7300: Loss = -9970.847873638075
Iteration 7400: Loss = -9970.846764839262
Iteration 7500: Loss = -9970.84580927231
Iteration 7600: Loss = -9970.84490068762
Iteration 7700: Loss = -9970.844126743781
Iteration 7800: Loss = -9970.84339620834
Iteration 7900: Loss = -9970.842727118916
Iteration 8000: Loss = -9970.84218064526
Iteration 8100: Loss = -9970.841614813105
Iteration 8200: Loss = -9970.841131272437
Iteration 8300: Loss = -9970.953512694434
1
Iteration 8400: Loss = -9970.840311078326
Iteration 8500: Loss = -9970.839970700634
Iteration 8600: Loss = -9970.83965775237
Iteration 8700: Loss = -9970.839342826444
Iteration 8800: Loss = -9970.839062432675
Iteration 8900: Loss = -9970.8388144133
Iteration 9000: Loss = -9970.838620597755
Iteration 9100: Loss = -9970.838391993884
Iteration 9200: Loss = -9970.838175346218
Iteration 9300: Loss = -9970.838033639362
Iteration 9400: Loss = -9970.838171832622
1
Iteration 9500: Loss = -9970.837713195737
Iteration 9600: Loss = -9970.837550663207
Iteration 9700: Loss = -9970.841297500603
1
Iteration 9800: Loss = -9970.837311460347
Iteration 9900: Loss = -9970.837221504626
Iteration 10000: Loss = -9970.850705476387
1
Iteration 10100: Loss = -9970.837018344475
Iteration 10200: Loss = -9970.836958561162
Iteration 10300: Loss = -9970.966603339575
1
Iteration 10400: Loss = -9970.836800983472
Iteration 10500: Loss = -9970.836697318417
Iteration 10600: Loss = -9970.83662113739
Iteration 10700: Loss = -9970.88245973523
1
Iteration 10800: Loss = -9970.836514796452
Iteration 10900: Loss = -9970.836457531694
Iteration 11000: Loss = -9970.836610228205
1
Iteration 11100: Loss = -9970.836409828165
Iteration 11200: Loss = -9970.836338884134
Iteration 11300: Loss = -9970.836280794027
Iteration 11400: Loss = -9970.845719987936
1
Iteration 11500: Loss = -9970.836202047642
Iteration 11600: Loss = -9970.836195609281
Iteration 11700: Loss = -9970.83615209862
Iteration 11800: Loss = -9970.846003350282
1
Iteration 11900: Loss = -9970.836093531838
Iteration 12000: Loss = -9970.83606496846
Iteration 12100: Loss = -9970.83602905994
Iteration 12200: Loss = -9970.888983643861
1
Iteration 12300: Loss = -9970.835974754933
Iteration 12400: Loss = -9970.835967292847
Iteration 12500: Loss = -9970.835962151687
Iteration 12600: Loss = -9970.836498022996
1
Iteration 12700: Loss = -9970.835922271559
Iteration 12800: Loss = -9970.835911931905
Iteration 12900: Loss = -9970.836368673656
1
Iteration 13000: Loss = -9970.835873180993
Iteration 13100: Loss = -9970.835866838757
Iteration 13200: Loss = -9970.838002058066
1
Iteration 13300: Loss = -9970.835825947992
Iteration 13400: Loss = -9970.83581642222
Iteration 13500: Loss = -9970.896496668747
1
Iteration 13600: Loss = -9970.83583136387
2
Iteration 13700: Loss = -9970.835809030701
Iteration 13800: Loss = -9970.835775747799
Iteration 13900: Loss = -9970.835799742417
1
Iteration 14000: Loss = -9970.835772567734
Iteration 14100: Loss = -9970.835745420438
Iteration 14200: Loss = -9970.83572970384
Iteration 14300: Loss = -9970.835904714722
1
Iteration 14400: Loss = -9970.835724137436
Iteration 14500: Loss = -9970.835727907697
1
Iteration 14600: Loss = -9970.838376241247
2
Iteration 14700: Loss = -9970.83573439708
3
Iteration 14800: Loss = -9970.835719820485
Iteration 14900: Loss = -9970.836157489033
1
Iteration 15000: Loss = -9970.835713044653
Iteration 15100: Loss = -9970.835695301594
Iteration 15200: Loss = -9970.838874508394
1
Iteration 15300: Loss = -9970.835730468707
2
Iteration 15400: Loss = -9970.835723837256
3
Iteration 15500: Loss = -9970.837646485244
4
Iteration 15600: Loss = -9970.840778029833
5
Iteration 15700: Loss = -9970.835751053324
6
Iteration 15800: Loss = -9970.835802486774
7
Iteration 15900: Loss = -9971.070411997427
8
Iteration 16000: Loss = -9970.835673025202
Iteration 16100: Loss = -9971.053471965306
1
Iteration 16200: Loss = -9970.835691142252
2
Iteration 16300: Loss = -9970.83568438143
3
Iteration 16400: Loss = -9970.835955387101
4
Iteration 16500: Loss = -9970.835687523044
5
Iteration 16600: Loss = -9970.835860313053
6
Iteration 16700: Loss = -9970.835746145267
7
Iteration 16800: Loss = -9970.83567215369
Iteration 16900: Loss = -9970.83569821339
1
Iteration 17000: Loss = -9970.835773919565
2
Iteration 17100: Loss = -9970.835692071436
3
Iteration 17200: Loss = -9970.835718921999
4
Iteration 17300: Loss = -9970.83573716588
5
Iteration 17400: Loss = -9970.835680000764
6
Iteration 17500: Loss = -9970.879872769226
7
Iteration 17600: Loss = -9970.835664629021
Iteration 17700: Loss = -9970.835703039458
1
Iteration 17800: Loss = -9970.835679001793
2
Iteration 17900: Loss = -9970.83564196584
Iteration 18000: Loss = -9970.840378282652
1
Iteration 18100: Loss = -9970.83566372992
2
Iteration 18200: Loss = -9970.835639060888
Iteration 18300: Loss = -9970.835630142075
Iteration 18400: Loss = -9970.835998272276
1
Iteration 18500: Loss = -9970.83563179501
2
Iteration 18600: Loss = -9970.83563912766
3
Iteration 18700: Loss = -9970.835771307073
4
Iteration 18800: Loss = -9970.835652562086
5
Iteration 18900: Loss = -9970.835628596516
Iteration 19000: Loss = -9970.835757594328
1
Iteration 19100: Loss = -9970.83564304951
2
Iteration 19200: Loss = -9970.83565027789
3
Iteration 19300: Loss = -9970.83572969872
4
Iteration 19400: Loss = -9970.835950124761
5
Iteration 19500: Loss = -9970.835642054019
6
Iteration 19600: Loss = -9970.838618515148
7
Iteration 19700: Loss = -9970.835652781743
8
Iteration 19800: Loss = -9970.835645394936
9
Iteration 19900: Loss = -9970.835981049451
10
Stopping early at iteration 19900 due to no improvement.
tensor([[ 2.2733, -3.6693],
        [ 4.1198, -5.7424],
        [ 3.7937, -5.2115],
        [ 2.7996, -4.4122],
        [ 4.6766, -6.0643],
        [ 3.2770, -4.7849],
        [ 3.2247, -6.2481],
        [ 4.1017, -5.7707],
        [ 3.5445, -5.4765],
        [-1.0141, -0.3842],
        [ 3.9582, -5.4996],
        [ 3.3337, -4.7882],
        [ 2.0362, -4.0112],
        [ 3.7965, -5.6350],
        [ 4.8146, -6.4005],
        [ 2.3866, -4.3634],
        [ 3.9723, -5.8317],
        [ 4.0369, -5.4342],
        [ 3.1578, -5.3703],
        [ 3.5523, -5.8951],
        [ 2.9410, -4.3391],
        [ 3.3090, -4.7484],
        [ 4.0587, -5.8549],
        [ 4.7965, -6.4128],
        [ 3.2912, -4.7887],
        [ 4.7051, -6.5248],
        [ 5.3780, -7.1634],
        [ 3.5998, -6.7799],
        [ 3.0348, -5.0121],
        [ 4.5377, -6.2221],
        [ 3.5408, -5.8778],
        [ 3.1415, -4.6063],
        [ 5.0808, -6.5597],
        [ 3.2982, -4.6860],
        [ 3.6027, -5.0508],
        [ 5.3747, -6.7799],
        [ 3.0919, -4.4956],
        [ 3.6950, -6.1969],
        [ 2.9117, -4.8792],
        [ 1.9869, -3.3753],
        [ 4.9436, -6.3324],
        [ 2.4941, -4.4881],
        [ 1.1872, -4.2601],
        [ 2.2803, -4.4000],
        [ 4.4769, -6.7396],
        [ 2.9261, -5.5460],
        [ 2.2805, -3.6708],
        [ 3.9770, -5.4195],
        [ 1.6133, -3.0031],
        [ 2.5965, -4.0965],
        [ 2.1286, -5.8826],
        [ 2.3374, -5.2385],
        [ 4.7535, -6.9391],
        [ 2.2634, -3.6901],
        [ 3.4167, -5.2520],
        [ 2.1705, -6.5177],
        [ 4.1889, -7.0622],
        [ 2.9215, -4.6868],
        [ 4.1915, -5.6731],
        [ 3.5203, -6.8299],
        [ 1.0584, -4.2785],
        [ 5.1815, -6.9098],
        [ 3.2450, -4.8752],
        [ 1.4655, -6.0807],
        [ 5.9983, -7.8711],
        [ 2.7063, -4.1553],
        [ 4.6209, -6.0142],
        [ 3.6002, -5.8555],
        [ 4.6965, -6.0934],
        [ 2.1341, -5.9053],
        [ 4.6847, -6.0906],
        [ 4.5599, -6.2453],
        [ 2.6491, -4.2725],
        [ 3.0736, -5.4285],
        [ 3.9916, -5.4068],
        [ 3.2137, -5.2988],
        [ 2.7795, -4.5313],
        [ 4.8562, -6.3917],
        [ 4.4136, -5.9044],
        [ 3.9321, -5.9511],
        [ 4.9539, -6.6993],
        [ 4.2327, -5.6213],
        [ 2.7400, -4.6726],
        [ 0.0923, -4.7075],
        [ 3.7562, -5.1688],
        [ 2.9251, -5.6165],
        [ 2.8052, -5.8339],
        [ 2.4113, -3.8670],
        [ 2.0768, -4.2946],
        [ 4.0469, -6.2549],
        [ 4.2126, -5.6410],
        [ 4.2639, -7.0087],
        [ 3.1048, -4.4984],
        [ 2.8431, -4.2445],
        [ 3.7496, -6.1343],
        [ 2.6756, -5.3470],
        [ 4.2506, -5.6381],
        [ 3.0344, -5.1604],
        [ 3.6186, -5.0448],
        [ 1.2594, -2.9258]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9895, 0.0105],
        [0.3563, 0.6437]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9926, 0.0074], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1362, 0.1972],
         [0.2676, 0.9758]],

        [[0.6631, 0.1805],
         [0.3069, 0.2820]],

        [[0.9882, 0.2190],
         [0.7687, 0.6326]],

        [[0.3525, 0.1772],
         [0.1709, 0.9699]],

        [[0.7371, 0.2560],
         [0.7931, 0.9826]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.0207567131845433
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: 0.0062187062454228965
Average Adjusted Rand Index: 0.005305907368143331
Iteration 0: Loss = -41804.54975449175
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.9922,    nan]],

        [[0.1474,    nan],
         [0.1694, 0.8227]],

        [[0.9457,    nan],
         [0.8066, 0.4776]],

        [[0.9864,    nan],
         [0.5873, 0.7440]],

        [[0.9350,    nan],
         [0.7139, 0.6653]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41801.795413880136
Iteration 100: Loss = -10001.54062040559
Iteration 200: Loss = -9988.411177340135
Iteration 300: Loss = -9982.783782103772
Iteration 400: Loss = -9981.29081135807
Iteration 500: Loss = -9980.391570666203
Iteration 600: Loss = -9979.792844561725
Iteration 700: Loss = -9979.365784961987
Iteration 800: Loss = -9979.04563928361
Iteration 900: Loss = -9978.797046380343
Iteration 1000: Loss = -9978.599007326171
Iteration 1100: Loss = -9978.438185708934
Iteration 1200: Loss = -9978.305522377737
Iteration 1300: Loss = -9978.194529261234
Iteration 1400: Loss = -9978.10072400221
Iteration 1500: Loss = -9978.020646597055
Iteration 1600: Loss = -9977.951711633134
Iteration 1700: Loss = -9977.89200649369
Iteration 1800: Loss = -9977.839906345413
Iteration 1900: Loss = -9977.79411365734
Iteration 2000: Loss = -9977.75365540292
Iteration 2100: Loss = -9977.717737367391
Iteration 2200: Loss = -9977.685631694301
Iteration 2300: Loss = -9977.65689069942
Iteration 2400: Loss = -9977.631002626798
Iteration 2500: Loss = -9977.607559018006
Iteration 2600: Loss = -9977.586280656153
Iteration 2700: Loss = -9977.566880754151
Iteration 2800: Loss = -9977.549079981116
Iteration 2900: Loss = -9977.532704032801
Iteration 3000: Loss = -9977.51761141324
Iteration 3100: Loss = -9977.5035827599
Iteration 3200: Loss = -9977.490491097604
Iteration 3300: Loss = -9977.478119901027
Iteration 3400: Loss = -9977.466409191951
Iteration 3500: Loss = -9977.45524989852
Iteration 3600: Loss = -9977.444432779237
Iteration 3700: Loss = -9977.440630304776
Iteration 3800: Loss = -9977.423284175924
Iteration 3900: Loss = -9977.412666693883
Iteration 4000: Loss = -9977.417580436375
1
Iteration 4100: Loss = -9977.390734722825
Iteration 4200: Loss = -9977.37947853371
Iteration 4300: Loss = -9977.368342705542
Iteration 4400: Loss = -9977.360347917942
Iteration 4500: Loss = -9977.347275737557
Iteration 4600: Loss = -9977.337607683237
Iteration 4700: Loss = -9977.328470781244
Iteration 4800: Loss = -9977.321022161921
Iteration 4900: Loss = -9977.311922695759
Iteration 5000: Loss = -9977.304362824365
Iteration 5100: Loss = -9977.297706107807
Iteration 5200: Loss = -9977.290409834723
Iteration 5300: Loss = -9977.28393368731
Iteration 5400: Loss = -9977.27770556264
Iteration 5500: Loss = -9977.29781674688
1
Iteration 5600: Loss = -9977.265920749338
Iteration 5700: Loss = -9977.260254572904
Iteration 5800: Loss = -9977.254690232268
Iteration 5900: Loss = -9977.249904534274
Iteration 6000: Loss = -9977.243653988713
Iteration 6100: Loss = -9977.238101393164
Iteration 6200: Loss = -9977.40188368014
1
Iteration 6300: Loss = -9977.226608147415
Iteration 6400: Loss = -9977.220529721957
Iteration 6500: Loss = -9977.214159997808
Iteration 6600: Loss = -9977.242064982858
1
Iteration 6700: Loss = -9977.199916936166
Iteration 6800: Loss = -9977.191821002003
Iteration 6900: Loss = -9977.182668871565
Iteration 7000: Loss = -9977.172677578057
Iteration 7100: Loss = -9977.159915929626
Iteration 7200: Loss = -9977.145185744757
Iteration 7300: Loss = -9977.155458255509
1
Iteration 7400: Loss = -9977.103362486727
Iteration 7500: Loss = -9977.071258670192
Iteration 7600: Loss = -9977.024745371373
Iteration 7700: Loss = -9976.952081216235
Iteration 7800: Loss = -9976.83009601128
Iteration 7900: Loss = -9976.613179390397
Iteration 8000: Loss = -9976.247573693578
Iteration 8100: Loss = -9975.795370983758
Iteration 8200: Loss = -9975.471589048528
Iteration 8300: Loss = -9975.30283231532
Iteration 8400: Loss = -9975.105315035631
Iteration 8500: Loss = -9974.96852127738
Iteration 8600: Loss = -9974.915231568188
Iteration 8700: Loss = -9974.78526021558
Iteration 8800: Loss = -9974.734912784015
Iteration 8900: Loss = -9974.711095244418
Iteration 9000: Loss = -9974.693168861464
Iteration 9100: Loss = -9974.669878405355
Iteration 9200: Loss = -9974.662649158292
Iteration 9300: Loss = -9974.65005894027
Iteration 9400: Loss = -9974.643653679548
Iteration 9500: Loss = -9974.665567050753
1
Iteration 9600: Loss = -9974.474062723884
Iteration 9700: Loss = -9974.168162804152
Iteration 9800: Loss = -9974.160791308821
Iteration 9900: Loss = -9974.153048911248
Iteration 10000: Loss = -9974.150432122487
Iteration 10100: Loss = -9974.147522408539
Iteration 10200: Loss = -9974.145839449553
Iteration 10300: Loss = -9974.14533112265
Iteration 10400: Loss = -9974.143627648307
Iteration 10500: Loss = -9974.143672619011
1
Iteration 10600: Loss = -9974.14216594004
Iteration 10700: Loss = -9974.14161238115
Iteration 10800: Loss = -9974.141151192724
Iteration 10900: Loss = -9974.140745451046
Iteration 11000: Loss = -9974.140358805013
Iteration 11100: Loss = -9974.14015622845
Iteration 11200: Loss = -9974.139882963183
Iteration 11300: Loss = -9974.139652114949
Iteration 11400: Loss = -9974.209767163544
1
Iteration 11500: Loss = -9974.13900482731
Iteration 11600: Loss = -9974.142758052647
1
Iteration 11700: Loss = -9974.138571790529
Iteration 11800: Loss = -9974.246549464107
1
Iteration 11900: Loss = -9974.138261453945
Iteration 12000: Loss = -9974.13810072754
Iteration 12100: Loss = -9974.137966131259
Iteration 12200: Loss = -9974.137807075927
Iteration 12300: Loss = -9974.244717558151
1
Iteration 12400: Loss = -9974.137534157588
Iteration 12500: Loss = -9974.13746523462
Iteration 12600: Loss = -9974.137938291322
1
Iteration 12700: Loss = -9974.13722648401
Iteration 12800: Loss = -9974.168904819975
1
Iteration 12900: Loss = -9974.137079992368
Iteration 13000: Loss = -9974.137588886415
1
Iteration 13100: Loss = -9974.136878163417
Iteration 13200: Loss = -9974.13985813536
1
Iteration 13300: Loss = -9974.136731486144
Iteration 13400: Loss = -9974.514125367381
1
Iteration 13500: Loss = -9974.136591579205
Iteration 13600: Loss = -9974.136567268963
Iteration 13700: Loss = -9974.151803369536
1
Iteration 13800: Loss = -9974.13648079502
Iteration 13900: Loss = -9974.13636848054
Iteration 14000: Loss = -9974.15151136462
1
Iteration 14100: Loss = -9974.13632542288
Iteration 14200: Loss = -9974.13625958802
Iteration 14300: Loss = -9974.146122874618
1
Iteration 14400: Loss = -9974.136179950103
Iteration 14500: Loss = -9974.136130900853
Iteration 14600: Loss = -9974.138378580976
1
Iteration 14700: Loss = -9974.136099351646
Iteration 14800: Loss = -9974.148600400731
1
Iteration 14900: Loss = -9974.136033037614
Iteration 15000: Loss = -9974.140252340956
1
Iteration 15100: Loss = -9974.135962614071
Iteration 15200: Loss = -9974.135941584465
Iteration 15300: Loss = -9974.24010911515
1
Iteration 15400: Loss = -9974.13585868195
Iteration 15500: Loss = -9974.135864003836
1
Iteration 15600: Loss = -9974.137834222642
2
Iteration 15700: Loss = -9974.135828420061
Iteration 15800: Loss = -9974.135835218742
1
Iteration 15900: Loss = -9974.13579291899
Iteration 16000: Loss = -9974.135788602165
Iteration 16100: Loss = -9974.16535993424
1
Iteration 16200: Loss = -9974.13578501234
Iteration 16300: Loss = -9974.281363098771
1
Iteration 16400: Loss = -9974.135759940407
Iteration 16500: Loss = -9974.135735811218
Iteration 16600: Loss = -9974.135827090842
1
Iteration 16700: Loss = -9974.135676799566
Iteration 16800: Loss = -9974.137159633494
1
Iteration 16900: Loss = -9974.135616050204
Iteration 17000: Loss = -9974.136194109084
1
Iteration 17100: Loss = -9974.135620044222
2
Iteration 17200: Loss = -9974.138658914842
3
Iteration 17300: Loss = -9974.135608372928
Iteration 17400: Loss = -9974.21001219043
1
Iteration 17500: Loss = -9974.135620264655
2
Iteration 17600: Loss = -9974.136278935699
3
Iteration 17700: Loss = -9974.135633871048
4
Iteration 17800: Loss = -9974.135576290922
Iteration 17900: Loss = -9974.13599561663
1
Iteration 18000: Loss = -9974.135567990888
Iteration 18100: Loss = -9974.146588292022
1
Iteration 18200: Loss = -9974.135626610856
2
Iteration 18300: Loss = -9974.13770325022
3
Iteration 18400: Loss = -9974.135644332968
4
Iteration 18500: Loss = -9974.1355707076
5
Iteration 18600: Loss = -9974.333190380989
6
Iteration 18700: Loss = -9974.135544668468
Iteration 18800: Loss = -9974.13714711573
1
Iteration 18900: Loss = -9974.135537224243
Iteration 19000: Loss = -9974.135553902757
1
Iteration 19100: Loss = -9974.135701536285
2
Iteration 19200: Loss = -9974.13552193442
Iteration 19300: Loss = -9974.161039925088
1
Iteration 19400: Loss = -9974.135525554186
2
Iteration 19500: Loss = -9974.13562584479
3
Iteration 19600: Loss = -9974.135706766529
4
Iteration 19700: Loss = -9974.135516372462
Iteration 19800: Loss = -9974.143480132221
1
Iteration 19900: Loss = -9974.135500581166
tensor([[-7.3046,  4.7320],
        [-5.6151,  4.2180],
        [-7.0703,  4.9345],
        [-9.6357,  6.8757],
        [-4.4740,  3.0185],
        [-7.7770,  6.3248],
        [-6.1233,  4.6769],
        [-5.7107,  4.3188],
        [-6.7569,  5.3091],
        [-8.9739,  7.5754],
        [-6.1807,  4.6131],
        [-7.8149,  6.3878],
        [-8.5922,  7.1899],
        [-6.2478,  4.5719],
        [-4.0592,  2.1972],
        [-8.4236,  6.9872],
        [-5.7421,  4.0108],
        [-6.2948,  4.5194],
        [-7.4372,  5.7307],
        [-7.2370,  3.6092],
        [-5.2561,  3.2093],
        [-7.3510,  5.7719],
        [-5.6833,  3.9758],
        [-4.3807,  1.7806],
        [-7.7919,  6.3611],
        [-4.2509,  1.9312],
        [-2.2444,  0.6515],
        [-6.1233,  2.4065],
        [-8.4408,  5.8216],
        [-4.3793,  2.9816],
        [-6.1062,  4.7015],
        [-4.5068,  2.9024],
        [-3.3924,  1.6292],
        [-7.8628,  6.3856],
        [-3.2583,  1.7692],
        [-2.6326,  1.2463],
        [-9.4331,  5.8979],
        [-5.7890,  3.9464],
        [-4.7856,  2.5339],
        [-9.0721,  6.9248],
        [-3.8122,  2.3830],
        [-5.0749,  3.4136],
        [-7.7473,  5.4999],
        [-8.6501,  7.2637],
        [-4.2073,  2.0238],
        [-7.3478,  5.8856],
        [-6.7171,  5.2666],
        [-6.1401,  4.7238],
        [-8.1089,  6.2608],
        [-8.6824,  7.2222],
        [-7.8032,  6.4143],
        [-4.5743,  2.7593],
        [-2.6693,  1.1339],
        [-6.8347,  5.0792],
        [-3.7081,  1.3956],
        [-3.7796,  1.4457],
        [-3.9234,  2.2631],
        [-8.8720,  6.5376],
        [-5.5467,  4.1490],
        [-5.3018,  3.1930],
        [-9.4038,  7.7370],
        [-3.1143,  0.7495],
        [-7.9668,  6.2803],
        [-8.3381,  6.7875],
        [-0.3065, -1.2359],
        [-5.5867,  4.1604],
        [-4.4024,  1.8845],
        [-6.1366,  4.7249],
        [-5.6473,  1.7131],
        [-7.9854,  6.2997],
        [-4.3817,  2.9874],
        [-4.3701,  2.9674],
        [-8.8910,  5.4557],
        [-8.0171,  5.2265],
        [-6.1564,  4.7327],
        [-8.0698,  5.1518],
        [-5.2437,  3.3882],
        [-3.8574,  2.3232],
        [-5.3250,  3.3021],
        [-5.8472,  4.1825],
        [-3.2490,  1.8555],
        [-5.5833,  4.0848],
        [-7.8485,  6.4608],
        [-5.6899,  3.9556],
        [-7.1155,  5.0561],
        [-7.8271,  5.4106],
        [-3.2276,  1.8273],
        [-8.7994,  7.2548],
        [-6.3806,  4.4510],
        [-4.9209,  3.5345],
        [-5.5318,  4.1220],
        [-4.0840,  2.1271],
        [-5.5432,  1.7962],
        [-8.7845,  6.8155],
        [-5.6416,  4.0600],
        [-7.9356,  6.4293],
        [-5.5914,  4.1851],
        [-5.1730,  1.0682],
        [-3.8202,  1.2561],
        [-7.2603,  5.8458]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.6334e-05, 9.9990e-01],
        [7.2352e-02, 9.2765e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0090, 0.9910], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0855, 0.0494],
         [0.9922, 0.1427]],

        [[0.1474, 0.1120],
         [0.1694, 0.8227]],

        [[0.9457, 0.1424],
         [0.8066, 0.4776]],

        [[0.9864, 0.1185],
         [0.5873, 0.7440]],

        [[0.9350, 0.0814],
         [0.7139, 0.6653]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
Global Adjusted Rand Index: -0.0004446725451460072
Average Adjusted Rand Index: 0.0004593523422826859
10087.563434266836
new:  [-0.004642911204330692, 0.023033235523174662, 0.0062187062454228965, -0.0004446725451460072] [0.0006556802549507461, 0.015637984288232803, 0.005305907368143331, 0.0004593523422826859] [9973.82350729648, 9969.252146663423, 9970.835981049451, 9974.13551606443]
prior:  [-0.00038523117239545255, -0.00038523117239545255, 6.064153498089786e-05, 0.0] [0.0003653940237608727, 0.0003653940237608727, -0.00010178366459815819, 0.0] [9974.205233711275, 9974.20612783846, 9975.413426803321, nan]
-----------------------------------------------------------------------------------------
This iteration is 14
True Objective function: Loss = -10033.4814440645
Iteration 0: Loss = -22174.00815335777
Iteration 10: Loss = -9906.786331953488
Iteration 20: Loss = -9906.785058197183
Iteration 30: Loss = -9906.785014427456
Iteration 40: Loss = -9906.784874823496
Iteration 50: Loss = -9906.784718646128
Iteration 60: Loss = -9906.784561061464
Iteration 70: Loss = -9906.784466494793
Iteration 80: Loss = -9906.784293419454
Iteration 90: Loss = -9906.784193753438
Iteration 100: Loss = -9906.784063528869
Iteration 110: Loss = -9906.783923385752
Iteration 120: Loss = -9906.78381508984
Iteration 130: Loss = -9906.783633291258
Iteration 140: Loss = -9906.783490348635
Iteration 150: Loss = -9906.783366380563
Iteration 160: Loss = -9906.783252271747
Iteration 170: Loss = -9906.783093479438
Iteration 180: Loss = -9906.78301957083
Iteration 190: Loss = -9906.782841438431
Iteration 200: Loss = -9906.782689700964
Iteration 210: Loss = -9906.782562840748
Iteration 220: Loss = -9906.78241656087
Iteration 230: Loss = -9906.782274130697
Iteration 240: Loss = -9906.782181394316
Iteration 250: Loss = -9906.78209779494
Iteration 260: Loss = -9906.781944217546
Iteration 270: Loss = -9906.781770705804
Iteration 280: Loss = -9906.781677497109
Iteration 290: Loss = -9906.78152906625
Iteration 300: Loss = -9906.781418801078
Iteration 310: Loss = -9906.781255790995
Iteration 320: Loss = -9906.781168504998
Iteration 330: Loss = -9906.781043278212
Iteration 340: Loss = -9906.780911921387
Iteration 350: Loss = -9906.780789944189
Iteration 360: Loss = -9906.780647048525
Iteration 370: Loss = -9906.78054137096
Iteration 380: Loss = -9906.780402137356
Iteration 390: Loss = -9906.780307418232
Iteration 400: Loss = -9906.780154812526
Iteration 410: Loss = -9906.780040779107
Iteration 420: Loss = -9906.779933231124
Iteration 430: Loss = -9906.779832280716
Iteration 440: Loss = -9906.7797028832
Iteration 450: Loss = -9906.779541855347
Iteration 460: Loss = -9906.779447062234
Iteration 470: Loss = -9906.779360304818
Iteration 480: Loss = -9906.7792427588
Iteration 490: Loss = -9906.779132363105
Iteration 500: Loss = -9906.778991992018
Iteration 510: Loss = -9906.778852950598
Iteration 520: Loss = -9906.778753960414
Iteration 530: Loss = -9906.778666499626
Iteration 540: Loss = -9906.778546831454
Iteration 550: Loss = -9906.77843841367
Iteration 560: Loss = -9906.778353783431
Iteration 570: Loss = -9906.778234387146
Iteration 580: Loss = -9906.778125309576
Iteration 590: Loss = -9906.778031649079
Iteration 600: Loss = -9906.77790132566
Iteration 610: Loss = -9906.777815481539
Iteration 620: Loss = -9906.777684379864
Iteration 630: Loss = -9906.777607225866
Iteration 640: Loss = -9906.777487009165
Iteration 650: Loss = -9906.777381019227
Iteration 660: Loss = -9906.777276949668
Iteration 670: Loss = -9906.777156967768
Iteration 680: Loss = -9906.777111731368
Iteration 690: Loss = -9906.777024119598
Iteration 700: Loss = -9906.776901729154
Iteration 710: Loss = -9906.776803830428
Iteration 720: Loss = -9906.776692236635
Iteration 730: Loss = -9906.776629549664
Iteration 740: Loss = -9906.776547330785
Iteration 750: Loss = -9906.776442862747
Iteration 760: Loss = -9906.776369094421
Iteration 770: Loss = -9906.776257257217
Iteration 780: Loss = -9906.776152025457
Iteration 790: Loss = -9906.776088018387
Iteration 800: Loss = -9906.776022522143
Iteration 810: Loss = -9906.775912792007
Iteration 820: Loss = -9906.775825045708
Iteration 830: Loss = -9906.775717141865
Iteration 840: Loss = -9906.775659663963
Iteration 850: Loss = -9906.775556033093
Iteration 860: Loss = -9906.775502355269
Iteration 870: Loss = -9906.77539280963
Iteration 880: Loss = -9906.7753552433
Iteration 890: Loss = -9906.775246718198
Iteration 900: Loss = -9906.775158217746
Iteration 910: Loss = -9906.775082099732
Iteration 920: Loss = -9906.774996994449
Iteration 930: Loss = -9906.774946115298
Iteration 940: Loss = -9906.77482678055
Iteration 950: Loss = -9906.774753842883
Iteration 960: Loss = -9906.774681463998
Iteration 970: Loss = -9906.774641518328
Iteration 980: Loss = -9906.774573931296
Iteration 990: Loss = -9906.774498563247
Iteration 1000: Loss = -9906.774409678625
Iteration 1010: Loss = -9906.774362252196
Iteration 1020: Loss = -9906.774285678099
Iteration 1030: Loss = -9906.774174805962
Iteration 1040: Loss = -9906.774142868591
Iteration 1050: Loss = -9906.774067471599
Iteration 1060: Loss = -9906.77397964773
Iteration 1070: Loss = -9906.773916292997
Iteration 1080: Loss = -9906.773855971574
Iteration 1090: Loss = -9906.773834574124
Iteration 1100: Loss = -9906.773753086201
Iteration 1110: Loss = -9906.773657662947
Iteration 1120: Loss = -9906.773629793224
Iteration 1130: Loss = -9906.773579618495
Iteration 1140: Loss = -9906.773512360467
Iteration 1150: Loss = -9906.773452633257
Iteration 1160: Loss = -9906.773369
Iteration 1170: Loss = -9906.773333950816
Iteration 1180: Loss = -9906.773272567243
Iteration 1190: Loss = -9906.773199094467
Iteration 1200: Loss = -9906.773136151121
Iteration 1210: Loss = -9906.773124231744
Iteration 1220: Loss = -9906.7730467614
Iteration 1230: Loss = -9906.773004536964
Iteration 1240: Loss = -9906.772937241158
Iteration 1250: Loss = -9906.77286759215
Iteration 1260: Loss = -9906.772846521042
Iteration 1270: Loss = -9906.772787407343
Iteration 1280: Loss = -9906.77273811723
Iteration 1290: Loss = -9906.772697792832
Iteration 1300: Loss = -9906.772637754442
Iteration 1310: Loss = -9906.772617882241
Iteration 1320: Loss = -9906.772558332292
Iteration 1330: Loss = -9906.772509217722
Iteration 1340: Loss = -9906.772464598193
Iteration 1350: Loss = -9906.772430870607
Iteration 1360: Loss = -9906.772376461988
Iteration 1370: Loss = -9906.772287585505
Iteration 1380: Loss = -9906.77226346006
Iteration 1390: Loss = -9906.77221512308
Iteration 1400: Loss = -9906.772193423683
Iteration 1410: Loss = -9906.772145216284
Iteration 1420: Loss = -9906.7721226628
Iteration 1430: Loss = -9906.772083998785
Iteration 1440: Loss = -9906.77207249986
Iteration 1450: Loss = -9906.772025085738
Iteration 1460: Loss = -9906.771939962126
Iteration 1470: Loss = -9906.77192270501
Iteration 1480: Loss = -9906.771891701019
Iteration 1490: Loss = -9906.771794842669
Iteration 1500: Loss = -9906.771815989456
1
Iteration 1510: Loss = -9906.771758850737
Iteration 1520: Loss = -9906.771738133823
Iteration 1530: Loss = -9906.771678522535
Iteration 1540: Loss = -9906.771664984555
Iteration 1550: Loss = -9906.771633740227
Iteration 1560: Loss = -9906.771590685134
Iteration 1570: Loss = -9906.771567618143
Iteration 1580: Loss = -9906.771525309368
Iteration 1590: Loss = -9906.771471834421
Iteration 1600: Loss = -9906.77144968354
Iteration 1610: Loss = -9906.771458646539
1
Iteration 1620: Loss = -9906.771396565397
Iteration 1630: Loss = -9906.771373560665
Iteration 1640: Loss = -9906.771339762463
Iteration 1650: Loss = -9906.77130541767
Iteration 1660: Loss = -9906.77125317155
Iteration 1670: Loss = -9906.771236037072
Iteration 1680: Loss = -9906.771240584912
1
Iteration 1690: Loss = -9906.771185204203
Iteration 1700: Loss = -9906.77115571946
Iteration 1710: Loss = -9906.771154585449
Iteration 1720: Loss = -9906.771114825238
Iteration 1730: Loss = -9906.771139129361
1
Iteration 1740: Loss = -9906.77104441732
Iteration 1750: Loss = -9906.771039751247
Iteration 1760: Loss = -9906.77097453834
Iteration 1770: Loss = -9906.770977870345
1
Iteration 1780: Loss = -9906.770967665432
Iteration 1790: Loss = -9906.770943530713
Iteration 1800: Loss = -9906.770895622765
Iteration 1810: Loss = -9906.77087814252
Iteration 1820: Loss = -9906.770857198728
Iteration 1830: Loss = -9906.770814301937
Iteration 1840: Loss = -9906.77084179003
1
Iteration 1850: Loss = -9906.770780709181
Iteration 1860: Loss = -9906.770774709526
Iteration 1870: Loss = -9906.770723151241
Iteration 1880: Loss = -9906.770718668173
Iteration 1890: Loss = -9906.770701548796
Iteration 1900: Loss = -9906.770667310946
Iteration 1910: Loss = -9906.770665067766
Iteration 1920: Loss = -9906.77062671385
Iteration 1930: Loss = -9906.770618450384
Iteration 1940: Loss = -9906.770594327283
Iteration 1950: Loss = -9906.770586215032
Iteration 1960: Loss = -9906.770546156655
Iteration 1970: Loss = -9906.770559854282
1
Iteration 1980: Loss = -9906.77049703751
Iteration 1990: Loss = -9906.770481149704
Iteration 2000: Loss = -9906.770510357168
1
Iteration 2010: Loss = -9906.770498220136
2
Iteration 2020: Loss = -9906.770481333419
3
Stopping early at iteration 2019 due to no improvement.
pi: tensor([[0.1749, 0.8251],
        [0.8969, 0.1031]], dtype=torch.float64)
alpha: tensor([0.5208, 0.4792])
beta: tensor([[[0.1361, 0.1352],
         [0.8740, 0.1360]],

        [[0.8463, 0.1404],
         [0.4683, 0.7123]],

        [[0.2824, 0.1344],
         [0.0660, 0.7859]],

        [[0.8204, 0.1284],
         [0.4080, 0.8012]],

        [[0.7437, 0.1420],
         [0.1753, 0.6576]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22710.87352340381
Iteration 100: Loss = -9912.272482314143
Iteration 200: Loss = -9909.791014450226
Iteration 300: Loss = -9908.936520538615
Iteration 400: Loss = -9908.524743793963
Iteration 500: Loss = -9908.28396558626
Iteration 600: Loss = -9908.091327744536
Iteration 700: Loss = -9908.002644468013
Iteration 800: Loss = -9907.953923591289
Iteration 900: Loss = -9907.916074570983
Iteration 1000: Loss = -9907.890567691784
Iteration 1100: Loss = -9907.872367484886
Iteration 1200: Loss = -9907.843924399358
Iteration 1300: Loss = -9907.324939341703
Iteration 1400: Loss = -9904.392151143426
Iteration 1500: Loss = -9903.965146462175
Iteration 1600: Loss = -9903.906650809331
Iteration 1700: Loss = -9903.86183356778
Iteration 1800: Loss = -9903.796477285236
Iteration 1900: Loss = -9903.773119544383
Iteration 2000: Loss = -9903.75545914981
Iteration 2100: Loss = -9903.741083897583
Iteration 2200: Loss = -9903.728943363409
Iteration 2300: Loss = -9903.71827273233
Iteration 2400: Loss = -9903.708295934848
Iteration 2500: Loss = -9903.698964095058
Iteration 2600: Loss = -9903.691225342749
Iteration 2700: Loss = -9903.685357233955
Iteration 2800: Loss = -9903.680059441209
Iteration 2900: Loss = -9903.675285986268
Iteration 3000: Loss = -9903.67085927037
Iteration 3100: Loss = -9903.665034333064
Iteration 3200: Loss = -9903.657723383592
Iteration 3300: Loss = -9903.653492508083
Iteration 3400: Loss = -9903.650200329714
Iteration 3500: Loss = -9903.647651153917
Iteration 3600: Loss = -9903.645253318695
Iteration 3700: Loss = -9903.642984203258
Iteration 3800: Loss = -9903.64090873722
Iteration 3900: Loss = -9903.63876963132
Iteration 4000: Loss = -9903.636784756549
Iteration 4100: Loss = -9903.635378225203
Iteration 4200: Loss = -9903.6334116065
Iteration 4300: Loss = -9903.63187798416
Iteration 4400: Loss = -9903.629550499047
Iteration 4500: Loss = -9903.627314683876
Iteration 4600: Loss = -9903.627804226515
1
Iteration 4700: Loss = -9903.625364721358
Iteration 4800: Loss = -9903.624449927496
Iteration 4900: Loss = -9903.623570006197
Iteration 5000: Loss = -9903.62273723565
Iteration 5100: Loss = -9903.62193250735
Iteration 5200: Loss = -9903.621220356416
Iteration 5300: Loss = -9903.62050894156
Iteration 5400: Loss = -9903.62107704782
1
Iteration 5500: Loss = -9903.619117429153
Iteration 5600: Loss = -9903.618499071708
Iteration 5700: Loss = -9903.618009213302
Iteration 5800: Loss = -9903.617455662701
Iteration 5900: Loss = -9903.617081131013
Iteration 6000: Loss = -9903.616544708097
Iteration 6100: Loss = -9903.616093586697
Iteration 6200: Loss = -9903.615716875578
Iteration 6300: Loss = -9903.61531313584
Iteration 6400: Loss = -9903.61511086393
Iteration 6500: Loss = -9903.614584817253
Iteration 6600: Loss = -9903.61419866109
Iteration 6700: Loss = -9903.613874361678
Iteration 6800: Loss = -9903.613436666883
Iteration 6900: Loss = -9903.613035662693
Iteration 7000: Loss = -9903.6125163261
Iteration 7100: Loss = -9903.612018631797
Iteration 7200: Loss = -9903.61147174318
Iteration 7300: Loss = -9903.61117375494
Iteration 7400: Loss = -9903.611148546424
Iteration 7500: Loss = -9903.610778130243
Iteration 7600: Loss = -9903.610614324965
Iteration 7700: Loss = -9903.61039923357
Iteration 7800: Loss = -9903.61022132457
Iteration 7900: Loss = -9903.6102165549
Iteration 8000: Loss = -9903.609894726742
Iteration 8100: Loss = -9903.609786135146
Iteration 8200: Loss = -9903.609609738749
Iteration 8300: Loss = -9903.609461546184
Iteration 8400: Loss = -9903.61034355102
1
Iteration 8500: Loss = -9903.609174397257
Iteration 8600: Loss = -9903.608996965591
Iteration 8700: Loss = -9903.60884528153
Iteration 8800: Loss = -9903.60873676123
Iteration 8900: Loss = -9903.609000267235
1
Iteration 9000: Loss = -9903.608453244282
Iteration 9100: Loss = -9903.60815836783
Iteration 9200: Loss = -9903.607957683545
Iteration 9300: Loss = -9903.607868699557
Iteration 9400: Loss = -9903.608444109854
1
Iteration 9500: Loss = -9903.60774611918
Iteration 9600: Loss = -9903.607753633662
1
Iteration 9700: Loss = -9903.607636932407
Iteration 9800: Loss = -9903.607676524247
1
Iteration 9900: Loss = -9903.607716677276
2
Iteration 10000: Loss = -9903.607496895636
Iteration 10100: Loss = -9903.621429377275
1
Iteration 10200: Loss = -9903.607313560422
Iteration 10300: Loss = -9903.647213019864
1
Iteration 10400: Loss = -9903.607231385322
Iteration 10500: Loss = -9903.610750522876
1
Iteration 10600: Loss = -9903.607082983439
Iteration 10700: Loss = -9903.607071346994
Iteration 10800: Loss = -9903.606975974104
Iteration 10900: Loss = -9903.60696685894
Iteration 11000: Loss = -9903.613932152204
1
Iteration 11100: Loss = -9903.60690597389
Iteration 11200: Loss = -9903.60689673536
Iteration 11300: Loss = -9903.609779486278
1
Iteration 11400: Loss = -9903.606848494106
Iteration 11500: Loss = -9903.60682424563
Iteration 11600: Loss = -9903.615796092323
1
Iteration 11700: Loss = -9903.606765752656
Iteration 11800: Loss = -9903.60673304503
Iteration 11900: Loss = -9903.661570171893
1
Iteration 12000: Loss = -9903.606664370694
Iteration 12100: Loss = -9903.606633932579
Iteration 12200: Loss = -9903.606615479546
Iteration 12300: Loss = -9903.607265347993
1
Iteration 12400: Loss = -9903.606577371416
Iteration 12500: Loss = -9903.60654483849
Iteration 12600: Loss = -9903.72877650439
1
Iteration 12700: Loss = -9903.606538862923
Iteration 12800: Loss = -9903.606544732822
1
Iteration 12900: Loss = -9903.606594089779
2
Iteration 13000: Loss = -9903.606543545991
3
Iteration 13100: Loss = -9903.606482597886
Iteration 13200: Loss = -9903.60654820077
1
Iteration 13300: Loss = -9903.606502316998
2
Iteration 13400: Loss = -9903.606485758419
3
Iteration 13500: Loss = -9903.61207444286
4
Iteration 13600: Loss = -9903.606543505874
5
Iteration 13700: Loss = -9903.606475112947
Iteration 13800: Loss = -9903.608045483932
1
Iteration 13900: Loss = -9903.606445220614
Iteration 14000: Loss = -9903.606414890683
Iteration 14100: Loss = -9903.606995268812
1
Iteration 14200: Loss = -9903.606415894381
2
Iteration 14300: Loss = -9903.606397427204
Iteration 14400: Loss = -9903.614068129187
1
Iteration 14500: Loss = -9903.606396121791
Iteration 14600: Loss = -9903.606395556919
Iteration 14700: Loss = -9903.607189190292
1
Iteration 14800: Loss = -9903.60638341782
Iteration 14900: Loss = -9903.606390939836
1
Iteration 15000: Loss = -9903.609758292989
2
Iteration 15100: Loss = -9903.606356425424
Iteration 15200: Loss = -9903.606331575893
Iteration 15300: Loss = -9903.62824705127
1
Iteration 15400: Loss = -9903.606341158424
2
Iteration 15500: Loss = -9903.606331423378
Iteration 15600: Loss = -9903.60636968503
1
Iteration 15700: Loss = -9903.606442188684
2
Iteration 15800: Loss = -9903.60631571354
Iteration 15900: Loss = -9903.606320614217
1
Iteration 16000: Loss = -9903.613662728732
2
Iteration 16100: Loss = -9903.606319638606
3
Iteration 16200: Loss = -9903.606320626037
4
Iteration 16300: Loss = -9903.606307203812
Iteration 16400: Loss = -9903.607741951602
1
Iteration 16500: Loss = -9903.606315443947
2
Iteration 16600: Loss = -9903.606354327545
3
Iteration 16700: Loss = -9903.6066006837
4
Iteration 16800: Loss = -9903.789047433294
5
Iteration 16900: Loss = -9903.606279504027
Iteration 17000: Loss = -9903.60695253182
1
Iteration 17100: Loss = -9903.606301923332
2
Iteration 17200: Loss = -9903.630661183572
3
Iteration 17300: Loss = -9903.606285131304
4
Iteration 17400: Loss = -9903.661609014895
5
Iteration 17500: Loss = -9903.60628909098
6
Iteration 17600: Loss = -9903.606290622958
7
Iteration 17700: Loss = -9903.606331453726
8
Iteration 17800: Loss = -9903.60627665788
Iteration 17900: Loss = -9903.607161996371
1
Iteration 18000: Loss = -9903.606331884084
2
Iteration 18100: Loss = -9903.60628446008
3
Iteration 18200: Loss = -9903.606265843795
Iteration 18300: Loss = -9903.606385502604
1
Iteration 18400: Loss = -9903.606246963454
Iteration 18500: Loss = -9903.60627130205
1
Iteration 18600: Loss = -9903.606263209093
2
Iteration 18700: Loss = -9903.606456202344
3
Iteration 18800: Loss = -9903.60626947641
4
Iteration 18900: Loss = -9903.606288344807
5
Iteration 19000: Loss = -9903.606261394503
6
Iteration 19100: Loss = -9903.606252150446
7
Iteration 19200: Loss = -9903.606253500053
8
Iteration 19300: Loss = -9903.606255450957
9
Iteration 19400: Loss = -9903.606767982983
10
Stopping early at iteration 19400 due to no improvement.
tensor([[ -5.4987,   0.8834],
        [ -6.8936,   2.2783],
        [-10.0644,   5.4492],
        [-11.3726,   6.7573],
        [ -4.5225,  -0.0927],
        [ -7.5870,   2.9718],
        [-11.9234,   7.3082],
        [  0.2738,  -4.8891],
        [-11.6048,   6.9896],
        [ -5.9041,   1.2889],
        [ -6.1950,   1.5798],
        [ -8.6746,   4.0593],
        [ -6.3753,   1.7601],
        [ -8.1240,   3.5087],
        [ -8.8554,   4.2402],
        [ -8.9154,   4.3001],
        [ -4.8817,   0.2665],
        [ -5.8363,   1.2211],
        [ -6.3171,   1.7019],
        [-11.9850,   7.3697],
        [-10.6372,   6.0220],
        [-10.4480,   5.8328],
        [-11.8773,   7.2620],
        [ -7.7332,   3.1180],
        [ -6.9808,   2.3656],
        [-11.7283,   7.1131],
        [ -6.3356,   1.7204],
        [ -1.5592,  -3.0560],
        [ -8.7175,   4.1023],
        [ -6.9884,   2.3732],
        [-11.9657,   7.3505],
        [-11.8612,   7.2460],
        [ -8.1606,   3.5453],
        [ -9.9477,   5.3325],
        [ -7.2574,   2.6421],
        [-11.3960,   6.7808],
        [-11.4266,   6.8114],
        [ -9.2222,   4.6070],
        [ -7.1270,   2.5118],
        [ -7.8094,   3.1942],
        [ -9.0085,   4.3932],
        [-11.7712,   7.1560],
        [ -6.3380,   1.7228],
        [-11.0314,   6.4162],
        [ -7.4318,   2.8165],
        [ -6.2795,   1.6643],
        [ -6.6895,   2.0743],
        [ -5.6560,   1.0408],
        [-11.6092,   6.9939],
        [ -6.2141,   1.5988],
        [-11.8007,   7.1855],
        [ -8.7987,   4.1835],
        [ -8.8149,   4.1997],
        [ -5.4414,   0.8262],
        [-10.3793,   5.7641],
        [ -9.0066,   4.3914],
        [ -5.0623,   0.4471],
        [-10.2340,   5.6188],
        [ -5.4341,   0.8189],
        [ -7.0382,   2.4230],
        [ -6.2092,   1.5939],
        [-11.6565,   7.0413],
        [ -3.6721,  -0.9431],
        [-11.0356,   6.4204],
        [ -8.2954,   3.6802],
        [ -7.0127,   2.3975],
        [-11.3133,   6.6981],
        [ -8.4163,   3.8010],
        [ -5.3755,   0.7603],
        [ -7.9321,   3.3169],
        [ -8.1092,   3.4940],
        [-11.7791,   7.1638],
        [ -8.3390,   3.7238],
        [ -4.6371,   0.0219],
        [ -7.8024,   3.1872],
        [-11.8131,   7.1979],
        [-11.5931,   6.9779],
        [-11.2606,   6.6454],
        [ -8.4581,   3.8429],
        [ -8.2066,   3.5914],
        [ -7.9759,   3.3607],
        [-11.7283,   7.1131],
        [-11.8961,   7.2809],
        [-10.9589,   6.3437],
        [ -9.8859,   5.2706],
        [ -6.1807,   1.5655],
        [ -7.0397,   2.4245],
        [ -7.6779,   3.0627],
        [ -6.6844,   2.0692],
        [-11.0895,   6.4743],
        [ -6.8239,   2.2087],
        [ -8.5993,   3.9841],
        [ -9.1733,   4.5581],
        [ -5.3080,   0.6928],
        [ -5.3408,   0.7256],
        [-11.9939,   7.3787],
        [-11.8127,   7.1975],
        [ -7.5082,   2.8930],
        [ -8.4669,   3.8517],
        [ -7.6968,   3.0816]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.0798e-07],
        [3.3467e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0192, 0.9808], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5311, 0.1825],
         [0.8740, 0.1356]],

        [[0.8463, 0.1860],
         [0.4683, 0.7123]],

        [[0.2824, 0.2138],
         [0.0660, 0.7859]],

        [[0.8204, 0.1241],
         [0.4080, 0.8012]],

        [[0.7437, 0.1988],
         [0.1753, 0.6576]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.018335442091456804
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: 0.011120442419829029
Average Adjusted Rand Index: 0.010867349304690565
Iteration 0: Loss = -17010.02319526268
Iteration 10: Loss = -9908.005102241985
Iteration 20: Loss = -9907.988132915303
Iteration 30: Loss = -9907.921195484414
Iteration 40: Loss = -9906.828355745492
Iteration 50: Loss = -9906.046118302133
Iteration 60: Loss = -9905.887145009136
Iteration 70: Loss = -9905.849803255633
Iteration 80: Loss = -9905.8403083529
Iteration 90: Loss = -9905.837853230689
Iteration 100: Loss = -9905.837301134556
Iteration 110: Loss = -9905.837187582378
Iteration 120: Loss = -9905.837229697938
1
Iteration 130: Loss = -9905.837216676091
2
Iteration 140: Loss = -9905.837313818278
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.1170, 0.8830],
        [0.0446, 0.9554]], dtype=torch.float64)
alpha: tensor([0.0479, 0.9521])
beta: tensor([[[0.1607, 0.1386],
         [0.8612, 0.1350]],

        [[0.4004, 0.1801],
         [0.9414, 0.6075]],

        [[0.8583, 0.1444],
         [0.8786, 0.5026]],

        [[0.0801, 0.0849],
         [0.1184, 0.7100]],

        [[0.4830, 0.1792],
         [0.9238, 0.2145]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17009.452634222504
Iteration 100: Loss = -9910.580092973303
Iteration 200: Loss = -9908.568704677193
Iteration 300: Loss = -9907.981290043343
Iteration 400: Loss = -9907.519409546952
Iteration 500: Loss = -9907.022903010788
Iteration 600: Loss = -9906.593328990553
Iteration 700: Loss = -9906.210323118386
Iteration 800: Loss = -9905.991007676766
Iteration 900: Loss = -9905.819403708752
Iteration 1000: Loss = -9905.665119406169
Iteration 1100: Loss = -9905.515101062476
Iteration 1200: Loss = -9905.400985981441
Iteration 1300: Loss = -9905.30120875505
Iteration 1400: Loss = -9905.19254789821
Iteration 1500: Loss = -9905.116208812029
Iteration 1600: Loss = -9905.138610034941
1
Iteration 1700: Loss = -9904.89482527618
Iteration 1800: Loss = -9904.065457320117
Iteration 1900: Loss = -9903.101482021671
Iteration 2000: Loss = -9902.506526942801
Iteration 2100: Loss = -9902.229923279427
Iteration 2200: Loss = -9902.072650165823
Iteration 2300: Loss = -9901.967049032992
Iteration 2400: Loss = -9901.891206616549
Iteration 2500: Loss = -9901.834484265955
Iteration 2600: Loss = -9901.790935922862
Iteration 2700: Loss = -9901.757628980642
Iteration 2800: Loss = -9901.729562757368
Iteration 2900: Loss = -9901.706321941805
Iteration 3000: Loss = -9901.699768014192
Iteration 3100: Loss = -9901.656900026162
Iteration 3200: Loss = -9901.640187252558
Iteration 3300: Loss = -9901.628618344215
Iteration 3400: Loss = -9901.616920847136
Iteration 3500: Loss = -9901.607529383919
Iteration 3600: Loss = -9901.612986779764
1
Iteration 3700: Loss = -9901.591543964081
Iteration 3800: Loss = -9901.584376206918
Iteration 3900: Loss = -9901.604858970024
1
Iteration 4000: Loss = -9901.569418726878
Iteration 4100: Loss = -9901.562958474055
Iteration 4200: Loss = -9901.55804947079
Iteration 4300: Loss = -9901.553805850568
Iteration 4400: Loss = -9901.550172432148
Iteration 4500: Loss = -9901.54696399527
Iteration 4600: Loss = -9901.544163077668
Iteration 4700: Loss = -9901.541390877532
Iteration 4800: Loss = -9901.538977719327
Iteration 4900: Loss = -9901.537099505636
Iteration 5000: Loss = -9901.534725278165
Iteration 5100: Loss = -9901.53284823955
Iteration 5200: Loss = -9901.531263081122
Iteration 5300: Loss = -9901.52934363671
Iteration 5400: Loss = -9901.527677163496
Iteration 5500: Loss = -9901.525987796118
Iteration 5600: Loss = -9901.5242342331
Iteration 5700: Loss = -9901.522461190576
Iteration 5800: Loss = -9901.520538053015
Iteration 5900: Loss = -9901.572944890284
1
Iteration 6000: Loss = -9901.51283753316
Iteration 6100: Loss = -9901.50580513979
Iteration 6200: Loss = -9901.504620230473
Iteration 6300: Loss = -9901.50224418835
Iteration 6400: Loss = -9901.499608619702
Iteration 6500: Loss = -9901.495208649869
Iteration 6600: Loss = -9901.493085807253
Iteration 6700: Loss = -9901.492060434526
Iteration 6800: Loss = -9901.527200942226
1
Iteration 6900: Loss = -9901.490516259419
Iteration 7000: Loss = -9901.48976185376
Iteration 7100: Loss = -9901.51247464895
1
Iteration 7200: Loss = -9901.48805133268
Iteration 7300: Loss = -9901.487035257083
Iteration 7400: Loss = -9901.485180709184
Iteration 7500: Loss = -9901.40013726097
Iteration 7600: Loss = -9901.399260155493
Iteration 7700: Loss = -9901.398817703344
Iteration 7800: Loss = -9901.398571270447
Iteration 7900: Loss = -9901.398047207154
Iteration 8000: Loss = -9901.39765053808
Iteration 8100: Loss = -9901.397175999395
Iteration 8200: Loss = -9901.396427138365
Iteration 8300: Loss = -9901.418162902433
1
Iteration 8400: Loss = -9901.395601518483
Iteration 8500: Loss = -9901.395234040521
Iteration 8600: Loss = -9901.584999377557
1
Iteration 8700: Loss = -9901.393798564146
Iteration 8800: Loss = -9901.393483402842
Iteration 8900: Loss = -9901.395386614124
1
Iteration 9000: Loss = -9901.396428839778
2
Iteration 9100: Loss = -9901.392914919848
Iteration 9200: Loss = -9901.392914564238
Iteration 9300: Loss = -9901.39254774783
Iteration 9400: Loss = -9901.393079356096
1
Iteration 9500: Loss = -9901.392048934977
Iteration 9600: Loss = -9901.396263844423
1
Iteration 9700: Loss = -9901.391654648934
Iteration 9800: Loss = -9901.456332993293
1
Iteration 9900: Loss = -9901.391370677033
Iteration 10000: Loss = -9901.391165986503
Iteration 10100: Loss = -9901.395657800158
1
Iteration 10200: Loss = -9901.390767417062
Iteration 10300: Loss = -9901.392427991996
1
Iteration 10400: Loss = -9901.388777439359
Iteration 10500: Loss = -9901.388562945744
Iteration 10600: Loss = -9901.39082013024
1
Iteration 10700: Loss = -9901.436344046735
2
Iteration 10800: Loss = -9901.388179018608
Iteration 10900: Loss = -9901.388165034818
Iteration 11000: Loss = -9901.421999305938
1
Iteration 11100: Loss = -9901.388014925078
Iteration 11200: Loss = -9901.392135540847
1
Iteration 11300: Loss = -9901.387997197056
Iteration 11400: Loss = -9901.387991834707
Iteration 11500: Loss = -9901.388955741468
1
Iteration 11600: Loss = -9901.387922473761
Iteration 11700: Loss = -9901.3892441853
1
Iteration 11800: Loss = -9901.387738361507
Iteration 11900: Loss = -9901.442176051598
1
Iteration 12000: Loss = -9901.387695401347
Iteration 12100: Loss = -9901.384054521657
Iteration 12200: Loss = -9901.385823596536
1
Iteration 12300: Loss = -9901.383667525384
Iteration 12400: Loss = -9901.38382705074
1
Iteration 12500: Loss = -9901.37929491398
Iteration 12600: Loss = -9901.376895962556
Iteration 12700: Loss = -9901.376727318568
Iteration 12800: Loss = -9901.368789238459
Iteration 12900: Loss = -9901.36536405059
Iteration 13000: Loss = -9901.366130216245
1
Iteration 13100: Loss = -9901.365637108789
2
Iteration 13200: Loss = -9901.3652407118
Iteration 13300: Loss = -9901.365685614228
1
Iteration 13400: Loss = -9901.365302686778
2
Iteration 13500: Loss = -9901.390628521953
3
Iteration 13600: Loss = -9901.365052281983
Iteration 13700: Loss = -9901.365088500026
1
Iteration 13800: Loss = -9901.389330507203
2
Iteration 13900: Loss = -9901.367258776792
3
Iteration 14000: Loss = -9901.36507583572
4
Iteration 14100: Loss = -9901.364856625123
Iteration 14200: Loss = -9901.364932000368
1
Iteration 14300: Loss = -9901.365042225476
2
Iteration 14400: Loss = -9901.364889273435
3
Iteration 14500: Loss = -9901.363684036705
Iteration 14600: Loss = -9901.363609293323
Iteration 14700: Loss = -9901.363156307752
Iteration 14800: Loss = -9901.439192166412
1
Iteration 14900: Loss = -9901.362672581727
Iteration 15000: Loss = -9901.36292522082
1
Iteration 15100: Loss = -9901.36263248039
Iteration 15200: Loss = -9901.363535217426
1
Iteration 15300: Loss = -9901.362471114002
Iteration 15400: Loss = -9901.363368294442
1
Iteration 15500: Loss = -9901.367657822619
2
Iteration 15600: Loss = -9901.362839284237
3
Iteration 15700: Loss = -9901.356019715113
Iteration 15800: Loss = -9901.356589430648
1
Iteration 15900: Loss = -9901.384836003099
2
Iteration 16000: Loss = -9901.35594203701
Iteration 16100: Loss = -9901.357411977093
1
Iteration 16200: Loss = -9901.355107606152
Iteration 16300: Loss = -9901.35518948589
1
Iteration 16400: Loss = -9901.354924898187
Iteration 16500: Loss = -9901.356467361311
1
Iteration 16600: Loss = -9901.354846597695
Iteration 16700: Loss = -9901.354951855741
1
Iteration 16800: Loss = -9901.387214012366
2
Iteration 16900: Loss = -9901.35483398936
Iteration 17000: Loss = -9901.35487562457
1
Iteration 17100: Loss = -9901.354949771976
2
Iteration 17200: Loss = -9901.35497478283
3
Iteration 17300: Loss = -9901.354919682512
4
Iteration 17400: Loss = -9901.438169842
5
Iteration 17500: Loss = -9901.353274833427
Iteration 17600: Loss = -9901.34996899208
Iteration 17700: Loss = -9901.34934108697
Iteration 17800: Loss = -9901.348954952247
Iteration 17900: Loss = -9901.348897046711
Iteration 18000: Loss = -9901.348863452406
Iteration 18100: Loss = -9901.34872940063
Iteration 18200: Loss = -9901.351356683235
1
Iteration 18300: Loss = -9901.349156654322
2
Iteration 18400: Loss = -9901.347969023776
Iteration 18500: Loss = -9901.348858115538
1
Iteration 18600: Loss = -9901.413622236725
2
Iteration 18700: Loss = -9901.34713186396
Iteration 18800: Loss = -9901.38604022069
1
Iteration 18900: Loss = -9901.347143609471
2
Iteration 19000: Loss = -9901.354719736553
3
Iteration 19100: Loss = -9901.34713184835
Iteration 19200: Loss = -9901.347630989603
1
Iteration 19300: Loss = -9901.35422246222
2
Iteration 19400: Loss = -9901.368523292274
3
Iteration 19500: Loss = -9901.377326399412
4
Iteration 19600: Loss = -9901.346909374064
Iteration 19700: Loss = -9901.392291116907
1
Iteration 19800: Loss = -9901.346882011383
Iteration 19900: Loss = -9901.356030195928
1
tensor([[-7.4282e-01, -6.4432e-01],
        [-3.7657e+00,  1.8663e+00],
        [-4.1302e+00,  2.1673e+00],
        [-4.3477e+00,  2.8803e+00],
        [-1.3533e+00, -2.0296e+00],
        [-3.1603e+00,  1.7645e+00],
        [-5.1763e+00,  3.1154e+00],
        [ 8.9911e-01, -2.4124e+00],
        [-9.0954e+00,  6.6901e+00],
        [-3.7364e+00,  1.4012e+00],
        [-3.4895e+00,  2.0934e+00],
        [-4.2543e+00,  2.1949e+00],
        [-2.2384e+00,  7.9637e-01],
        [-4.8703e+00,  1.3457e+00],
        [-3.6701e+00,  2.1655e+00],
        [-6.2226e+00,  1.8477e+00],
        [-4.9032e-01, -9.8907e-01],
        [-1.9193e+00,  1.8758e-01],
        [-3.4000e+00, -3.8907e-01],
        [-4.9244e+00,  3.2871e+00],
        [-3.7547e+00,  2.3402e+00],
        [-9.8762e+00,  6.2029e+00],
        [-5.7247e+00,  2.5301e+00],
        [-3.9212e+00,  2.4259e+00],
        [-3.0470e+00,  1.1766e+00],
        [-4.1165e+00,  2.6257e+00],
        [-1.3856e+00, -2.0405e-01],
        [-7.3306e-01, -1.8911e+00],
        [-5.0779e+00,  2.6323e+00],
        [-8.5989e+00,  7.2124e+00],
        [-3.4261e+00,  1.9656e+00],
        [-4.3475e+00,  1.6458e+00],
        [-1.0927e+01,  6.3113e+00],
        [-9.2187e+00,  7.5206e+00],
        [-3.2296e+00,  1.8277e+00],
        [-9.1534e+00,  7.2972e+00],
        [-9.4401e+00,  7.0839e+00],
        [-4.2189e+00,  2.5267e+00],
        [-2.7910e+00,  1.1106e+00],
        [-4.1665e+00,  2.0243e+00],
        [-9.0348e+00,  6.6087e+00],
        [-4.9817e+00,  3.1827e+00],
        [-3.3022e+00, -5.7276e-01],
        [-4.2176e+00,  2.8313e+00],
        [-4.1820e+00,  2.7957e+00],
        [-1.9256e+00,  2.8794e-01],
        [-2.5219e+00,  1.0726e+00],
        [-4.2103e+00,  8.0448e-01],
        [-3.7425e+00,  1.7012e+00],
        [-3.3390e+00,  1.2005e+00],
        [-2.4643e+00,  1.0539e+00],
        [-5.0229e+00,  1.6189e+00],
        [-4.5659e+00,  2.9062e+00],
        [-3.0533e+00,  8.4360e-01],
        [-2.2174e+00,  1.7597e-01],
        [-8.8564e+00,  7.4656e+00],
        [-6.8597e-03, -2.1285e+00],
        [-4.3082e+00,  1.4884e+00],
        [-2.3625e+00,  8.6462e-02],
        [-1.6769e+00,  7.4336e-02],
        [-3.4246e+00,  1.7633e+00],
        [-3.9203e+00,  2.4993e+00],
        [-1.0911e+00, -1.1381e+00],
        [-4.5144e+00,  2.0386e+00],
        [-3.4277e+00,  2.0367e+00],
        [-3.4305e+00,  1.9861e+00],
        [-9.1440e+00,  7.2800e+00],
        [-5.1595e+00,  1.3128e+00],
        [-1.5216e+00,  1.3201e-01],
        [-3.8724e+00,  2.4341e+00],
        [-4.8208e+00,  3.4341e+00],
        [-5.8845e+00,  1.3656e+00],
        [-3.9104e+00,  1.8970e+00],
        [-3.4591e-01, -1.8692e+00],
        [-4.6431e+00,  3.0827e+00],
        [-8.8881e+00,  7.5017e+00],
        [-4.1515e+00,  2.6022e+00],
        [-8.9402e+00,  7.5429e+00],
        [-3.5275e+00,  1.2673e+00],
        [-5.3770e+00,  3.2039e+00],
        [-2.9552e+00,  1.4404e+00],
        [-3.9698e+00,  1.9333e+00],
        [-8.9379e+00,  7.5367e+00],
        [-2.6848e+00,  8.9765e-01],
        [-3.8701e+00,  2.4829e+00],
        [-1.5954e+00,  7.8407e-04],
        [-4.5208e+00, -9.4373e-02],
        [-2.5404e+00, -1.2725e-01],
        [-2.7538e+00,  1.3663e+00],
        [-8.6823e+00,  6.9414e+00],
        [-3.9850e+00,  2.5957e+00],
        [-1.0204e+01,  6.9960e+00],
        [-4.3806e+00,  1.4622e+00],
        [-3.6966e+00,  1.0692e+00],
        [-3.3665e+00, -5.7443e-01],
        [-5.1016e+00,  3.2123e+00],
        [-1.0246e+01,  6.9892e+00],
        [-3.4027e+00,  1.4004e+00],
        [-3.6010e+00,  2.0743e+00],
        [-9.6184e+00,  6.1236e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 6.6967e-07],
        [1.3120e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0741, 0.9259], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1768, 0.1731],
         [0.8612, 0.1335]],

        [[0.4004, 0.1719],
         [0.9414, 0.6075]],

        [[0.8583, 0.1671],
         [0.8786, 0.5026]],

        [[0.0801, 0.1011],
         [0.1184, 0.7100]],

        [[0.4830, 0.1904],
         [0.9238, 0.2145]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.013859261779027774
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0011225368650757482
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.023323334729290386
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.023323334729290386
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.003485103334991517
Global Adjusted Rand Index: 0.014090369211748933
Average Adjusted Rand Index: 0.013022714287535162
Iteration 0: Loss = -36558.84348881508
Iteration 10: Loss = -9908.011935169558
Iteration 20: Loss = -9908.011935169601
1
Iteration 30: Loss = -9908.01193516976
2
Iteration 40: Loss = -9908.011935170773
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 6.3488e-14],
        [1.0000e+00, 2.6394e-17]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 5.9706e-14])
beta: tensor([[[0.1361, 0.1297],
         [0.2540, 0.1487]],

        [[0.3244, 0.1976],
         [0.7506, 0.0213]],

        [[0.3822, 0.1481],
         [0.9738, 0.4888]],

        [[0.8235, 0.0725],
         [0.2337, 0.1755]],

        [[0.0744, 0.2087],
         [0.1064, 0.0109]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36558.26264673163
Iteration 100: Loss = -9968.5148509805
Iteration 200: Loss = -9935.53262538944
Iteration 300: Loss = -9921.921507433188
Iteration 400: Loss = -9914.726149199341
Iteration 500: Loss = -9912.644881390019
Iteration 600: Loss = -9911.410520751297
Iteration 700: Loss = -9910.598297443352
Iteration 800: Loss = -9910.03096254595
Iteration 900: Loss = -9909.617183616316
Iteration 1000: Loss = -9909.305405155126
Iteration 1100: Loss = -9909.064388188757
Iteration 1200: Loss = -9908.874217020211
Iteration 1300: Loss = -9908.721545185785
Iteration 1400: Loss = -9908.5973801789
Iteration 1500: Loss = -9908.495293502252
Iteration 1600: Loss = -9908.410514592379
Iteration 1700: Loss = -9908.339501369703
Iteration 1800: Loss = -9908.27948078169
Iteration 1900: Loss = -9908.228508064582
Iteration 2000: Loss = -9908.184855401661
Iteration 2100: Loss = -9908.147247803647
Iteration 2200: Loss = -9908.114661887392
Iteration 2300: Loss = -9908.086335913384
Iteration 2400: Loss = -9908.061428106643
Iteration 2500: Loss = -9908.039357986434
Iteration 2600: Loss = -9908.019661666985
Iteration 2700: Loss = -9908.001929599535
Iteration 2800: Loss = -9907.985962275685
Iteration 2900: Loss = -9907.971326024019
Iteration 3000: Loss = -9907.958026579989
Iteration 3100: Loss = -9907.945785826572
Iteration 3200: Loss = -9907.934397891266
Iteration 3300: Loss = -9907.923759443558
Iteration 3400: Loss = -9907.913669589812
Iteration 3500: Loss = -9907.903833776218
Iteration 3600: Loss = -9907.89309608953
Iteration 3700: Loss = -9907.872288828094
Iteration 3800: Loss = -9907.833115999929
Iteration 3900: Loss = -9907.811667449821
Iteration 4000: Loss = -9907.795796368197
Iteration 4100: Loss = -9907.782021062394
Iteration 4200: Loss = -9907.765603605594
Iteration 4300: Loss = -9907.74093915859
Iteration 4400: Loss = -9907.695736908081
Iteration 4500: Loss = -9907.581456947342
Iteration 4600: Loss = -9907.461125506105
Iteration 4700: Loss = -9907.373851899989
Iteration 4800: Loss = -9907.315513321066
Iteration 4900: Loss = -9907.236835463926
Iteration 5000: Loss = -9907.115726039092
Iteration 5100: Loss = -9907.038103344368
Iteration 5200: Loss = -9906.96438190272
Iteration 5300: Loss = -9906.862099393558
Iteration 5400: Loss = -9906.78169135847
Iteration 5500: Loss = -9906.726924004206
Iteration 5600: Loss = -9906.57563164308
Iteration 5700: Loss = -9906.4813205376
Iteration 5800: Loss = -9906.401419267535
Iteration 5900: Loss = -9906.326270767346
Iteration 6000: Loss = -9906.244699438854
Iteration 6100: Loss = -9906.164764762376
Iteration 6200: Loss = -9906.123936727889
Iteration 6300: Loss = -9906.080461187912
Iteration 6400: Loss = -9906.049616875478
Iteration 6500: Loss = -9906.020176549475
Iteration 6600: Loss = -9905.97029286459
Iteration 6700: Loss = -9905.904154430935
Iteration 6800: Loss = -9905.766966463943
Iteration 6900: Loss = -9905.679092109289
Iteration 7000: Loss = -9905.50389925274
Iteration 7100: Loss = -9905.248336489845
Iteration 7200: Loss = -9905.186816594382
Iteration 7300: Loss = -9905.058112342173
Iteration 7400: Loss = -9905.02586887284
Iteration 7500: Loss = -9904.995069127986
Iteration 7600: Loss = -9904.96825813314
Iteration 7700: Loss = -9904.895218105996
Iteration 7800: Loss = -9904.805175794696
Iteration 7900: Loss = -9904.661884082449
Iteration 8000: Loss = -9903.726714727645
Iteration 8100: Loss = -9903.347311597085
Iteration 8200: Loss = -9903.116506726134
Iteration 8300: Loss = -9901.732988393636
Iteration 8400: Loss = -9901.643566172
Iteration 8500: Loss = -9901.60286313986
Iteration 8600: Loss = -9901.572689569488
Iteration 8700: Loss = -9901.498142476687
Iteration 8800: Loss = -9901.481360744552
Iteration 8900: Loss = -9901.499506254093
1
Iteration 9000: Loss = -9901.468115071886
Iteration 9100: Loss = -9901.4620209462
Iteration 9200: Loss = -9901.454582119213
Iteration 9300: Loss = -9901.45028189751
Iteration 9400: Loss = -9901.447195321563
Iteration 9500: Loss = -9901.44434262667
Iteration 9600: Loss = -9901.441920643112
Iteration 9700: Loss = -9901.439861170993
Iteration 9800: Loss = -9901.446562200283
1
Iteration 9900: Loss = -9901.436268067651
Iteration 10000: Loss = -9901.434677233803
Iteration 10100: Loss = -9901.464374327361
1
Iteration 10200: Loss = -9901.431358414582
Iteration 10300: Loss = -9901.427902690446
Iteration 10400: Loss = -9901.488358328199
1
Iteration 10500: Loss = -9901.421208598886
Iteration 10600: Loss = -9901.420024852026
Iteration 10700: Loss = -9901.63745289779
1
Iteration 10800: Loss = -9901.417726392374
Iteration 10900: Loss = -9901.417062265686
Iteration 11000: Loss = -9901.416415732465
Iteration 11100: Loss = -9901.415737427871
Iteration 11200: Loss = -9901.41429146638
Iteration 11300: Loss = -9901.41325330241
Iteration 11400: Loss = -9901.412875887981
Iteration 11500: Loss = -9901.412474444089
Iteration 11600: Loss = -9901.598644857972
1
Iteration 11700: Loss = -9901.411843385946
Iteration 11800: Loss = -9901.41173254694
Iteration 11900: Loss = -9901.411843720423
1
Iteration 12000: Loss = -9901.41580439981
2
Iteration 12100: Loss = -9901.41076647653
Iteration 12200: Loss = -9901.404703243295
Iteration 12300: Loss = -9901.404719258768
1
Iteration 12400: Loss = -9901.40321150509
Iteration 12500: Loss = -9901.406065104242
1
Iteration 12600: Loss = -9901.4031023325
Iteration 12700: Loss = -9901.403908162874
1
Iteration 12800: Loss = -9901.402013938523
Iteration 12900: Loss = -9901.401828239974
Iteration 13000: Loss = -9901.402800476017
1
Iteration 13100: Loss = -9901.40182919108
2
Iteration 13200: Loss = -9901.504486983144
3
Iteration 13300: Loss = -9901.40083869018
Iteration 13400: Loss = -9901.5047144963
1
Iteration 13500: Loss = -9901.400679698356
Iteration 13600: Loss = -9901.400638928419
Iteration 13700: Loss = -9901.459251262844
1
Iteration 13800: Loss = -9901.400350241112
Iteration 13900: Loss = -9901.40409670085
1
Iteration 14000: Loss = -9901.40018680789
Iteration 14100: Loss = -9901.402045014314
1
Iteration 14200: Loss = -9901.408987125671
2
Iteration 14300: Loss = -9901.403202703224
3
Iteration 14400: Loss = -9901.399948755157
Iteration 14500: Loss = -9901.411588151934
1
Iteration 14600: Loss = -9901.399610123195
Iteration 14700: Loss = -9901.399531178804
Iteration 14800: Loss = -9901.40057952226
1
Iteration 14900: Loss = -9901.39473213339
Iteration 15000: Loss = -9901.456348886239
1
Iteration 15100: Loss = -9901.394623937416
Iteration 15200: Loss = -9901.397172899178
1
Iteration 15300: Loss = -9901.394568706439
Iteration 15400: Loss = -9901.425037842446
1
Iteration 15500: Loss = -9901.394496554476
Iteration 15600: Loss = -9901.423286985908
1
Iteration 15700: Loss = -9901.394443235245
Iteration 15800: Loss = -9901.394570786015
1
Iteration 15900: Loss = -9901.397517376545
2
Iteration 16000: Loss = -9901.41228994023
3
Iteration 16100: Loss = -9901.395685413123
4
Iteration 16200: Loss = -9901.394508274518
5
Iteration 16300: Loss = -9901.394394099882
Iteration 16400: Loss = -9901.39430528559
Iteration 16500: Loss = -9901.474074383023
1
Iteration 16600: Loss = -9901.425735837545
2
Iteration 16700: Loss = -9901.388211387775
Iteration 16800: Loss = -9901.3894765127
1
Iteration 16900: Loss = -9901.389884663151
2
Iteration 17000: Loss = -9901.56618462903
3
Iteration 17100: Loss = -9901.388186515358
Iteration 17200: Loss = -9901.388411950322
1
Iteration 17300: Loss = -9901.388126934833
Iteration 17400: Loss = -9901.389098281681
1
Iteration 17500: Loss = -9901.388125134294
Iteration 17600: Loss = -9901.514004065893
1
Iteration 17700: Loss = -9901.388113452738
Iteration 17800: Loss = -9901.388095226617
Iteration 17900: Loss = -9901.3966339309
1
Iteration 18000: Loss = -9901.3880762268
Iteration 18100: Loss = -9901.389009696875
1
Iteration 18200: Loss = -9901.388083881398
2
Iteration 18300: Loss = -9901.64892446786
3
Iteration 18400: Loss = -9901.388066034882
Iteration 18500: Loss = -9901.389119638943
1
Iteration 18600: Loss = -9901.3862526524
Iteration 18700: Loss = -9901.386293995323
1
Iteration 18800: Loss = -9901.387318454312
2
Iteration 18900: Loss = -9901.38700268944
3
Iteration 19000: Loss = -9901.386265373423
4
Iteration 19100: Loss = -9901.389254433376
5
Iteration 19200: Loss = -9901.38623387837
Iteration 19300: Loss = -9901.398738525091
1
Iteration 19400: Loss = -9901.388030540988
2
Iteration 19500: Loss = -9901.38618704204
Iteration 19600: Loss = -9901.389235020923
1
Iteration 19700: Loss = -9901.389492057155
2
Iteration 19800: Loss = -9901.38620989253
3
Iteration 19900: Loss = -9901.382222305236
tensor([[-0.6562, -0.7555],
        [ 5.6158, -8.4862],
        [ 2.4072, -3.9217],
        [ 2.6680, -4.6142],
        [-0.9783, -0.4321],
        [ 1.5337, -3.4255],
        [ 3.4001, -4.9468],
        [-2.3282,  0.9189],
        [ 6.1097, -8.0082],
        [ 1.8755, -3.3882],
        [ 2.1625, -3.5513],
        [ 2.4099, -4.1181],
        [-0.3417, -3.4262],
        [ 6.2276, -8.3061],
        [ 2.1502, -3.7725],
        [ 6.6102, -7.9969],
        [-1.1378, -0.6684],
        [ 0.3268, -1.9278],
        [ 0.8811, -2.3364],
        [ 3.2918, -5.0425],
        [ 1.9509, -4.1885],
        [ 6.2046, -7.9638],
        [ 3.3329, -5.0258],
        [ 2.2737, -4.1117],
        [ 1.0284, -3.3043],
        [ 6.1016, -8.3530],
        [-0.0824, -1.3068],
        [-1.3281, -0.1420],
        [ 6.8603, -8.2478],
        [ 6.2286, -7.9001],
        [ 1.9973, -3.4060],
        [ 2.2699, -3.8359],
        [ 1.3162, -2.9134],
        [ 6.7769, -8.3212],
        [ 1.5403, -3.5653],
        [ 1.5939, -3.1777],
        [ 6.2681, -8.3206],
        [ 2.5260, -4.3119],
        [ 1.2910, -2.7025],
        [ 1.8041, -4.4403],
        [ 6.3723, -7.7648],
        [ 3.3810, -4.8224],
        [ 0.3092, -2.4683],
        [ 4.7979, -9.4132],
        [ 2.7374, -4.3750],
        [-0.2063, -2.3743],
        [ 4.2370, -8.3227],
        [ 1.0112, -4.0801],
        [ 1.9667, -3.5439],
        [ 1.5925, -3.0072],
        [ 0.8673, -2.6838],
        [ 2.6288, -4.0592],
        [ 6.2925, -7.9851],
        [ 0.5572, -3.4847],
        [ 0.2016, -2.3056],
        [ 2.6286, -4.0342],
        [-1.7917,  0.3718],
        [ 2.2453, -3.6317],
        [ 0.3883, -2.1184],
        [-0.3454, -2.0430],
        [ 5.9120, -8.2921],
        [ 0.9439, -5.5592],
        [-1.6093, -1.6294],
        [ 6.8435, -8.2299],
        [ 1.6891, -3.8132],
        [ 1.9705, -3.5846],
        [ 6.2032, -8.4366],
        [ 2.5574, -4.0617],
        [-0.1214, -1.8136],
        [ 6.7250, -8.4808],
        [ 6.4664, -8.9375],
        [ 2.3486, -4.9460],
        [ 6.0099, -8.6409],
        [-1.5523,  0.0193],
        [ 3.2003, -4.6398],
        [ 6.6317, -8.5091],
        [ 6.1094, -7.7009],
        [ 6.8237, -8.2157],
        [ 1.2602, -3.6222],
        [ 6.7689, -8.1563],
        [ 0.8435, -3.5527],
        [ 2.2698, -3.6976],
        [ 6.7375, -8.1272],
        [ 1.1022, -2.6025],
        [ 2.4754, -3.9865],
        [-0.3301, -1.9961],
        [ 1.1964, -3.1843],
        [ 0.4416, -1.9728],
        [ 5.5835, -7.1775],
        [ 5.2346, -9.0960],
        [ 2.1282, -4.5739],
        [ 1.3276, -3.9190],
        [ 6.3453, -7.7665],
        [ 1.7184, -3.1250],
        [ 0.6408, -2.2170],
        [ 6.8945, -8.2881],
        [ 2.8992, -5.7693],
        [ 1.0025, -3.8758],
        [ 2.0932, -3.6221],
        [ 5.8150, -8.0807]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 5.4224e-07],
        [3.2010e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9274, 0.0726], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1336, 0.1734],
         [0.2540, 0.1774]],

        [[0.3244, 0.1712],
         [0.7506, 0.0213]],

        [[0.3822, 0.1674],
         [0.9738, 0.4888]],

        [[0.8235, 0.1001],
         [0.2337, 0.1755]],

        [[0.0744, 0.1913],
         [0.1064, 0.0109]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.005247724183445045
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0049666994303792225
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.015253231171422693
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.015253231171422693
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
Global Adjusted Rand Index: 0.010217422905468154
Average Adjusted Rand Index: 0.008347175203645061
Iteration 0: Loss = -21139.852973569996
Iteration 10: Loss = -9908.011935169541
Iteration 20: Loss = -9908.011935169543
1
Iteration 30: Loss = -9908.011935169554
2
Iteration 40: Loss = -9908.011935169632
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 4.7611e-15],
        [1.0000e+00, 1.4745e-16]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 4.4747e-15])
beta: tensor([[[0.1361, 0.1297],
         [0.8276, 0.1471]],

        [[0.7537, 0.1976],
         [0.7833, 0.1728]],

        [[0.4206, 0.1465],
         [0.9077, 0.9016]],

        [[0.5508, 0.0718],
         [0.0594, 0.5770]],

        [[0.0804, 0.2087],
         [0.0504, 0.9764]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21139.142593635068
Iteration 100: Loss = -9914.798305314733
Iteration 200: Loss = -9909.762151348645
Iteration 300: Loss = -9908.89016078735
Iteration 400: Loss = -9908.487063845552
Iteration 500: Loss = -9908.229416752894
Iteration 600: Loss = -9908.043896891604
Iteration 700: Loss = -9907.893818274693
Iteration 800: Loss = -9907.761359766211
Iteration 900: Loss = -9907.641281348744
Iteration 1000: Loss = -9907.535060944143
Iteration 1100: Loss = -9907.440105960435
Iteration 1200: Loss = -9907.345341907858
Iteration 1300: Loss = -9907.24634772267
Iteration 1400: Loss = -9907.134370720838
Iteration 1500: Loss = -9907.002328882596
Iteration 1600: Loss = -9906.839904125476
Iteration 1700: Loss = -9906.637047250639
Iteration 1800: Loss = -9906.40436540042
Iteration 1900: Loss = -9906.185238619008
Iteration 2000: Loss = -9906.00680269644
Iteration 2100: Loss = -9905.852539944011
Iteration 2200: Loss = -9905.736903866635
Iteration 2300: Loss = -9905.657095643364
Iteration 2400: Loss = -9905.600225822358
Iteration 2500: Loss = -9905.549079163582
Iteration 2600: Loss = -9905.481832180114
Iteration 2700: Loss = -9905.324201133923
Iteration 2800: Loss = -9905.209311283872
Iteration 2900: Loss = -9905.132080622798
Iteration 3000: Loss = -9905.0706973328
Iteration 3100: Loss = -9905.020558161117
Iteration 3200: Loss = -9904.968764525624
Iteration 3300: Loss = -9904.924546705995
Iteration 3400: Loss = -9904.82331113285
Iteration 3500: Loss = -9903.85955256714
Iteration 3600: Loss = -9902.725517896968
Iteration 3700: Loss = -9902.366930490638
Iteration 3800: Loss = -9902.19011699257
Iteration 3900: Loss = -9902.465281912511
1
Iteration 4000: Loss = -9901.998278401701
Iteration 4100: Loss = -9901.919642125924
Iteration 4200: Loss = -9901.85977683329
Iteration 4300: Loss = -9901.826008209528
Iteration 4400: Loss = -9901.798550062153
Iteration 4500: Loss = -9901.769546260193
Iteration 4600: Loss = -9901.712270808892
Iteration 4700: Loss = -9901.646007894626
Iteration 4800: Loss = -9901.593919829475
Iteration 4900: Loss = -9901.544956643444
Iteration 5000: Loss = -9901.532067882152
Iteration 5100: Loss = -9901.519930621773
Iteration 5200: Loss = -9901.509510965961
Iteration 5300: Loss = -9901.501459345649
Iteration 5400: Loss = -9901.497655040901
Iteration 5500: Loss = -9901.48618608301
Iteration 5600: Loss = -9901.48119507819
Iteration 5700: Loss = -9901.474410892728
Iteration 5800: Loss = -9901.469287473132
Iteration 5900: Loss = -9901.462456784393
Iteration 6000: Loss = -9901.452957201818
Iteration 6100: Loss = -9901.448141963565
Iteration 6200: Loss = -9901.561007723698
1
Iteration 6300: Loss = -9901.440111873444
Iteration 6400: Loss = -9901.436830422523
Iteration 6500: Loss = -9901.599217002573
1
Iteration 6600: Loss = -9901.431215959257
Iteration 6700: Loss = -9901.428267200603
Iteration 6800: Loss = -9901.703453949747
1
Iteration 6900: Loss = -9901.422772496608
Iteration 7000: Loss = -9901.42071095918
Iteration 7100: Loss = -9901.433128942397
1
Iteration 7200: Loss = -9901.418215128853
Iteration 7300: Loss = -9901.414746002423
Iteration 7400: Loss = -9901.432242394321
1
Iteration 7500: Loss = -9901.411548429778
Iteration 7600: Loss = -9901.555380762127
1
Iteration 7700: Loss = -9901.406825013208
Iteration 7800: Loss = -9901.398661456376
Iteration 7900: Loss = -9901.396676039436
Iteration 8000: Loss = -9901.393641606748
Iteration 8100: Loss = -9901.392537376616
Iteration 8200: Loss = -9901.390592002943
Iteration 8300: Loss = -9901.388862858199
Iteration 8400: Loss = -9901.8756304599
1
Iteration 8500: Loss = -9901.383572184466
Iteration 8600: Loss = -9901.382254158281
Iteration 8700: Loss = -9901.38115058152
Iteration 8800: Loss = -9901.38148031985
1
Iteration 8900: Loss = -9901.379445853852
Iteration 9000: Loss = -9901.378864126917
Iteration 9100: Loss = -9901.378764911624
Iteration 9200: Loss = -9901.377726974579
Iteration 9300: Loss = -9901.377101456177
Iteration 9400: Loss = -9901.377189712022
1
Iteration 9500: Loss = -9901.374747811496
Iteration 9600: Loss = -9901.368687596912
Iteration 9700: Loss = -9901.367570469069
Iteration 9800: Loss = -9901.366817166923
Iteration 9900: Loss = -9901.387446010325
1
Iteration 10000: Loss = -9901.365846391842
Iteration 10100: Loss = -9901.3654884059
Iteration 10200: Loss = -9901.373684051621
1
Iteration 10300: Loss = -9901.364876706059
Iteration 10400: Loss = -9901.364556548431
Iteration 10500: Loss = -9901.364240063896
Iteration 10600: Loss = -9901.36372000094
Iteration 10700: Loss = -9901.362578672159
Iteration 10800: Loss = -9901.364847662411
1
Iteration 10900: Loss = -9901.36213968939
Iteration 11000: Loss = -9901.361579702001
Iteration 11100: Loss = -9901.380060407564
1
Iteration 11200: Loss = -9901.360881746563
Iteration 11300: Loss = -9901.364108897835
1
Iteration 11400: Loss = -9901.364238128775
2
Iteration 11500: Loss = -9901.357191001069
Iteration 11600: Loss = -9901.395741193433
1
Iteration 11700: Loss = -9901.35680708334
Iteration 11800: Loss = -9901.413063816575
1
Iteration 11900: Loss = -9901.356553999389
Iteration 12000: Loss = -9901.400112890144
1
Iteration 12100: Loss = -9901.34101655885
Iteration 12200: Loss = -9901.340461399504
Iteration 12300: Loss = -9901.340288977393
Iteration 12400: Loss = -9901.36748560627
1
Iteration 12500: Loss = -9901.340106603666
Iteration 12600: Loss = -9901.483879871943
1
Iteration 12700: Loss = -9901.339979725664
Iteration 12800: Loss = -9901.339967394442
Iteration 12900: Loss = -9901.347870026751
1
Iteration 13000: Loss = -9901.33981100587
Iteration 13100: Loss = -9901.33972978036
Iteration 13200: Loss = -9901.339965470292
1
Iteration 13300: Loss = -9901.338617960006
Iteration 13400: Loss = -9901.33824804817
Iteration 13500: Loss = -9901.338312434831
1
Iteration 13600: Loss = -9901.440467055827
2
Iteration 13700: Loss = -9901.338157990425
Iteration 13800: Loss = -9901.496435972203
1
Iteration 13900: Loss = -9901.347915051316
2
Iteration 14000: Loss = -9901.357428827394
3
Iteration 14100: Loss = -9901.339768638492
4
Iteration 14200: Loss = -9901.345156356872
5
Iteration 14300: Loss = -9901.337024012273
Iteration 14400: Loss = -9901.336536416738
Iteration 14500: Loss = -9901.40066403552
1
Iteration 14600: Loss = -9901.347486800007
2
Iteration 14700: Loss = -9901.36155275444
3
Iteration 14800: Loss = -9901.3366344785
4
Iteration 14900: Loss = -9901.335934074074
Iteration 15000: Loss = -9901.336390164
1
Iteration 15100: Loss = -9901.336968341231
2
Iteration 15200: Loss = -9901.34690984718
3
Iteration 15300: Loss = -9901.334762105485
Iteration 15400: Loss = -9901.33481766926
1
Iteration 15500: Loss = -9901.333712840828
Iteration 15600: Loss = -9901.33342187293
Iteration 15700: Loss = -9901.364278356523
1
Iteration 15800: Loss = -9901.333373997182
Iteration 15900: Loss = -9901.499694478804
1
Iteration 16000: Loss = -9901.333389957803
2
Iteration 16100: Loss = -9901.334578793647
3
Iteration 16200: Loss = -9901.333913132075
4
Iteration 16300: Loss = -9901.513393738915
5
Iteration 16400: Loss = -9901.331942310351
Iteration 16500: Loss = -9901.361282694525
1
Iteration 16600: Loss = -9901.33191926054
Iteration 16700: Loss = -9901.334432380549
1
Iteration 16800: Loss = -9901.331867758421
Iteration 16900: Loss = -9901.343907195416
1
Iteration 17000: Loss = -9901.331836621057
Iteration 17100: Loss = -9901.317026960693
Iteration 17200: Loss = -9901.32054485555
1
Iteration 17300: Loss = -9901.316846630189
Iteration 17400: Loss = -9901.316716790752
Iteration 17500: Loss = -9901.324996228435
1
Iteration 17600: Loss = -9901.4559830207
2
Iteration 17700: Loss = -9901.316702941986
Iteration 17800: Loss = -9901.316749655107
1
Iteration 17900: Loss = -9901.332634212655
2
Iteration 18000: Loss = -9901.316629392417
Iteration 18100: Loss = -9901.383104960994
1
Iteration 18200: Loss = -9901.316454164178
Iteration 18300: Loss = -9901.321294441585
1
Iteration 18400: Loss = -9901.316447616615
Iteration 18500: Loss = -9901.317188661696
1
Iteration 18600: Loss = -9901.317523906304
2
Iteration 18700: Loss = -9901.316071185363
Iteration 18800: Loss = -9901.316013520478
Iteration 18900: Loss = -9901.316019530312
1
Iteration 19000: Loss = -9901.366319440665
2
Iteration 19100: Loss = -9901.33022199737
3
Iteration 19200: Loss = -9901.427196534256
4
Iteration 19300: Loss = -9901.31595892368
Iteration 19400: Loss = -9901.316247436704
1
Iteration 19500: Loss = -9901.317328274012
2
Iteration 19600: Loss = -9901.316006567338
3
Iteration 19700: Loss = -9901.318959828499
4
Iteration 19800: Loss = -9901.315941260296
Iteration 19900: Loss = -9901.31899372992
1
tensor([[-0.8689, -0.9361],
        [ 1.6401, -3.9269],
        [ 2.1510, -4.0825],
        [ 2.5845, -4.5634],
        [-1.0779, -0.3998],
        [ 1.4784, -3.3867],
        [ 3.2926, -4.9329],
        [-2.5485,  0.7555],
        [ 1.3185, -5.6562],
        [ 0.6682, -4.4045],
        [ 1.9455, -3.5865],
        [ 1.8014, -4.5894],
        [ 0.4722, -2.4988],
        [ 2.3151, -3.8303],
        [ 1.1726, -4.5980],
        [ 6.5050, -8.5236],
        [-1.1408, -0.6336],
        [ 0.3490, -1.7404],
        [ 0.5201, -2.4767],
        [ 7.1176, -9.0197],
        [ 1.5902, -4.4294],
        [ 6.6866, -8.2955],
        [ 3.0979, -5.0805],
        [ 2.4284, -3.8486],
        [ 1.3797, -2.7940],
        [ 2.5697, -4.0796],
        [-0.5063, -1.6398],
        [-1.3074, -0.1076],
        [ 3.0881, -4.5177],
        [ 1.5314, -3.6372],
        [ 1.8929, -3.4436],
        [ 2.1998, -3.7249],
        [ 1.3680, -2.7999],
        [ 7.0374, -8.4928],
        [ 1.6789, -3.3161],
        [ 1.6486, -3.0619],
        [ 6.4373, -8.9152],
        [ 2.5803, -4.1073],
        [ 0.9539, -2.9012],
        [ 2.3327, -3.7897],
        [ 2.5680, -4.2858],
        [ 3.2831, -4.7904],
        [ 0.5959, -2.0792],
        [ 2.5731, -4.4258],
        [ 2.1957, -4.7079],
        [-1.1204, -3.2837],
        [ 0.3561, -3.1906],
        [ 1.4176, -3.5480],
        [ 1.5005, -3.8945],
        [ 0.5244, -3.9572],
        [ 1.0344, -2.4302],
        [ 1.0679, -5.4998],
        [ 2.6058, -4.8145],
        [ 0.7063, -3.1522],
        [ 0.4578, -1.9162],
        [ 1.6405, -4.9030],
        [-1.9774,  0.1646],
        [ 6.8690, -9.9058],
        [-0.0152, -2.4283],
        [ 0.1594, -1.5502],
        [ 1.5273, -3.6015],
        [ 2.4811, -3.8683],
        [-0.7277, -0.6599],
        [ 2.5008, -3.9855],
        [ 1.8899, -3.5103],
        [ 1.8915, -3.4678],
        [ 6.5135, -9.0888],
        [ 2.4475, -3.9744],
        [-0.3522, -1.9610],
        [ 2.3075, -3.9142],
        [ 3.2955, -4.9032],
        [ 2.7103, -4.4501],
        [ 2.0596, -3.6907],
        [-1.8623, -0.3068],
        [ 3.0064, -4.6295],
        [ 6.8907, -8.4937],
        [ 2.6576, -4.0444],
        [ 6.8717, -9.2137],
        [ 1.6762, -3.0723],
        [ 7.1239, -8.6600],
        [ 1.4573, -2.8751],
        [ 6.5940, -8.6630],
        [ 4.1876, -5.5754],
        [ 1.0419, -2.4977],
        [ 1.8985, -4.3810],
        [-0.1522, -1.7140],
        [ 0.2898, -4.0659],
        [ 0.4371, -1.9200],
        [ 1.3092, -2.7821],
        [ 6.2423, -8.3302],
        [ 0.9552, -5.5705],
        [ 1.7912, -3.3109],
        [ 1.9998, -3.7863],
        [ 1.6422, -3.0656],
        [ 0.6517, -2.0991],
        [ 7.2739, -9.1814],
        [ 3.4926, -5.0160],
        [ 1.6421, -3.1128],
        [ 2.1082, -3.4955],
        [ 6.5275, -8.0016]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 2.6479e-07],
        [1.6881e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9258, 0.0742], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1342, 0.1733],
         [0.8276, 0.1767]],

        [[0.7537, 0.1717],
         [0.7833, 0.1728]],

        [[0.4206, 0.1676],
         [0.9077, 0.9016]],

        [[0.5508, 0.1013],
         [0.0594, 0.5770]],

        [[0.0804, 0.1905],
         [0.0504, 0.9764]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.013859261779027774
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0011225368650757482
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.023323334729290386
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.023323334729290386
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.003485103334991517
Global Adjusted Rand Index: 0.014090369211748933
Average Adjusted Rand Index: 0.013022714287535162
Iteration 0: Loss = -28683.829548555434
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.3164,    nan]],

        [[0.2770,    nan],
         [0.9589, 0.7070]],

        [[0.5573,    nan],
         [0.3202, 0.2311]],

        [[0.7603,    nan],
         [0.5731, 0.9166]],

        [[0.1210,    nan],
         [0.4987, 0.4733]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28683.060296933876
Iteration 100: Loss = -9947.62571505821
Iteration 200: Loss = -9920.505336595434
Iteration 300: Loss = -9915.902086895294
Iteration 400: Loss = -9911.022800994358
Iteration 500: Loss = -9910.06068124079
Iteration 600: Loss = -9909.435240722836
Iteration 700: Loss = -9908.852218199583
Iteration 800: Loss = -9908.563250896766
Iteration 900: Loss = -9908.38617794904
Iteration 1000: Loss = -9908.262841779544
Iteration 1100: Loss = -9908.1666397547
Iteration 1200: Loss = -9908.085218401879
Iteration 1300: Loss = -9908.013163440206
Iteration 1400: Loss = -9907.947715625773
Iteration 1500: Loss = -9907.887604572375
Iteration 1600: Loss = -9907.831867398207
Iteration 1700: Loss = -9907.77553492939
Iteration 1800: Loss = -9907.709993727713
Iteration 1900: Loss = -9907.625063084672
Iteration 2000: Loss = -9907.743146636805
1
Iteration 2100: Loss = -9907.372784340709
Iteration 2200: Loss = -9907.228097793492
Iteration 2300: Loss = -9907.094598887794
Iteration 2400: Loss = -9906.988092123107
Iteration 2500: Loss = -9906.885439731634
Iteration 2600: Loss = -9906.782767355584
Iteration 2700: Loss = -9906.874633984262
1
Iteration 2800: Loss = -9906.488061673803
Iteration 2900: Loss = -9906.338303781993
Iteration 3000: Loss = -9906.204731652451
Iteration 3100: Loss = -9906.082962733635
Iteration 3200: Loss = -9905.959320222746
Iteration 3300: Loss = -9905.842560830031
Iteration 3400: Loss = -9905.893972436228
1
Iteration 3500: Loss = -9905.653591501896
Iteration 3600: Loss = -9905.575794326456
Iteration 3700: Loss = -9905.487896244267
Iteration 3800: Loss = -9905.433451864172
Iteration 3900: Loss = -9905.39368942228
Iteration 4000: Loss = -9905.360397317481
Iteration 4100: Loss = -9905.332601020382
Iteration 4200: Loss = -9905.255177593992
Iteration 4300: Loss = -9905.214432183164
Iteration 4400: Loss = -9905.077552749073
Iteration 4500: Loss = -9904.494205732772
Iteration 4600: Loss = -9904.262261478618
Iteration 4700: Loss = -9904.136816598659
Iteration 4800: Loss = -9904.051328666133
Iteration 4900: Loss = -9903.948060550822
Iteration 5000: Loss = -9903.847114690496
Iteration 5100: Loss = -9903.828919193522
Iteration 5200: Loss = -9903.638876081946
Iteration 5300: Loss = -9903.516221052132
Iteration 5400: Loss = -9903.42407428728
Iteration 5500: Loss = -9903.32196081023
Iteration 5600: Loss = -9903.25412258783
Iteration 5700: Loss = -9903.206530698382
Iteration 5800: Loss = -9903.167249148517
Iteration 5900: Loss = -9903.09726267998
Iteration 6000: Loss = -9903.059426845723
Iteration 6100: Loss = -9903.099072414612
1
Iteration 6200: Loss = -9903.016127128341
Iteration 6300: Loss = -9903.002924786742
Iteration 6400: Loss = -9902.991297085762
Iteration 6500: Loss = -9902.974902196702
Iteration 6600: Loss = -9902.949010942128
Iteration 6700: Loss = -9902.927604796101
Iteration 6800: Loss = -9902.915704787087
Iteration 6900: Loss = -9902.909039592443
Iteration 7000: Loss = -9902.892688722424
Iteration 7100: Loss = -9902.885290856419
Iteration 7200: Loss = -9902.888473449122
1
Iteration 7300: Loss = -9902.84904327209
Iteration 7400: Loss = -9902.841435520639
Iteration 7500: Loss = -9902.833566952972
Iteration 7600: Loss = -9902.819682774194
Iteration 7700: Loss = -9902.841523684987
1
Iteration 7800: Loss = -9902.807903330398
Iteration 7900: Loss = -9902.804495197617
Iteration 8000: Loss = -9902.80072103198
Iteration 8100: Loss = -9902.796136749746
Iteration 8200: Loss = -9902.8813254518
1
Iteration 8300: Loss = -9902.79014350375
Iteration 8400: Loss = -9902.788894416162
Iteration 8500: Loss = -9902.784800089215
Iteration 8600: Loss = -9902.74999690284
Iteration 8700: Loss = -9902.775109711529
1
Iteration 8800: Loss = -9902.733672406419
Iteration 8900: Loss = -9902.733012285746
Iteration 9000: Loss = -9902.732583704466
Iteration 9100: Loss = -9902.731800694668
Iteration 9200: Loss = -9902.731267842788
Iteration 9300: Loss = -9902.730728977425
Iteration 9400: Loss = -9902.730103354863
Iteration 9500: Loss = -9902.729362080596
Iteration 9600: Loss = -9902.728060509096
Iteration 9700: Loss = -9902.726835084812
Iteration 9800: Loss = -9902.7261415068
Iteration 9900: Loss = -9902.725723351621
Iteration 10000: Loss = -9902.725080755466
Iteration 10100: Loss = -9902.723298403924
Iteration 10200: Loss = -9902.708794870188
Iteration 10300: Loss = -9902.68629953783
Iteration 10400: Loss = -9902.682905051393
Iteration 10500: Loss = -9902.680180803765
Iteration 10600: Loss = -9902.6736412279
Iteration 10700: Loss = -9902.670972090064
Iteration 10800: Loss = -9902.670534797335
Iteration 10900: Loss = -9902.666072338277
Iteration 11000: Loss = -9902.640934715275
Iteration 11100: Loss = -9902.640176916624
Iteration 11200: Loss = -9902.639328196927
Iteration 11300: Loss = -9902.638908246023
Iteration 11400: Loss = -9902.638830264557
Iteration 11500: Loss = -9902.63857781201
Iteration 11600: Loss = -9902.638820892767
1
Iteration 11700: Loss = -9902.638284212764
Iteration 11800: Loss = -9902.637975366637
Iteration 11900: Loss = -9902.63739183741
Iteration 12000: Loss = -9902.634512000595
Iteration 12100: Loss = -9902.634377044815
Iteration 12200: Loss = -9902.635946829638
1
Iteration 12300: Loss = -9902.634064686614
Iteration 12400: Loss = -9902.638803382517
1
Iteration 12500: Loss = -9902.633200143315
Iteration 12600: Loss = -9902.633066943143
Iteration 12700: Loss = -9902.632536185949
Iteration 12800: Loss = -9902.63227734526
Iteration 12900: Loss = -9902.632225971902
Iteration 13000: Loss = -9902.630541621982
Iteration 13100: Loss = -9902.632351160803
1
Iteration 13200: Loss = -9902.636808961612
2
Iteration 13300: Loss = -9902.63026624389
Iteration 13400: Loss = -9902.630272188208
1
Iteration 13500: Loss = -9902.66008518577
2
Iteration 13600: Loss = -9902.629334273905
Iteration 13700: Loss = -9902.864157997881
1
Iteration 13800: Loss = -9902.629181849019
Iteration 13900: Loss = -9902.629243078984
1
Iteration 14000: Loss = -9902.629068678276
Iteration 14100: Loss = -9902.629878015396
1
Iteration 14200: Loss = -9902.703384649325
2
Iteration 14300: Loss = -9902.629270934658
3
Iteration 14400: Loss = -9902.648513043057
4
Iteration 14500: Loss = -9902.62774315252
Iteration 14600: Loss = -9902.626394192146
Iteration 14700: Loss = -9902.62891710002
1
Iteration 14800: Loss = -9902.648344692323
2
Iteration 14900: Loss = -9902.624817343747
Iteration 15000: Loss = -9902.62703042134
1
Iteration 15100: Loss = -9902.624749170809
Iteration 15200: Loss = -9902.633072027209
1
Iteration 15300: Loss = -9902.624510966916
Iteration 15400: Loss = -9902.624961592088
1
Iteration 15500: Loss = -9902.62536522902
2
Iteration 15600: Loss = -9902.623928813757
Iteration 15700: Loss = -9902.614037353875
Iteration 15800: Loss = -9902.61236116012
Iteration 15900: Loss = -9902.615973997314
1
Iteration 16000: Loss = -9902.61226356576
Iteration 16100: Loss = -9902.613122545807
1
Iteration 16200: Loss = -9902.612425397128
2
Iteration 16300: Loss = -9902.61275885512
3
Iteration 16400: Loss = -9902.615616688
4
Iteration 16500: Loss = -9902.613698245523
5
Iteration 16600: Loss = -9902.643269823155
6
Iteration 16700: Loss = -9902.612887215464
7
Iteration 16800: Loss = -9902.623359160125
8
Iteration 16900: Loss = -9902.611406546852
Iteration 17000: Loss = -9902.61914947767
1
Iteration 17100: Loss = -9902.611853299219
2
Iteration 17200: Loss = -9902.615043220903
3
Iteration 17300: Loss = -9902.688198174237
4
Iteration 17400: Loss = -9902.611398085106
Iteration 17500: Loss = -9902.61157275317
1
Iteration 17600: Loss = -9902.610237181632
Iteration 17700: Loss = -9902.615601036796
1
Iteration 17800: Loss = -9902.617071922621
2
Iteration 17900: Loss = -9902.613350569443
3
Iteration 18000: Loss = -9902.620496131327
4
Iteration 18100: Loss = -9902.617293478508
5
Iteration 18200: Loss = -9902.619972498065
6
Iteration 18300: Loss = -9902.609743309926
Iteration 18400: Loss = -9902.608038335033
Iteration 18500: Loss = -9902.609473898461
1
Iteration 18600: Loss = -9902.608603500561
2
Iteration 18700: Loss = -9902.675419294903
3
Iteration 18800: Loss = -9902.60826299792
4
Iteration 18900: Loss = -9902.607892322316
Iteration 19000: Loss = -9902.6103021449
1
Iteration 19100: Loss = -9902.614956191206
2
Iteration 19200: Loss = -9902.671244059313
3
Iteration 19300: Loss = -9902.612904515729
4
Iteration 19400: Loss = -9902.641841563538
5
Iteration 19500: Loss = -9902.628856379924
6
Iteration 19600: Loss = -9902.60802133673
7
Iteration 19700: Loss = -9902.607046217481
Iteration 19800: Loss = -9902.81310469243
1
Iteration 19900: Loss = -9902.606840502785
tensor([[ -4.8376,   3.0072],
        [ -3.8501,   1.9602],
        [-10.3886,   8.1090],
        [ -9.4243,   7.4710],
        [ -3.5553,   2.1668],
        [ -9.3328,   7.9451],
        [ -8.5618,   7.1130],
        [ -4.0655,   2.4405],
        [ -9.1881,   7.5787],
        [ -5.2908,   3.8809],
        [ -0.0636,  -2.0854],
        [ -4.4520,   3.0515],
        [ -2.4271,   0.4621],
        [-10.0903,   6.9967],
        [ -5.6923,   2.4139],
        [ -9.1114,   7.6863],
        [ -6.8611,   5.4543],
        [ -4.8386,   2.6064],
        [ -3.1015,   1.1679],
        [ -8.9402,   6.8234],
        [ -8.9817,   7.5909],
        [ -9.8455,   7.8126],
        [ -8.8998,   7.4976],
        [ -9.2885,   7.3603],
        [ -5.8162,   4.3135],
        [ -8.9242,   7.5212],
        [ -9.5301,   8.0742],
        [ -5.4730,   3.9883],
        [ -4.8901,   2.7174],
        [ -9.2408,   7.6606],
        [ -5.5434,   2.8832],
        [ -9.6550,   6.8246],
        [ -9.0459,   7.5640],
        [ -8.8560,   7.3472],
        [ -8.6832,   7.0599],
        [ -9.1552,   7.2984],
        [ -9.2517,   7.8216],
        [ -8.8075,   7.3019],
        [ -3.3032,   0.7478],
        [ -9.0434,   7.6037],
        [ -9.5196,   8.0049],
        [ -8.6632,   6.8557],
        [ -3.9804,  -0.6348],
        [ -9.9267,   6.6400],
        [ -3.0906,   1.6777],
        [ -8.9229,   7.4714],
        [ -3.6943,   1.7010],
        [ -8.8488,   6.9859],
        [ -3.9506,   2.5534],
        [ -4.1787,   2.7038],
        [ -4.9031,   3.1406],
        [ -8.9480,   7.5462],
        [ -5.3621,   3.9661],
        [  0.8575,  -2.2718],
        [ -6.2887,   4.7541],
        [ -9.2517,   7.5224],
        [ -9.7653,   8.3346],
        [ -9.2988,   5.7054],
        [ -1.9329,   0.5366],
        [ -8.6249,   6.8650],
        [ -4.5941,   2.7532],
        [ -8.5825,   6.7418],
        [ -5.4530,   4.0380],
        [ -4.5619,  -0.0533],
        [-11.9768,   7.3615],
        [ -5.4500,   3.7065],
        [ -9.6933,   7.5417],
        [ -3.8471,   1.2872],
        [ -4.7850,   3.3451],
        [ -8.7217,   7.3336],
        [ -4.7218,   3.2904],
        [ -8.7401,   7.3499],
        [ -5.7636,   3.4571],
        [ -9.3903,   7.8798],
        [ -5.4225,   3.8782],
        [ -7.1921,   4.7789],
        [ -5.8646,   4.4155],
        [ -8.8725,   7.1290],
        [ -8.9810,   7.2761],
        [ -5.4088,   2.0745],
        [-10.6224,   6.9191],
        [ -6.3387,   4.0271],
        [ -8.9854,   6.9265],
        [ -0.2646,  -1.1572],
        [ -2.6232,   0.9672],
        [-10.0515,   7.8439],
        [-10.0059,   7.9122],
        [ -9.2852,   7.2463],
        [-10.5365,   8.0901],
        [ -9.5608,   7.9567],
        [ -1.6986,   0.1391],
        [ -4.7650,   3.2257],
        [ -5.1470,   3.5749],
        [ -4.8015,   3.3975],
        [ -5.6719,   4.1624],
        [ -8.7664,   7.3373],
        [ -8.6369,   7.2168],
        [ -9.5343,   7.8410],
        [ -9.2634,   7.5233],
        [ -5.3815,   3.6189]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.2390e-06],
        [2.0753e-02, 9.7925e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0298, 0.9702], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1988, 0.0695],
         [0.3164, 0.1343]],

        [[0.2770, 0.2113],
         [0.9589, 0.7070]],

        [[0.5573, 0.1678],
         [0.3202, 0.2311]],

        [[0.7603, 0.1325],
         [0.5731, 0.9166]],

        [[0.1210, 0.1591],
         [0.4987, 0.4733]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: -0.004294123202807246
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0009975514204148314
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0015169269193791859
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0015169269193791859
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
Global Adjusted Rand Index: 0.0023197131756090947
Average Adjusted Rand Index: -0.00010880744275213235
10033.4814440645
new:  [0.014090369211748933, 0.010217422905468154, 0.014090369211748933, 0.0023197131756090947] [0.013022714287535162, 0.008347175203645061, 0.013022714287535162, -0.00010880744275213235] [9901.34686824419, 9901.380983269812, 9901.387655411925, 9902.637916969981]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [9905.837313818278, 9908.011935170773, 9908.011935169632, nan]
-----------------------------------------------------------------------------------------
This iteration is 15
True Objective function: Loss = -9892.029275103168
Iteration 0: Loss = -16118.229542808755
Iteration 10: Loss = -9749.587200871134
Iteration 20: Loss = -9749.428779225858
Iteration 30: Loss = -9749.38817757374
Iteration 40: Loss = -9749.371279635308
Iteration 50: Loss = -9749.361218695169
Iteration 60: Loss = -9749.354004412244
Iteration 70: Loss = -9749.348180687643
Iteration 80: Loss = -9749.343132034102
Iteration 90: Loss = -9749.33856467583
Iteration 100: Loss = -9749.334541079275
Iteration 110: Loss = -9749.331082318573
Iteration 120: Loss = -9749.328288080855
Iteration 130: Loss = -9749.326415650612
Iteration 140: Loss = -9749.325366451456
Iteration 150: Loss = -9749.325129604633
Iteration 160: Loss = -9749.3255662458
1
Iteration 170: Loss = -9749.326463310712
2
Iteration 180: Loss = -9749.327776227736
3
Stopping early at iteration 179 due to no improvement.
pi: tensor([[0.9332, 0.0668],
        [0.9284, 0.0716]], dtype=torch.float64)
alpha: tensor([0.9334, 0.0666])
beta: tensor([[[0.1306, 0.1296],
         [0.9663, 0.1619]],

        [[0.3426, 0.1491],
         [0.6002, 0.5549]],

        [[0.2157, 0.1290],
         [0.8932, 0.8788]],

        [[0.9847, 0.1429],
         [0.3656, 0.8158]],

        [[0.2988, 0.1791],
         [0.1852, 0.2850]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0006579886660132697
Average Adjusted Rand Index: -0.0004529465619428516
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15970.187872784662
Iteration 100: Loss = -9750.60411987167
Iteration 200: Loss = -9749.06093449783
Iteration 300: Loss = -9748.267694718375
Iteration 400: Loss = -9747.36717250087
Iteration 500: Loss = -9746.61349335131
Iteration 600: Loss = -9746.11599239673
Iteration 700: Loss = -9745.865537154212
Iteration 800: Loss = -9745.724048623166
Iteration 900: Loss = -9745.637914573257
Iteration 1000: Loss = -9745.579825678276
Iteration 1100: Loss = -9745.537591460079
Iteration 1200: Loss = -9745.504996451617
Iteration 1300: Loss = -9745.478906541937
Iteration 1400: Loss = -9745.457616738215
Iteration 1500: Loss = -9745.439941584658
Iteration 1600: Loss = -9745.425156988607
Iteration 1700: Loss = -9745.412584951231
Iteration 1800: Loss = -9745.401906953792
Iteration 1900: Loss = -9745.392754659817
Iteration 2000: Loss = -9745.384888811315
Iteration 2100: Loss = -9745.378096492439
Iteration 2200: Loss = -9745.37220144488
Iteration 2300: Loss = -9745.367135013461
Iteration 2400: Loss = -9745.362727522095
Iteration 2500: Loss = -9745.358904591882
Iteration 2600: Loss = -9745.355586964242
Iteration 2700: Loss = -9745.352620693515
Iteration 2800: Loss = -9745.350057107931
Iteration 2900: Loss = -9745.347838704372
Iteration 3000: Loss = -9745.345866708276
Iteration 3100: Loss = -9745.344120026837
Iteration 3200: Loss = -9745.34261106751
Iteration 3300: Loss = -9745.341258093462
Iteration 3400: Loss = -9745.340151154127
Iteration 3500: Loss = -9745.339060399907
Iteration 3600: Loss = -9745.338186206242
Iteration 3700: Loss = -9745.337350465883
Iteration 3800: Loss = -9745.336668287371
Iteration 3900: Loss = -9745.336119429354
Iteration 4000: Loss = -9745.335506533833
Iteration 4100: Loss = -9745.335046199158
Iteration 4200: Loss = -9745.334633634951
Iteration 4300: Loss = -9745.334240207989
Iteration 4400: Loss = -9745.333948487701
Iteration 4500: Loss = -9745.333681219992
Iteration 4600: Loss = -9745.333402380575
Iteration 4700: Loss = -9745.333166507142
Iteration 4800: Loss = -9745.332995963476
Iteration 4900: Loss = -9745.332828182762
Iteration 5000: Loss = -9745.332659917025
Iteration 5100: Loss = -9745.332529011219
Iteration 5200: Loss = -9745.33238453142
Iteration 5300: Loss = -9745.332281062058
Iteration 5400: Loss = -9745.332214272636
Iteration 5500: Loss = -9745.332088964205
Iteration 5600: Loss = -9745.332041416219
Iteration 5700: Loss = -9745.332078314976
1
Iteration 5800: Loss = -9745.331886985683
Iteration 5900: Loss = -9745.331979113284
1
Iteration 6000: Loss = -9745.331757368296
Iteration 6100: Loss = -9745.331953118786
1
Iteration 6200: Loss = -9745.331661574379
Iteration 6300: Loss = -9745.336615363667
1
Iteration 6400: Loss = -9745.331566823103
Iteration 6500: Loss = -9745.331604509865
1
Iteration 6600: Loss = -9745.331501225084
Iteration 6700: Loss = -9745.3314710066
Iteration 6800: Loss = -9745.331437283136
Iteration 6900: Loss = -9745.331402999893
Iteration 7000: Loss = -9745.331393147462
Iteration 7100: Loss = -9745.33135138453
Iteration 7200: Loss = -9745.331373134719
1
Iteration 7300: Loss = -9745.331305128406
Iteration 7400: Loss = -9745.33136550252
1
Iteration 7500: Loss = -9745.331270516632
Iteration 7600: Loss = -9745.332294817475
1
Iteration 7700: Loss = -9745.331252925049
Iteration 7800: Loss = -9745.331214488326
Iteration 7900: Loss = -9745.335946337536
1
Iteration 8000: Loss = -9745.331376709704
2
Iteration 8100: Loss = -9745.331265269366
3
Iteration 8200: Loss = -9745.339601446163
4
Iteration 8300: Loss = -9745.333125741365
5
Iteration 8400: Loss = -9745.331270635226
6
Iteration 8500: Loss = -9745.331146648328
Iteration 8600: Loss = -9745.33245422327
1
Iteration 8700: Loss = -9745.331118044094
Iteration 8800: Loss = -9745.339612632653
1
Iteration 8900: Loss = -9745.331129135884
2
Iteration 9000: Loss = -9745.348900405013
3
Iteration 9100: Loss = -9745.331112610163
Iteration 9200: Loss = -9745.331087874407
Iteration 9300: Loss = -9745.336615757555
1
Iteration 9400: Loss = -9745.331067020787
Iteration 9500: Loss = -9745.343977702842
1
Iteration 9600: Loss = -9745.33107719534
2
Iteration 9700: Loss = -9745.331046566936
Iteration 9800: Loss = -9745.331075573586
1
Iteration 9900: Loss = -9745.331640473341
2
Iteration 10000: Loss = -9745.331495792345
3
Iteration 10100: Loss = -9745.331043672008
Iteration 10200: Loss = -9745.33105789821
1
Iteration 10300: Loss = -9745.331113213613
2
Iteration 10400: Loss = -9745.331041982869
Iteration 10500: Loss = -9745.331249969093
1
Iteration 10600: Loss = -9745.331081644905
2
Iteration 10700: Loss = -9745.382653707342
3
Iteration 10800: Loss = -9745.33106039514
4
Iteration 10900: Loss = -9745.33505292037
5
Iteration 11000: Loss = -9745.331030916866
Iteration 11100: Loss = -9745.331011481025
Iteration 11200: Loss = -9745.331123280821
1
Iteration 11300: Loss = -9745.33102359208
2
Iteration 11400: Loss = -9745.512496758538
3
Iteration 11500: Loss = -9745.331031580441
4
Iteration 11600: Loss = -9745.337063786337
5
Iteration 11700: Loss = -9745.331028426677
6
Iteration 11800: Loss = -9745.333680152227
7
Iteration 11900: Loss = -9745.331019313788
8
Iteration 12000: Loss = -9745.331508131336
9
Iteration 12100: Loss = -9745.331047430658
10
Stopping early at iteration 12100 due to no improvement.
tensor([[ -9.8015,   5.1863],
        [ -8.9146,   4.2994],
        [-10.7553,   6.1401],
        [-10.8894,   6.2741],
        [ -9.0555,   4.4403],
        [-10.6034,   5.9882],
        [ -4.6771,   0.0619],
        [-10.7256,   6.1104],
        [ -5.6974,   1.0822],
        [-10.7823,   6.1671],
        [-10.5745,   5.9593],
        [-10.5001,   5.8849],
        [ -8.5488,   3.9336],
        [-10.9415,   6.3263],
        [ -7.2432,   2.6280],
        [ -8.4120,   3.7968],
        [-10.6059,   5.9907],
        [-10.6520,   6.0368],
        [ -8.1887,   3.5735],
        [ -8.5249,   3.9096],
        [ -5.4135,   0.7982],
        [ -9.9650,   5.3497],
        [ -9.6255,   5.0102],
        [ -9.1407,   4.5255],
        [-10.4716,   5.8563],
        [ -7.3349,   2.7197],
        [ -8.1278,   3.5126],
        [-10.3096,   5.6944],
        [-10.4258,   5.8106],
        [-10.2890,   5.6738],
        [ -5.0819,   0.4667],
        [ -6.9707,   2.3555],
        [ -8.2837,   3.6685],
        [ -9.7615,   5.1463],
        [ -9.9887,   5.3735],
        [-11.1918,   6.5766],
        [ -9.8675,   5.2523],
        [ -7.1362,   2.5210],
        [ -8.4683,   3.8531],
        [-10.1577,   5.5425],
        [-10.1466,   5.5314],
        [ -9.3318,   4.7166],
        [ -6.6144,   1.9991],
        [ -8.8062,   4.1910],
        [ -8.5020,   3.8868],
        [ -1.1049,  -3.5104],
        [ -7.3530,   2.7377],
        [ -9.1957,   4.5805],
        [-10.8433,   6.2281],
        [-10.3970,   5.7818],
        [ -9.2638,   4.6486],
        [-11.2334,   6.6182],
        [-10.6877,   6.0725],
        [-10.7401,   6.1249],
        [ -6.9708,   2.3556],
        [-10.2973,   5.6820],
        [ -8.2007,   3.5855],
        [ -7.7811,   3.1658],
        [ -6.4304,   1.8151],
        [-11.0112,   6.3960],
        [ -8.5844,   3.9692],
        [ -8.7969,   4.1817],
        [-10.2110,   5.5958],
        [ -7.0957,   2.4804],
        [-10.6908,   6.0756],
        [ -9.8127,   5.1975],
        [-11.0592,   6.4440],
        [ -5.9701,   1.3549],
        [ -9.9185,   5.3033],
        [-10.7502,   6.1350],
        [-10.7541,   6.1388],
        [ -8.1391,   3.5238],
        [-10.3095,   5.6943],
        [ -9.0513,   4.4360],
        [ -9.1435,   4.5282],
        [ -7.4415,   2.8263],
        [ -7.6083,   2.9931],
        [ -9.1948,   4.5796],
        [ -7.5055,   2.8903],
        [-10.3240,   5.7087],
        [ -9.6523,   5.0371],
        [-10.5779,   5.9626],
        [ -9.3734,   4.7581],
        [ -6.1236,   1.5084],
        [-10.2181,   5.6029],
        [ -9.4828,   4.8676],
        [ -8.8035,   4.1883],
        [-10.5633,   5.9481],
        [ -8.2250,   3.6098],
        [-10.5466,   5.9314],
        [-10.7788,   6.1636],
        [ -9.9590,   5.3438],
        [ -8.6194,   4.0042],
        [ -7.9765,   3.3612],
        [ -4.2725,  -0.3427],
        [ -9.5253,   4.9101],
        [ -9.8242,   5.2090],
        [-10.4378,   5.8226],
        [-10.6085,   5.9932],
        [ -5.7059,   1.0907]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.3953e-06],
        [7.2431e-03, 9.9276e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0096, 0.9904], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0158, 0.0630],
         [0.9663, 0.1347]],

        [[0.3426, 0.1235],
         [0.6002, 0.5549]],

        [[0.2157, 0.0850],
         [0.8932, 0.8788]],

        [[0.9847, 0.0653],
         [0.3656, 0.8158]],

        [[0.2988, 0.2021],
         [0.1852, 0.2850]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0009975514204148314
Global Adjusted Rand Index: -0.0008018420765110036
Average Adjusted Rand Index: -0.0007803048475515604
Iteration 0: Loss = -35500.84572899494
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.8126,    nan]],

        [[0.5911,    nan],
         [0.9063, 0.0907]],

        [[0.6774,    nan],
         [0.4548, 0.6536]],

        [[0.4140,    nan],
         [0.3689, 0.8982]],

        [[0.2031,    nan],
         [0.4990, 0.8123]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35500.524501390544
Iteration 100: Loss = -9767.947119624461
Iteration 200: Loss = -9756.505206525851
Iteration 300: Loss = -9753.912155023656
Iteration 400: Loss = -9752.450075066954
Iteration 500: Loss = -9751.726735784367
Iteration 600: Loss = -9751.406333556095
Iteration 700: Loss = -9751.200405180833
Iteration 800: Loss = -9751.057294442946
Iteration 900: Loss = -9750.94724988354
Iteration 1000: Loss = -9750.854040315884
Iteration 1100: Loss = -9750.765824511693
Iteration 1200: Loss = -9750.658072298867
Iteration 1300: Loss = -9750.447182114822
Iteration 1400: Loss = -9750.213442333546
Iteration 1500: Loss = -9750.091716538167
Iteration 1600: Loss = -9750.007570804395
Iteration 1700: Loss = -9749.943822741569
Iteration 1800: Loss = -9749.892544709164
Iteration 1900: Loss = -9749.84926112562
Iteration 2000: Loss = -9749.811212132401
Iteration 2100: Loss = -9749.776419189686
Iteration 2200: Loss = -9749.740726092732
Iteration 2300: Loss = -9749.710696395701
Iteration 2400: Loss = -9749.682176928014
Iteration 2500: Loss = -9749.657058425459
Iteration 2600: Loss = -9749.633891168267
Iteration 2700: Loss = -9749.612081855343
Iteration 2800: Loss = -9749.59132466696
Iteration 2900: Loss = -9749.571363615281
Iteration 3000: Loss = -9749.551933609253
Iteration 3100: Loss = -9749.532765125407
Iteration 3200: Loss = -9749.513731103903
Iteration 3300: Loss = -9749.494552902406
Iteration 3400: Loss = -9749.475531909702
Iteration 3500: Loss = -9749.457177247003
Iteration 3600: Loss = -9749.439209340686
Iteration 3700: Loss = -9749.421401061189
Iteration 3800: Loss = -9749.403648878022
Iteration 3900: Loss = -9749.38596174229
Iteration 4000: Loss = -9749.368276213307
Iteration 4100: Loss = -9749.350605207084
Iteration 4200: Loss = -9749.332889152794
Iteration 4300: Loss = -9749.315057708282
Iteration 4400: Loss = -9749.296453213006
Iteration 4500: Loss = -9749.272732483387
Iteration 4600: Loss = -9749.249962848595
Iteration 4700: Loss = -9749.23361853965
Iteration 4800: Loss = -9749.218098730078
Iteration 4900: Loss = -9749.203399049004
Iteration 5000: Loss = -9749.189593687055
Iteration 5100: Loss = -9749.176662416863
Iteration 5200: Loss = -9749.164649351158
Iteration 5300: Loss = -9749.153562313906
Iteration 5400: Loss = -9749.14330927482
Iteration 5500: Loss = -9749.133780325805
Iteration 5600: Loss = -9749.12487888123
Iteration 5700: Loss = -9749.116532826924
Iteration 5800: Loss = -9749.108537985401
Iteration 5900: Loss = -9749.100840900044
Iteration 6000: Loss = -9749.093365536182
Iteration 6100: Loss = -9749.08601463269
Iteration 6200: Loss = -9749.078781419194
Iteration 6300: Loss = -9749.071731833326
Iteration 6400: Loss = -9749.06493560445
Iteration 6500: Loss = -9749.058316305633
Iteration 6600: Loss = -9749.05190456087
Iteration 6700: Loss = -9749.045547492806
Iteration 6800: Loss = -9749.039220580526
Iteration 6900: Loss = -9749.032717872195
Iteration 7000: Loss = -9749.026297167953
Iteration 7100: Loss = -9749.019542356033
Iteration 7200: Loss = -9749.012888961488
Iteration 7300: Loss = -9749.00659700327
Iteration 7400: Loss = -9749.00040890717
Iteration 7500: Loss = -9748.994401539507
Iteration 7600: Loss = -9748.989232922575
Iteration 7700: Loss = -9748.985081953482
Iteration 7800: Loss = -9748.981844586026
Iteration 7900: Loss = -9748.979908990195
Iteration 8000: Loss = -9748.987628877268
1
Iteration 8100: Loss = -9748.988541984985
2
Iteration 8200: Loss = -9748.976728373687
Iteration 8300: Loss = -9748.978256972674
1
Iteration 8400: Loss = -9748.976205165907
Iteration 8500: Loss = -9748.976367841533
1
Iteration 8600: Loss = -9748.97605244015
Iteration 8700: Loss = -9748.976077040641
1
Iteration 8800: Loss = -9748.975949914005
Iteration 8900: Loss = -9748.975866152708
Iteration 9000: Loss = -9749.021816610477
1
Iteration 9100: Loss = -9748.975774984718
Iteration 9200: Loss = -9748.975718574082
Iteration 9300: Loss = -9748.97782269085
1
Iteration 9400: Loss = -9748.975591699837
Iteration 9500: Loss = -9748.975541220734
Iteration 9600: Loss = -9748.980252561429
1
Iteration 9700: Loss = -9748.975444045147
Iteration 9800: Loss = -9748.975373299156
Iteration 9900: Loss = -9748.985059453222
1
Iteration 10000: Loss = -9748.975232005103
Iteration 10100: Loss = -9748.97511904581
Iteration 10200: Loss = -9748.977260070898
1
Iteration 10300: Loss = -9748.974930456929
Iteration 10400: Loss = -9748.974825889938
Iteration 10500: Loss = -9748.99374801111
1
Iteration 10600: Loss = -9748.974481249297
Iteration 10700: Loss = -9748.97430464755
Iteration 10800: Loss = -9748.974274974855
Iteration 10900: Loss = -9748.973748840828
Iteration 11000: Loss = -9748.973375039752
Iteration 11100: Loss = -9748.97440289298
1
Iteration 11200: Loss = -9748.972143601308
Iteration 11300: Loss = -9748.971078928083
Iteration 11400: Loss = -9748.969420779611
Iteration 11500: Loss = -9748.9657950844
Iteration 11600: Loss = -9748.978352172373
1
Iteration 11700: Loss = -9748.910092236447
Iteration 11800: Loss = -9748.709269524992
Iteration 11900: Loss = -9748.628793787671
Iteration 12000: Loss = -9748.614215898391
Iteration 12100: Loss = -9748.584623213947
Iteration 12200: Loss = -9747.770872520041
Iteration 12300: Loss = -9747.720485761156
Iteration 12400: Loss = -9747.713970300945
Iteration 12500: Loss = -9747.707965568896
Iteration 12600: Loss = -9747.730603444988
1
Iteration 12700: Loss = -9747.704636719038
Iteration 12800: Loss = -9747.703627953264
Iteration 12900: Loss = -9747.702979013255
Iteration 13000: Loss = -9747.702329646667
Iteration 13100: Loss = -9747.701894352662
Iteration 13200: Loss = -9747.701568648257
Iteration 13300: Loss = -9747.701170930173
Iteration 13400: Loss = -9747.700897535557
Iteration 13500: Loss = -9747.700859996028
Iteration 13600: Loss = -9747.700460775242
Iteration 13700: Loss = -9747.700275469484
Iteration 13800: Loss = -9747.700245654163
Iteration 13900: Loss = -9747.699979621388
Iteration 14000: Loss = -9747.699896108137
Iteration 14100: Loss = -9747.702929311352
1
Iteration 14200: Loss = -9747.699634982537
Iteration 14300: Loss = -9747.699572065745
Iteration 14400: Loss = -9747.703888636795
1
Iteration 14500: Loss = -9747.699376000528
Iteration 14600: Loss = -9747.699319374955
Iteration 14700: Loss = -9747.701953309159
1
Iteration 14800: Loss = -9747.699149478447
Iteration 14900: Loss = -9748.049883271522
1
Iteration 15000: Loss = -9747.699064469296
Iteration 15100: Loss = -9747.699021223478
Iteration 15200: Loss = -9747.706780395683
1
Iteration 15300: Loss = -9747.69895472351
Iteration 15400: Loss = -9747.698884036832
Iteration 15500: Loss = -9747.705270025872
1
Iteration 15600: Loss = -9747.698759321169
Iteration 15700: Loss = -9747.698564212618
Iteration 15800: Loss = -9747.700353818858
1
Iteration 15900: Loss = -9747.698402168815
Iteration 16000: Loss = -9747.698812896486
1
Iteration 16100: Loss = -9747.698529364798
2
Iteration 16200: Loss = -9747.711771267308
3
Iteration 16300: Loss = -9747.69848962801
4
Iteration 16400: Loss = -9747.698350151933
Iteration 16500: Loss = -9747.6980611584
Iteration 16600: Loss = -9747.697839511608
Iteration 16700: Loss = -9747.697900377028
1
Iteration 16800: Loss = -9747.699319598478
2
Iteration 16900: Loss = -9747.697767479714
Iteration 17000: Loss = -9747.6978712164
1
Iteration 17100: Loss = -9747.698846864123
2
Iteration 17200: Loss = -9747.69799665242
3
Iteration 17300: Loss = -9747.756197578145
4
Iteration 17400: Loss = -9747.697716734798
Iteration 17500: Loss = -9747.699120189282
1
Iteration 17600: Loss = -9747.697671464335
Iteration 17700: Loss = -9747.697613994962
Iteration 17800: Loss = -9747.707149546575
1
Iteration 17900: Loss = -9747.697600140707
Iteration 18000: Loss = -9747.69759871447
Iteration 18100: Loss = -9747.698779730019
1
Iteration 18200: Loss = -9747.697759827228
2
Iteration 18300: Loss = -9747.697587252562
Iteration 18400: Loss = -9747.697806318607
1
Iteration 18500: Loss = -9747.697578497997
Iteration 18600: Loss = -9747.904859284454
1
Iteration 18700: Loss = -9747.697523643448
Iteration 18800: Loss = -9747.697544225104
1
Iteration 18900: Loss = -9747.697740786843
2
Iteration 19000: Loss = -9747.697529978028
3
Iteration 19100: Loss = -9747.85948364273
4
Iteration 19200: Loss = -9747.697535037016
5
Iteration 19300: Loss = -9747.699236655184
6
Iteration 19400: Loss = -9747.697759266264
7
Iteration 19500: Loss = -9747.69782485519
8
Iteration 19600: Loss = -9747.697534533745
9
Iteration 19700: Loss = -9747.698343578119
10
Stopping early at iteration 19700 due to no improvement.
tensor([[ -4.3969,   3.0105],
        [ -4.3212,   2.9316],
        [ -4.2408,   2.8208],
        [ -3.7714,   2.3196],
        [ -2.7103,   1.2172],
        [ -3.6233,   2.0490],
        [ -6.4815,   2.7861],
        [ -5.5376,   3.9535],
        [ -3.1070,   1.7071],
        [ -5.6281,   4.2193],
        [ -4.9611,   3.5336],
        [ -4.9091,   2.5327],
        [ -6.3434,   4.0439],
        [ -4.6463,   3.2596],
        [ -2.8686,   1.4539],
        [ -6.2772,   4.3599],
        [ -7.2703,   4.5871],
        [ -3.4174,   1.7544],
        [ -3.3412,   1.9340],
        [ -3.2115,   0.4374],
        [ -4.0129,   1.6739],
        [ -5.1448,   3.1144],
        [ -5.5518,   0.9366],
        [ -4.3379,   2.3707],
        [ -5.6252,   4.2300],
        [ -0.7111,  -0.8127],
        [ -5.0808,   3.6810],
        [ -4.5015,   3.0656],
        [ -3.6065,   2.0873],
        [ -3.7542,   2.3649],
        [ -7.8969,   5.3071],
        [ -3.0305,   0.9424],
        [ -6.2268,   4.5768],
        [ -3.4436,   1.9834],
        [ -3.4120,   2.0191],
        [ -4.3370,   1.8242],
        [ -9.1636,   7.3507],
        [ -4.7081,   3.3026],
        [ -4.6911,   3.2505],
        [ -6.3341,   4.9091],
        [ -3.9804,   1.8547],
        [ -8.5549,   7.0689],
        [ -3.9547,   2.0202],
        [  0.3326,  -1.7726],
        [ -5.6030,   3.4437],
        [ -3.6709,   2.2256],
        [ -4.7952,   3.3881],
        [ -6.0205,   4.1917],
        [ -5.0429,   2.3743],
        [ -9.1532,   6.9396],
        [  0.6936,  -2.1061],
        [ -5.8930,   3.7970],
        [ -7.2883,   4.8317],
        [ -3.3813,   1.3620],
        [ -5.2648,   0.6496],
        [ -5.8654,   4.4220],
        [ -4.4712,   1.6003],
        [ -1.8211,   0.4302],
        [ -3.2105,   1.6106],
        [ -6.0666,   3.1225],
        [ -5.8846,   4.3366],
        [ -3.4616,   1.6334],
        [ -4.8220,   3.4009],
        [ -3.0420,   1.4187],
        [ -6.6999,   3.5942],
        [ -3.9485,   2.0771],
        [ -4.0862,   2.4897],
        [ -6.2289,   4.8119],
        [ -4.6908,   3.2011],
        [ -6.4289,   5.0009],
        [ -2.7571,   0.3198],
        [ -4.4336,   3.0387],
        [ -8.8158,   6.5193],
        [ -2.4601,   0.6135],
        [ -4.0774,   2.6910],
        [ -6.7143,   5.1842],
        [ -5.5606,   4.1716],
        [ -7.4089,   4.2248],
        [ -6.4590,   4.8453],
        [ -3.5699,   2.1696],
        [ -4.4444,   2.9960],
        [ -6.6393,   5.1945],
        [ -4.1466,   2.7597],
        [ -5.3444,   3.6125],
        [ -3.9160,   1.9132],
        [ -5.5736,   3.4845],
        [ -6.5543,   5.1567],
        [ -4.8370,   2.9495],
        [ -4.6448,   2.7092],
        [ -5.6205,   3.8788],
        [ -5.0551,   3.5949],
        [ -3.6531,   2.2668],
        [ -9.1211,   6.6267],
        [ -6.1468,   4.3252],
        [ -4.0643,   2.5628],
        [ -6.3918,   5.0053],
        [ -6.5633,   4.5934],
        [ -6.0354,   4.5905],
        [-10.4121,   6.1955],
        [ -3.8401,   2.4066]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.7809e-06],
        [1.3691e-05, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0275, 0.9725], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2625, 0.0882],
         [0.8126, 0.1327]],

        [[0.5911, 0.1741],
         [0.9063, 0.0907]],

        [[0.6774, 0.1185],
         [0.4548, 0.6536]],

        [[0.4140, 0.1841],
         [0.3689, 0.8982]],

        [[0.2031, 0.1960],
         [0.4990, 0.8123]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.004267232452421997
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.004371511787304062
Global Adjusted Rand Index: -0.00015240596172097878
Average Adjusted Rand Index: -0.003512280899301514
Iteration 0: Loss = -26665.597201019278
Iteration 10: Loss = -9749.417354383264
Iteration 20: Loss = -9749.354752342942
Iteration 30: Loss = -9749.339419261672
Iteration 40: Loss = -9749.334268205932
Iteration 50: Loss = -9749.333115642748
Iteration 60: Loss = -9749.333612304423
1
Iteration 70: Loss = -9749.334873635698
2
Iteration 80: Loss = -9749.336442798965
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.9239, 0.0761],
        [0.9216, 0.0784]], dtype=torch.float64)
alpha: tensor([0.9242, 0.0758])
beta: tensor([[[0.1305, 0.1298],
         [0.5343, 0.1595]],

        [[0.2001, 0.1481],
         [0.0595, 0.9100]],

        [[0.5861, 0.1296],
         [0.8907, 0.0381]],

        [[0.3586, 0.1412],
         [0.8746, 0.0359]],

        [[0.1349, 0.1756],
         [0.5881, 0.0042]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0006579886660132697
Average Adjusted Rand Index: -0.0004529465619428516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26563.337479532693
Iteration 100: Loss = -9770.90462537851
Iteration 200: Loss = -9759.48204069139
Iteration 300: Loss = -9755.240461961088
Iteration 400: Loss = -9753.234561960326
Iteration 500: Loss = -9752.138024387094
Iteration 600: Loss = -9751.47376600064
Iteration 700: Loss = -9751.028647244948
Iteration 800: Loss = -9750.70900700491
Iteration 900: Loss = -9750.490505542475
Iteration 1000: Loss = -9750.277666935091
Iteration 1100: Loss = -9750.12083267348
Iteration 1200: Loss = -9749.987045569693
Iteration 1300: Loss = -9749.86940869118
Iteration 1400: Loss = -9749.762865586523
Iteration 1500: Loss = -9749.665372191957
Iteration 1600: Loss = -9749.762188985356
1
Iteration 1700: Loss = -9749.496500652314
Iteration 1800: Loss = -9749.427582542143
Iteration 1900: Loss = -9749.371274963158
Iteration 2000: Loss = -9749.327103507283
Iteration 2100: Loss = -9749.29217712204
Iteration 2200: Loss = -9749.26345256214
Iteration 2300: Loss = -9749.260493436706
Iteration 2400: Loss = -9749.216816655548
Iteration 2500: Loss = -9749.197128199521
Iteration 2600: Loss = -9749.17932277342
Iteration 2700: Loss = -9749.162828330782
Iteration 2800: Loss = -9749.147700593474
Iteration 2900: Loss = -9749.133689542614
Iteration 3000: Loss = -9749.123099368782
Iteration 3100: Loss = -9749.1083438793
Iteration 3200: Loss = -9749.096760397544
Iteration 3300: Loss = -9749.182371731195
1
Iteration 3400: Loss = -9749.075365964476
Iteration 3500: Loss = -9749.065573129565
Iteration 3600: Loss = -9749.05634054214
Iteration 3700: Loss = -9749.04763079716
Iteration 3800: Loss = -9749.039327612856
Iteration 3900: Loss = -9749.031628706816
Iteration 4000: Loss = -9749.025542549238
Iteration 4100: Loss = -9749.01816208721
Iteration 4200: Loss = -9749.012600920112
Iteration 4300: Loss = -9749.009188319898
Iteration 4400: Loss = -9749.003621277107
Iteration 4500: Loss = -9749.000223933368
Iteration 4600: Loss = -9749.030955710847
1
Iteration 4700: Loss = -9748.994934117247
Iteration 4800: Loss = -9748.99296311648
Iteration 4900: Loss = -9748.991290916656
Iteration 5000: Loss = -9749.011137617852
1
Iteration 5100: Loss = -9748.988549893551
Iteration 5200: Loss = -9748.987342236782
Iteration 5300: Loss = -9749.015862258171
1
Iteration 5400: Loss = -9748.985335225769
Iteration 5500: Loss = -9748.984407202573
Iteration 5600: Loss = -9749.005344423716
1
Iteration 5700: Loss = -9748.982794583098
Iteration 5800: Loss = -9748.982062419082
Iteration 5900: Loss = -9748.985886608225
1
Iteration 6000: Loss = -9748.980735957632
Iteration 6100: Loss = -9748.980123852944
Iteration 6200: Loss = -9748.982933324145
1
Iteration 6300: Loss = -9748.97898248056
Iteration 6400: Loss = -9748.978417459086
Iteration 6500: Loss = -9748.979675082866
1
Iteration 6600: Loss = -9748.977417069163
Iteration 6700: Loss = -9748.976933931519
Iteration 6800: Loss = -9748.97765310618
1
Iteration 6900: Loss = -9748.975910742109
Iteration 7000: Loss = -9748.975356087478
Iteration 7100: Loss = -9748.983122054025
1
Iteration 7200: Loss = -9748.97402172791
Iteration 7300: Loss = -9748.973196400746
Iteration 7400: Loss = -9748.972144673517
Iteration 7500: Loss = -9748.970461473386
Iteration 7600: Loss = -9748.967771815278
Iteration 7700: Loss = -9749.231170599654
1
Iteration 7800: Loss = -9748.943705649197
Iteration 7900: Loss = -9748.851028308914
Iteration 8000: Loss = -9748.876111814805
1
Iteration 8100: Loss = -9748.63614719895
Iteration 8200: Loss = -9748.628541757582
Iteration 8300: Loss = -9748.624907504272
Iteration 8400: Loss = -9748.622629393758
Iteration 8500: Loss = -9748.623696031838
1
Iteration 8600: Loss = -9748.619759932957
Iteration 8700: Loss = -9748.618704313452
Iteration 8800: Loss = -9748.617962907687
Iteration 8900: Loss = -9748.61699988138
Iteration 9000: Loss = -9748.616813789757
Iteration 9100: Loss = -9748.61558136342
Iteration 9200: Loss = -9748.614965543027
Iteration 9300: Loss = -9748.841052619528
1
Iteration 9400: Loss = -9748.613919199575
Iteration 9500: Loss = -9748.613525446644
Iteration 9600: Loss = -9748.613186011991
Iteration 9700: Loss = -9748.612795499614
Iteration 9800: Loss = -9748.612488762761
Iteration 9900: Loss = -9748.61447230647
1
Iteration 10000: Loss = -9748.624206301858
2
Iteration 10100: Loss = -9748.611635776724
Iteration 10200: Loss = -9748.612016007351
1
Iteration 10300: Loss = -9748.611178224413
Iteration 10400: Loss = -9748.619191684455
1
Iteration 10500: Loss = -9748.610754294214
Iteration 10600: Loss = -9748.68833899832
1
Iteration 10700: Loss = -9748.610401341624
Iteration 10800: Loss = -9748.610291078861
Iteration 10900: Loss = -9748.610359285865
1
Iteration 11000: Loss = -9748.609967463415
Iteration 11100: Loss = -9748.61016786112
1
Iteration 11200: Loss = -9748.60970491224
Iteration 11300: Loss = -9748.610968155595
1
Iteration 11400: Loss = -9748.60946469369
Iteration 11500: Loss = -9748.638319922185
1
Iteration 11600: Loss = -9748.609309943371
Iteration 11700: Loss = -9748.6093015667
Iteration 11800: Loss = -9748.609158697896
Iteration 11900: Loss = -9748.754894712483
1
Iteration 12000: Loss = -9748.609136895971
Iteration 12100: Loss = -9748.61873668396
1
Iteration 12200: Loss = -9748.609085429822
Iteration 12300: Loss = -9748.608883136567
Iteration 12400: Loss = -9748.671357216159
1
Iteration 12500: Loss = -9748.610828551706
2
Iteration 12600: Loss = -9748.630913584333
3
Iteration 12700: Loss = -9748.60852312396
Iteration 12800: Loss = -9748.611267979604
1
Iteration 12900: Loss = -9748.608426748347
Iteration 13000: Loss = -9748.61011349432
1
Iteration 13100: Loss = -9748.609123440574
2
Iteration 13200: Loss = -9748.612765107471
3
Iteration 13300: Loss = -9748.713380881376
4
Iteration 13400: Loss = -9748.608272359972
Iteration 13500: Loss = -9748.608351194373
1
Iteration 13600: Loss = -9748.61563379117
2
Iteration 13700: Loss = -9748.6125442937
3
Iteration 13800: Loss = -9748.6081932249
Iteration 13900: Loss = -9748.608867871542
1
Iteration 14000: Loss = -9748.620997032496
2
Iteration 14100: Loss = -9748.608168105095
Iteration 14200: Loss = -9748.610833308907
1
Iteration 14300: Loss = -9748.608170341342
2
Iteration 14400: Loss = -9748.635651898669
3
Iteration 14500: Loss = -9748.614400331391
4
Iteration 14600: Loss = -9748.61140088764
5
Iteration 14700: Loss = -9748.60804701351
Iteration 14800: Loss = -9748.60920737265
1
Iteration 14900: Loss = -9748.617670445848
2
Iteration 15000: Loss = -9748.61259735269
3
Iteration 15100: Loss = -9748.608230438253
4
Iteration 15200: Loss = -9748.607917477932
Iteration 15300: Loss = -9748.608004862148
1
Iteration 15400: Loss = -9748.608214152748
2
Iteration 15500: Loss = -9748.627652902622
3
Iteration 15600: Loss = -9748.612616582768
4
Iteration 15700: Loss = -9748.60844572401
5
Iteration 15800: Loss = -9748.618538628281
6
Iteration 15900: Loss = -9748.610803813668
7
Iteration 16000: Loss = -9748.608451884254
8
Iteration 16100: Loss = -9748.702682091207
9
Iteration 16200: Loss = -9748.613960038889
10
Stopping early at iteration 16200 due to no improvement.
tensor([[ 6.5963, -8.0059],
        [ 5.5581, -8.8582],
        [ 6.7622, -8.1969],
        [ 5.1563, -9.7716],
        [ 3.9869, -8.1764],
        [ 5.7604, -7.2980],
        [ 6.5030, -8.7339],
        [ 6.3643, -7.7588],
        [ 6.0058, -8.0777],
        [ 6.1884, -8.7216],
        [ 6.2220, -7.6913],
        [ 5.7173, -7.8412],
        [ 6.6356, -8.3746],
        [ 5.8415, -7.2477],
        [ 5.6249, -7.0200],
        [ 7.0833, -8.5515],
        [ 6.8890, -8.7632],
        [ 5.6463, -7.1498],
        [ 6.4961, -8.4636],
        [ 5.5394, -7.0098],
        [ 7.0285, -8.4283],
        [ 6.3257, -7.9199],
        [ 6.6445, -8.0513],
        [ 5.9826, -7.3935],
        [ 6.9347, -8.4124],
        [ 4.5430, -6.7116],
        [ 6.2724, -7.7229],
        [ 6.4921, -8.0101],
        [ 5.4525, -6.9920],
        [ 5.5509, -7.5318],
        [ 6.4017, -8.5175],
        [ 6.7396, -8.5307],
        [ 6.4017, -8.2441],
        [ 5.8843, -7.3513],
        [ 6.9914, -8.4058],
        [ 6.4774, -8.0998],
        [ 6.4379, -7.8506],
        [ 6.2839, -8.4685],
        [ 6.7067, -8.4707],
        [ 6.3415, -7.8070],
        [ 5.3302, -6.7350],
        [ 6.3049, -9.6649],
        [ 6.4906, -8.1826],
        [ 4.6785, -6.3609],
        [ 6.3578, -7.7445],
        [ 6.3771, -8.4657],
        [ 6.3868, -7.8402],
        [ 6.9611, -8.3662],
        [ 5.8277, -7.2608],
        [ 6.7658, -8.3909],
        [ 3.6184, -7.2698],
        [ 5.7406, -7.9631],
        [ 6.7319, -8.2963],
        [ 5.9320, -8.0822],
        [ 5.6886, -7.8091],
        [ 5.9656, -7.8458],
        [ 6.0092, -7.4708],
        [ 5.0713, -6.6870],
        [ 5.2959, -8.2969],
        [ 5.5318, -8.0535],
        [ 6.3566, -8.3290],
        [ 5.4131, -9.3090],
        [ 6.0345, -7.4253],
        [ 5.1864, -6.8400],
        [ 6.3604, -7.9116],
        [ 4.9900, -7.9890],
        [ 5.7140, -7.1127],
        [ 5.5058, -9.4437],
        [ 5.9141, -7.5557],
        [ 5.8009, -9.6038],
        [ 5.1535, -6.5398],
        [ 6.3603, -7.7721],
        [ 6.2344, -8.7508],
        [ 5.2699, -6.6973],
        [ 5.5388, -7.3207],
        [ 7.0397, -8.4512],
        [ 6.4126, -7.9162],
        [ 6.4510, -8.7796],
        [ 7.1110, -8.5122],
        [ 5.2681, -7.6585],
        [ 6.2382, -7.6783],
        [ 6.3111, -8.5460],
        [ 5.7978, -7.5642],
        [ 6.5398, -8.3990],
        [ 5.8321, -9.0312],
        [ 6.2616, -7.8625],
        [ 6.9487, -8.3927],
        [ 6.3878, -7.7952],
        [ 6.2415, -7.6278],
        [ 7.0008, -8.4510],
        [ 5.8070, -7.3209],
        [ 6.9984, -8.4144],
        [ 6.5654, -8.1448],
        [ 5.1247, -9.7399],
        [ 5.7419, -9.1885],
        [ 6.5588, -8.9858],
        [ 6.3168, -8.2046],
        [ 6.1855, -9.1087],
        [ 6.9600, -8.3491],
        [ 5.7355, -8.1753]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.6336e-01, 3.6645e-02],
        [7.2380e-05, 9.9993e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.6586e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1312, 0.1188],
         [0.5343, 0.1919]],

        [[0.2001, 0.1609],
         [0.0595, 0.9100]],

        [[0.5861, 0.1287],
         [0.8907, 0.0381]],

        [[0.3586, 0.1501],
         [0.8746, 0.0359]],

        [[0.1349, 0.1631],
         [0.5881, 0.0042]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
Global Adjusted Rand Index: 0.0036101405255143247
Average Adjusted Rand Index: -0.00014671034033994024
Iteration 0: Loss = -15830.728838498491
Iteration 10: Loss = -9749.479439540417
Iteration 20: Loss = -9749.387032435145
Iteration 30: Loss = -9749.366708346213
Iteration 40: Loss = -9749.357794102607
Iteration 50: Loss = -9749.351443578978
Iteration 60: Loss = -9749.346053568574
Iteration 70: Loss = -9749.341265928275
Iteration 80: Loss = -9749.336935140334
Iteration 90: Loss = -9749.333111478476
Iteration 100: Loss = -9749.329884877869
Iteration 110: Loss = -9749.327463201622
Iteration 120: Loss = -9749.325903722125
Iteration 130: Loss = -9749.325211227433
Iteration 140: Loss = -9749.325219857255
1
Iteration 150: Loss = -9749.325881644028
2
Iteration 160: Loss = -9749.326915541867
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[0.0699, 0.9301],
        [0.0654, 0.9346]], dtype=torch.float64)
alpha: tensor([0.0653, 0.9347])
beta: tensor([[[0.1625, 0.1295],
         [0.0888, 0.1306]],

        [[0.5474, 0.1493],
         [0.9971, 0.0575]],

        [[0.4955, 0.1289],
         [0.9856, 0.6202]],

        [[0.4634, 0.1434],
         [0.5522, 0.2071]],

        [[0.1407, 0.1797],
         [0.5789, 0.5824]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0006579886660132697
Average Adjusted Rand Index: -0.0004529465619428516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15830.433877502099
Iteration 100: Loss = -9785.344770762278
Iteration 200: Loss = -9763.961031676155
Iteration 300: Loss = -9754.011860262704
Iteration 400: Loss = -9750.853429266106
Iteration 500: Loss = -9749.897068383136
Iteration 600: Loss = -9749.459140321977
Iteration 700: Loss = -9749.171544241082
Iteration 800: Loss = -9748.964725098003
Iteration 900: Loss = -9748.816636334259
Iteration 1000: Loss = -9748.712114541122
Iteration 1100: Loss = -9748.651664325438
Iteration 1200: Loss = -9748.60648148434
Iteration 1300: Loss = -9748.570061106962
Iteration 1400: Loss = -9748.541891815325
Iteration 1500: Loss = -9748.519876441442
Iteration 1600: Loss = -9748.501358469763
Iteration 1700: Loss = -9748.486915630336
Iteration 1800: Loss = -9748.474666356147
Iteration 1900: Loss = -9748.464151926679
Iteration 2000: Loss = -9748.455163134518
Iteration 2100: Loss = -9748.447626865205
Iteration 2200: Loss = -9748.441180421343
Iteration 2300: Loss = -9748.435697965748
Iteration 2400: Loss = -9748.430972696271
Iteration 2500: Loss = -9748.426973626538
Iteration 2600: Loss = -9748.423426172552
Iteration 2700: Loss = -9748.420356276278
Iteration 2800: Loss = -9748.417662872744
Iteration 2900: Loss = -9748.415306845563
Iteration 3000: Loss = -9748.413178316232
Iteration 3100: Loss = -9748.411240426094
Iteration 3200: Loss = -9748.409514372514
Iteration 3300: Loss = -9748.408311195039
Iteration 3400: Loss = -9748.406654431332
Iteration 3500: Loss = -9748.405399244391
Iteration 3600: Loss = -9748.514418682436
1
Iteration 3700: Loss = -9748.40323297881
Iteration 3800: Loss = -9748.402227647568
Iteration 3900: Loss = -9748.40134751458
Iteration 4000: Loss = -9748.400520725347
Iteration 4100: Loss = -9748.399750571743
Iteration 4200: Loss = -9748.39902681704
Iteration 4300: Loss = -9748.629940405915
1
Iteration 4400: Loss = -9748.397812494028
Iteration 4500: Loss = -9748.397261905362
Iteration 4600: Loss = -9748.396716865278
Iteration 4700: Loss = -9748.406536379836
1
Iteration 4800: Loss = -9748.395761336207
Iteration 4900: Loss = -9748.395328059529
Iteration 5000: Loss = -9748.447186151609
1
Iteration 5100: Loss = -9748.394464760804
Iteration 5200: Loss = -9748.394096948943
Iteration 5300: Loss = -9748.393742074266
Iteration 5400: Loss = -9748.393462398759
Iteration 5500: Loss = -9748.393072121171
Iteration 5600: Loss = -9748.392745562332
Iteration 5700: Loss = -9748.411068767755
1
Iteration 5800: Loss = -9748.392185180122
Iteration 5900: Loss = -9748.391914405736
Iteration 6000: Loss = -9748.391661761578
Iteration 6100: Loss = -9748.391446391246
Iteration 6200: Loss = -9748.391159504165
Iteration 6300: Loss = -9748.390881496065
Iteration 6400: Loss = -9748.39100893867
1
Iteration 6500: Loss = -9748.390391918032
Iteration 6600: Loss = -9748.390162438698
Iteration 6700: Loss = -9748.390067442024
Iteration 6800: Loss = -9748.38963870962
Iteration 6900: Loss = -9748.389321412309
Iteration 7000: Loss = -9748.393281331973
1
Iteration 7100: Loss = -9748.388623314848
Iteration 7200: Loss = -9748.388245501577
Iteration 7300: Loss = -9748.402765557132
1
Iteration 7400: Loss = -9748.387867537162
Iteration 7500: Loss = -9748.390013348155
1
Iteration 7600: Loss = -9748.390084365938
2
Iteration 7700: Loss = -9748.384605124853
Iteration 7800: Loss = -9748.383200173828
Iteration 7900: Loss = -9748.380462358951
Iteration 8000: Loss = -9748.376735755868
Iteration 8100: Loss = -9748.369732630676
Iteration 8200: Loss = -9748.352527768227
Iteration 8300: Loss = -9748.120233304746
Iteration 8400: Loss = -9747.788523927304
Iteration 8500: Loss = -9747.785828556944
Iteration 8600: Loss = -9747.78764729748
1
Iteration 8700: Loss = -9747.784238644439
Iteration 8800: Loss = -9747.785259281796
1
Iteration 8900: Loss = -9747.783540486173
Iteration 9000: Loss = -9747.784277168179
1
Iteration 9100: Loss = -9747.783131620623
Iteration 9200: Loss = -9747.785805721198
1
Iteration 9300: Loss = -9747.78302802581
Iteration 9400: Loss = -9747.92747875871
1
Iteration 9500: Loss = -9747.782881680123
Iteration 9600: Loss = -9747.782792005202
Iteration 9700: Loss = -9747.803360503865
1
Iteration 9800: Loss = -9747.78266515521
Iteration 9900: Loss = -9747.782602433435
Iteration 10000: Loss = -9747.78675703807
1
Iteration 10100: Loss = -9747.782396577808
Iteration 10200: Loss = -9747.782292374535
Iteration 10300: Loss = -9747.782324845255
1
Iteration 10400: Loss = -9747.781880970817
Iteration 10500: Loss = -9747.799763953324
1
Iteration 10600: Loss = -9747.781116446402
Iteration 10700: Loss = -9747.809527131434
1
Iteration 10800: Loss = -9747.82980706756
2
Iteration 10900: Loss = -9747.810149413688
3
Iteration 11000: Loss = -9747.751491030103
Iteration 11100: Loss = -9747.572608079809
Iteration 11200: Loss = -9745.48638291895
Iteration 11300: Loss = -9745.366233884975
Iteration 11400: Loss = -9745.351808799483
Iteration 11500: Loss = -9745.342082323135
Iteration 11600: Loss = -9745.338856493376
Iteration 11700: Loss = -9745.340065370989
1
Iteration 11800: Loss = -9745.335850564119
Iteration 11900: Loss = -9745.335054383348
Iteration 12000: Loss = -9745.374661151523
1
Iteration 12100: Loss = -9745.333991624124
Iteration 12200: Loss = -9745.333606009637
Iteration 12300: Loss = -9745.333333915185
Iteration 12400: Loss = -9745.333906357213
1
Iteration 12500: Loss = -9745.332873535368
Iteration 12600: Loss = -9745.33268829751
Iteration 12700: Loss = -9745.33252782547
Iteration 12800: Loss = -9745.332441239216
Iteration 12900: Loss = -9745.33229554638
Iteration 13000: Loss = -9745.332175610936
Iteration 13100: Loss = -9745.335234317377
1
Iteration 13200: Loss = -9745.331999033691
Iteration 13300: Loss = -9745.331910132834
Iteration 13400: Loss = -9745.334630584777
1
Iteration 13500: Loss = -9745.331812004335
Iteration 13600: Loss = -9745.331766734545
Iteration 13700: Loss = -9745.331741570242
Iteration 13800: Loss = -9745.331729446923
Iteration 13900: Loss = -9745.331618421429
Iteration 14000: Loss = -9745.331580048072
Iteration 14100: Loss = -9745.332207429485
1
Iteration 14200: Loss = -9745.331501785598
Iteration 14300: Loss = -9745.331464690216
Iteration 14400: Loss = -9745.4090657499
1
Iteration 14500: Loss = -9745.331435214648
Iteration 14600: Loss = -9745.331404933613
Iteration 14700: Loss = -9745.331370988792
Iteration 14800: Loss = -9745.331427889267
1
Iteration 14900: Loss = -9745.33131905305
Iteration 15000: Loss = -9745.331347624016
1
Iteration 15100: Loss = -9745.332397315971
2
Iteration 15200: Loss = -9745.33127804452
Iteration 15300: Loss = -9745.331287899107
1
Iteration 15400: Loss = -9745.334114857067
2
Iteration 15500: Loss = -9745.331250859135
Iteration 15600: Loss = -9745.331245260459
Iteration 15700: Loss = -9745.331957578592
1
Iteration 15800: Loss = -9745.331208221412
Iteration 15900: Loss = -9745.331224750154
1
Iteration 16000: Loss = -9745.44168443503
2
Iteration 16100: Loss = -9745.33120038247
Iteration 16200: Loss = -9745.33119246591
Iteration 16300: Loss = -9745.354641914482
1
Iteration 16400: Loss = -9745.331144207697
Iteration 16500: Loss = -9745.350687330827
1
Iteration 16600: Loss = -9745.331151055763
2
Iteration 16700: Loss = -9745.359017825742
3
Iteration 16800: Loss = -9745.33133922224
4
Iteration 16900: Loss = -9745.331168573131
5
Iteration 17000: Loss = -9745.375224844265
6
Iteration 17100: Loss = -9745.331103011513
Iteration 17200: Loss = -9745.33204424142
1
Iteration 17300: Loss = -9745.331151418404
2
Iteration 17400: Loss = -9745.331111605756
3
Iteration 17500: Loss = -9745.33492697869
4
Iteration 17600: Loss = -9745.331086501754
Iteration 17700: Loss = -9745.383862953944
1
Iteration 17800: Loss = -9745.331111743715
2
Iteration 17900: Loss = -9745.331594347965
3
Iteration 18000: Loss = -9745.333341060208
4
Iteration 18100: Loss = -9745.331102412492
5
Iteration 18200: Loss = -9745.346515838924
6
Iteration 18300: Loss = -9745.331093833869
7
Iteration 18400: Loss = -9745.332021894015
8
Iteration 18500: Loss = -9745.33110347178
9
Iteration 18600: Loss = -9745.331093520044
10
Stopping early at iteration 18600 due to no improvement.
tensor([[-8.3090,  6.7574],
        [-7.3212,  5.9337],
        [-8.9125,  7.4095],
        [-8.9883,  7.5917],
        [-7.6836,  5.8625],
        [-9.0861,  7.4793],
        [-3.1295,  1.6097],
        [-9.2119,  7.6532],
        [-4.1705,  2.6092],
        [-9.0644,  7.6774],
        [-9.4703,  6.9311],
        [-9.3721,  7.2519],
        [-7.9680,  4.5362],
        [-9.2821,  7.3906],
        [-5.6356,  4.2294],
        [-7.8398,  4.3590],
        [-9.3465,  7.2696],
        [-8.9084,  6.9911],
        [-6.5819,  5.1926],
        [-7.1692,  5.2882],
        [-3.8931,  2.3191],
        [-8.3134,  6.9271],
        [-8.9020,  5.7141],
        [-7.6981,  5.9352],
        [-8.7374,  7.3270],
        [-6.3424,  3.7083],
        [-7.2039,  4.4515],
        [-8.7042,  7.3162],
        [-9.6944,  6.2830],
        [-8.7355,  7.0614],
        [-3.5280,  2.0209],
        [-5.4851,  3.8415],
        [-6.6769,  5.2902],
        [-8.5119,  6.5474],
        [-8.7038,  5.6540],
        [-9.3996,  7.4236],
        [-8.4891,  6.8201],
        [-5.9141,  3.7453],
        [-6.9498,  5.4419],
        [-8.9743,  6.8589],
        [-9.5864,  6.1986],
        [-8.0196,  6.4061],
        [-5.6520,  2.9617],
        [-7.4070,  5.5679],
        [-7.7753,  4.6151],
        [-0.4261, -2.8315],
        [-5.9633,  4.1293],
        [-7.7403,  6.2303],
        [-9.3634,  7.2366],
        [-9.7798,  6.6241],
        [-8.2835,  6.3435],
        [-9.8257,  7.0272],
        [-9.2723,  7.2968],
        [-9.2965,  7.1476],
        [-5.3586,  3.9691],
        [-8.6567,  7.1263],
        [-7.3749,  4.3318],
        [-6.3736,  4.5593],
        [-4.8176,  3.4283],
        [-9.3393,  7.4210],
        [-7.1409,  5.5012],
        [-7.1561,  5.7173],
        [-8.5544,  7.1096],
        [-6.0770,  3.5026],
        [-9.6372,  6.8309],
        [-8.6674,  6.7506],
        [-9.7659,  7.0842],
        [-5.1216,  2.2040],
        [-8.9256,  6.4410],
        [-8.9491,  7.5505],
        [-9.0316,  7.5301],
        [-6.5604,  5.1097],
        [-8.7889,  7.3855],
        [-7.3376,  5.9366],
        [-7.6940,  6.1520],
        [-7.1062,  3.1712],
        [-6.2641,  4.3397],
        [-8.0034,  5.8700],
        [-5.8936,  4.4783],
        [-9.0406,  6.9042],
        [-8.3025,  6.4980],
        [-8.9420,  6.7972],
        [-7.4862,  6.0603],
        [-4.8303,  2.8023],
        [-8.8042,  7.0335],
        [-7.9817,  6.5584],
        [-7.2837,  5.7948],
        [-9.2108,  7.3906],
        [-6.6594,  5.1784],
        [-8.7731,  7.1215],
        [-9.0751,  7.6290],
        [-8.4141,  6.8659],
        [-7.1128,  5.6019],
        [-6.4095,  4.9309],
        [-2.7992,  1.1306],
        [-8.8922,  5.5691],
        [-8.9207,  6.5036],
        [-8.6700,  7.1834],
        [-9.4478,  7.0029],
        [-4.0916,  2.7048]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 9.5244e-06],
        [7.2421e-03, 9.9276e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0096, 0.9904], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0158, 0.0630],
         [0.0888, 0.1347]],

        [[0.5474, 0.1235],
         [0.9971, 0.0575]],

        [[0.4955, 0.0850],
         [0.9856, 0.6202]],

        [[0.4634, 0.0652],
         [0.5522, 0.2071]],

        [[0.1407, 0.2021],
         [0.5789, 0.5824]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0009975514204148314
Global Adjusted Rand Index: -0.0008018420765110036
Average Adjusted Rand Index: -0.0007803048475515604
Iteration 0: Loss = -24275.22384130203
Iteration 10: Loss = -9750.65585438129
Iteration 20: Loss = -9750.65585545534
1
Iteration 30: Loss = -9750.65592763816
2
Iteration 40: Loss = -9750.655258515138
Iteration 50: Loss = -9750.519835983603
Iteration 60: Loss = -9749.652160904629
Iteration 70: Loss = -9749.474242037168
Iteration 80: Loss = -9749.414953659561
Iteration 90: Loss = -9749.387863347187
Iteration 100: Loss = -9749.374824280518
Iteration 110: Loss = -9749.368772618634
Iteration 120: Loss = -9749.366413151201
Iteration 130: Loss = -9749.366102534324
Iteration 140: Loss = -9749.36688092833
1
Iteration 150: Loss = -9749.368297689456
2
Iteration 160: Loss = -9749.369884887108
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[9.5494e-01, 4.5060e-02],
        [1.0000e+00, 4.8656e-25]], dtype=torch.float64)
alpha: tensor([0.9572, 0.0428])
beta: tensor([[[0.1309, 0.1287],
         [0.4765, 0.1740]],

        [[0.4984, 0.1528],
         [0.7557, 0.2751]],

        [[0.5673, 0.1253],
         [0.6620, 0.9045]],

        [[0.7014, 0.1562],
         [0.2974, 0.4707]],

        [[0.1715, 0.1904],
         [0.9564, 0.3776]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0006579886660132697
Average Adjusted Rand Index: -0.0004529465619428516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24276.2352134907
Iteration 100: Loss = -9760.814603177407
Iteration 200: Loss = -9753.728946722285
Iteration 300: Loss = -9752.092586045379
Iteration 400: Loss = -9751.316341435915
Iteration 500: Loss = -9750.751272353727
Iteration 600: Loss = -9750.442427924216
Iteration 700: Loss = -9750.249321272271
Iteration 800: Loss = -9750.116927296167
Iteration 900: Loss = -9750.019919766642
Iteration 1000: Loss = -9749.944478147901
Iteration 1100: Loss = -9749.88311181131
Iteration 1200: Loss = -9749.831441639873
Iteration 1300: Loss = -9749.786411181427
Iteration 1400: Loss = -9749.745152278672
Iteration 1500: Loss = -9749.705035915093
Iteration 1600: Loss = -9749.663534817026
Iteration 1700: Loss = -9749.61836712204
Iteration 1800: Loss = -9749.568459178408
Iteration 1900: Loss = -9749.515020322633
Iteration 2000: Loss = -9749.461484897
Iteration 2100: Loss = -9749.411004135069
Iteration 2200: Loss = -9749.364765755548
Iteration 2300: Loss = -9749.322486945639
Iteration 2400: Loss = -9749.283321681052
Iteration 2500: Loss = -9749.245973933805
Iteration 2600: Loss = -9749.20941080884
Iteration 2700: Loss = -9749.172769163653
Iteration 2800: Loss = -9749.135399816267
Iteration 2900: Loss = -9749.0968006421
Iteration 3000: Loss = -9749.05646910444
Iteration 3100: Loss = -9749.015619275993
Iteration 3200: Loss = -9748.97542945358
Iteration 3300: Loss = -9748.936065146443
Iteration 3400: Loss = -9748.898923519793
Iteration 3500: Loss = -9748.865315756797
Iteration 3600: Loss = -9748.8357913113
Iteration 3700: Loss = -9748.81052729286
Iteration 3800: Loss = -9748.789369561431
Iteration 3900: Loss = -9748.77198301099
Iteration 4000: Loss = -9748.75799939318
Iteration 4100: Loss = -9748.74689455481
Iteration 4200: Loss = -9748.73808626833
Iteration 4300: Loss = -9748.73089277356
Iteration 4400: Loss = -9748.724600907452
Iteration 4500: Loss = -9748.718484299143
Iteration 4600: Loss = -9748.711647598986
Iteration 4700: Loss = -9748.70269405991
Iteration 4800: Loss = -9748.688377692522
Iteration 4900: Loss = -9748.657179457032
Iteration 5000: Loss = -9748.52792298782
Iteration 5100: Loss = -9747.809868150132
Iteration 5200: Loss = -9746.503217375805
Iteration 5300: Loss = -9746.032287590911
Iteration 5400: Loss = -9745.817363928069
Iteration 5500: Loss = -9745.697455672149
Iteration 5600: Loss = -9745.62170151723
Iteration 5700: Loss = -9745.568442553662
Iteration 5800: Loss = -9745.529956661952
Iteration 5900: Loss = -9745.544700366503
1
Iteration 6000: Loss = -9745.478130625035
Iteration 6100: Loss = -9745.459930889181
Iteration 6200: Loss = -9745.445237016775
Iteration 6300: Loss = -9745.433201807158
Iteration 6400: Loss = -9745.423201691361
Iteration 6500: Loss = -9745.414862353802
Iteration 6600: Loss = -9745.494508054993
1
Iteration 6700: Loss = -9745.401410391474
Iteration 6800: Loss = -9745.395987047661
Iteration 6900: Loss = -9745.391365159634
Iteration 7000: Loss = -9745.389316515826
Iteration 7100: Loss = -9745.384377890015
Iteration 7200: Loss = -9745.381560079128
Iteration 7300: Loss = -9745.42793475547
1
Iteration 7400: Loss = -9745.376934144258
Iteration 7500: Loss = -9745.375034089348
Iteration 7600: Loss = -9745.373322565343
Iteration 7700: Loss = -9745.371911036711
Iteration 7800: Loss = -9745.370410700833
Iteration 7900: Loss = -9745.369157586983
Iteration 8000: Loss = -9745.376542099935
1
Iteration 8100: Loss = -9745.367079074204
Iteration 8200: Loss = -9745.366173182503
Iteration 8300: Loss = -9745.367481076637
1
Iteration 8400: Loss = -9745.364585603867
Iteration 8500: Loss = -9745.363916327227
Iteration 8600: Loss = -9745.363305153165
Iteration 8700: Loss = -9745.442440901494
1
Iteration 8800: Loss = -9745.362233526917
Iteration 8900: Loss = -9745.361762190638
Iteration 9000: Loss = -9745.361309175987
Iteration 9100: Loss = -9745.361968256708
1
Iteration 9200: Loss = -9745.360525575321
Iteration 9300: Loss = -9745.360138729808
Iteration 9400: Loss = -9745.35986289016
Iteration 9500: Loss = -9745.359639076602
Iteration 9600: Loss = -9745.359246344573
Iteration 9700: Loss = -9745.358978253833
Iteration 9800: Loss = -9745.372640036356
1
Iteration 9900: Loss = -9745.358463624574
Iteration 10000: Loss = -9745.358263144424
Iteration 10100: Loss = -9745.500916251836
1
Iteration 10200: Loss = -9745.357825214622
Iteration 10300: Loss = -9745.35762371379
Iteration 10400: Loss = -9745.357412094461
Iteration 10500: Loss = -9745.358901170246
1
Iteration 10600: Loss = -9745.357001959974
Iteration 10700: Loss = -9745.35680990257
Iteration 10800: Loss = -9745.791635049576
1
Iteration 10900: Loss = -9745.356377281034
Iteration 11000: Loss = -9745.356121695015
Iteration 11100: Loss = -9745.355936824471
Iteration 11200: Loss = -9745.356492789911
1
Iteration 11300: Loss = -9745.355707880353
Iteration 11400: Loss = -9745.355581170168
Iteration 11500: Loss = -9745.476072071247
1
Iteration 11600: Loss = -9745.355400276287
Iteration 11700: Loss = -9745.355317383703
Iteration 11800: Loss = -9745.35523434495
Iteration 11900: Loss = -9745.358651988832
1
Iteration 12000: Loss = -9745.355066006097
Iteration 12100: Loss = -9745.355021610263
Iteration 12200: Loss = -9745.76911219166
1
Iteration 12300: Loss = -9745.354916267379
Iteration 12400: Loss = -9745.354805173809
Iteration 12500: Loss = -9745.354784456631
Iteration 12600: Loss = -9745.357632238287
1
Iteration 12700: Loss = -9745.35462554259
Iteration 12800: Loss = -9745.354596860949
Iteration 12900: Loss = -9745.35454689207
Iteration 13000: Loss = -9745.355213397017
1
Iteration 13100: Loss = -9745.354437093023
Iteration 13200: Loss = -9745.354408963593
Iteration 13300: Loss = -9745.376976139802
1
Iteration 13400: Loss = -9745.354342209821
Iteration 13500: Loss = -9745.354300536266
Iteration 13600: Loss = -9745.354244487386
Iteration 13700: Loss = -9745.360342691793
1
Iteration 13800: Loss = -9745.353742327658
Iteration 13900: Loss = -9745.352714716726
Iteration 14000: Loss = -9745.352326868973
Iteration 14100: Loss = -9745.815222406376
1
Iteration 14200: Loss = -9745.352239905753
Iteration 14300: Loss = -9745.352170192939
Iteration 14400: Loss = -9745.352160204004
Iteration 14500: Loss = -9745.353547093882
1
Iteration 14600: Loss = -9745.351892141798
Iteration 14700: Loss = -9745.351279822744
Iteration 14800: Loss = -9745.351303744605
1
Iteration 14900: Loss = -9745.35123160862
Iteration 15000: Loss = -9745.351231859062
1
Iteration 15100: Loss = -9745.35130310933
2
Iteration 15200: Loss = -9745.351218334163
Iteration 15300: Loss = -9745.351149294213
Iteration 15400: Loss = -9745.351172293786
1
Iteration 15500: Loss = -9745.376138060285
2
Iteration 15600: Loss = -9745.351128916935
Iteration 15700: Loss = -9745.35111774709
Iteration 15800: Loss = -9745.392199940385
1
Iteration 15900: Loss = -9745.351061444098
Iteration 16000: Loss = -9745.351088667654
1
Iteration 16100: Loss = -9745.352245900922
2
Iteration 16200: Loss = -9745.401959255063
3
Iteration 16300: Loss = -9745.351004500744
Iteration 16400: Loss = -9745.393229872234
1
Iteration 16500: Loss = -9745.350993448976
Iteration 16600: Loss = -9745.394047917829
1
Iteration 16700: Loss = -9745.350921669487
Iteration 16800: Loss = -9745.51613354675
1
Iteration 16900: Loss = -9745.350823978897
Iteration 17000: Loss = -9745.35046921253
Iteration 17100: Loss = -9745.351649468876
1
Iteration 17200: Loss = -9745.350425423687
Iteration 17300: Loss = -9745.35047534122
1
Iteration 17400: Loss = -9745.350440137112
2
Iteration 17500: Loss = -9745.35044686
3
Iteration 17600: Loss = -9745.350362783289
Iteration 17700: Loss = -9745.36456981807
1
Iteration 17800: Loss = -9745.350399406901
2
Iteration 17900: Loss = -9745.350225404833
Iteration 18000: Loss = -9745.350765133353
1
Iteration 18100: Loss = -9745.350145875867
Iteration 18200: Loss = -9745.350154802698
1
Iteration 18300: Loss = -9745.350536210248
2
Iteration 18400: Loss = -9745.35013518531
Iteration 18500: Loss = -9745.350104800042
Iteration 18600: Loss = -9745.498529982931
1
Iteration 18700: Loss = -9745.350146873801
2
Iteration 18800: Loss = -9745.350131511015
3
Iteration 18900: Loss = -9745.36062683055
4
Iteration 19000: Loss = -9745.350130944349
5
Iteration 19100: Loss = -9745.350112109685
6
Iteration 19200: Loss = -9745.683010426492
7
Iteration 19300: Loss = -9745.350147990097
8
Iteration 19400: Loss = -9745.350989826376
9
Iteration 19500: Loss = -9745.350386071384
10
Stopping early at iteration 19500 due to no improvement.
tensor([[  7.8366, -10.3330],
        [  5.7765,  -7.6816],
        [  7.4041,  -8.7920],
        [  7.9338,  -9.3493],
        [  7.6253,  -9.2444],
        [  7.9857,  -9.4842],
        [  1.6880,  -3.1940],
        [  8.6022, -10.1630],
        [  2.4242,  -4.4572],
        [  7.6201, -10.0280],
        [  7.5202, -10.2276],
        [  7.7484,  -9.5695],
        [  5.5621,  -7.1844],
        [  8.3542, -10.2713],
        [  6.7408,  -8.1763],
        [  7.0654,  -8.4886],
        [  7.8378,  -9.3626],
        [  7.2541,  -9.3831],
        [  4.4390,  -7.4802],
        [  7.3260,  -9.7626],
        [  1.6681,  -4.6438],
        [  7.2449,  -8.7342],
        [  7.5034, -10.2407],
        [  6.9647,  -8.9444],
        [  7.6305,  -9.0172],
        [  6.4659,  -7.8753],
        [  5.2098,  -6.6111],
        [  8.7222, -10.2086],
        [  7.4547,  -9.2891],
        [  7.4365, -10.3679],
        [  2.1550,  -3.5457],
        [  4.0462,  -5.5091],
        [  7.1478, -10.1714],
        [  7.7082,  -9.0946],
        [  6.6551,  -8.7313],
        [  8.2374,  -9.7861],
        [  7.0684,  -8.4554],
        [  2.6106,  -7.2259],
        [  7.4674,  -9.7538],
        [  7.2480,  -8.8468],
        [  8.0646,  -9.7572],
        [  7.0317,  -9.5175],
        [  2.8275,  -5.9328],
        [  6.9832,  -8.6905],
        [  5.5941,  -6.9819],
        [ -2.4266,  -0.0539],
        [  4.3424,  -5.8282],
        [  5.8932,  -8.3679],
        [  7.7327,  -9.1583],
        [  7.8711,  -9.6911],
        [  6.3150,  -9.1857],
        [  8.3368, -10.4748],
        [  7.9598, -10.1947],
        [  7.7468,  -9.8752],
        [  3.8808,  -5.6338],
        [  7.5885,  -8.9750],
        [  6.4789,  -8.3133],
        [  4.0373,  -6.9887],
        [  3.4701,  -4.9492],
        [  8.0758, -10.4863],
        [  5.7052,  -7.1247],
        [  6.9156,  -8.3440],
        [  7.6832,  -9.1416],
        [  4.1735,  -5.5606],
        [  7.9122,  -9.3427],
        [  7.5298, -10.7729],
        [  8.7152, -10.1214],
        [  3.1055,  -4.5475],
        [  7.6549,  -9.0443],
        [  8.0100, -10.0744],
        [  6.8411, -11.4563],
        [  7.7198,  -9.4321],
        [  7.6597,  -9.1175],
        [  6.0841,  -9.1386],
        [  6.1483,  -7.7574],
        [  4.3388,  -6.2266],
        [  4.5598,  -6.2201],
        [  7.3335,  -9.6192],
        [  6.4284,  -7.9975],
        [  7.0935, -11.1914],
        [  7.8573,  -9.2453],
        [  7.9422,  -9.3287],
        [  6.4497,  -8.5038],
        [  2.7230,  -5.0628],
        [  7.2323, -10.2010],
        [  6.6186,  -8.0151],
        [  5.8958,  -7.3007],
        [  7.9731,  -9.5899],
        [  4.6475,  -7.3945],
        [  7.4296,  -9.4785],
        [  8.0710,  -9.7259],
        [  7.6341,  -9.2136],
        [  7.9393,  -9.8677],
        [  5.0121,  -6.5193],
        [  7.7955, -10.6872],
        [  6.5963,  -8.2576],
        [  6.8114,  -8.5089],
        [  7.3564,  -9.4264],
        [  8.1223,  -9.6955],
        [  2.6591,  -4.2521]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9267e-01, 7.3265e-03],
        [4.5827e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9907, 0.0093], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1347, 0.0614],
         [0.4765, 0.0158]],

        [[0.4984, 0.1234],
         [0.7557, 0.2751]],

        [[0.5673, 0.0849],
         [0.6620, 0.9045]],

        [[0.7014, 0.0652],
         [0.2974, 0.4707]],

        [[0.1715, 0.2022],
         [0.9564, 0.3776]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0009975514204148314
Global Adjusted Rand Index: -0.0008018420765110036
Average Adjusted Rand Index: -0.0007803048475515604
9892.029275103168
new:  [-0.00015240596172097878, 0.0036101405255143247, -0.0008018420765110036, -0.0008018420765110036] [-0.003512280899301514, -0.00014671034033994024, -0.0007803048475515604, -0.0007803048475515604] [9747.698343578119, 9748.613960038889, 9745.331093520044, 9745.350386071384]
prior:  [0.0, 0.0006579886660132697, 0.0006579886660132697, 0.0006579886660132697] [0.0, -0.0004529465619428516, -0.0004529465619428516, -0.0004529465619428516] [nan, 9749.336442798965, 9749.326915541867, 9749.369884887108]
-----------------------------------------------------------------------------------------
This iteration is 16
True Objective function: Loss = -9903.887632493004
Iteration 0: Loss = -18261.41517898564
Iteration 10: Loss = -9783.371577858612
Iteration 20: Loss = -9783.222165437084
Iteration 30: Loss = -9783.182399968258
Iteration 40: Loss = -9783.16938323888
Iteration 50: Loss = -9783.16465481566
Iteration 60: Loss = -9783.162710054263
Iteration 70: Loss = -9783.161948269191
Iteration 80: Loss = -9783.161534999119
Iteration 90: Loss = -9783.161371757644
Iteration 100: Loss = -9783.161236846572
Iteration 110: Loss = -9783.161185749188
Iteration 120: Loss = -9783.161142269682
Iteration 130: Loss = -9783.16114566708
1
Iteration 140: Loss = -9783.161123498056
Iteration 150: Loss = -9783.161154415555
1
Iteration 160: Loss = -9783.161103936905
Iteration 170: Loss = -9783.161101864804
Iteration 180: Loss = -9783.161061314679
Iteration 190: Loss = -9783.16108675426
1
Iteration 200: Loss = -9783.16107418162
2
Iteration 210: Loss = -9783.161090568636
3
Stopping early at iteration 209 due to no improvement.
pi: tensor([[8.1305e-01, 1.8695e-01],
        [9.9996e-01, 3.9167e-05]], dtype=torch.float64)
alpha: tensor([0.8425, 0.1575])
beta: tensor([[[0.1336, 0.1299],
         [0.9959, 0.1324]],

        [[0.2725, 0.1429],
         [0.5790, 0.1912]],

        [[0.4519, 0.1408],
         [0.1568, 0.4822]],

        [[0.4391, 0.1206],
         [0.9133, 0.4936]],

        [[0.6509, 0.1303],
         [0.0299, 0.0859]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18189.949248909692
Iteration 100: Loss = -9786.124704332933
Iteration 200: Loss = -9784.61741190908
Iteration 300: Loss = -9784.160295729413
Iteration 400: Loss = -9783.860609365804
Iteration 500: Loss = -9783.230590078296
Iteration 600: Loss = -9781.163095742166
Iteration 700: Loss = -9780.515762305322
Iteration 800: Loss = -9780.270663448702
Iteration 900: Loss = -9780.155479999004
Iteration 1000: Loss = -9780.083095298603
Iteration 1100: Loss = -9780.030114384856
Iteration 1200: Loss = -9779.988314629627
Iteration 1300: Loss = -9779.953653184652
Iteration 1400: Loss = -9779.92395013313
Iteration 1500: Loss = -9779.897683891691
Iteration 1600: Loss = -9779.873977374316
Iteration 1700: Loss = -9779.852182379274
Iteration 1800: Loss = -9779.831799811569
Iteration 1900: Loss = -9779.812622545525
Iteration 2000: Loss = -9779.794301128131
Iteration 2100: Loss = -9779.776670332145
Iteration 2200: Loss = -9779.759676737849
Iteration 2300: Loss = -9779.743133578257
Iteration 2400: Loss = -9779.727061346923
Iteration 2500: Loss = -9779.711435321668
Iteration 2600: Loss = -9779.696149067004
Iteration 2700: Loss = -9779.681228813497
Iteration 2800: Loss = -9779.666715448559
Iteration 2900: Loss = -9779.652527945742
Iteration 3000: Loss = -9779.63873638248
Iteration 3100: Loss = -9779.625346493078
Iteration 3200: Loss = -9779.612325806716
Iteration 3300: Loss = -9779.599713508493
Iteration 3400: Loss = -9779.587486213617
Iteration 3500: Loss = -9779.575668597798
Iteration 3600: Loss = -9779.564205747061
Iteration 3700: Loss = -9779.553090252022
Iteration 3800: Loss = -9779.54213453806
Iteration 3900: Loss = -9779.531113743495
Iteration 4000: Loss = -9779.518518112407
Iteration 4100: Loss = -9779.501762424516
Iteration 4200: Loss = -9779.48979670765
Iteration 4300: Loss = -9779.480168613842
Iteration 4400: Loss = -9779.471232154121
Iteration 4500: Loss = -9779.462753666397
Iteration 4600: Loss = -9779.454660128282
Iteration 4700: Loss = -9779.446977740574
Iteration 4800: Loss = -9779.439687591565
Iteration 4900: Loss = -9779.432823672463
Iteration 5000: Loss = -9779.426311843987
Iteration 5100: Loss = -9779.420158780635
Iteration 5200: Loss = -9779.414341440464
Iteration 5300: Loss = -9779.408868575241
Iteration 5400: Loss = -9779.403633986356
Iteration 5500: Loss = -9779.398691871747
Iteration 5600: Loss = -9779.394481289903
Iteration 5700: Loss = -9779.389507573422
Iteration 5800: Loss = -9779.385243091287
Iteration 5900: Loss = -9779.392084388614
1
Iteration 6000: Loss = -9779.37740030667
Iteration 6100: Loss = -9779.373752261565
Iteration 6200: Loss = -9779.370305163484
Iteration 6300: Loss = -9779.367050732682
Iteration 6400: Loss = -9779.363832226087
Iteration 6500: Loss = -9779.360848501403
Iteration 6600: Loss = -9779.363797802398
1
Iteration 6700: Loss = -9779.355313769007
Iteration 6800: Loss = -9779.352767371074
Iteration 6900: Loss = -9779.354306192907
1
Iteration 7000: Loss = -9779.34797455829
Iteration 7100: Loss = -9779.345745557472
Iteration 7200: Loss = -9779.34359222939
Iteration 7300: Loss = -9779.341855881066
Iteration 7400: Loss = -9779.33947042776
Iteration 7500: Loss = -9779.337562971466
Iteration 7600: Loss = -9779.345773366995
1
Iteration 7700: Loss = -9779.334133549472
Iteration 7800: Loss = -9779.332582836136
Iteration 7900: Loss = -9779.427227241744
1
Iteration 8000: Loss = -9779.329761397865
Iteration 8100: Loss = -9779.328384661674
Iteration 8200: Loss = -9779.32711751369
Iteration 8300: Loss = -9779.326011487246
Iteration 8400: Loss = -9779.324745549606
Iteration 8500: Loss = -9779.323628316164
Iteration 8600: Loss = -9779.363083327351
1
Iteration 8700: Loss = -9779.321544710108
Iteration 8800: Loss = -9779.320569698748
Iteration 8900: Loss = -9779.319677882722
Iteration 9000: Loss = -9779.318843480823
Iteration 9100: Loss = -9779.318015779552
Iteration 9200: Loss = -9779.317254331014
Iteration 9300: Loss = -9779.318961079245
1
Iteration 9400: Loss = -9779.315819842946
Iteration 9500: Loss = -9779.315133494682
Iteration 9600: Loss = -9779.314512057994
Iteration 9700: Loss = -9779.313942050921
Iteration 9800: Loss = -9779.313334836514
Iteration 9900: Loss = -9779.312801684002
Iteration 10000: Loss = -9779.372260295348
1
Iteration 10100: Loss = -9779.31172919421
Iteration 10200: Loss = -9779.311002755796
Iteration 10300: Loss = -9779.309698286292
Iteration 10400: Loss = -9779.31266791244
1
Iteration 10500: Loss = -9779.308782764141
Iteration 10600: Loss = -9779.308383539328
Iteration 10700: Loss = -9779.350699759378
1
Iteration 10800: Loss = -9779.307557162441
Iteration 10900: Loss = -9779.307028740477
Iteration 11000: Loss = -9779.306619966923
Iteration 11100: Loss = -9779.307212813053
1
Iteration 11200: Loss = -9779.306039094081
Iteration 11300: Loss = -9779.305728760346
Iteration 11400: Loss = -9779.328695061688
1
Iteration 11500: Loss = -9779.305265016541
Iteration 11600: Loss = -9779.305030900216
Iteration 11700: Loss = -9779.304791188575
Iteration 11800: Loss = -9779.30556577965
1
Iteration 11900: Loss = -9779.304426446342
Iteration 12000: Loss = -9779.304228475161
Iteration 12100: Loss = -9779.820475552604
1
Iteration 12200: Loss = -9779.303859261066
Iteration 12300: Loss = -9779.303706212095
Iteration 12400: Loss = -9779.30355474853
Iteration 12500: Loss = -9779.305242734765
1
Iteration 12600: Loss = -9779.303305544308
Iteration 12700: Loss = -9779.303115772896
Iteration 12800: Loss = -9779.302972568858
Iteration 12900: Loss = -9779.310249215661
1
Iteration 13000: Loss = -9779.302752310847
Iteration 13100: Loss = -9779.302648002344
Iteration 13200: Loss = -9779.30254750417
Iteration 13300: Loss = -9779.30265611498
1
Iteration 13400: Loss = -9779.302364209785
Iteration 13500: Loss = -9779.30228728289
Iteration 13600: Loss = -9779.302274553911
Iteration 13700: Loss = -9779.302112136536
Iteration 13800: Loss = -9779.302016949488
Iteration 13900: Loss = -9779.30196644292
Iteration 14000: Loss = -9779.302294442248
1
Iteration 14100: Loss = -9779.301911399287
Iteration 14200: Loss = -9779.301762539626
Iteration 14300: Loss = -9779.301714376914
Iteration 14400: Loss = -9779.302103780708
1
Iteration 14500: Loss = -9779.301604950775
Iteration 14600: Loss = -9779.301568749846
Iteration 14700: Loss = -9779.319455994457
1
Iteration 14800: Loss = -9779.301490716629
Iteration 14900: Loss = -9779.301433954404
Iteration 15000: Loss = -9779.301377726939
Iteration 15100: Loss = -9779.302108051696
1
Iteration 15200: Loss = -9779.301295320085
Iteration 15300: Loss = -9779.301283131714
Iteration 15400: Loss = -9779.317589528504
1
Iteration 15500: Loss = -9779.301232754091
Iteration 15600: Loss = -9779.30120627874
Iteration 15700: Loss = -9779.6833537626
1
Iteration 15800: Loss = -9779.301160627469
Iteration 15900: Loss = -9779.3010909831
Iteration 16000: Loss = -9779.301098721799
1
Iteration 16100: Loss = -9779.301102697173
2
Iteration 16200: Loss = -9779.301033353713
Iteration 16300: Loss = -9779.301011365897
Iteration 16400: Loss = -9779.307089480344
1
Iteration 16500: Loss = -9779.30097329995
Iteration 16600: Loss = -9779.300980411927
1
Iteration 16700: Loss = -9779.3009661562
Iteration 16800: Loss = -9779.301011418367
1
Iteration 16900: Loss = -9779.300915882905
Iteration 17000: Loss = -9779.300912674778
Iteration 17100: Loss = -9779.329094821667
1
Iteration 17200: Loss = -9779.300861765489
Iteration 17300: Loss = -9779.30089132662
1
Iteration 17400: Loss = -9779.300881329522
2
Iteration 17500: Loss = -9779.300882544492
3
Iteration 17600: Loss = -9779.30083688893
Iteration 17700: Loss = -9779.300849962328
1
Iteration 17800: Loss = -9779.310259550935
2
Iteration 17900: Loss = -9779.30082234096
Iteration 18000: Loss = -9779.300808012862
Iteration 18100: Loss = -9779.300818781043
1
Iteration 18200: Loss = -9779.315280434876
2
Iteration 18300: Loss = -9779.300790326914
Iteration 18400: Loss = -9779.300786793823
Iteration 18500: Loss = -9779.30080111955
1
Iteration 18600: Loss = -9779.300805309327
2
Iteration 18700: Loss = -9779.300785593012
Iteration 18800: Loss = -9779.300772118924
Iteration 18900: Loss = -9779.478276003427
1
Iteration 19000: Loss = -9779.300754620132
Iteration 19100: Loss = -9779.300762465562
1
Iteration 19200: Loss = -9779.300746419955
Iteration 19300: Loss = -9779.309495343134
1
Iteration 19400: Loss = -9779.30075273594
2
Iteration 19500: Loss = -9779.300755558366
3
Iteration 19600: Loss = -9779.30075045791
4
Iteration 19700: Loss = -9779.30074576699
Iteration 19800: Loss = -9779.300734011755
Iteration 19900: Loss = -9779.300744189153
1
tensor([[  5.9026, -10.5178],
        [  7.3794, -11.9946],
        [  3.1121,  -7.7273],
        [  5.7833, -10.3985],
        [  6.3713, -10.9865],
        [  7.5366, -12.1518],
        [  5.9611, -10.5763],
        [  4.2959,  -8.9111],
        [  3.3072,  -7.9224],
        [  4.8423,  -9.4575],
        [  0.9966,  -5.6118],
        [  7.2114, -11.8266],
        [  6.1274, -10.7427],
        [  7.2620, -11.8772],
        [  2.8054,  -7.4206],
        [  6.2643, -10.8795],
        [  3.8616,  -8.4768],
        [  1.0347,  -5.6499],
        [  6.0788, -10.6941],
        [  4.5084,  -9.1236],
        [ -3.2011,  -1.4141],
        [  5.7336, -10.3489],
        [  6.2534, -10.8686],
        [  8.7032, -13.3184],
        [ -4.8825,   0.2673],
        [  5.3876, -10.0028],
        [  6.0606, -10.6759],
        [  3.2753,  -7.8905],
        [  6.6004, -11.2156],
        [  5.9131, -10.5283],
        [  5.8582, -10.4735],
        [  7.1761, -11.7914],
        [  2.6617,  -7.2770],
        [  6.7145, -11.3297],
        [  6.2930, -10.9082],
        [  0.7679,  -5.3831],
        [  5.9827, -10.5979],
        [  7.4838, -12.0990],
        [  1.0506,  -5.6659],
        [  5.8199, -10.4351],
        [  6.7797, -11.3949],
        [  6.4530, -11.0682],
        [  1.4981,  -6.1133],
        [  5.6402, -10.2555],
        [  3.1495,  -7.7647],
        [  5.6341, -10.2494],
        [  6.0501, -10.6654],
        [  7.6772, -12.2924],
        [  4.7880,  -9.4032],
        [  8.0310, -12.6463],
        [  1.2273,  -5.8425],
        [  7.5584, -12.1736],
        [  6.5037, -11.1189],
        [  4.3628,  -8.9780],
        [  8.1995, -12.8147],
        [  6.7347, -11.3499],
        [  2.3327,  -6.9480],
        [  5.8620, -10.4772],
        [  6.3302, -10.9454],
        [  5.1496,  -9.7648],
        [  2.9929,  -7.6081],
        [  6.2554, -10.8706],
        [  2.1005,  -6.7157],
        [  6.3484, -10.9636],
        [  2.8965,  -7.5117],
        [  7.7888, -12.4041],
        [  6.1318, -10.7470],
        [  7.3091, -11.9244],
        [  3.5880,  -8.2032],
        [  8.1002, -12.7154],
        [  8.3127, -12.9279],
        [  5.9736, -10.5888],
        [  5.7972, -10.4124],
        [  8.3012, -12.9165],
        [  8.0399, -12.6551],
        [  3.3549,  -7.9701],
        [  2.2586,  -6.8738],
        [  6.1017, -10.7169],
        [  6.1610, -10.7762],
        [  6.2042, -10.8194],
        [  0.2879,  -4.9031],
        [  1.7607,  -6.3760],
        [  0.9089,  -5.5241],
        [ -0.5375,  -4.0777],
        [  5.8451, -10.4604],
        [  5.9101, -10.5253],
        [  7.9805, -12.5957],
        [  6.4489, -11.0641],
        [  5.9878, -10.6030],
        [  6.1814, -10.7966],
        [  5.6220, -10.2372],
        [  4.3329,  -8.9481],
        [  0.2466,  -4.8618],
        [  6.2864, -10.9017],
        [  6.5515, -11.1667],
        [  6.2648, -10.8800],
        [  7.7478, -12.3631],
        [  1.5048,  -6.1200],
        [  5.8172, -10.4325],
        [  5.0750,  -9.6902]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.5969e-09],
        [1.5919e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9812, 0.0188], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.3552e-01, 1.1213e-01],
         [9.9585e-01, 3.9257e-05]],

        [[2.7250e-01, 8.0578e-02],
         [5.7897e-01, 1.9121e-01]],

        [[4.5188e-01, 1.5437e-01],
         [1.5684e-01, 4.8225e-01]],

        [[4.3910e-01, 4.6662e-02],
         [9.1334e-01, 4.9361e-01]],

        [[6.5090e-01, 9.8142e-02],
         [2.9882e-02, 8.5918e-02]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0008670219930345769
Average Adjusted Rand Index: 0.0012209402535248042
Iteration 0: Loss = -23410.21398642029
Iteration 10: Loss = -9784.186148083772
Iteration 20: Loss = -9784.177346544702
Iteration 30: Loss = -9784.050852751981
Iteration 40: Loss = -9783.682079129816
Iteration 50: Loss = -9783.530329417887
Iteration 60: Loss = -9783.472776739498
Iteration 70: Loss = -9783.417749829985
Iteration 80: Loss = -9783.304620270765
Iteration 90: Loss = -9783.128304822472
Iteration 100: Loss = -9783.07567348778
Iteration 110: Loss = -9783.052708019746
Iteration 120: Loss = -9783.040047591829
Iteration 130: Loss = -9783.03263264132
Iteration 140: Loss = -9783.028145343322
Iteration 150: Loss = -9783.025332173122
Iteration 160: Loss = -9783.02351800352
Iteration 170: Loss = -9783.022352376662
Iteration 180: Loss = -9783.021571606885
Iteration 190: Loss = -9783.021032259401
Iteration 200: Loss = -9783.020677497305
Iteration 210: Loss = -9783.020375104796
Iteration 220: Loss = -9783.020231113709
Iteration 230: Loss = -9783.020069950364
Iteration 240: Loss = -9783.020010151711
Iteration 250: Loss = -9783.019907088863
Iteration 260: Loss = -9783.019850865598
Iteration 270: Loss = -9783.019839975226
Iteration 280: Loss = -9783.019798309455
Iteration 290: Loss = -9783.019747690432
Iteration 300: Loss = -9783.01973551625
Iteration 310: Loss = -9783.019733977208
Iteration 320: Loss = -9783.019728658172
Iteration 330: Loss = -9783.019701569969
Iteration 340: Loss = -9783.019699298407
Iteration 350: Loss = -9783.019722959289
1
Iteration 360: Loss = -9783.01972052402
2
Iteration 370: Loss = -9783.019667770266
Iteration 380: Loss = -9783.019704282815
1
Iteration 390: Loss = -9783.01968172622
2
Iteration 400: Loss = -9783.0196749965
3
Stopping early at iteration 399 due to no improvement.
pi: tensor([[0.0507, 0.9493],
        [0.0499, 0.9501]], dtype=torch.float64)
alpha: tensor([0.0500, 0.9500])
beta: tensor([[[0.1263, 0.1210],
         [0.4750, 0.1340]],

        [[0.4140, 0.1506],
         [0.7332, 0.3806]],

        [[0.6733, 0.1530],
         [0.4693, 0.2136]],

        [[0.0846, 0.0898],
         [0.2000, 0.0973]],

        [[0.1511, 0.1254],
         [0.5615, 0.2212]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23409.260904715506
Iteration 100: Loss = -9842.955573735384
Iteration 200: Loss = -9801.5722584234
Iteration 300: Loss = -9787.676907996642
Iteration 400: Loss = -9785.973874962983
Iteration 500: Loss = -9785.241439511286
Iteration 600: Loss = -9784.781703795883
Iteration 700: Loss = -9784.460767979517
Iteration 800: Loss = -9784.2430963225
Iteration 900: Loss = -9784.12095454703
Iteration 1000: Loss = -9784.045981537924
Iteration 1100: Loss = -9783.98937138896
Iteration 1200: Loss = -9783.931287879765
Iteration 1300: Loss = -9783.85992495554
Iteration 1400: Loss = -9783.780621722328
Iteration 1500: Loss = -9783.703913551046
Iteration 1600: Loss = -9783.637180861482
Iteration 1700: Loss = -9783.58426052534
Iteration 1800: Loss = -9783.542547248468
Iteration 1900: Loss = -9783.509213721407
Iteration 2000: Loss = -9783.48449348796
Iteration 2100: Loss = -9783.455093109162
Iteration 2200: Loss = -9783.430720961884
Iteration 2300: Loss = -9783.408379159528
Iteration 2400: Loss = -9783.391565319396
Iteration 2500: Loss = -9783.375334268776
Iteration 2600: Loss = -9783.363528390699
Iteration 2700: Loss = -9783.411321559608
1
Iteration 2800: Loss = -9783.344104544918
Iteration 2900: Loss = -9783.336068778626
Iteration 3000: Loss = -9783.328937722732
Iteration 3100: Loss = -9783.32264061089
Iteration 3200: Loss = -9783.316901974102
Iteration 3300: Loss = -9783.311790723335
Iteration 3400: Loss = -9783.381366741049
1
Iteration 3500: Loss = -9783.30274166064
Iteration 3600: Loss = -9783.298561040914
Iteration 3700: Loss = -9783.294628470585
Iteration 3800: Loss = -9783.340186152298
1
Iteration 3900: Loss = -9783.28712385196
Iteration 4000: Loss = -9783.283574797772
Iteration 4100: Loss = -9783.280117980925
Iteration 4200: Loss = -9783.276757253845
Iteration 4300: Loss = -9783.272825877975
Iteration 4400: Loss = -9783.268242443013
Iteration 4500: Loss = -9783.259385485619
Iteration 4600: Loss = -9783.251663319308
Iteration 4700: Loss = -9783.246660641453
Iteration 4800: Loss = -9783.240985023449
Iteration 4900: Loss = -9783.337401340175
1
Iteration 5000: Loss = -9783.22420340307
Iteration 5100: Loss = -9783.208552612461
Iteration 5200: Loss = -9783.17955636225
Iteration 5300: Loss = -9783.126623643511
Iteration 5400: Loss = -9783.057167997718
Iteration 5500: Loss = -9782.997331382272
Iteration 5600: Loss = -9782.974627278216
Iteration 5700: Loss = -9782.963457273509
Iteration 5800: Loss = -9782.955107576687
Iteration 5900: Loss = -9782.9492969344
Iteration 6000: Loss = -9782.944855336502
Iteration 6100: Loss = -9782.94103413902
Iteration 6200: Loss = -9782.928962729136
Iteration 6300: Loss = -9782.878056663152
Iteration 6400: Loss = -9782.875771052517
Iteration 6500: Loss = -9782.86886571561
Iteration 6600: Loss = -9782.862562149136
Iteration 6700: Loss = -9782.862471282322
Iteration 6800: Loss = -9782.855638243132
Iteration 6900: Loss = -9782.847288606472
Iteration 7000: Loss = -9782.808490477835
Iteration 7100: Loss = -9782.787233048204
Iteration 7200: Loss = -9782.781988722363
Iteration 7300: Loss = -9782.78055005136
Iteration 7400: Loss = -9782.778590534963
Iteration 7500: Loss = -9782.786934960177
1
Iteration 7600: Loss = -9782.778191237394
Iteration 7700: Loss = -9782.778235149228
1
Iteration 7800: Loss = -9782.778144734042
Iteration 7900: Loss = -9782.78775286889
1
Iteration 8000: Loss = -9782.777980536186
Iteration 8100: Loss = -9782.778401342242
1
Iteration 8200: Loss = -9782.941925516643
2
Iteration 8300: Loss = -9782.788523286259
3
Iteration 8400: Loss = -9782.807282624202
4
Iteration 8500: Loss = -9782.778561515554
5
Iteration 8600: Loss = -9783.073970763397
6
Iteration 8700: Loss = -9782.779383375799
7
Iteration 8800: Loss = -9782.777869156142
Iteration 8900: Loss = -9782.778560289553
1
Iteration 9000: Loss = -9782.777830237126
Iteration 9100: Loss = -9783.129431789504
1
Iteration 9200: Loss = -9782.77928084498
2
Iteration 9300: Loss = -9782.788632440648
3
Iteration 9400: Loss = -9782.781436328156
4
Iteration 9500: Loss = -9782.787002427647
5
Iteration 9600: Loss = -9782.79100128856
6
Iteration 9700: Loss = -9782.779622363068
7
Iteration 9800: Loss = -9782.778170728954
8
Iteration 9900: Loss = -9782.829600612133
9
Iteration 10000: Loss = -9782.778122656708
10
Stopping early at iteration 10000 due to no improvement.
tensor([[-1.1677, -0.2549],
        [-1.2632, -0.1321],
        [-1.1995, -0.3408],
        [-1.1574, -0.3191],
        [-1.4051, -0.2096],
        [-1.0687, -0.3766],
        [-1.2063, -0.1956],
        [-2.0374, -0.8942],
        [-1.4198, -0.4733],
        [-1.5106, -0.4331],
        [-1.0423, -0.3526],
        [-2.8378, -1.4833],
        [-1.3824, -0.3961],
        [-1.3334, -0.4961],
        [-1.8376, -0.8559],
        [-1.3858, -0.2411],
        [-1.1193, -0.2861],
        [-1.2092, -0.2059],
        [-1.8995, -0.9129],
        [-1.5684, -0.8540],
        [-2.7759, -1.8393],
        [-1.1717, -0.2684],
        [-1.2509, -0.1632],
        [-1.3463, -0.0522],
        [-2.4436, -1.6158],
        [-1.2182, -0.4652],
        [-1.8912, -0.9990],
        [-1.2127, -0.2249],
        [-1.3909, -0.2331],
        [-1.1753, -0.3179],
        [-2.7342, -1.8810],
        [-2.0026, -1.0312],
        [-1.1864, -0.2765],
        [-1.5424,  0.0285],
        [-1.2474, -0.2013],
        [-2.7137, -1.5699],
        [-1.2676, -0.1902],
        [-1.3398, -0.0865],
        [-1.2163, -0.1741],
        [-1.3373, -0.4436],
        [-1.3398, -0.0558],
        [-1.7677, -0.5648],
        [-2.0751, -1.0358],
        [-2.3679, -1.3810],
        [-1.2219, -0.2829],
        [-1.5737, -0.6785],
        [-1.2770, -0.1340],
        [-1.5155, -0.2704],
        [-1.2072, -0.2666],
        [-1.3786, -0.2679],
        [-1.6039, -0.7858],
        [-1.3298, -0.0916],
        [-1.3169, -0.1167],
        [-1.1243, -0.2688],
        [-1.2717, -0.3397],
        [-1.8604, -0.5929],
        [-1.7263, -1.0257],
        [-2.0134, -1.1248],
        [-1.5384, -0.4022],
        [-1.1954, -0.2403],
        [-1.5918, -0.6042],
        [-1.4567, -0.3501],
        [-1.1013, -0.3529],
        [-1.2781, -0.1264],
        [-1.2510, -0.4117],
        [-1.0994, -0.3818],
        [-2.2038, -1.1655],
        [-1.4285,  0.0098],
        [-1.4532, -0.2557],
        [-1.9777, -0.8457],
        [-1.6230, -0.1712],
        [-1.1576, -0.2419],
        [-1.1446, -0.2764],
        [-1.5339, -0.5253],
        [-1.4337, -0.4230],
        [-1.2114, -0.2451],
        [-1.3016, -0.0915],
        [-1.7022, -0.7796],
        [-1.4284, -0.4141],
        [-1.5733, -0.5104],
        [-1.5454, -0.5560],
        [-1.6008, -0.5376],
        [-1.8506, -0.7737],
        [-1.1656, -0.2717],
        [-1.8368, -0.8431],
        [-1.6630, -0.5429],
        [-1.2882, -0.3365],
        [-1.3197, -0.1861],
        [-1.2896, -0.3621],
        [-1.2623, -0.2484],
        [-2.1993, -1.2219],
        [-1.4958, -0.4604],
        [-1.7858, -0.9587],
        [-1.3520, -0.2233],
        [-1.5778, -0.4765],
        [-2.1292, -1.0888],
        [-1.6670, -0.6345],
        [-1.3025, -0.1777],
        [-1.1385, -0.2525],
        [-1.3329, -0.6231]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0038, 0.9962],
        [0.0592, 0.9408]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2677, 0.7323], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1252, 0.1304],
         [0.4750, 0.1360]],

        [[0.4140, 0.1528],
         [0.7332, 0.3806]],

        [[0.6733, 0.1495],
         [0.4693, 0.2136]],

        [[0.0846, 0.0914],
         [0.2000, 0.0973]],

        [[0.1511, 0.1262],
         [0.5615, 0.2212]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -21635.169464362516
Iteration 10: Loss = -9784.186276540058
Iteration 20: Loss = -9784.183249933189
Iteration 30: Loss = -9784.130188670975
Iteration 40: Loss = -9783.78290584511
Iteration 50: Loss = -9783.512903682798
Iteration 60: Loss = -9783.437865806754
Iteration 70: Loss = -9783.407023354688
Iteration 80: Loss = -9783.368269377346
Iteration 90: Loss = -9783.216616753503
Iteration 100: Loss = -9783.114190898474
Iteration 110: Loss = -9783.083829990728
Iteration 120: Loss = -9783.067542090717
Iteration 130: Loss = -9783.05834938176
Iteration 140: Loss = -9783.053045292021
Iteration 150: Loss = -9783.049966853872
Iteration 160: Loss = -9783.04813838322
Iteration 170: Loss = -9783.047061269725
Iteration 180: Loss = -9783.04644946752
Iteration 190: Loss = -9783.046080050106
Iteration 200: Loss = -9783.045893945222
Iteration 210: Loss = -9783.04576227586
Iteration 220: Loss = -9783.045672932705
Iteration 230: Loss = -9783.04558708508
Iteration 240: Loss = -9783.045495945978
Iteration 250: Loss = -9783.04525881538
Iteration 260: Loss = -9783.044946166738
Iteration 270: Loss = -9783.044480763452
Iteration 280: Loss = -9783.04365654505
Iteration 290: Loss = -9783.042424856898
Iteration 300: Loss = -9783.040730239944
Iteration 310: Loss = -9783.038528673007
Iteration 320: Loss = -9783.035744130408
Iteration 330: Loss = -9783.032629338782
Iteration 340: Loss = -9783.029451211933
Iteration 350: Loss = -9783.026633703417
Iteration 360: Loss = -9783.024318794149
Iteration 370: Loss = -9783.022629361465
Iteration 380: Loss = -9783.02148340669
Iteration 390: Loss = -9783.02074215873
Iteration 400: Loss = -9783.02030100599
Iteration 410: Loss = -9783.020018285571
Iteration 420: Loss = -9783.01992206532
Iteration 430: Loss = -9783.019804983915
Iteration 440: Loss = -9783.019739413898
Iteration 450: Loss = -9783.019695934585
Iteration 460: Loss = -9783.01969365939
Iteration 470: Loss = -9783.019686243666
Iteration 480: Loss = -9783.019693195882
1
Iteration 490: Loss = -9783.01969867013
2
Iteration 500: Loss = -9783.019670586475
Iteration 510: Loss = -9783.019681508496
1
Iteration 520: Loss = -9783.019685074461
2
Iteration 530: Loss = -9783.01967551796
3
Stopping early at iteration 529 due to no improvement.
pi: tensor([[0.0507, 0.9493],
        [0.0500, 0.9500]], dtype=torch.float64)
alpha: tensor([0.0501, 0.9499])
beta: tensor([[[0.1262, 0.1211],
         [0.9016, 0.1340]],

        [[0.2257, 0.1506],
         [0.6661, 0.3842]],

        [[0.3050, 0.1529],
         [0.1855, 0.2367]],

        [[0.7459, 0.0898],
         [0.3657, 0.5270]],

        [[0.3448, 0.1254],
         [0.8137, 0.0572]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21634.794121032402
Iteration 100: Loss = -9802.032689125403
Iteration 200: Loss = -9787.684006829044
Iteration 300: Loss = -9786.018226335076
Iteration 400: Loss = -9785.210442730006
Iteration 500: Loss = -9784.696237393131
Iteration 600: Loss = -9784.364515891735
Iteration 700: Loss = -9784.143108569275
Iteration 800: Loss = -9783.986861327297
Iteration 900: Loss = -9783.86208067971
Iteration 1000: Loss = -9783.748867776483
Iteration 1100: Loss = -9783.663752856251
Iteration 1200: Loss = -9783.592387820021
Iteration 1300: Loss = -9783.531614446836
Iteration 1400: Loss = -9783.480323461496
Iteration 1500: Loss = -9783.436303751425
Iteration 1600: Loss = -9783.397817311077
Iteration 1700: Loss = -9783.363181099105
Iteration 1800: Loss = -9783.330793311645
Iteration 1900: Loss = -9783.299658142798
Iteration 2000: Loss = -9783.269841466268
Iteration 2100: Loss = -9783.24111218956
Iteration 2200: Loss = -9783.213154576822
Iteration 2300: Loss = -9783.185619541802
Iteration 2400: Loss = -9783.158286857279
Iteration 2500: Loss = -9783.13105986882
Iteration 2600: Loss = -9783.103892490706
Iteration 2700: Loss = -9783.07702068344
Iteration 2800: Loss = -9783.050776992586
Iteration 2900: Loss = -9783.025491806586
Iteration 3000: Loss = -9783.001265281859
Iteration 3100: Loss = -9782.97866490474
Iteration 3200: Loss = -9782.958325918966
Iteration 3300: Loss = -9782.939336904177
Iteration 3400: Loss = -9782.92125807387
Iteration 3500: Loss = -9782.905837992115
Iteration 3600: Loss = -9782.893732045068
Iteration 3700: Loss = -9782.883686777765
Iteration 3800: Loss = -9782.87534421648
Iteration 3900: Loss = -9782.868827832297
Iteration 4000: Loss = -9782.863717535036
Iteration 4100: Loss = -9782.859659133102
Iteration 4200: Loss = -9782.856310805937
Iteration 4300: Loss = -9782.853458466643
Iteration 4400: Loss = -9782.85093229916
Iteration 4500: Loss = -9782.84864178145
Iteration 4600: Loss = -9782.846661568807
Iteration 4700: Loss = -9782.844754072597
Iteration 4800: Loss = -9782.84298755796
Iteration 4900: Loss = -9782.84133686147
Iteration 5000: Loss = -9782.839700881901
Iteration 5100: Loss = -9782.84031899395
1
Iteration 5200: Loss = -9782.836687446761
Iteration 5300: Loss = -9782.835223973178
Iteration 5400: Loss = -9782.833793339001
Iteration 5500: Loss = -9782.83246692357
Iteration 5600: Loss = -9782.830664395033
Iteration 5700: Loss = -9782.829235843194
Iteration 5800: Loss = -9782.831522664279
1
Iteration 5900: Loss = -9782.821891538862
Iteration 6000: Loss = -9782.81217875854
Iteration 6100: Loss = -9782.801247664813
Iteration 6200: Loss = -9782.785097424341
Iteration 6300: Loss = -9782.78428145271
Iteration 6400: Loss = -9782.781396661889
Iteration 6500: Loss = -9782.780388272009
Iteration 6600: Loss = -9782.778799354883
Iteration 6700: Loss = -9782.788052976504
1
Iteration 6800: Loss = -9782.778861208992
2
Iteration 6900: Loss = -9782.779807461566
3
Iteration 7000: Loss = -9782.779208000913
4
Iteration 7100: Loss = -9782.77939873573
5
Iteration 7200: Loss = -9782.778755165822
Iteration 7300: Loss = -9782.807057860464
1
Iteration 7400: Loss = -9782.778963670518
2
Iteration 7500: Loss = -9782.77879641017
3
Iteration 7600: Loss = -9782.778656644838
Iteration 7700: Loss = -9782.778640573413
Iteration 7800: Loss = -9782.779480724223
1
Iteration 7900: Loss = -9782.814845084606
2
Iteration 8000: Loss = -9782.783402423394
3
Iteration 8100: Loss = -9782.7786678177
4
Iteration 8200: Loss = -9782.778644131327
5
Iteration 8300: Loss = -9782.797724138176
6
Iteration 8400: Loss = -9782.779208369462
7
Iteration 8500: Loss = -9782.779079203181
8
Iteration 8600: Loss = -9782.778549518207
Iteration 8700: Loss = -9782.7790180991
1
Iteration 8800: Loss = -9782.785318019096
2
Iteration 8900: Loss = -9782.781422713602
3
Iteration 9000: Loss = -9782.77929659447
4
Iteration 9100: Loss = -9782.825165390028
5
Iteration 9200: Loss = -9782.781569633593
6
Iteration 9300: Loss = -9782.800347318302
7
Iteration 9400: Loss = -9782.778853023061
8
Iteration 9500: Loss = -9782.780116452408
9
Iteration 9600: Loss = -9782.778647698584
10
Stopping early at iteration 9600 due to no improvement.
tensor([[-1.7858, -0.8706],
        [-1.3729, -0.2474],
        [-1.4855, -0.6229],
        [-1.5617, -0.7221],
        [-2.2946, -1.1021],
        [-1.3550, -0.6536],
        [-1.3627, -0.3575],
        [-1.9034, -0.7617],
        [-1.1718, -0.2225],
        [-1.2405, -0.1618],
        [-1.0463, -0.3519],
        [-2.8636, -1.5164],
        [-1.2635, -0.2745],
        [-1.5592, -0.7200],
        [-1.5761, -0.5912],
        [-1.6781, -0.5351],
        [-1.1142, -0.2741],
        [-1.5562, -0.5522],
        [-1.8056, -0.8163],
        [-1.3597, -0.6387],
        [-1.2010, -0.2606],
        [-1.2200, -0.3131],
        [-1.2550, -0.1674],
        [-1.3386, -0.0494],
        [-1.1445, -0.3093],
        [-1.2786, -0.5186],
        [-1.4255, -0.5286],
        [-1.9188, -0.9283],
        [-1.2706, -0.1158],
        [-1.4712, -0.6095],
        [-1.2609, -0.4030],
        [-1.3813, -0.4103],
        [-1.6473, -0.7347],
        [-1.4946,  0.0563],
        [-1.4134, -0.3720],
        [-2.1916, -1.0496],
        [-1.2431, -0.1645],
        [-1.3182, -0.0700],
        [-1.5851, -0.5420],
        [-1.7932, -0.9002],
        [-1.3409, -0.0661],
        [-1.6082, -0.4092],
        [-1.2432, -0.2032],
        [-1.1896, -0.2001],
        [-1.7221, -0.7793],
        [-1.2138, -0.3140],
        [-1.5238, -0.3825],
        [-2.6947, -1.4537],
        [-1.1666, -0.2225],
        [-1.3709, -0.2629],
        [-1.1600, -0.3379],
        [-2.1978, -0.9628],
        [-1.3617, -0.1653],
        [-1.5271, -0.6671],
        [-1.1854, -0.2488],
        [-1.9825, -0.7277],
        [-1.0740, -0.3649],
        [-1.1474, -0.2584],
        [-1.3797, -0.2445],
        [-1.2476, -0.2907],
        [-1.4019, -0.4119],
        [-1.8639, -0.7594],
        [-1.0713, -0.3152],
        [-1.2758, -0.1270],
        [-1.1415, -0.2958],
        [-1.0568, -0.3330],
        [-1.2121, -0.1775],
        [-1.6914, -0.2604],
        [-1.4995, -0.3110],
        [-1.2592, -0.1274],
        [-1.5553, -0.1127],
        [-1.3080, -0.3903],
        [-1.1531, -0.2821],
        [-2.0458, -1.0423],
        [-1.6389, -0.6287],
        [-1.1863, -0.2194],
        [-1.3777, -0.1725],
        [-2.1166, -1.1927],
        [-1.2459, -0.2325],
        [-1.3502, -0.2890],
        [-1.5634, -0.5715],
        [-1.2492, -0.1881],
        [-1.2418, -0.1636],
        [-1.1426, -0.2440],
        [-1.4596, -0.4641],
        [-1.2769, -0.1560],
        [-1.3258, -0.3715],
        [-1.5053, -0.3722],
        [-1.2523, -0.3245],
        [-1.2235, -0.2100],
        [-1.6794, -0.6982],
        [-1.7104, -0.6737],
        [-1.1435, -0.3087],
        [-1.8677, -0.7445],
        [-1.3320, -0.2325],
        [-1.2357, -0.1946],
        [-1.2106, -0.1764],
        [-1.3296, -0.2044],
        [-1.1547, -0.2683],
        [-2.6662, -1.9490]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0092, 0.9908],
        [0.0582, 0.9418]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2673, 0.7327], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1254, 0.1304],
         [0.9016, 0.1360]],

        [[0.2257, 0.1525],
         [0.6661, 0.3842]],

        [[0.3050, 0.1498],
         [0.1855, 0.2367]],

        [[0.7459, 0.0911],
         [0.3657, 0.5270]],

        [[0.3448, 0.1261],
         [0.8137, 0.0572]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -22229.62428505854
Iteration 10: Loss = -9784.184909518066
Iteration 20: Loss = -9784.146037884848
Iteration 30: Loss = -9783.852347054764
Iteration 40: Loss = -9783.57886408825
Iteration 50: Loss = -9783.49722043917
Iteration 60: Loss = -9783.445865937321
Iteration 70: Loss = -9783.377804153008
Iteration 80: Loss = -9783.200190680627
Iteration 90: Loss = -9783.095673233183
Iteration 100: Loss = -9783.062621088091
Iteration 110: Loss = -9783.04565815882
Iteration 120: Loss = -9783.035947877801
Iteration 130: Loss = -9783.030151796553
Iteration 140: Loss = -9783.026619692928
Iteration 150: Loss = -9783.024341960343
Iteration 160: Loss = -9783.022893656758
Iteration 170: Loss = -9783.021931133175
Iteration 180: Loss = -9783.021271703836
Iteration 190: Loss = -9783.020825237521
Iteration 200: Loss = -9783.020536415333
Iteration 210: Loss = -9783.020315316877
Iteration 220: Loss = -9783.020146353401
Iteration 230: Loss = -9783.020047924954
Iteration 240: Loss = -9783.019972081178
Iteration 250: Loss = -9783.01987566548
Iteration 260: Loss = -9783.019826290989
Iteration 270: Loss = -9783.019809471349
Iteration 280: Loss = -9783.019799108835
Iteration 290: Loss = -9783.019758571969
Iteration 300: Loss = -9783.019757762388
Iteration 310: Loss = -9783.01972225076
Iteration 320: Loss = -9783.019753219744
1
Iteration 330: Loss = -9783.019741654965
2
Iteration 340: Loss = -9783.019719792506
Iteration 350: Loss = -9783.019686663682
Iteration 360: Loss = -9783.019687573895
1
Iteration 370: Loss = -9783.019692084557
2
Iteration 380: Loss = -9783.019703595248
3
Stopping early at iteration 379 due to no improvement.
pi: tensor([[0.9501, 0.0499],
        [0.9494, 0.0506]], dtype=torch.float64)
alpha: tensor([0.9500, 0.0500])
beta: tensor([[[0.1340, 0.1210],
         [0.0019, 0.1263]],

        [[0.1704, 0.1507],
         [0.4347, 0.1377]],

        [[0.5238, 0.1530],
         [0.8381, 0.7419]],

        [[0.1447, 0.0898],
         [0.4418, 0.2580]],

        [[0.1401, 0.1254],
         [0.7567, 0.2025]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22229.613696240453
Iteration 100: Loss = -9785.089489507718
Iteration 200: Loss = -9784.318550341
Iteration 300: Loss = -9784.073355881115
Iteration 400: Loss = -9783.915873694561
Iteration 500: Loss = -9783.797388913128
Iteration 600: Loss = -9783.69827004341
Iteration 700: Loss = -9783.609012048675
Iteration 800: Loss = -9783.51842885809
Iteration 900: Loss = -9783.423959662747
Iteration 1000: Loss = -9783.331537096958
Iteration 1100: Loss = -9783.24873643509
Iteration 1200: Loss = -9783.178661786345
Iteration 1300: Loss = -9783.1202498767
Iteration 1400: Loss = -9783.072017358118
Iteration 1500: Loss = -9783.031566068626
Iteration 1600: Loss = -9782.996814543516
Iteration 1700: Loss = -9782.966986461102
Iteration 1800: Loss = -9782.94151931755
Iteration 1900: Loss = -9782.92012295766
Iteration 2000: Loss = -9782.902621743133
Iteration 2100: Loss = -9782.888480166903
Iteration 2200: Loss = -9782.877284285743
Iteration 2300: Loss = -9782.868401912963
Iteration 2400: Loss = -9782.861245716316
Iteration 2500: Loss = -9782.855393976517
Iteration 2600: Loss = -9782.85058029323
Iteration 2700: Loss = -9782.846855470667
Iteration 2800: Loss = -9782.844068613122
Iteration 2900: Loss = -9782.842097742325
Iteration 3000: Loss = -9782.840667797995
Iteration 3100: Loss = -9782.839553664722
Iteration 3200: Loss = -9782.838600494206
Iteration 3300: Loss = -9782.837716590959
Iteration 3400: Loss = -9782.836879777025
Iteration 3500: Loss = -9782.836062801664
Iteration 3600: Loss = -9782.835192075137
Iteration 3700: Loss = -9782.834314620453
Iteration 3800: Loss = -9782.83346183766
Iteration 3900: Loss = -9782.83248905362
Iteration 4000: Loss = -9782.831505890246
Iteration 4100: Loss = -9782.830353115176
Iteration 4200: Loss = -9782.829086121183
Iteration 4300: Loss = -9782.827576637723
Iteration 4400: Loss = -9782.82570057562
Iteration 4500: Loss = -9782.823192252165
Iteration 4600: Loss = -9782.81945641216
Iteration 4700: Loss = -9782.813604601834
Iteration 4800: Loss = -9782.80452553197
Iteration 4900: Loss = -9782.793961502444
Iteration 5000: Loss = -9782.786412751984
Iteration 5100: Loss = -9782.782286948119
Iteration 5200: Loss = -9782.780088988811
Iteration 5300: Loss = -9782.778850640645
Iteration 5400: Loss = -9782.778004796643
Iteration 5500: Loss = -9782.777212194876
Iteration 5600: Loss = -9782.776775954448
Iteration 5700: Loss = -9782.775385364657
Iteration 5800: Loss = -9782.771115420821
Iteration 5900: Loss = -9782.767209787324
Iteration 6000: Loss = -9782.748777401399
Iteration 6100: Loss = -9782.726091723194
Iteration 6200: Loss = -9782.699283977057
Iteration 6300: Loss = -9782.667105440745
Iteration 6400: Loss = -9782.633494648006
Iteration 6500: Loss = -9782.621635176767
Iteration 6600: Loss = -9782.59165913052
Iteration 6700: Loss = -9782.577783306504
Iteration 6800: Loss = -9782.571024865762
Iteration 6900: Loss = -9782.562683879978
Iteration 7000: Loss = -9782.558101880415
Iteration 7100: Loss = -9782.5547401174
Iteration 7200: Loss = -9782.55218687859
Iteration 7300: Loss = -9782.551156787777
Iteration 7400: Loss = -9782.548724175711
Iteration 7500: Loss = -9782.562171503312
1
Iteration 7600: Loss = -9782.546516755132
Iteration 7700: Loss = -9782.545740891761
Iteration 7800: Loss = -9782.545063527537
Iteration 7900: Loss = -9782.54450787698
Iteration 8000: Loss = -9782.544111589623
Iteration 8100: Loss = -9782.546150587876
1
Iteration 8200: Loss = -9782.543431989103
Iteration 8300: Loss = -9782.543579712623
1
Iteration 8400: Loss = -9782.542727767763
Iteration 8500: Loss = -9782.542468416657
Iteration 8600: Loss = -9782.542668394099
1
Iteration 8700: Loss = -9782.542309686194
Iteration 8800: Loss = -9782.541891083994
Iteration 8900: Loss = -9782.542198029754
1
Iteration 9000: Loss = -9782.548096876608
2
Iteration 9100: Loss = -9782.549992911447
3
Iteration 9200: Loss = -9782.550584329354
4
Iteration 9300: Loss = -9782.621834832804
5
Iteration 9400: Loss = -9782.541153987137
Iteration 9500: Loss = -9782.543766434886
1
Iteration 9600: Loss = -9782.540929265824
Iteration 9700: Loss = -9782.54107001097
1
Iteration 9800: Loss = -9782.69650627866
2
Iteration 9900: Loss = -9782.54074103273
Iteration 10000: Loss = -9782.636260848196
1
Iteration 10100: Loss = -9782.540622446586
Iteration 10200: Loss = -9782.560480266964
1
Iteration 10300: Loss = -9782.540514517279
Iteration 10400: Loss = -9782.541172447898
1
Iteration 10500: Loss = -9782.547040524445
2
Iteration 10600: Loss = -9782.540401600885
Iteration 10700: Loss = -9782.540479852416
1
Iteration 10800: Loss = -9782.613491575243
2
Iteration 10900: Loss = -9782.54027842862
Iteration 11000: Loss = -9782.542508870632
1
Iteration 11100: Loss = -9782.58261968631
2
Iteration 11200: Loss = -9782.540201478561
Iteration 11300: Loss = -9782.541123722494
1
Iteration 11400: Loss = -9782.540151365078
Iteration 11500: Loss = -9782.541185506845
1
Iteration 11600: Loss = -9782.540093472002
Iteration 11700: Loss = -9782.543653603909
1
Iteration 11800: Loss = -9782.540265416928
2
Iteration 11900: Loss = -9782.706633777112
3
Iteration 12000: Loss = -9782.540046706694
Iteration 12100: Loss = -9782.589949715491
1
Iteration 12200: Loss = -9782.54279237512
2
Iteration 12300: Loss = -9782.552121569894
3
Iteration 12400: Loss = -9782.539965877837
Iteration 12500: Loss = -9782.539986405602
1
Iteration 12600: Loss = -9782.540063950968
2
Iteration 12700: Loss = -9782.540167866287
3
Iteration 12800: Loss = -9782.541132838802
4
Iteration 12900: Loss = -9782.612011185563
5
Iteration 13000: Loss = -9782.545156742135
6
Iteration 13100: Loss = -9782.623206704433
7
Iteration 13200: Loss = -9782.540267699895
8
Iteration 13300: Loss = -9782.540210647509
9
Iteration 13400: Loss = -9782.56003910207
10
Stopping early at iteration 13400 due to no improvement.
tensor([[-2.4109,  0.9782],
        [-3.0565,  0.8290],
        [-2.5162,  1.1188],
        [-2.3419,  0.9010],
        [-2.7772,  1.2994],
        [-2.3021,  0.7148],
        [-2.6702,  0.8969],
        [-3.0889,  0.9538],
        [-2.4674,  1.0807],
        [-2.5971,  1.1922],
        [-2.8660,  0.0705],
        [-3.3407,  1.1809],
        [-2.7194,  0.8934],
        [-2.5022,  0.7678],
        [-2.5013,  1.1045],
        [-2.6835,  1.2596],
        [-2.9820,  0.3033],
        [-2.5783,  1.0992],
        [-2.6151,  1.0394],
        [-3.5805, -0.5724],
        [-2.8535,  1.3851],
        [-2.3941,  1.0039],
        [-2.8278,  0.9794],
        [-3.0471,  1.2695],
        [-3.0429,  1.4121],
        [-3.5409, -0.4486],
        [-2.3945,  1.0081],
        [-2.5783,  1.0163],
        [-2.7779,  1.1773],
        [-2.3958,  0.9265],
        [-2.3899,  0.9925],
        [-2.7478,  0.7602],
        [-2.4829,  0.9206],
        [-3.1823,  1.6699],
        [-2.6394,  1.0562],
        [-3.6513,  0.4842],
        [-2.5950,  1.1936],
        [-2.8338,  1.3311],
        [-2.6921,  1.1160],
        [-2.8460,  0.4641],
        [-2.8168,  1.3456],
        [-3.0641,  0.9734],
        [-2.8576,  0.8526],
        [-3.9205, -0.3476],
        [-3.0940,  0.3917],
        [-2.4528,  1.0167],
        [-2.7218,  1.2773],
        [-2.8388,  1.3421],
        [-3.9696, -0.4686],
        [-2.6359,  1.2266],
        [-2.6403,  0.6110],
        [-3.3738,  0.8357],
        [-3.2135,  0.8302],
        [-2.3555,  0.9635],
        [-3.1694,  0.3191],
        [-3.4781,  0.6513],
        [-2.2528,  0.7850],
        [-2.3875,  0.9872],
        [-3.1327,  0.7861],
        [-2.4938,  1.0287],
        [-2.5584,  1.0460],
        [-2.6356,  1.2144],
        [-2.8756,  0.2149],
        [-3.2212,  0.6832],
        [-2.4095,  1.0091],
        [-2.8342,  0.1437],
        [-2.7661,  0.9134],
        [-3.3152,  1.2305],
        [-2.7686,  1.2814],
        [-3.3706,  0.5445],
        [-2.9959,  1.6061],
        [-2.4045,  0.9925],
        [-2.5423,  0.7625],
        [-2.5251,  1.0366],
        [-2.6627,  0.9383],
        [-3.2837,  0.2766],
        [-3.2358,  0.9319],
        [-2.7275,  0.7063],
        [-2.6023,  1.0048],
        [-2.5707,  1.1743],
        [-3.4276,  0.2938],
        [-2.9451,  0.8425],
        [-3.4820,  0.3346],
        [-2.3967,  1.0078],
        [-2.9883,  0.6677],
        [-2.6433,  1.2501],
        [-2.4841,  1.0978],
        [-3.0655,  0.8562],
        [-2.4654,  0.9546],
        [-3.1267,  0.5331],
        [-2.5323,  1.1458],
        [-3.0883,  0.6632],
        [-3.0558,  0.2290],
        [-2.6928,  1.2032],
        [-2.6385,  1.1618],
        [-2.5457,  1.1583],
        [-2.5942,  1.1700],
        [-2.6564,  1.2642],
        [-2.4195,  0.9133],
        [-2.3785,  0.6316]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 1.0381e-05],
        [7.0500e-01, 2.9500e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0255, 0.9745], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1346, 0.1223],
         [0.0019, 0.1342]],

        [[0.1704, 0.1408],
         [0.4347, 0.1377]],

        [[0.5238, 0.1533],
         [0.8381, 0.7419]],

        [[0.1447, 0.0684],
         [0.4418, 0.2580]],

        [[0.1401, 0.1054],
         [0.7567, 0.2025]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00031554881220337836
Average Adjusted Rand Index: 0.001211421106403762
Iteration 0: Loss = -14157.3648763407
Iteration 10: Loss = -9783.977479998783
Iteration 20: Loss = -9783.528792641218
Iteration 30: Loss = -9783.45770409325
Iteration 40: Loss = -9783.388265385793
Iteration 50: Loss = -9783.208311817198
Iteration 60: Loss = -9783.09527345824
Iteration 70: Loss = -9783.06143108014
Iteration 80: Loss = -9783.044930306307
Iteration 90: Loss = -9783.035543890774
Iteration 100: Loss = -9783.029956102706
Iteration 110: Loss = -9783.02649185959
Iteration 120: Loss = -9783.024255611233
Iteration 130: Loss = -9783.022849487688
Iteration 140: Loss = -9783.02187075987
Iteration 150: Loss = -9783.02127778206
Iteration 160: Loss = -9783.020830179548
Iteration 170: Loss = -9783.020509158416
Iteration 180: Loss = -9783.020281846673
Iteration 190: Loss = -9783.020142253794
Iteration 200: Loss = -9783.0200246661
Iteration 210: Loss = -9783.01994221447
Iteration 220: Loss = -9783.019888783065
Iteration 230: Loss = -9783.019826960615
Iteration 240: Loss = -9783.019823848606
Iteration 250: Loss = -9783.019780750968
Iteration 260: Loss = -9783.01975958641
Iteration 270: Loss = -9783.019746169604
Iteration 280: Loss = -9783.019748248056
1
Iteration 290: Loss = -9783.019730337344
Iteration 300: Loss = -9783.01969208052
Iteration 310: Loss = -9783.019700194855
1
Iteration 320: Loss = -9783.019709523445
2
Iteration 330: Loss = -9783.019704385117
3
Stopping early at iteration 329 due to no improvement.
pi: tensor([[0.0506, 0.9494],
        [0.0499, 0.9501]], dtype=torch.float64)
alpha: tensor([0.0500, 0.9500])
beta: tensor([[[0.1263, 0.1210],
         [0.6110, 0.1340]],

        [[0.9245, 0.1507],
         [0.2642, 0.6240]],

        [[0.7559, 0.1530],
         [0.4785, 0.3704]],

        [[0.7418, 0.0897],
         [0.1519, 0.2018]],

        [[0.0738, 0.1254],
         [0.5612, 0.5705]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14156.824414729486
Iteration 100: Loss = -9784.44596396535
Iteration 200: Loss = -9783.430912039232
Iteration 300: Loss = -9783.175063867058
Iteration 400: Loss = -9783.04493901962
Iteration 500: Loss = -9782.970184986296
Iteration 600: Loss = -9782.917266257231
Iteration 700: Loss = -9782.883747479838
Iteration 800: Loss = -9782.86453636993
Iteration 900: Loss = -9782.851934092592
Iteration 1000: Loss = -9782.843539610301
Iteration 1100: Loss = -9782.837775499047
Iteration 1200: Loss = -9782.83387675622
Iteration 1300: Loss = -9782.831173585437
Iteration 1400: Loss = -9782.829236071859
Iteration 1500: Loss = -9782.827705038773
Iteration 1600: Loss = -9782.826401795905
Iteration 1700: Loss = -9782.825206406551
Iteration 1800: Loss = -9782.824087904244
Iteration 1900: Loss = -9782.822897049575
Iteration 2000: Loss = -9782.821614232522
Iteration 2100: Loss = -9782.820128779747
Iteration 2200: Loss = -9782.818361267893
Iteration 2300: Loss = -9782.816213154778
Iteration 2400: Loss = -9782.813570336373
Iteration 2500: Loss = -9782.81039946288
Iteration 2600: Loss = -9782.806573524942
Iteration 2700: Loss = -9782.802191288747
Iteration 2800: Loss = -9782.797566753761
Iteration 2900: Loss = -9782.793200301607
Iteration 3000: Loss = -9782.789413164055
Iteration 3100: Loss = -9782.786416400773
Iteration 3200: Loss = -9782.784311528101
Iteration 3300: Loss = -9782.78586369329
1
Iteration 3400: Loss = -9782.783606531206
Iteration 3500: Loss = -9782.780685413283
Iteration 3600: Loss = -9782.78187313701
1
Iteration 3700: Loss = -9782.77953287799
Iteration 3800: Loss = -9782.77922270596
Iteration 3900: Loss = -9782.7792701069
1
Iteration 4000: Loss = -9782.778887305229
Iteration 4100: Loss = -9782.778744667421
Iteration 4200: Loss = -9782.782106072515
1
Iteration 4300: Loss = -9782.7786707408
Iteration 4400: Loss = -9782.778585881062
Iteration 4500: Loss = -9782.779004749014
1
Iteration 4600: Loss = -9782.778470288335
Iteration 4700: Loss = -9782.784569631664
1
Iteration 4800: Loss = -9782.780377857096
2
Iteration 4900: Loss = -9782.777815608666
Iteration 5000: Loss = -9782.777500865672
Iteration 5100: Loss = -9782.80133060955
1
Iteration 5200: Loss = -9782.779733626734
2
Iteration 5300: Loss = -9782.775546484554
Iteration 5400: Loss = -9782.772999158507
Iteration 5500: Loss = -9782.77192086529
Iteration 5600: Loss = -9782.75866326664
Iteration 5700: Loss = -9782.74151291998
Iteration 5800: Loss = -9782.723052244419
Iteration 5900: Loss = -9782.694062784207
Iteration 6000: Loss = -9782.664279457458
Iteration 6100: Loss = -9782.635079448879
Iteration 6200: Loss = -9782.608750968138
Iteration 6300: Loss = -9782.592152872796
Iteration 6400: Loss = -9782.579857910787
Iteration 6500: Loss = -9782.571035640274
Iteration 6600: Loss = -9782.567451410574
Iteration 6700: Loss = -9782.559683763584
Iteration 6800: Loss = -9782.556332778682
Iteration 6900: Loss = -9782.553359566311
Iteration 7000: Loss = -9782.551502138303
Iteration 7100: Loss = -9782.549571980533
Iteration 7200: Loss = -9782.557604434664
1
Iteration 7300: Loss = -9782.548817418969
Iteration 7400: Loss = -9782.546335400442
Iteration 7500: Loss = -9782.54562590994
Iteration 7600: Loss = -9782.545408027663
Iteration 7700: Loss = -9782.54467762134
Iteration 7800: Loss = -9782.544351535254
Iteration 7900: Loss = -9782.543670721227
Iteration 8000: Loss = -9782.556762692435
1
Iteration 8100: Loss = -9782.54303468166
Iteration 8200: Loss = -9782.544607599819
1
Iteration 8300: Loss = -9782.544706991204
2
Iteration 8400: Loss = -9782.542508548435
Iteration 8500: Loss = -9782.58221187689
1
Iteration 8600: Loss = -9782.541988063842
Iteration 8700: Loss = -9782.542241246892
1
Iteration 8800: Loss = -9782.545099295778
2
Iteration 8900: Loss = -9782.541541425677
Iteration 9000: Loss = -9782.552558002415
1
Iteration 9100: Loss = -9782.541298100083
Iteration 9200: Loss = -9782.541342665538
1
Iteration 9300: Loss = -9782.541334088857
2
Iteration 9400: Loss = -9782.541010015162
Iteration 9500: Loss = -9782.542120907854
1
Iteration 9600: Loss = -9782.54607598432
2
Iteration 9700: Loss = -9782.5412655546
3
Iteration 9800: Loss = -9782.542228880002
4
Iteration 9900: Loss = -9782.543670523073
5
Iteration 10000: Loss = -9782.540676372397
Iteration 10100: Loss = -9782.5655903355
1
Iteration 10200: Loss = -9782.550939208964
2
Iteration 10300: Loss = -9782.54755579461
3
Iteration 10400: Loss = -9782.549096796902
4
Iteration 10500: Loss = -9782.598961877282
5
Iteration 10600: Loss = -9782.55705557412
6
Iteration 10700: Loss = -9782.613979530412
7
Iteration 10800: Loss = -9782.55785362716
8
Iteration 10900: Loss = -9782.652840459326
9
Iteration 11000: Loss = -9782.547175225489
10
Stopping early at iteration 11000 due to no improvement.
tensor([[ 1.0087e+00, -2.4025e+00],
        [ 1.2550e+00, -2.6516e+00],
        [ 9.9041e-01, -2.6626e+00],
        [-3.2080e-01, -3.5825e+00],
        [ 1.1803e+00, -2.9172e+00],
        [ 6.9619e-01, -2.3344e+00],
        [ 1.0839e+00, -2.5112e+00],
        [ 1.1521e+00, -2.9116e+00],
        [ 1.0621e+00, -2.5102e+00],
        [ 1.2147e+00, -2.6011e+00],
        [-8.3297e-01, -3.7823e+00],
        [ 1.5026e+00, -3.0399e+00],
        [ 3.8083e-01, -3.2591e+00],
        [ 8.9643e-01, -2.3942e+00],
        [ 8.9730e-01, -2.7340e+00],
        [ 1.2600e+00, -2.7040e+00],
        [ 2.6874e-01, -3.0340e+00],
        [ 1.0314e+00, -2.6739e+00],
        [ 7.1082e-01, -2.9677e+00],
        [ 7.3927e-01, -2.2827e+00],
        [ 1.4267e+00, -2.8194e+00],
        [ 6.6184e-01, -2.7566e+00],
        [ 1.0239e+00, -2.8090e+00],
        [ 1.4211e+00, -2.9167e+00],
        [ 1.5378e+00, -2.9242e+00],
        [ 5.2055e-01, -2.5873e+00],
        [ 8.8521e-01, -2.5373e+00],
        [ 1.1124e+00, -2.5075e+00],
        [ 1.2902e+00, -2.6863e+00],
        [ 6.9558e-01, -2.6465e+00],
        [ 1.0068e+00, -2.3957e+00],
        [ 9.0289e-01, -2.6329e+00],
        [ 1.0148e+00, -2.4104e+00],
        [ 1.7446e+00, -3.1310e+00],
        [ 9.3913e-01, -2.7804e+00],
        [ 1.2987e+00, -2.8545e+00],
        [ 1.1515e+00, -2.6587e+00],
        [ 8.9186e-01, -3.2938e+00],
        [ 1.0643e+00, -2.7696e+00],
        [ 9.6226e-01, -2.3700e+00],
        [ 1.3713e+00, -2.8119e+00],
        [-1.4038e-01, -4.1992e+00],
        [-4.3897e-01, -4.1762e+00],
        [ 9.6109e-01, -2.6372e+00],
        [ 4.6425e-01, -3.0446e+00],
        [ 8.8783e-01, -2.6044e+00],
        [ 9.7084e-01, -3.0500e+00],
        [ 9.7337e-01, -3.2286e+00],
        [ 9.9474e-01, -2.5284e+00],
        [ 9.8248e-01, -2.9031e+00],
        [ 8.7023e-01, -2.3986e+00],
        [ 1.6466e-01, -4.0661e+00],
        [ 1.3121e+00, -2.7528e+00],
        [ 8.6345e-01, -2.4750e+00],
        [ 6.8672e-01, -2.8225e+00],
        [ 1.3804e+00, -2.7689e+00],
        [ 6.2137e-01, -2.4306e+00],
        [ 9.8492e-01, -2.4142e+00],
        [ 1.2748e+00, -2.6678e+00],
        [ 1.0696e+00, -2.4749e+00],
        [ 5.3438e-01, -3.0945e+00],
        [ 1.1848e+00, -2.6875e+00],
        [ 7.5751e-01, -2.3481e+00],
        [ 1.2626e+00, -2.6647e+00],
        [ 9.3086e-01, -2.5030e+00],
        [ 7.0240e-01, -2.2899e+00],
        [ 9.7497e-01, -2.7302e+00],
        [ 1.2110e+00, -3.3583e+00],
        [ 6.4129e-01, -3.4280e+00],
        [ 7.6282e-01, -3.1758e+00],
        [ 1.4571e+00, -3.1681e+00],
        [ 3.3914e-01, -3.0786e+00],
        [ 8.3610e-01, -2.4887e+00],
        [ 9.4225e-01, -2.6461e+00],
        [ 1.0464e+00, -2.5813e+00],
        [ 1.0497e+00, -2.5396e+00],
        [ 1.1608e+00, -3.0257e+00],
        [ 7.5655e-01, -2.7018e+00],
        [ 9.5273e-01, -2.6854e+00],
        [ 1.1603e+00, -2.6096e+00],
        [ 1.1676e+00, -2.5801e+00],
        [ 1.2108e+00, -2.5987e+00],
        [ 1.2280e+00, -2.6161e+00],
        [ 8.6132e-01, -2.5618e+00],
        [ 7.9784e-01, -2.8895e+00],
        [ 1.2541e+00, -2.6643e+00],
        [ 3.9714e-01, -3.2119e+00],
        [ 1.1046e+00, -2.8408e+00],
        [ 1.0155e+00, -2.4292e+00],
        [ 8.9071e-01, -2.7979e+00],
        [ 1.1560e+00, -2.5474e+00],
        [-4.1999e-01, -4.1952e+00],
        [ 9.5752e-01, -2.3444e+00],
        [ 1.0936e+00, -2.8217e+00],
        [ 1.1636e+00, -2.6582e+00],
        [-2.4739e-04, -3.7335e+00],
        [ 8.5970e-01, -2.9261e+00],
        [ 1.2108e+00, -2.7296e+00],
        [ 1.0382e-01, -3.2508e+00],
        [ 8.0259e-01, -2.2214e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[3.0384e-01, 6.9616e-01],
        [3.4553e-05, 9.9997e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9745, 0.0255], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1344, 0.1221],
         [0.6110, 0.1347]],

        [[0.9245, 0.1406],
         [0.2642, 0.6240]],

        [[0.7559, 0.1525],
         [0.4785, 0.3704]],

        [[0.7418, 0.0692],
         [0.1519, 0.2018]],

        [[0.0738, 0.1056],
         [0.5612, 0.5705]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00031554881220337836
Average Adjusted Rand Index: 0.001211421106403762
9903.887632493004
new:  [0.0, 0.0, -0.00031554881220337836, -0.00031554881220337836] [0.0, 0.0, 0.001211421106403762, 0.001211421106403762] [9782.778122656708, 9782.778647698584, 9782.56003910207, 9782.547175225489]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [9783.0196749965, 9783.01967551796, 9783.019703595248, 9783.019704385117]
-----------------------------------------------------------------------------------------
This iteration is 17
True Objective function: Loss = -9972.982863569872
Iteration 0: Loss = -23711.12667172934
Iteration 10: Loss = -9798.733027976727
Iteration 20: Loss = -9798.521829840874
Iteration 30: Loss = -9798.369480001575
Iteration 40: Loss = -9798.282386383227
Iteration 50: Loss = -9798.237313834929
Iteration 60: Loss = -9798.206938648214
Iteration 70: Loss = -9798.172033705763
Iteration 80: Loss = -9798.115510393738
Iteration 90: Loss = -9798.039769995314
Iteration 100: Loss = -9797.971994139729
Iteration 110: Loss = -9797.912656587656
Iteration 120: Loss = -9797.854790409674
Iteration 130: Loss = -9797.797551440703
Iteration 140: Loss = -9797.743779645014
Iteration 150: Loss = -9797.694807158381
Iteration 160: Loss = -9797.650129787295
Iteration 170: Loss = -9797.608896335405
Iteration 180: Loss = -9797.570511597376
Iteration 190: Loss = -9797.534510302867
Iteration 200: Loss = -9797.500606606522
Iteration 210: Loss = -9797.468579508124
Iteration 220: Loss = -9797.438185848905
Iteration 230: Loss = -9797.409251647594
Iteration 240: Loss = -9797.38170384877
Iteration 250: Loss = -9797.35539007904
Iteration 260: Loss = -9797.330195959325
Iteration 270: Loss = -9797.306048988346
Iteration 280: Loss = -9797.282870645278
Iteration 290: Loss = -9797.260598852205
Iteration 300: Loss = -9797.239157150892
Iteration 310: Loss = -9797.218524774953
Iteration 320: Loss = -9797.19858052303
Iteration 330: Loss = -9797.179334931156
Iteration 340: Loss = -9797.160822220383
Iteration 350: Loss = -9797.142863605377
Iteration 360: Loss = -9797.12553149582
Iteration 370: Loss = -9797.108719249562
Iteration 380: Loss = -9797.092543678018
Iteration 390: Loss = -9797.076824835738
Iteration 400: Loss = -9797.061618932183
Iteration 410: Loss = -9797.046890160022
Iteration 420: Loss = -9797.032597709926
Iteration 430: Loss = -9797.018761889443
Iteration 440: Loss = -9797.005402039187
Iteration 450: Loss = -9796.992347161071
Iteration 460: Loss = -9796.979757614643
Iteration 470: Loss = -9796.967557439135
Iteration 480: Loss = -9796.955664359053
Iteration 490: Loss = -9796.944169502727
Iteration 500: Loss = -9796.93299859802
Iteration 510: Loss = -9796.922221100745
Iteration 520: Loss = -9796.911736704364
Iteration 530: Loss = -9796.901552164065
Iteration 540: Loss = -9796.891690700848
Iteration 550: Loss = -9796.882189864884
Iteration 560: Loss = -9796.872837189525
Iteration 570: Loss = -9796.863878583674
Iteration 580: Loss = -9796.855098836857
Iteration 590: Loss = -9796.846633691366
Iteration 600: Loss = -9796.838438583833
Iteration 610: Loss = -9796.830454662959
Iteration 620: Loss = -9796.822746853
Iteration 630: Loss = -9796.815202970787
Iteration 640: Loss = -9796.808001269828
Iteration 650: Loss = -9796.80089098052
Iteration 660: Loss = -9796.794059945138
Iteration 670: Loss = -9796.787474343377
Iteration 680: Loss = -9796.780986916965
Iteration 690: Loss = -9796.774803091028
Iteration 700: Loss = -9796.76872904123
Iteration 710: Loss = -9796.762839037341
Iteration 720: Loss = -9796.757208419722
Iteration 730: Loss = -9796.751697438765
Iteration 740: Loss = -9796.746336113694
Iteration 750: Loss = -9796.741160932414
Iteration 760: Loss = -9796.73616592253
Iteration 770: Loss = -9796.731283619167
Iteration 780: Loss = -9796.726580703835
Iteration 790: Loss = -9796.721980409706
Iteration 800: Loss = -9796.717563759576
Iteration 810: Loss = -9796.71326169161
Iteration 820: Loss = -9796.709120224068
Iteration 830: Loss = -9796.70503861449
Iteration 840: Loss = -9796.701133488901
Iteration 850: Loss = -9796.697339468581
Iteration 860: Loss = -9796.693638225188
Iteration 870: Loss = -9796.690075107897
Iteration 880: Loss = -9796.686639367488
Iteration 890: Loss = -9796.683270052692
Iteration 900: Loss = -9796.680014202811
Iteration 910: Loss = -9796.676925416421
Iteration 920: Loss = -9796.673818266654
Iteration 930: Loss = -9796.670865972856
Iteration 940: Loss = -9796.668018798971
Iteration 950: Loss = -9796.66521907016
Iteration 960: Loss = -9796.662536875661
Iteration 970: Loss = -9796.65989880452
Iteration 980: Loss = -9796.657375832077
Iteration 990: Loss = -9796.65489008296
Iteration 1000: Loss = -9796.652574860356
Iteration 1010: Loss = -9796.650244046692
Iteration 1020: Loss = -9796.64799615131
Iteration 1030: Loss = -9796.645858890633
Iteration 1040: Loss = -9796.643726160784
Iteration 1050: Loss = -9796.641665598368
Iteration 1060: Loss = -9796.639681782199
Iteration 1070: Loss = -9796.637780515563
Iteration 1080: Loss = -9796.63589159345
Iteration 1090: Loss = -9796.634105513776
Iteration 1100: Loss = -9796.63236343127
Iteration 1110: Loss = -9796.630642330005
Iteration 1120: Loss = -9796.62902530589
Iteration 1130: Loss = -9796.62741823879
Iteration 1140: Loss = -9796.625862579229
Iteration 1150: Loss = -9796.624354223823
Iteration 1160: Loss = -9796.622901899624
Iteration 1170: Loss = -9796.621483069364
Iteration 1180: Loss = -9796.620137949934
Iteration 1190: Loss = -9796.618813217603
Iteration 1200: Loss = -9796.617494907243
Iteration 1210: Loss = -9796.61628486384
Iteration 1220: Loss = -9796.615047805763
Iteration 1230: Loss = -9796.613891460456
Iteration 1240: Loss = -9796.612718476614
Iteration 1250: Loss = -9796.611645120232
Iteration 1260: Loss = -9796.610551562804
Iteration 1270: Loss = -9796.609520662221
Iteration 1280: Loss = -9796.608517090168
Iteration 1290: Loss = -9796.607513922549
Iteration 1300: Loss = -9796.606573256513
Iteration 1310: Loss = -9796.605615794108
Iteration 1320: Loss = -9796.60479749563
Iteration 1330: Loss = -9796.603907040624
Iteration 1340: Loss = -9796.603033087076
Iteration 1350: Loss = -9796.602220608007
Iteration 1360: Loss = -9796.60145522115
Iteration 1370: Loss = -9796.600693593802
Iteration 1380: Loss = -9796.599943092964
Iteration 1390: Loss = -9796.599187894297
Iteration 1400: Loss = -9796.59850937936
Iteration 1410: Loss = -9796.597811470163
Iteration 1420: Loss = -9796.597172147964
Iteration 1430: Loss = -9796.596543131041
Iteration 1440: Loss = -9796.59590716619
Iteration 1450: Loss = -9796.595302041453
Iteration 1460: Loss = -9796.594700646974
Iteration 1470: Loss = -9796.594133084915
Iteration 1480: Loss = -9796.593599856147
Iteration 1490: Loss = -9796.593061195763
Iteration 1500: Loss = -9796.592548291877
Iteration 1510: Loss = -9796.59200848126
Iteration 1520: Loss = -9796.591565338409
Iteration 1530: Loss = -9796.591076626193
Iteration 1540: Loss = -9796.59061725113
Iteration 1550: Loss = -9796.590154535625
Iteration 1560: Loss = -9796.589736132692
Iteration 1570: Loss = -9796.589303253633
Iteration 1580: Loss = -9796.588878204357
Iteration 1590: Loss = -9796.588522477663
Iteration 1600: Loss = -9796.588047919873
Iteration 1610: Loss = -9796.587730150684
Iteration 1620: Loss = -9796.587356569366
Iteration 1630: Loss = -9796.586985241915
Iteration 1640: Loss = -9796.586658869317
Iteration 1650: Loss = -9796.586321290626
Iteration 1660: Loss = -9796.585979383406
Iteration 1670: Loss = -9796.585701810665
Iteration 1680: Loss = -9796.585415776659
Iteration 1690: Loss = -9796.585086494906
Iteration 1700: Loss = -9796.58481480735
Iteration 1710: Loss = -9796.584505799427
Iteration 1720: Loss = -9796.58426336038
Iteration 1730: Loss = -9796.583993891476
Iteration 1740: Loss = -9796.583740218108
Iteration 1750: Loss = -9796.58346375923
Iteration 1760: Loss = -9796.583265516985
Iteration 1770: Loss = -9796.5830143258
Iteration 1780: Loss = -9796.582803781117
Iteration 1790: Loss = -9796.582582355206
Iteration 1800: Loss = -9796.582375805912
Iteration 1810: Loss = -9796.582161862641
Iteration 1820: Loss = -9796.5819637876
Iteration 1830: Loss = -9796.581733063322
Iteration 1840: Loss = -9796.58155881955
Iteration 1850: Loss = -9796.581357110901
Iteration 1860: Loss = -9796.581181530526
Iteration 1870: Loss = -9796.58103473227
Iteration 1880: Loss = -9796.580894371764
Iteration 1890: Loss = -9796.58069359209
Iteration 1900: Loss = -9796.580543742386
Iteration 1910: Loss = -9796.580347799129
Iteration 1920: Loss = -9796.580253040427
Iteration 1930: Loss = -9796.580099291272
Iteration 1940: Loss = -9796.579939339736
Iteration 1950: Loss = -9796.57980956594
Iteration 1960: Loss = -9796.579668427574
Iteration 1970: Loss = -9796.579550743745
Iteration 1980: Loss = -9796.579396711128
Iteration 1990: Loss = -9796.579298925328
Iteration 2000: Loss = -9796.57918481372
Iteration 2010: Loss = -9796.579070311716
Iteration 2020: Loss = -9796.578960629848
Iteration 2030: Loss = -9796.578846701796
Iteration 2040: Loss = -9796.578744405248
Iteration 2050: Loss = -9796.578628773836
Iteration 2060: Loss = -9796.578549580707
Iteration 2070: Loss = -9796.5784215513
Iteration 2080: Loss = -9796.578350520595
Iteration 2090: Loss = -9796.578259259728
Iteration 2100: Loss = -9796.57818898781
Iteration 2110: Loss = -9796.578088792676
Iteration 2120: Loss = -9796.578026303721
Iteration 2130: Loss = -9796.577895766737
Iteration 2140: Loss = -9796.577827685789
Iteration 2150: Loss = -9796.57775660433
Iteration 2160: Loss = -9796.577680623845
Iteration 2170: Loss = -9796.577602539015
Iteration 2180: Loss = -9796.577549385953
Iteration 2190: Loss = -9796.577513314518
Iteration 2200: Loss = -9796.577443657001
Iteration 2210: Loss = -9796.577367013655
Iteration 2220: Loss = -9796.577301170946
Iteration 2230: Loss = -9796.577226223424
Iteration 2240: Loss = -9796.57714018292
Iteration 2250: Loss = -9796.577127060898
Iteration 2260: Loss = -9796.577099271179
Iteration 2270: Loss = -9796.577033363055
Iteration 2280: Loss = -9796.576975552252
Iteration 2290: Loss = -9796.576900469554
Iteration 2300: Loss = -9796.57687618771
Iteration 2310: Loss = -9796.57680590703
Iteration 2320: Loss = -9796.57676877504
Iteration 2330: Loss = -9796.576709170542
Iteration 2340: Loss = -9796.57669203007
Iteration 2350: Loss = -9796.576664567823
Iteration 2360: Loss = -9796.576570390112
Iteration 2370: Loss = -9796.576563082688
Iteration 2380: Loss = -9796.576530525963
Iteration 2390: Loss = -9796.576464563683
Iteration 2400: Loss = -9796.576416016398
Iteration 2410: Loss = -9796.576417661401
1
Iteration 2420: Loss = -9796.576339613213
Iteration 2430: Loss = -9796.576368763126
1
Iteration 2440: Loss = -9796.576290862815
Iteration 2450: Loss = -9796.576259785394
Iteration 2460: Loss = -9796.576235091534
Iteration 2470: Loss = -9796.576207905488
Iteration 2480: Loss = -9796.576198930949
Iteration 2490: Loss = -9796.576147467202
Iteration 2500: Loss = -9796.576119200954
Iteration 2510: Loss = -9796.576104297586
Iteration 2520: Loss = -9796.576051785134
Iteration 2530: Loss = -9796.576043479508
Iteration 2540: Loss = -9796.576024152946
Iteration 2550: Loss = -9796.575996675485
Iteration 2560: Loss = -9796.575960114917
Iteration 2570: Loss = -9796.575962143253
1
Iteration 2580: Loss = -9796.57594050093
Iteration 2590: Loss = -9796.575933471284
Iteration 2600: Loss = -9796.57588455037
Iteration 2610: Loss = -9796.575879499176
Iteration 2620: Loss = -9796.575856932071
Iteration 2630: Loss = -9796.575834363084
Iteration 2640: Loss = -9796.575831202557
Iteration 2650: Loss = -9796.57579451154
Iteration 2660: Loss = -9796.57574994786
Iteration 2670: Loss = -9796.575759220594
1
Iteration 2680: Loss = -9796.57576872796
2
Iteration 2690: Loss = -9796.575727875996
Iteration 2700: Loss = -9796.575726464576
Iteration 2710: Loss = -9796.575730797003
1
Iteration 2720: Loss = -9796.575678087442
Iteration 2730: Loss = -9796.575680544807
1
Iteration 2740: Loss = -9796.575648735958
Iteration 2750: Loss = -9796.575695589107
1
Iteration 2760: Loss = -9796.575678795933
2
Iteration 2770: Loss = -9796.575642904902
Iteration 2780: Loss = -9796.575628964558
Iteration 2790: Loss = -9796.575631276892
1
Iteration 2800: Loss = -9796.575574331137
Iteration 2810: Loss = -9796.575572387907
Iteration 2820: Loss = -9796.575567932648
Iteration 2830: Loss = -9796.575547233193
Iteration 2840: Loss = -9796.57555164925
1
Iteration 2850: Loss = -9796.575557008991
2
Iteration 2860: Loss = -9796.575534630838
Iteration 2870: Loss = -9796.575515369412
Iteration 2880: Loss = -9796.575512383035
Iteration 2890: Loss = -9796.575541192931
1
Iteration 2900: Loss = -9796.575501362371
Iteration 2910: Loss = -9796.575496699392
Iteration 2920: Loss = -9796.575487334427
Iteration 2930: Loss = -9796.575451249595
Iteration 2940: Loss = -9796.575472222885
1
Iteration 2950: Loss = -9796.575487974058
2
Iteration 2960: Loss = -9796.575460043026
3
Stopping early at iteration 2959 due to no improvement.
pi: tensor([[0.6613, 0.3387],
        [0.6867, 0.3133]], dtype=torch.float64)
alpha: tensor([0.6687, 0.3313])
beta: tensor([[[0.1160, 0.1463],
         [0.2302, 0.1727]],

        [[0.8631, 0.1414],
         [0.4215, 0.7601]],

        [[0.7715, 0.1367],
         [0.0131, 0.0938]],

        [[0.7018, 0.1428],
         [0.3019, 0.4328]],

        [[0.5294, 0.1424],
         [0.0524, 0.3101]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.02379663096291535
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.007069157284353335
time is 2
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 65
Adjusted Rand Index: 0.07904845352107101
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.03208286804600833
time is 4
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.024979443551724323
Global Adjusted Rand Index: 0.02893078709592487
Average Adjusted Rand Index: 0.030567647759473137
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23870.555619701743
Iteration 100: Loss = -9801.907185121616
Iteration 200: Loss = -9800.65511731604
Iteration 300: Loss = -9800.114813556118
Iteration 400: Loss = -9799.782349375842
Iteration 500: Loss = -9799.552857518498
Iteration 600: Loss = -9799.38113268012
Iteration 700: Loss = -9799.242596825403
Iteration 800: Loss = -9799.126554139735
Iteration 900: Loss = -9799.032651292995
Iteration 1000: Loss = -9798.968690375785
Iteration 1100: Loss = -9798.932001500423
Iteration 1200: Loss = -9798.911476290636
Iteration 1300: Loss = -9798.89899454194
Iteration 1400: Loss = -9798.890281169039
Iteration 1500: Loss = -9798.88342568766
Iteration 1600: Loss = -9798.87736950773
Iteration 1700: Loss = -9798.871808867205
Iteration 1800: Loss = -9798.866575406872
Iteration 1900: Loss = -9798.861552847593
Iteration 2000: Loss = -9798.856685115808
Iteration 2100: Loss = -9798.851885352733
Iteration 2200: Loss = -9798.847226378428
Iteration 2300: Loss = -9798.842599360023
Iteration 2400: Loss = -9798.837945045465
Iteration 2500: Loss = -9798.833261326425
Iteration 2600: Loss = -9798.828436827693
Iteration 2700: Loss = -9798.823480645417
Iteration 2800: Loss = -9798.818266894898
Iteration 2900: Loss = -9798.812690263901
Iteration 3000: Loss = -9798.806711136634
Iteration 3100: Loss = -9798.800176088318
Iteration 3200: Loss = -9798.793007685066
Iteration 3300: Loss = -9798.785062741234
Iteration 3400: Loss = -9798.776424444502
Iteration 3500: Loss = -9798.766825884442
Iteration 3600: Loss = -9798.756343820449
Iteration 3700: Loss = -9798.744823294177
Iteration 3800: Loss = -9798.732264336077
Iteration 3900: Loss = -9798.718378924155
Iteration 4000: Loss = -9798.703124733336
Iteration 4100: Loss = -9798.686435774602
Iteration 4200: Loss = -9798.669193086675
Iteration 4300: Loss = -9798.652392813698
Iteration 4400: Loss = -9798.63711714509
Iteration 4500: Loss = -9798.624325251374
Iteration 4600: Loss = -9798.614690583452
Iteration 4700: Loss = -9798.60793499046
Iteration 4800: Loss = -9798.603114474226
Iteration 4900: Loss = -9798.600269861023
Iteration 5000: Loss = -9798.598211670545
Iteration 5100: Loss = -9798.596509610861
Iteration 5200: Loss = -9798.593744347694
Iteration 5300: Loss = -9798.592719278346
Iteration 5400: Loss = -9798.596822704993
1
Iteration 5500: Loss = -9798.590885091256
Iteration 5600: Loss = -9798.619821205524
1
Iteration 5700: Loss = -9798.589672133818
Iteration 5800: Loss = -9798.589141094917
Iteration 5900: Loss = -9798.610988299382
1
Iteration 6000: Loss = -9798.588266206738
Iteration 6100: Loss = -9798.587870093266
Iteration 6200: Loss = -9798.587482023675
Iteration 6300: Loss = -9798.587262027888
Iteration 6400: Loss = -9798.58664649998
Iteration 6500: Loss = -9798.586227924156
Iteration 6600: Loss = -9798.587412247061
1
Iteration 6700: Loss = -9798.585422981938
Iteration 6800: Loss = -9798.585031040244
Iteration 6900: Loss = -9798.62263142786
1
Iteration 7000: Loss = -9798.584273490547
Iteration 7100: Loss = -9798.583942166932
Iteration 7200: Loss = -9798.693309077877
1
Iteration 7300: Loss = -9798.583310179867
Iteration 7400: Loss = -9798.583059308987
Iteration 7500: Loss = -9798.600643739557
1
Iteration 7600: Loss = -9798.582664718377
Iteration 7700: Loss = -9798.582448790417
Iteration 7800: Loss = -9798.582278933365
Iteration 7900: Loss = -9798.582217817408
Iteration 8000: Loss = -9798.582033189405
Iteration 8100: Loss = -9798.581927714631
Iteration 8200: Loss = -9798.587209666797
1
Iteration 8300: Loss = -9798.581753543605
Iteration 8400: Loss = -9798.581665537535
Iteration 8500: Loss = -9798.581604086263
Iteration 8600: Loss = -9798.58178136848
1
Iteration 8700: Loss = -9798.581443662293
Iteration 8800: Loss = -9798.581831085728
1
Iteration 8900: Loss = -9798.58132300652
Iteration 9000: Loss = -9798.5814289299
1
Iteration 9100: Loss = -9798.581187109916
Iteration 9200: Loss = -9798.5811087672
Iteration 9300: Loss = -9798.581088294375
Iteration 9400: Loss = -9798.58102631475
Iteration 9500: Loss = -9798.744073367394
1
Iteration 9600: Loss = -9798.580874837242
Iteration 9700: Loss = -9798.584957489158
1
Iteration 9800: Loss = -9798.583699845878
2
Iteration 9900: Loss = -9798.580879564977
3
Iteration 10000: Loss = -9798.581147379924
4
Iteration 10100: Loss = -9798.591009869888
5
Iteration 10200: Loss = -9798.580573839869
Iteration 10300: Loss = -9798.61748115995
1
Iteration 10400: Loss = -9798.580486045996
Iteration 10500: Loss = -9798.91620571157
1
Iteration 10600: Loss = -9798.580425543321
Iteration 10700: Loss = -9798.580376052356
Iteration 10800: Loss = -9798.586903463742
1
Iteration 10900: Loss = -9798.580590170273
2
Iteration 11000: Loss = -9798.580236156105
Iteration 11100: Loss = -9798.591175071802
1
Iteration 11200: Loss = -9798.580188317039
Iteration 11300: Loss = -9798.580706572156
1
Iteration 11400: Loss = -9798.580117599064
Iteration 11500: Loss = -9798.581440090804
1
Iteration 11600: Loss = -9798.580293348372
2
Iteration 11700: Loss = -9798.580069006732
Iteration 11800: Loss = -9798.679079476831
1
Iteration 11900: Loss = -9798.58001012733
Iteration 12000: Loss = -9798.581892576734
1
Iteration 12100: Loss = -9798.579989364609
Iteration 12200: Loss = -9798.579995484582
1
Iteration 12300: Loss = -9798.579947227889
Iteration 12400: Loss = -9798.579985776649
1
Iteration 12500: Loss = -9798.583342851449
2
Iteration 12600: Loss = -9798.580336311887
3
Iteration 12700: Loss = -9798.57995523296
4
Iteration 12800: Loss = -9798.580535875211
5
Iteration 12900: Loss = -9798.579903280712
Iteration 13000: Loss = -9798.580028299804
1
Iteration 13100: Loss = -9798.672306094051
2
Iteration 13200: Loss = -9798.57986211264
Iteration 13300: Loss = -9798.638120506903
1
Iteration 13400: Loss = -9798.57980754885
Iteration 13500: Loss = -9798.57985626486
1
Iteration 13600: Loss = -9798.57985352331
2
Iteration 13700: Loss = -9798.579808512566
3
Iteration 13800: Loss = -9798.581722613531
4
Iteration 13900: Loss = -9798.57978530187
Iteration 14000: Loss = -9798.579788262023
1
Iteration 14100: Loss = -9798.580065487251
2
Iteration 14200: Loss = -9798.579751801459
Iteration 14300: Loss = -9798.584700873404
1
Iteration 14400: Loss = -9798.5797935939
2
Iteration 14500: Loss = -9798.580838554137
3
Iteration 14600: Loss = -9798.590302421695
4
Iteration 14700: Loss = -9798.579800813946
5
Iteration 14800: Loss = -9798.717210791061
6
Iteration 14900: Loss = -9798.579774578251
7
Iteration 15000: Loss = -9798.579771534649
8
Iteration 15100: Loss = -9798.579770190247
9
Iteration 15200: Loss = -9798.579764196698
10
Stopping early at iteration 15200 due to no improvement.
tensor([[-1.0943, -3.5209],
        [-1.1516, -3.4636],
        [-1.2713, -3.3439],
        [-1.1470, -3.4682],
        [-1.0932, -3.5220],
        [-1.0518, -3.5635],
        [-1.2849, -3.3303],
        [-1.1213, -3.4939],
        [-1.3002, -3.3150],
        [-1.3628, -3.2524],
        [-1.5152, -3.1000],
        [-0.9953, -3.6199],
        [-1.1418, -3.4735],
        [-1.3947, -3.2206],
        [-1.3618, -3.2534],
        [-1.4509, -3.1643],
        [-1.2035, -3.4117],
        [-1.3143, -3.3009],
        [-1.1112, -3.5040],
        [-1.2391, -3.3761],
        [-0.8863, -3.7290],
        [-1.1052, -3.5100],
        [-1.0470, -3.5682],
        [-1.0182, -3.5970],
        [-1.1090, -3.5062],
        [-1.4295, -3.1857],
        [-1.1697, -3.4455],
        [-1.1166, -3.4986],
        [-0.8063, -3.8089],
        [-1.5179, -3.0973],
        [-1.3662, -3.2491],
        [-1.4838, -3.1314],
        [-1.4772, -3.1380],
        [-1.5862, -3.0291],
        [-1.3050, -3.3102],
        [-1.2117, -3.4035],
        [-1.3013, -3.3139],
        [-1.0987, -3.5165],
        [-1.2554, -3.3598],
        [-0.9228, -3.6924],
        [-1.2026, -3.4126],
        [-1.5224, -3.0928],
        [-1.3413, -3.2739],
        [-1.5177, -3.0975],
        [-1.5291, -3.0861],
        [-1.1463, -3.4689],
        [-1.4995, -3.1158],
        [-1.2537, -3.3615],
        [-1.1330, -3.4822],
        [-1.1989, -3.4163],
        [-1.5876, -3.0276],
        [-1.3420, -3.2732],
        [-1.1935, -3.4217],
        [-1.0441, -3.5711],
        [-1.4882, -3.1270],
        [-1.3095, -3.3057],
        [-1.6372, -2.9780],
        [-1.0830, -3.5322],
        [-1.1532, -3.4620],
        [-1.5520, -3.0632],
        [-1.2094, -3.4058],
        [-1.3884, -3.2268],
        [-1.6225, -2.9927],
        [-1.2202, -3.3950],
        [-1.5001, -3.1151],
        [-1.4657, -3.1496],
        [-1.6520, -2.9632],
        [-1.3789, -3.2363],
        [-1.7641, -2.8511],
        [-0.9921, -3.6231],
        [-1.3953, -3.2200],
        [-1.3186, -3.2966],
        [-1.3630, -3.2522],
        [-1.5311, -3.0841],
        [-1.3708, -3.2445],
        [-1.3893, -3.2259],
        [-1.2056, -3.4096],
        [-1.0954, -3.5199],
        [-1.2461, -3.3691],
        [-1.0968, -3.5184],
        [-1.0993, -3.5159],
        [-1.4734, -3.1418],
        [-1.3390, -3.2762],
        [-1.2429, -3.3723],
        [-1.7468, -2.8684],
        [-1.3185, -3.2967],
        [-1.0718, -3.5434],
        [-1.0982, -3.5170],
        [-1.3186, -3.2966],
        [-1.6366, -2.9786],
        [-1.1043, -3.5110],
        [-1.6141, -3.0011],
        [-1.1281, -3.4871],
        [-1.1664, -3.4488],
        [-1.5540, -3.0612],
        [-0.8936, -3.7216],
        [-1.3381, -3.2771],
        [-1.4644, -3.1508],
        [-1.0520, -3.5632],
        [-1.6015, -3.0137]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.2484e-07],
        [8.7281e-01, 1.2719e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8779, 0.1221], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1346, 0.1479],
         [0.2302, 0.1589]],

        [[0.8631, 0.0986],
         [0.4215, 0.7601]],

        [[0.7715, 0.1734],
         [0.0131, 0.0938]],

        [[0.7018, 0.0970],
         [0.3019, 0.4328]],

        [[0.5294, 0.6712],
         [0.0524, 0.3101]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -29873.10460287686
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.2937,    nan]],

        [[0.3583,    nan],
         [0.2269, 0.7579]],

        [[0.8602,    nan],
         [0.0844, 0.5281]],

        [[0.8708,    nan],
         [0.2998, 0.2208]],

        [[0.0149,    nan],
         [0.3409, 0.1603]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29871.703187519866
Iteration 100: Loss = -9814.399933994568
Iteration 200: Loss = -9805.62145931084
Iteration 300: Loss = -9802.720152151387
Iteration 400: Loss = -9801.469559520932
Iteration 500: Loss = -9800.718192132841
Iteration 600: Loss = -9800.218223813961
Iteration 700: Loss = -9799.858314092846
Iteration 800: Loss = -9799.603145316287
Iteration 900: Loss = -9799.429801110731
Iteration 1000: Loss = -9799.309188761914
Iteration 1100: Loss = -9799.224126452875
Iteration 1200: Loss = -9799.161906442658
Iteration 1300: Loss = -9799.114383276326
Iteration 1400: Loss = -9799.075680310432
Iteration 1500: Loss = -9799.043265064554
Iteration 1600: Loss = -9799.015650168847
Iteration 1700: Loss = -9798.99116505832
Iteration 1800: Loss = -9798.968609522379
Iteration 1900: Loss = -9798.946748011309
Iteration 2000: Loss = -9798.924312256953
Iteration 2100: Loss = -9798.899683886364
Iteration 2200: Loss = -9798.871048695639
Iteration 2300: Loss = -9798.837436506761
Iteration 2400: Loss = -9798.801523573196
Iteration 2500: Loss = -9798.768088351891
Iteration 2600: Loss = -9798.738362513895
Iteration 2700: Loss = -9798.71145417694
Iteration 2800: Loss = -9798.686551951196
Iteration 2900: Loss = -9798.669989075519
Iteration 3000: Loss = -9798.641017895836
Iteration 3100: Loss = -9798.61990842361
Iteration 3200: Loss = -9798.600128148202
Iteration 3300: Loss = -9798.57972567598
Iteration 3400: Loss = -9798.560220540061
Iteration 3500: Loss = -9798.540814139165
Iteration 3600: Loss = -9798.523434622335
Iteration 3700: Loss = -9798.501321428535
Iteration 3800: Loss = -9798.481184130407
Iteration 3900: Loss = -9798.461957746435
Iteration 4000: Loss = -9798.439944877187
Iteration 4100: Loss = -9798.418730205869
Iteration 4200: Loss = -9798.397184079582
Iteration 4300: Loss = -9798.376026216594
Iteration 4400: Loss = -9798.353702389895
Iteration 4500: Loss = -9798.332256801288
Iteration 4600: Loss = -9798.730864587993
1
Iteration 4700: Loss = -9798.29098019041
Iteration 4800: Loss = -9798.271560832545
Iteration 4900: Loss = -9798.253002885609
Iteration 5000: Loss = -9798.23524842214
Iteration 5100: Loss = -9798.217592352154
Iteration 5200: Loss = -9798.200141956606
Iteration 5300: Loss = -9798.25127971603
1
Iteration 5400: Loss = -9798.163205877718
Iteration 5500: Loss = -9798.142471902882
Iteration 5600: Loss = -9798.11928627081
Iteration 5700: Loss = -9798.092702817667
Iteration 5800: Loss = -9798.062118279397
Iteration 5900: Loss = -9798.026278069143
Iteration 6000: Loss = -9798.039496429737
1
Iteration 6100: Loss = -9797.906742248779
Iteration 6200: Loss = -9797.713512861239
Iteration 6300: Loss = -9797.535927228218
Iteration 6400: Loss = -9797.339328785569
Iteration 6500: Loss = -9797.130248467176
Iteration 6600: Loss = -9796.949377267398
Iteration 6700: Loss = -9796.818018250224
Iteration 6800: Loss = -9796.769160374264
Iteration 6900: Loss = -9796.734058117898
Iteration 7000: Loss = -9796.721483031188
Iteration 7100: Loss = -9796.693843523826
Iteration 7200: Loss = -9796.64117659305
Iteration 7300: Loss = -9796.535095182753
Iteration 7400: Loss = -9796.3612230592
Iteration 7500: Loss = -9796.167838807407
Iteration 7600: Loss = -9796.052198798508
Iteration 7700: Loss = -9795.99588882679
Iteration 7800: Loss = -9795.965658066245
Iteration 7900: Loss = -9795.953509131168
Iteration 8000: Loss = -9795.964640302576
1
Iteration 8100: Loss = -9795.944273745545
Iteration 8200: Loss = -9795.942661956466
Iteration 8300: Loss = -9795.942590754767
Iteration 8400: Loss = -9795.941346979325
Iteration 8500: Loss = -9795.941202097718
Iteration 8600: Loss = -9795.941022192468
Iteration 8700: Loss = -9795.941425433044
1
Iteration 8800: Loss = -9795.94119110112
2
Iteration 8900: Loss = -9795.941173771558
3
Iteration 9000: Loss = -9795.948516367062
4
Iteration 9100: Loss = -9795.943730245825
5
Iteration 9200: Loss = -9795.9409346068
Iteration 9300: Loss = -9795.943281677393
1
Iteration 9400: Loss = -9795.945236273696
2
Iteration 9500: Loss = -9795.940939478847
3
Iteration 9600: Loss = -9795.943835677454
4
Iteration 9700: Loss = -9795.969044529329
5
Iteration 9800: Loss = -9795.940917494301
Iteration 9900: Loss = -9795.953948099766
1
Iteration 10000: Loss = -9795.970931378144
2
Iteration 10100: Loss = -9795.941999535307
3
Iteration 10200: Loss = -9795.955309868326
4
Iteration 10300: Loss = -9795.94890775171
5
Iteration 10400: Loss = -9795.95228437268
6
Iteration 10500: Loss = -9795.943517010928
7
Iteration 10600: Loss = -9795.951174306132
8
Iteration 10700: Loss = -9795.955240152884
9
Iteration 10800: Loss = -9795.942502353977
10
Stopping early at iteration 10800 due to no improvement.
tensor([[-0.6444, -0.7667],
        [-0.7196, -0.6682],
        [-1.2714, -0.5785],
        [-0.6887, -0.7107],
        [-0.9173, -1.1525],
        [-2.1599, -2.4554],
        [-1.0569, -0.4240],
        [-2.2951, -2.3201],
        [-1.1205, -0.3595],
        [-1.1156, -0.2759],
        [-1.8367, -0.2236],
        [-0.6998, -1.3519],
        [-0.7650, -0.6879],
        [-1.3229, -0.1612],
        [-1.9423, -0.9176],
        [-1.9360, -0.5767],
        [-1.0698, -0.7762],
        [-1.1185, -0.3110],
        [-1.1047, -1.1848],
        [-1.5721, -1.0696],
        [-0.2855, -1.4175],
        [-0.6274, -0.7673],
        [-0.7181, -1.0258],
        [-0.6664, -1.2335],
        [-0.6781, -0.7396],
        [-2.5791, -1.2576],
        [-0.8604, -0.8722],
        [-0.7613, -0.7608],
        [-0.1066, -1.5702],
        [-1.6953, -0.0326],
        [-1.6113, -0.5883],
        [-1.3592, -0.0564],
        [-1.4943,  0.0123],
        [-1.5754, -0.0163],
        [-1.0430, -0.3611],
        [-0.9588, -0.6485],
        [-0.8955, -0.5030],
        [-2.2236, -2.3916],
        [-0.9745, -0.4672],
        [-0.4079, -1.2419],
        [-1.1107, -0.8254],
        [-1.6594,  0.1130],
        [-1.1736, -0.2432],
        [-1.5594,  0.1165],
        [-3.0627, -1.3334],
        [-1.3151, -1.2453],
        [-1.7650, -0.1550],
        [-0.9501, -0.4541],
        [-0.9903, -0.9359],
        [-0.9267, -0.7248],
        [-1.6044,  0.1837],
        [-1.5381, -0.5296],
        [-0.8343, -0.5694],
        [-1.0420, -1.4253],
        [-1.4635, -0.0135],
        [-1.5116, -0.7169],
        [-1.5084, -0.2611],
        [-0.7873, -0.9956],
        [-1.1262, -0.9850],
        [-2.3467, -0.4990],
        [-0.9475, -0.6547],
        [-1.0360, -0.3510],
        [-1.6673,  0.1734],
        [-1.0982, -0.6440],
        [-1.5057,  0.1057],
        [-1.4012,  0.0076],
        [-2.0266,  0.1681],
        [-1.2829, -0.1045],
        [-2.7547,  0.0226],
        [-0.7347, -1.4139],
        [-1.2920, -0.1558],
        [-2.3154, -1.4051],
        [-1.1929, -0.2186],
        [-1.6759,  0.1563],
        [-1.4324, -0.3763],
        [-1.2970, -0.1529],
        [-1.4545, -1.1056],
        [-0.6221, -0.7703],
        [-1.2839, -0.8880],
        [-1.0067, -1.1868],
        [-0.6705, -0.7193],
        [-1.6848, -0.1159],
        [-1.3858, -0.3525],
        [-1.1487, -0.5204],
        [-2.2835,  0.2761],
        [-1.1008, -0.2927],
        [-0.6061, -0.8227],
        [-0.6429, -0.7892],
        [-1.1641, -0.2953],
        [-1.8694,  0.2039],
        [-0.6723, -0.7467],
        [-1.8446,  0.2907],
        [-0.9888, -0.9293],
        [-0.8346, -0.6332],
        [-1.6971,  0.3040],
        [-0.4530, -1.4731],
        [-2.3823, -1.4448],
        [-2.8470, -1.7682],
        [-0.9732, -1.3485],
        [-2.4639, -0.5557]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.2386, 0.7614],
        [0.4900, 0.5100]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3622, 0.6378], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1028, 0.1314],
         [0.2937, 0.1575]],

        [[0.3583, 0.1278],
         [0.2269, 0.7579]],

        [[0.8602, 0.1234],
         [0.0844, 0.5281]],

        [[0.8708, 0.1276],
         [0.2998, 0.2208]],

        [[0.0149, 0.1273],
         [0.3409, 0.1603]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.02334946830923364
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.03115331411624301
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.009099092785950311
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.0701013211886409
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.04957635074872084
Global Adjusted Rand Index: 0.041296639816264
Average Adjusted Rand Index: 0.03665590942975774
Iteration 0: Loss = -24631.988368135684
Iteration 10: Loss = -9797.943820640088
Iteration 20: Loss = -9797.884526478249
Iteration 30: Loss = -9797.827513369306
Iteration 40: Loss = -9797.77154892772
Iteration 50: Loss = -9797.719669411343
Iteration 60: Loss = -9797.672651958319
Iteration 70: Loss = -9797.62969426072
Iteration 80: Loss = -9797.589867769106
Iteration 90: Loss = -9797.552651830138
Iteration 100: Loss = -9797.517712398178
Iteration 110: Loss = -9797.484763349632
Iteration 120: Loss = -9797.453538346119
Iteration 130: Loss = -9797.423852766047
Iteration 140: Loss = -9797.39568304742
Iteration 150: Loss = -9797.368724095904
Iteration 160: Loss = -9797.342977470338
Iteration 170: Loss = -9797.318288453442
Iteration 180: Loss = -9797.294632607089
Iteration 190: Loss = -9797.271880852024
Iteration 200: Loss = -9797.250024094259
Iteration 210: Loss = -9797.228959864593
Iteration 220: Loss = -9797.208687654685
Iteration 230: Loss = -9797.18913873116
Iteration 240: Loss = -9797.170235783342
Iteration 250: Loss = -9797.1519575133
Iteration 260: Loss = -9797.134321083451
Iteration 270: Loss = -9797.117285291155
Iteration 280: Loss = -9797.10075297512
Iteration 290: Loss = -9797.084822652963
Iteration 300: Loss = -9797.069328489693
Iteration 310: Loss = -9797.054355829196
Iteration 320: Loss = -9797.039857427775
Iteration 330: Loss = -9797.025796597609
Iteration 340: Loss = -9797.012173008192
Iteration 350: Loss = -9796.998963015767
Iteration 360: Loss = -9796.986143977718
Iteration 370: Loss = -9796.973729344862
Iteration 380: Loss = -9796.96175233564
Iteration 390: Loss = -9796.95006182872
Iteration 400: Loss = -9796.93870244105
Iteration 410: Loss = -9796.927688811676
Iteration 420: Loss = -9796.917059503683
Iteration 430: Loss = -9796.906770463493
Iteration 440: Loss = -9796.896699500856
Iteration 450: Loss = -9796.88698705028
Iteration 460: Loss = -9796.877551965124
Iteration 470: Loss = -9796.868410465237
Iteration 480: Loss = -9796.859513462463
Iteration 490: Loss = -9796.850967622171
Iteration 500: Loss = -9796.842610197291
Iteration 510: Loss = -9796.83450174235
Iteration 520: Loss = -9796.826672131212
Iteration 530: Loss = -9796.819059574349
Iteration 540: Loss = -9796.811630109722
Iteration 550: Loss = -9796.804444660967
Iteration 560: Loss = -9796.797535883816
Iteration 570: Loss = -9796.79081393025
Iteration 580: Loss = -9796.784278227162
Iteration 590: Loss = -9796.777940944039
Iteration 600: Loss = -9796.771825361346
Iteration 610: Loss = -9796.765837000667
Iteration 620: Loss = -9796.760085625521
Iteration 630: Loss = -9796.754463500256
Iteration 640: Loss = -9796.749053697627
Iteration 650: Loss = -9796.743800902494
Iteration 660: Loss = -9796.738744692242
Iteration 670: Loss = -9796.73375791574
Iteration 680: Loss = -9796.728988867822
Iteration 690: Loss = -9796.724340059085
Iteration 700: Loss = -9796.719773619554
Iteration 710: Loss = -9796.715443794801
Iteration 720: Loss = -9796.71124517165
Iteration 730: Loss = -9796.707062304029
Iteration 740: Loss = -9796.703143839617
Iteration 750: Loss = -9796.699256039956
Iteration 760: Loss = -9796.695501479026
Iteration 770: Loss = -9796.691910259438
Iteration 780: Loss = -9796.688368370784
Iteration 790: Loss = -9796.684995572869
Iteration 800: Loss = -9796.681690454925
Iteration 810: Loss = -9796.67847055539
Iteration 820: Loss = -9796.675407653413
Iteration 830: Loss = -9796.672394351734
Iteration 840: Loss = -9796.669452855358
Iteration 850: Loss = -9796.66663725616
Iteration 860: Loss = -9796.663893486026
Iteration 870: Loss = -9796.661246569041
Iteration 880: Loss = -9796.658676631394
Iteration 890: Loss = -9796.656153201377
Iteration 900: Loss = -9796.653765712528
Iteration 910: Loss = -9796.65138657002
Iteration 920: Loss = -9796.649137223956
Iteration 930: Loss = -9796.646910058136
Iteration 940: Loss = -9796.64480013181
Iteration 950: Loss = -9796.642695361194
Iteration 960: Loss = -9796.640702454684
Iteration 970: Loss = -9796.638727674115
Iteration 980: Loss = -9796.636821832148
Iteration 990: Loss = -9796.635035386216
Iteration 1000: Loss = -9796.633306283507
Iteration 1010: Loss = -9796.631548947213
Iteration 1020: Loss = -9796.629850466199
Iteration 1030: Loss = -9796.628267897291
Iteration 1040: Loss = -9796.626636399673
Iteration 1050: Loss = -9796.625115024004
Iteration 1060: Loss = -9796.623643716941
Iteration 1070: Loss = -9796.622218025574
Iteration 1080: Loss = -9796.620871107529
Iteration 1090: Loss = -9796.619485767409
Iteration 1100: Loss = -9796.61818795046
Iteration 1110: Loss = -9796.616929299866
Iteration 1120: Loss = -9796.61565508918
Iteration 1130: Loss = -9796.614485048509
Iteration 1140: Loss = -9796.61332979626
Iteration 1150: Loss = -9796.612163464264
Iteration 1160: Loss = -9796.611119469017
Iteration 1170: Loss = -9796.610082314806
Iteration 1180: Loss = -9796.60901619717
Iteration 1190: Loss = -9796.608058402458
Iteration 1200: Loss = -9796.607100609035
Iteration 1210: Loss = -9796.606130402224
Iteration 1220: Loss = -9796.605225504778
Iteration 1230: Loss = -9796.60434375477
Iteration 1240: Loss = -9796.603487988812
Iteration 1250: Loss = -9796.602689070829
Iteration 1260: Loss = -9796.601866300358
Iteration 1270: Loss = -9796.601084071863
Iteration 1280: Loss = -9796.600279733562
Iteration 1290: Loss = -9796.599577591056
Iteration 1300: Loss = -9796.598865986194
Iteration 1310: Loss = -9796.598164485036
Iteration 1320: Loss = -9796.59749973195
Iteration 1330: Loss = -9796.596862043361
Iteration 1340: Loss = -9796.596220729567
Iteration 1350: Loss = -9796.595598337297
Iteration 1360: Loss = -9796.595018247046
Iteration 1370: Loss = -9796.594451356366
Iteration 1380: Loss = -9796.59389438262
Iteration 1390: Loss = -9796.593313966076
Iteration 1400: Loss = -9796.592804701377
Iteration 1410: Loss = -9796.592262249438
Iteration 1420: Loss = -9796.591777799471
Iteration 1430: Loss = -9796.591303081603
Iteration 1440: Loss = -9796.590836872827
Iteration 1450: Loss = -9796.590375345579
Iteration 1460: Loss = -9796.589931694516
Iteration 1470: Loss = -9796.589520981672
Iteration 1480: Loss = -9796.589094403384
Iteration 1490: Loss = -9796.588691239407
Iteration 1500: Loss = -9796.588271633322
Iteration 1510: Loss = -9796.58794004904
Iteration 1520: Loss = -9796.587535583016
Iteration 1530: Loss = -9796.587223503942
Iteration 1540: Loss = -9796.58687730021
Iteration 1550: Loss = -9796.586504384535
Iteration 1560: Loss = -9796.58618395471
Iteration 1570: Loss = -9796.585870803974
Iteration 1580: Loss = -9796.585537456642
Iteration 1590: Loss = -9796.58525288325
Iteration 1600: Loss = -9796.584957825056
Iteration 1610: Loss = -9796.584682012828
Iteration 1620: Loss = -9796.58439910515
Iteration 1630: Loss = -9796.584091541097
Iteration 1640: Loss = -9796.583878832227
Iteration 1650: Loss = -9796.583638571505
Iteration 1660: Loss = -9796.583376281476
Iteration 1670: Loss = -9796.58313275341
Iteration 1680: Loss = -9796.58294240152
Iteration 1690: Loss = -9796.582688795233
Iteration 1700: Loss = -9796.582473328499
Iteration 1710: Loss = -9796.582283117903
Iteration 1720: Loss = -9796.582069969494
Iteration 1730: Loss = -9796.581863467669
Iteration 1740: Loss = -9796.581653290355
Iteration 1750: Loss = -9796.58148176538
Iteration 1760: Loss = -9796.581319956349
Iteration 1770: Loss = -9796.581136980087
Iteration 1780: Loss = -9796.58094656201
Iteration 1790: Loss = -9796.580764683347
Iteration 1800: Loss = -9796.5806216695
Iteration 1810: Loss = -9796.580481147366
Iteration 1820: Loss = -9796.580338631471
Iteration 1830: Loss = -9796.58016769085
Iteration 1840: Loss = -9796.580029288083
Iteration 1850: Loss = -9796.579893088367
Iteration 1860: Loss = -9796.579774464873
Iteration 1870: Loss = -9796.579632838459
Iteration 1880: Loss = -9796.57948156123
Iteration 1890: Loss = -9796.579387822907
Iteration 1900: Loss = -9796.579221923022
Iteration 1910: Loss = -9796.579137950164
Iteration 1920: Loss = -9796.579015839712
Iteration 1930: Loss = -9796.578885474604
Iteration 1940: Loss = -9796.578823738742
Iteration 1950: Loss = -9796.578689596892
Iteration 1960: Loss = -9796.578568598563
Iteration 1970: Loss = -9796.578481296443
Iteration 1980: Loss = -9796.578412329005
Iteration 1990: Loss = -9796.578294126353
Iteration 2000: Loss = -9796.578190017852
Iteration 2010: Loss = -9796.578192135226
1
Iteration 2020: Loss = -9796.578064878207
Iteration 2030: Loss = -9796.577957453912
Iteration 2040: Loss = -9796.577901147277
Iteration 2050: Loss = -9796.577775515612
Iteration 2060: Loss = -9796.57774692282
Iteration 2070: Loss = -9796.577641264996
Iteration 2080: Loss = -9796.577611933213
Iteration 2090: Loss = -9796.577546936187
Iteration 2100: Loss = -9796.577452658626
Iteration 2110: Loss = -9796.577388478441
Iteration 2120: Loss = -9796.577326757219
Iteration 2130: Loss = -9796.577239108125
Iteration 2140: Loss = -9796.577211191365
Iteration 2150: Loss = -9796.577168184253
Iteration 2160: Loss = -9796.57708493067
Iteration 2170: Loss = -9796.577035269109
Iteration 2180: Loss = -9796.576974107422
Iteration 2190: Loss = -9796.576931867523
Iteration 2200: Loss = -9796.576890504299
Iteration 2210: Loss = -9796.576827568615
Iteration 2220: Loss = -9796.576797961854
Iteration 2230: Loss = -9796.576755490962
Iteration 2240: Loss = -9796.576693436371
Iteration 2250: Loss = -9796.576634467638
Iteration 2260: Loss = -9796.576624680001
Iteration 2270: Loss = -9796.576552124016
Iteration 2280: Loss = -9796.576554365478
1
Iteration 2290: Loss = -9796.576489755016
Iteration 2300: Loss = -9796.57645375139
Iteration 2310: Loss = -9796.576440628749
Iteration 2320: Loss = -9796.576380160952
Iteration 2330: Loss = -9796.576376751329
Iteration 2340: Loss = -9796.576328564122
Iteration 2350: Loss = -9796.57630175786
Iteration 2360: Loss = -9796.576255088365
Iteration 2370: Loss = -9796.576200388094
Iteration 2380: Loss = -9796.576209161705
1
Iteration 2390: Loss = -9796.576188380983
Iteration 2400: Loss = -9796.576154948045
Iteration 2410: Loss = -9796.576143270866
Iteration 2420: Loss = -9796.576087258194
Iteration 2430: Loss = -9796.57605698635
Iteration 2440: Loss = -9796.576065235917
1
Iteration 2450: Loss = -9796.576019775075
Iteration 2460: Loss = -9796.575976772529
Iteration 2470: Loss = -9796.575995049854
1
Iteration 2480: Loss = -9796.575948280402
Iteration 2490: Loss = -9796.575931512438
Iteration 2500: Loss = -9796.575903867391
Iteration 2510: Loss = -9796.575884708012
Iteration 2520: Loss = -9796.575870218208
Iteration 2530: Loss = -9796.575848524692
Iteration 2540: Loss = -9796.575855088784
1
Iteration 2550: Loss = -9796.575821284696
Iteration 2560: Loss = -9796.575773892899
Iteration 2570: Loss = -9796.575786021318
1
Iteration 2580: Loss = -9796.575751755085
Iteration 2590: Loss = -9796.575723923916
Iteration 2600: Loss = -9796.575740594953
1
Iteration 2610: Loss = -9796.575707506527
Iteration 2620: Loss = -9796.575689897325
Iteration 2630: Loss = -9796.575712489823
1
Iteration 2640: Loss = -9796.575657546466
Iteration 2650: Loss = -9796.575673031495
1
Iteration 2660: Loss = -9796.575655748833
Iteration 2670: Loss = -9796.575616628175
Iteration 2680: Loss = -9796.575628984765
1
Iteration 2690: Loss = -9796.57561581771
Iteration 2700: Loss = -9796.575598396095
Iteration 2710: Loss = -9796.575578950979
Iteration 2720: Loss = -9796.57557706301
Iteration 2730: Loss = -9796.575553895158
Iteration 2740: Loss = -9796.575554348745
1
Iteration 2750: Loss = -9796.575565953539
2
Iteration 2760: Loss = -9796.57554374719
Iteration 2770: Loss = -9796.575511395095
Iteration 2780: Loss = -9796.575533584444
1
Iteration 2790: Loss = -9796.57552044389
2
Iteration 2800: Loss = -9796.575506704861
Iteration 2810: Loss = -9796.575487666603
Iteration 2820: Loss = -9796.575491700602
1
Iteration 2830: Loss = -9796.57545221172
Iteration 2840: Loss = -9796.575464319152
1
Iteration 2850: Loss = -9796.575452846691
2
Iteration 2860: Loss = -9796.575460130849
3
Stopping early at iteration 2859 due to no improvement.
pi: tensor([[0.3133, 0.6867],
        [0.3387, 0.6613]], dtype=torch.float64)
alpha: tensor([0.3313, 0.6687])
beta: tensor([[[0.1727, 0.1463],
         [0.2764, 0.1160]],

        [[0.5357, 0.1414],
         [0.0221, 0.6940]],

        [[0.5885, 0.1367],
         [0.0980, 0.1768]],

        [[0.8895, 0.1428],
         [0.7470, 0.4069]],

        [[0.4716, 0.1424],
         [0.3535, 0.5297]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.02379663096291535
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.007069157284353335
time is 2
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.07904845352107101
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.03208286804600833
time is 4
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.024979443551724323
Global Adjusted Rand Index: 0.02893078709592487
Average Adjusted Rand Index: 0.030567647759473137
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24619.433836679462
Iteration 100: Loss = -9918.219143520559
Iteration 200: Loss = -9882.77805499407
Iteration 300: Loss = -9861.827822280793
Iteration 400: Loss = -9845.259219293139
Iteration 500: Loss = -9828.09881837371
Iteration 600: Loss = -9816.969784813053
Iteration 700: Loss = -9813.036969519562
Iteration 800: Loss = -9809.307014798977
Iteration 900: Loss = -9804.993032654558
Iteration 1000: Loss = -9800.823159712183
Iteration 1100: Loss = -9800.284030593977
Iteration 1200: Loss = -9799.968380292514
Iteration 1300: Loss = -9799.760941579205
Iteration 1400: Loss = -9799.608433035033
Iteration 1500: Loss = -9799.490338643669
Iteration 1600: Loss = -9799.395269460432
Iteration 1700: Loss = -9799.316401328346
Iteration 1800: Loss = -9799.24881491865
Iteration 1900: Loss = -9799.189046042397
Iteration 2000: Loss = -9799.134052932515
Iteration 2100: Loss = -9799.080882900967
Iteration 2200: Loss = -9799.026733290139
Iteration 2300: Loss = -9798.970618194362
Iteration 2400: Loss = -9798.917337553474
Iteration 2500: Loss = -9798.876148198613
Iteration 2600: Loss = -9798.845472329123
Iteration 2700: Loss = -9798.820944323708
Iteration 2800: Loss = -9798.800668112082
Iteration 2900: Loss = -9798.783585673851
Iteration 3000: Loss = -9798.768926135872
Iteration 3100: Loss = -9798.755882778058
Iteration 3200: Loss = -9798.744236489643
Iteration 3300: Loss = -9798.73384655699
Iteration 3400: Loss = -9798.724037965092
Iteration 3500: Loss = -9798.71516255752
Iteration 3600: Loss = -9798.70678420892
Iteration 3700: Loss = -9798.698803755991
Iteration 3800: Loss = -9798.690852757356
Iteration 3900: Loss = -9798.682919326195
Iteration 4000: Loss = -9798.674794589204
Iteration 4100: Loss = -9798.668490159398
Iteration 4200: Loss = -9798.663019268704
Iteration 4300: Loss = -9798.665161391338
1
Iteration 4400: Loss = -9798.653040173198
Iteration 4500: Loss = -9798.648399792864
Iteration 4600: Loss = -9798.648269986008
Iteration 4700: Loss = -9798.637449482241
Iteration 4800: Loss = -9798.63126212647
Iteration 4900: Loss = -9798.627065315952
Iteration 5000: Loss = -9798.62329323127
Iteration 5100: Loss = -9798.619503372076
Iteration 5200: Loss = -9798.615652153485
Iteration 5300: Loss = -9798.612042105657
Iteration 5400: Loss = -9798.60784648052
Iteration 5500: Loss = -9798.599942170327
Iteration 5600: Loss = -9798.518206338547
Iteration 5700: Loss = -9798.43851035812
Iteration 5800: Loss = -9798.428571461032
Iteration 5900: Loss = -9798.423075668614
Iteration 6000: Loss = -9798.436079751698
1
Iteration 6100: Loss = -9798.4153490931
Iteration 6200: Loss = -9798.412431308894
Iteration 6300: Loss = -9798.411721150873
Iteration 6400: Loss = -9798.407696152753
Iteration 6500: Loss = -9798.405632507965
Iteration 6600: Loss = -9798.403700875017
Iteration 6700: Loss = -9798.402126976407
Iteration 6800: Loss = -9798.400388860764
Iteration 6900: Loss = -9798.39898488691
Iteration 7000: Loss = -9798.399429137437
1
Iteration 7100: Loss = -9798.396445115313
Iteration 7200: Loss = -9798.395341255879
Iteration 7300: Loss = -9798.394344903916
Iteration 7400: Loss = -9798.393637182406
Iteration 7500: Loss = -9798.39255441156
Iteration 7600: Loss = -9798.3917250131
Iteration 7700: Loss = -9798.391605606264
Iteration 7800: Loss = -9798.390250347547
Iteration 7900: Loss = -9798.389561461647
Iteration 8000: Loss = -9798.389528079537
Iteration 8100: Loss = -9798.388420740737
Iteration 8200: Loss = -9798.387861501897
Iteration 8300: Loss = -9798.390494815325
1
Iteration 8400: Loss = -9798.386996513942
Iteration 8500: Loss = -9798.386490870615
Iteration 8600: Loss = -9798.435817198417
1
Iteration 8700: Loss = -9798.38564242907
Iteration 8800: Loss = -9798.38510188996
Iteration 8900: Loss = -9798.384639836833
Iteration 9000: Loss = -9798.38434706999
Iteration 9100: Loss = -9798.384007786235
Iteration 9200: Loss = -9798.38370517354
Iteration 9300: Loss = -9798.383480948976
Iteration 9400: Loss = -9798.383240463332
Iteration 9500: Loss = -9798.383001302565
Iteration 9600: Loss = -9798.386323630111
1
Iteration 9700: Loss = -9798.382573092787
Iteration 9800: Loss = -9798.382372638576
Iteration 9900: Loss = -9798.502108725821
1
Iteration 10000: Loss = -9798.381965905204
Iteration 10100: Loss = -9798.381766766963
Iteration 10200: Loss = -9798.417335359702
1
Iteration 10300: Loss = -9798.380557204515
Iteration 10400: Loss = -9798.380327933182
Iteration 10500: Loss = -9798.379675529735
Iteration 10600: Loss = -9798.377582677478
Iteration 10700: Loss = -9798.377274342163
Iteration 10800: Loss = -9798.377179799088
Iteration 10900: Loss = -9798.378369035103
1
Iteration 11000: Loss = -9798.376949098669
Iteration 11100: Loss = -9798.376841270694
Iteration 11200: Loss = -9798.377228304484
1
Iteration 11300: Loss = -9798.376641840165
Iteration 11400: Loss = -9798.376559569648
Iteration 11500: Loss = -9798.388387191331
1
Iteration 11600: Loss = -9798.37639772282
Iteration 11700: Loss = -9798.375706163974
Iteration 11800: Loss = -9798.47279158364
1
Iteration 11900: Loss = -9798.373356521315
Iteration 12000: Loss = -9798.37330417466
Iteration 12100: Loss = -9798.52527778958
1
Iteration 12200: Loss = -9798.373237198359
Iteration 12300: Loss = -9798.373702298108
1
Iteration 12400: Loss = -9798.37384307948
2
Iteration 12500: Loss = -9798.373054488346
Iteration 12600: Loss = -9798.37360510076
1
Iteration 12700: Loss = -9798.372974365113
Iteration 12800: Loss = -9798.516102237854
1
Iteration 12900: Loss = -9798.37288306262
Iteration 13000: Loss = -9798.372832642219
Iteration 13100: Loss = -9798.373611527843
1
Iteration 13200: Loss = -9798.373254525846
2
Iteration 13300: Loss = -9798.372755235092
Iteration 13400: Loss = -9798.372747663698
Iteration 13500: Loss = -9798.372660692277
Iteration 13600: Loss = -9798.372889083294
1
Iteration 13700: Loss = -9798.372670699957
2
Iteration 13800: Loss = -9798.37261075254
Iteration 13900: Loss = -9798.387192705834
1
Iteration 14000: Loss = -9798.372585947285
Iteration 14100: Loss = -9798.376480129544
1
Iteration 14200: Loss = -9798.372517907703
Iteration 14300: Loss = -9798.372664458702
1
Iteration 14400: Loss = -9798.372479782303
Iteration 14500: Loss = -9798.373075711774
1
Iteration 14600: Loss = -9798.372762365927
2
Iteration 14700: Loss = -9798.373063669176
3
Iteration 14800: Loss = -9798.372321831956
Iteration 14900: Loss = -9798.371672601323
Iteration 15000: Loss = -9798.365616670771
Iteration 15100: Loss = -9798.36640954887
1
Iteration 15200: Loss = -9798.369388791943
2
Iteration 15300: Loss = -9798.365546711113
Iteration 15400: Loss = -9798.365808666593
1
Iteration 15500: Loss = -9798.365562682775
2
Iteration 15600: Loss = -9798.365522252478
Iteration 15700: Loss = -9798.365546403582
1
Iteration 15800: Loss = -9798.365561936504
2
Iteration 15900: Loss = -9798.36660767948
3
Iteration 16000: Loss = -9798.364872021633
Iteration 16100: Loss = -9798.366007841374
1
Iteration 16200: Loss = -9798.365822242413
2
Iteration 16300: Loss = -9798.365036156109
3
Iteration 16400: Loss = -9798.604560240063
4
Iteration 16500: Loss = -9798.364802002427
Iteration 16600: Loss = -9798.368660406504
1
Iteration 16700: Loss = -9798.3639055453
Iteration 16800: Loss = -9797.945742190157
Iteration 16900: Loss = -9797.299398724837
Iteration 17000: Loss = -9797.300719932085
1
Iteration 17100: Loss = -9797.295770231449
Iteration 17200: Loss = -9797.41980689583
1
Iteration 17300: Loss = -9797.295122998525
Iteration 17400: Loss = -9797.294945776213
Iteration 17500: Loss = -9797.294921384124
Iteration 17600: Loss = -9797.294720041695
Iteration 17700: Loss = -9797.312649800202
1
Iteration 17800: Loss = -9797.294575416068
Iteration 17900: Loss = -9797.294527825075
Iteration 18000: Loss = -9797.295091825927
1
Iteration 18100: Loss = -9797.294502170656
Iteration 18200: Loss = -9797.29440938791
Iteration 18300: Loss = -9797.294377885532
Iteration 18400: Loss = -9797.294984275915
1
Iteration 18500: Loss = -9797.294315602758
Iteration 18600: Loss = -9797.294307731365
Iteration 18700: Loss = -9797.314637724947
1
Iteration 18800: Loss = -9797.294307401058
Iteration 18900: Loss = -9797.294302678652
Iteration 19000: Loss = -9797.294290498203
Iteration 19100: Loss = -9797.295725776146
1
Iteration 19200: Loss = -9797.294276446328
Iteration 19300: Loss = -9797.294262720992
Iteration 19400: Loss = -9797.670338432888
1
Iteration 19500: Loss = -9797.294547263562
2
Iteration 19600: Loss = -9797.294205370472
Iteration 19700: Loss = -9797.294200999766
Iteration 19800: Loss = -9797.295023121964
1
Iteration 19900: Loss = -9797.294189869568
tensor([[-8.4938,  6.8438],
        [-8.4427,  7.0555],
        [-7.7981,  6.3947],
        [-8.0792,  6.5986],
        [-8.4811,  6.9171],
        [-9.0403,  6.3610],
        [-7.7318,  6.0668],
        [-8.2408,  6.7476],
        [-7.4686,  5.5575],
        [-7.4779,  6.0411],
        [-6.8878,  5.4009],
        [-8.6106,  6.8534],
        [-7.8807,  6.4830],
        [-7.3668,  5.8352],
        [-7.2197,  5.8234],
        [-6.9874,  4.9182],
        [-8.5552,  5.5526],
        [-8.0055,  5.8498],
        [-8.4808,  6.2975],
        [-7.9115,  6.1899],
        [-8.7096,  7.2290],
        [-8.0531,  6.6240],
        [-8.5626,  7.1247],
        [-8.9855,  6.2646],
        [-9.7244,  6.0030],
        [-8.4020,  4.8568],
        [-7.9421,  6.3823],
        [-9.0532,  6.2228],
        [-8.6702,  7.1669],
        [-6.8400,  5.4311],
        [-7.4858,  5.8294],
        [-7.3741,  5.9860],
        [-6.8704,  5.4075],
        [-7.1143,  5.3520],
        [-7.9892,  6.0623],
        [-7.4458,  5.9257],
        [-7.8535,  6.3800],
        [-9.9592,  5.3440],
        [-8.2400,  6.7243],
        [-9.5903,  6.4469],
        [-8.3099,  6.5384],
        [-6.8138,  5.4273],
        [-7.3316,  5.8850],
        [-7.4137,  4.8145],
        [-6.7191,  5.0235],
        [-8.3090,  6.8048],
        [-7.3633,  5.0775],
        [-8.0325,  6.4344],
        [-8.1426,  6.7545],
        [-8.1297,  6.6983],
        [-7.1550,  5.5910],
        [-7.8948,  5.7169],
        [-8.3322,  6.8510],
        [-8.5407,  7.1355],
        [-6.6754,  5.2359],
        [-9.1302,  5.5443],
        [-6.8691,  5.0753],
        [-9.5593,  5.2122],
        [-9.2553,  5.8277],
        [-6.3820,  4.7692],
        [-8.3102,  6.8658],
        [-7.9459,  6.0402],
        [-7.0025,  5.0171],
        [-8.0674,  6.3195],
        [-7.2620,  3.8143],
        [-7.3969,  5.8611],
        [ 0.4371, -1.9664],
        [-7.4410,  5.9031],
        [-6.5717,  4.1501],
        [-8.7009,  7.0824],
        [-7.6630,  4.9125],
        [-8.1471,  5.6480],
        [-7.7525,  6.3384],
        [-7.0220,  5.0819],
        [-7.9036,  6.0032],
        [-7.4182,  5.1689],
        [-8.4649,  6.8389],
        [-8.4504,  6.8223],
        [-8.2675,  6.8594],
        [-8.4520,  6.9640],
        [-8.3775,  6.9215],
        [-7.1230,  5.5897],
        [-7.6363,  6.0255],
        [-8.0027,  6.2820],
        [-5.9028,  4.3738],
        [-7.9660,  6.4595],
        [-8.6070,  7.1426],
        [-7.7434,  6.2518],
        [-7.6284,  6.2319],
        [-6.7415,  4.2248],
        [-8.2865,  6.8146],
        [-7.0532,  4.2356],
        [-8.5065,  6.6438],
        [-8.3498,  6.5284],
        [-6.6100,  5.2236],
        [-9.5827,  6.6957],
        [-7.7738,  6.1730],
        [-6.5998,  5.1625],
        [-8.4679,  7.0201],
        [-6.9977,  5.4767]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9991e-01, 9.1465e-05],
        [1.5811e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0092, 0.9908], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1303, 0.2020],
         [0.2764, 0.1349]],

        [[0.5357, 0.1010],
         [0.0221, 0.6940]],

        [[0.5885, 0.0909],
         [0.0980, 0.1768]],

        [[0.8895, 0.2320],
         [0.7470, 0.4069]],

        [[0.4716, 0.1010],
         [0.3535, 0.5297]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.0003567551354141363
Average Adjusted Rand Index: 0.0005525063461823855
Iteration 0: Loss = -28782.83796724885
Iteration 10: Loss = -9798.713923298705
Iteration 20: Loss = -9798.460159847995
Iteration 30: Loss = -9798.345742380901
Iteration 40: Loss = -9798.295987500278
Iteration 50: Loss = -9798.268423976651
Iteration 60: Loss = -9798.250071159355
Iteration 70: Loss = -9798.235567329559
Iteration 80: Loss = -9798.221911369139
Iteration 90: Loss = -9798.20611387689
Iteration 100: Loss = -9798.182992935941
Iteration 110: Loss = -9798.141524679633
Iteration 120: Loss = -9798.071487591424
Iteration 130: Loss = -9797.996810545963
Iteration 140: Loss = -9797.93413601656
Iteration 150: Loss = -9797.875499535541
Iteration 160: Loss = -9797.81765681799
Iteration 170: Loss = -9797.76241620414
Iteration 180: Loss = -9797.711846842454
Iteration 190: Loss = -9797.6657775553
Iteration 200: Loss = -9797.623407466985
Iteration 210: Loss = -9797.584054433284
Iteration 220: Loss = -9797.54722471071
Iteration 230: Loss = -9797.512615168145
Iteration 240: Loss = -9797.479896403693
Iteration 250: Loss = -9797.44895224644
Iteration 260: Loss = -9797.41951942432
Iteration 270: Loss = -9797.391499689822
Iteration 280: Loss = -9797.36474746575
Iteration 290: Loss = -9797.339218300964
Iteration 300: Loss = -9797.314645305036
Iteration 310: Loss = -9797.291135807007
Iteration 320: Loss = -9797.268533743023
Iteration 330: Loss = -9797.24680532984
Iteration 340: Loss = -9797.225866656292
Iteration 350: Loss = -9797.205695771669
Iteration 360: Loss = -9797.186245622444
Iteration 370: Loss = -9797.167437840628
Iteration 380: Loss = -9797.149282293356
Iteration 390: Loss = -9797.131729517752
Iteration 400: Loss = -9797.114731384147
Iteration 410: Loss = -9797.098346772194
Iteration 420: Loss = -9797.082429620943
Iteration 430: Loss = -9797.067022241024
Iteration 440: Loss = -9797.052182097426
Iteration 450: Loss = -9797.037691174552
Iteration 460: Loss = -9797.023711742067
Iteration 470: Loss = -9797.01013454011
Iteration 480: Loss = -9796.997025745663
Iteration 490: Loss = -9796.984270161778
Iteration 500: Loss = -9796.971916039
Iteration 510: Loss = -9796.959923642824
Iteration 520: Loss = -9796.94830213102
Iteration 530: Loss = -9796.937029213803
Iteration 540: Loss = -9796.926078053928
Iteration 550: Loss = -9796.915472242737
Iteration 560: Loss = -9796.905223225629
Iteration 570: Loss = -9796.895233688252
Iteration 580: Loss = -9796.885562706448
Iteration 590: Loss = -9796.876169979438
Iteration 600: Loss = -9796.86706772754
Iteration 610: Loss = -9796.858238098022
Iteration 620: Loss = -9796.84966472709
Iteration 630: Loss = -9796.841364610402
Iteration 640: Loss = -9796.833259858948
Iteration 650: Loss = -9796.82549546457
Iteration 660: Loss = -9796.81792710678
Iteration 670: Loss = -9796.810555395083
Iteration 680: Loss = -9796.803441065747
Iteration 690: Loss = -9796.796541434773
Iteration 700: Loss = -9796.789828457424
Iteration 710: Loss = -9796.783340317772
Iteration 720: Loss = -9796.77702586392
Iteration 730: Loss = -9796.77090128962
Iteration 740: Loss = -9796.764980789032
Iteration 750: Loss = -9796.759205593162
Iteration 760: Loss = -9796.753642486152
Iteration 770: Loss = -9796.74822482469
Iteration 780: Loss = -9796.743028636562
Iteration 790: Loss = -9796.737953017599
Iteration 800: Loss = -9796.733058849617
Iteration 810: Loss = -9796.728274407442
Iteration 820: Loss = -9796.723637445471
Iteration 830: Loss = -9796.719152398113
Iteration 840: Loss = -9796.714822137708
Iteration 850: Loss = -9796.710582663798
Iteration 860: Loss = -9796.706468282155
Iteration 870: Loss = -9796.702515449855
Iteration 880: Loss = -9796.698712971984
Iteration 890: Loss = -9796.694967225454
Iteration 900: Loss = -9796.691365308507
Iteration 910: Loss = -9796.687864603171
Iteration 920: Loss = -9796.684449077571
Iteration 930: Loss = -9796.681217302936
Iteration 940: Loss = -9796.678015835321
Iteration 950: Loss = -9796.67492431955
Iteration 960: Loss = -9796.671948848098
Iteration 970: Loss = -9796.669024213928
Iteration 980: Loss = -9796.666240686309
Iteration 990: Loss = -9796.663514804037
Iteration 1000: Loss = -9796.660819509161
Iteration 1010: Loss = -9796.658272609651
Iteration 1020: Loss = -9796.655813302055
Iteration 1030: Loss = -9796.65336736201
Iteration 1040: Loss = -9796.651079428353
Iteration 1050: Loss = -9796.648801578014
Iteration 1060: Loss = -9796.646598124244
Iteration 1070: Loss = -9796.644451913948
Iteration 1080: Loss = -9796.642417282721
Iteration 1090: Loss = -9796.64041384635
Iteration 1100: Loss = -9796.638424300112
Iteration 1110: Loss = -9796.63657031982
Iteration 1120: Loss = -9796.63477653511
Iteration 1130: Loss = -9796.632976091963
Iteration 1140: Loss = -9796.631259912623
Iteration 1150: Loss = -9796.629613674218
Iteration 1160: Loss = -9796.627990416813
Iteration 1170: Loss = -9796.626454646592
Iteration 1180: Loss = -9796.62488973259
Iteration 1190: Loss = -9796.623445053145
Iteration 1200: Loss = -9796.621999936646
Iteration 1210: Loss = -9796.620645822552
Iteration 1220: Loss = -9796.619285146606
Iteration 1230: Loss = -9796.61798939797
Iteration 1240: Loss = -9796.616709460923
Iteration 1250: Loss = -9796.61551337453
Iteration 1260: Loss = -9796.61431149035
Iteration 1270: Loss = -9796.613169293603
Iteration 1280: Loss = -9796.612052909386
Iteration 1290: Loss = -9796.610952421659
Iteration 1300: Loss = -9796.609908502898
Iteration 1310: Loss = -9796.608917779371
Iteration 1320: Loss = -9796.607904223298
Iteration 1330: Loss = -9796.606929357866
Iteration 1340: Loss = -9796.606020606208
Iteration 1350: Loss = -9796.605078958954
Iteration 1360: Loss = -9796.604179946995
Iteration 1370: Loss = -9796.6033484909
Iteration 1380: Loss = -9796.602541521946
Iteration 1390: Loss = -9796.601732354951
Iteration 1400: Loss = -9796.600967164944
Iteration 1410: Loss = -9796.600205161758
Iteration 1420: Loss = -9796.599486730887
Iteration 1430: Loss = -9796.598759416764
Iteration 1440: Loss = -9796.598101005424
Iteration 1450: Loss = -9796.597405812996
Iteration 1460: Loss = -9796.59675343618
Iteration 1470: Loss = -9796.59612699691
Iteration 1480: Loss = -9796.595519860179
Iteration 1490: Loss = -9796.594887457766
Iteration 1500: Loss = -9796.594346910344
Iteration 1510: Loss = -9796.593827498278
Iteration 1520: Loss = -9796.593228579179
Iteration 1530: Loss = -9796.592739586835
Iteration 1540: Loss = -9796.592217191937
Iteration 1550: Loss = -9796.59171505701
Iteration 1560: Loss = -9796.591234959498
Iteration 1570: Loss = -9796.590786601337
Iteration 1580: Loss = -9796.590283234962
Iteration 1590: Loss = -9796.58986666208
Iteration 1600: Loss = -9796.589428365747
Iteration 1610: Loss = -9796.5890432448
Iteration 1620: Loss = -9796.588636070812
Iteration 1630: Loss = -9796.588296670918
Iteration 1640: Loss = -9796.58783755594
Iteration 1650: Loss = -9796.587488437435
Iteration 1660: Loss = -9796.587155637157
Iteration 1670: Loss = -9796.586799160083
Iteration 1680: Loss = -9796.586458800331
Iteration 1690: Loss = -9796.58610841454
Iteration 1700: Loss = -9796.585807917088
Iteration 1710: Loss = -9796.585480992555
Iteration 1720: Loss = -9796.585188012541
Iteration 1730: Loss = -9796.584911326028
Iteration 1740: Loss = -9796.584630009385
Iteration 1750: Loss = -9796.584361182591
Iteration 1760: Loss = -9796.584094847514
Iteration 1770: Loss = -9796.583846163767
Iteration 1780: Loss = -9796.583604601006
Iteration 1790: Loss = -9796.58333216598
Iteration 1800: Loss = -9796.583112070166
Iteration 1810: Loss = -9796.582872811878
Iteration 1820: Loss = -9796.582654897136
Iteration 1830: Loss = -9796.582449481211
Iteration 1840: Loss = -9796.582218157886
Iteration 1850: Loss = -9796.582015090979
Iteration 1860: Loss = -9796.581822630435
Iteration 1870: Loss = -9796.581653510946
Iteration 1880: Loss = -9796.581460603351
Iteration 1890: Loss = -9796.581270417782
Iteration 1900: Loss = -9796.581086667476
Iteration 1910: Loss = -9796.580933449817
Iteration 1920: Loss = -9796.580768687878
Iteration 1930: Loss = -9796.580595545316
Iteration 1940: Loss = -9796.58043368471
Iteration 1950: Loss = -9796.58032775388
Iteration 1960: Loss = -9796.580148405083
Iteration 1970: Loss = -9796.57999495834
Iteration 1980: Loss = -9796.579846530349
Iteration 1990: Loss = -9796.5797234145
Iteration 2000: Loss = -9796.579564612473
Iteration 2010: Loss = -9796.579467740077
Iteration 2020: Loss = -9796.579333319643
Iteration 2030: Loss = -9796.579231140619
Iteration 2040: Loss = -9796.579122896283
Iteration 2050: Loss = -9796.579007598075
Iteration 2060: Loss = -9796.578914950494
Iteration 2070: Loss = -9796.578771263405
Iteration 2080: Loss = -9796.578676534165
Iteration 2090: Loss = -9796.578581329033
Iteration 2100: Loss = -9796.578517683021
Iteration 2110: Loss = -9796.578371988086
Iteration 2120: Loss = -9796.578288633777
Iteration 2130: Loss = -9796.578237758164
Iteration 2140: Loss = -9796.57814327878
Iteration 2150: Loss = -9796.57802612049
Iteration 2160: Loss = -9796.577969566617
Iteration 2170: Loss = -9796.577884135484
Iteration 2180: Loss = -9796.577797888604
Iteration 2190: Loss = -9796.577733707712
Iteration 2200: Loss = -9796.577654625382
Iteration 2210: Loss = -9796.577571321544
Iteration 2220: Loss = -9796.577503248347
Iteration 2230: Loss = -9796.57745160511
Iteration 2240: Loss = -9796.577370497871
Iteration 2250: Loss = -9796.577330462807
Iteration 2260: Loss = -9796.577221709531
Iteration 2270: Loss = -9796.577196592907
Iteration 2280: Loss = -9796.577109706494
Iteration 2290: Loss = -9796.57706516959
Iteration 2300: Loss = -9796.577028946638
Iteration 2310: Loss = -9796.576972347671
Iteration 2320: Loss = -9796.5769195813
Iteration 2330: Loss = -9796.576875954323
Iteration 2340: Loss = -9796.576851729616
Iteration 2350: Loss = -9796.576784816041
Iteration 2360: Loss = -9796.576719997911
Iteration 2370: Loss = -9796.576692675613
Iteration 2380: Loss = -9796.576681907994
Iteration 2390: Loss = -9796.576599353908
Iteration 2400: Loss = -9796.576548275516
Iteration 2410: Loss = -9796.57655668728
1
Iteration 2420: Loss = -9796.576519012217
Iteration 2430: Loss = -9796.576447385754
Iteration 2440: Loss = -9796.576436815103
Iteration 2450: Loss = -9796.576374381879
Iteration 2460: Loss = -9796.576366797986
Iteration 2470: Loss = -9796.576336121267
Iteration 2480: Loss = -9796.576302987758
Iteration 2490: Loss = -9796.576297259726
Iteration 2500: Loss = -9796.57619684113
Iteration 2510: Loss = -9796.576205453084
1
Iteration 2520: Loss = -9796.5761272873
Iteration 2530: Loss = -9796.576146842204
1
Iteration 2540: Loss = -9796.576111543998
Iteration 2550: Loss = -9796.576095240558
Iteration 2560: Loss = -9796.5760831688
Iteration 2570: Loss = -9796.5760370362
Iteration 2580: Loss = -9796.576006583491
Iteration 2590: Loss = -9796.575981975904
Iteration 2600: Loss = -9796.575936547455
Iteration 2610: Loss = -9796.57596974929
1
Iteration 2620: Loss = -9796.57592320613
Iteration 2630: Loss = -9796.575913003659
Iteration 2640: Loss = -9796.575863252756
Iteration 2650: Loss = -9796.575869896033
1
Iteration 2660: Loss = -9796.57583274573
Iteration 2670: Loss = -9796.57584117926
1
Iteration 2680: Loss = -9796.575801317606
Iteration 2690: Loss = -9796.575771531352
Iteration 2700: Loss = -9796.575769799287
Iteration 2710: Loss = -9796.575766559225
Iteration 2720: Loss = -9796.575731121991
Iteration 2730: Loss = -9796.575715398303
Iteration 2740: Loss = -9796.575711391486
Iteration 2750: Loss = -9796.5757254602
1
Iteration 2760: Loss = -9796.575678850937
Iteration 2770: Loss = -9796.57570703062
1
Iteration 2780: Loss = -9796.57563428169
Iteration 2790: Loss = -9796.575661898156
1
Iteration 2800: Loss = -9796.575622623384
Iteration 2810: Loss = -9796.575632800053
1
Iteration 2820: Loss = -9796.575594087333
Iteration 2830: Loss = -9796.575557590802
Iteration 2840: Loss = -9796.575565687666
1
Iteration 2850: Loss = -9796.575601394205
2
Iteration 2860: Loss = -9796.575588169791
3
Stopping early at iteration 2859 due to no improvement.
pi: tensor([[0.3133, 0.6867],
        [0.3386, 0.6614]], dtype=torch.float64)
alpha: tensor([0.3313, 0.6687])
beta: tensor([[[1.7267e-01, 1.4628e-01],
         [8.8961e-01, 1.1603e-01]],

        [[4.6091e-01, 1.4144e-01],
         [9.6850e-01, 9.7387e-01]],

        [[5.0875e-01, 1.3669e-01],
         [1.0698e-01, 6.3839e-01]],

        [[2.6673e-01, 1.4278e-01],
         [2.9781e-01, 6.2356e-01]],

        [[7.2652e-04, 1.4240e-01],
         [8.7020e-01, 9.3130e-01]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.02379663096291535
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.007069157284353335
time is 2
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.07904845352107101
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.03208286804600833
time is 4
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.024979443551724323
Global Adjusted Rand Index: 0.02893078709592487
Average Adjusted Rand Index: 0.030567647759473137
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28782.32143161635
Iteration 100: Loss = -9855.648080470972
Iteration 200: Loss = -9810.829036574978
Iteration 300: Loss = -9805.27056631286
Iteration 400: Loss = -9803.818183218576
Iteration 500: Loss = -9803.018122350839
Iteration 600: Loss = -9802.48762553906
Iteration 700: Loss = -9802.076993646791
Iteration 800: Loss = -9801.729961224939
Iteration 900: Loss = -9801.407421914626
Iteration 1000: Loss = -9799.341795921815
Iteration 1100: Loss = -9799.170646487692
Iteration 1200: Loss = -9799.075621836975
Iteration 1300: Loss = -9799.009130387572
Iteration 1400: Loss = -9798.957866037217
Iteration 1500: Loss = -9798.916521878808
Iteration 1600: Loss = -9798.881874564391
Iteration 1700: Loss = -9798.853664099604
Iteration 1800: Loss = -9798.83139527693
Iteration 1900: Loss = -9798.812887709277
Iteration 2000: Loss = -9798.79602293193
Iteration 2100: Loss = -9798.779558349437
Iteration 2200: Loss = -9798.763006171203
Iteration 2300: Loss = -9798.747499447825
Iteration 2400: Loss = -9798.732965186578
Iteration 2500: Loss = -9798.718050867697
Iteration 2600: Loss = -9798.702375711073
Iteration 2700: Loss = -9798.686667630234
Iteration 2800: Loss = -9798.671815027226
Iteration 2900: Loss = -9798.657764449894
Iteration 3000: Loss = -9798.64445649285
Iteration 3100: Loss = -9798.631566675711
Iteration 3200: Loss = -9798.618686814161
Iteration 3300: Loss = -9798.6057406447
Iteration 3400: Loss = -9798.592002795825
Iteration 3500: Loss = -9798.577532559984
Iteration 3600: Loss = -9798.563519233512
Iteration 3700: Loss = -9798.550115993996
Iteration 3800: Loss = -9798.533930461674
Iteration 3900: Loss = -9798.455774315973
Iteration 4000: Loss = -9798.43016852058
Iteration 4100: Loss = -9798.41204549891
Iteration 4200: Loss = -9798.395976571888
Iteration 4300: Loss = -9798.380932402993
Iteration 4400: Loss = -9798.366329907827
Iteration 4500: Loss = -9798.35200895227
Iteration 4600: Loss = -9798.33779300401
Iteration 4700: Loss = -9798.322986619673
Iteration 4800: Loss = -9798.307185397656
Iteration 4900: Loss = -9798.28347664191
Iteration 5000: Loss = -9798.266228356077
Iteration 5100: Loss = -9798.248503533716
Iteration 5200: Loss = -9798.229415546784
Iteration 5300: Loss = -9798.19457277657
Iteration 5400: Loss = -9797.97754324704
Iteration 5500: Loss = -9797.846051904566
Iteration 5600: Loss = -9797.754099281256
Iteration 5700: Loss = -9797.670503213205
Iteration 5800: Loss = -9797.56160655526
Iteration 5900: Loss = -9797.468818690968
Iteration 6000: Loss = -9797.362775829708
Iteration 6100: Loss = -9797.295152369757
Iteration 6200: Loss = -9797.230528240012
Iteration 6300: Loss = -9797.181412264588
Iteration 6400: Loss = -9797.03712878514
Iteration 6500: Loss = -9796.960898512803
Iteration 6600: Loss = -9796.775355831272
Iteration 6700: Loss = -9796.728614671412
Iteration 6800: Loss = -9796.710933974475
Iteration 6900: Loss = -9796.673782292219
Iteration 7000: Loss = -9796.548866255149
Iteration 7100: Loss = -9796.188068826657
Iteration 7200: Loss = -9796.007293065677
Iteration 7300: Loss = -9795.960266623617
Iteration 7400: Loss = -9795.948378284169
Iteration 7500: Loss = -9795.949209343979
1
Iteration 7600: Loss = -9795.942094483531
Iteration 7700: Loss = -9795.954857809822
1
Iteration 7800: Loss = -9795.94106698923
Iteration 7900: Loss = -9795.941906876054
1
Iteration 8000: Loss = -9795.946540507137
2
Iteration 8100: Loss = -9795.953434744424
3
Iteration 8200: Loss = -9795.942596507199
4
Iteration 8300: Loss = -9795.942246937238
5
Iteration 8400: Loss = -9795.950749556703
6
Iteration 8500: Loss = -9795.951010849087
7
Iteration 8600: Loss = -9795.94146462879
8
Iteration 8700: Loss = -9795.94700743736
9
Iteration 8800: Loss = -9795.942031102715
10
Stopping early at iteration 8800 due to no improvement.
tensor([[-0.8587, -0.7355],
        [-0.9934, -1.0439],
        [-0.3510, -1.0429],
        [-0.7549, -0.7325],
        [-0.9925, -0.7567],
        [-1.7179, -1.4219],
        [-0.3922, -1.0233],
        [-0.9830, -0.9574],
        [-0.3901, -1.1498],
        [-0.2745, -1.1129],
        [ 0.1053, -1.5078],
        [-1.0301, -0.3774],
        [-0.7962, -0.8725],
        [-0.2862, -1.4480],
        [-0.3505, -1.3745],
        [-0.5911, -1.9492],
        [-0.9394, -1.2331],
        [-0.4707, -1.2777],
        [-0.7842, -0.7026],
        [-0.5254, -1.0280],
        [-1.9861, -0.8534],
        [-1.6342, -1.4935],
        [-2.4613, -2.1539],
        [-1.5801, -1.0126],
        [-0.8692, -0.8070],
        [-0.0701, -1.3925],
        [-0.6996, -0.6867],
        [-0.7686, -0.7685],
        [-1.8208, -0.3553],
        [ 0.0245, -1.6365],
        [-0.1843, -1.2067],
        [-0.0453, -1.3461],
        [-0.0412, -1.5459],
        [ 0.0492, -1.5095],
        [-0.6418, -1.3228],
        [-0.6208, -0.9307],
        [-1.1976, -1.5888],
        [-1.0135, -0.8450],
        [-0.7342, -1.2407],
        [-1.1124, -0.2779],
        [-0.5555, -0.8405],
        [ 0.1294, -1.6414],
        [-1.3329, -2.2630],
        [-0.7738, -2.4473],
        [-1.0595, -2.7887],
        [-0.9531, -1.0225],
        [ 0.0781, -1.5307],
        [-1.0293, -1.5247],
        [-1.2617, -1.3157],
        [-0.6119, -0.8132],
        [-1.2501, -3.0370],
        [-0.6024, -1.6108],
        [-0.5719, -0.8359],
        [-1.0758, -0.6916],
        [-0.5959, -2.0439],
        [-0.4940, -1.2889],
        [-0.0813, -1.3266],
        [-0.8415, -0.6325],
        [-1.1019, -1.2425],
        [ 0.1851, -1.6623],
        [-0.5532, -0.8451],
        [-0.3532, -1.0374],
        [-0.4253, -2.2636],
        [-0.7754, -1.2288],
        [-1.0721, -2.6819],
        [-0.1091, -1.5162],
        [ 0.2331, -1.9589],
        [-0.1402, -1.3172],
        [ 0.2957, -2.4808],
        [-1.3190, -0.6390],
        [-1.4621, -2.5980],
        [-1.4649, -2.3758],
        [-0.2182, -1.1912],
        [ 0.1322, -1.6976],
        [-0.3529, -1.4092],
        [-0.1341, -1.2767],
        [-1.0224, -1.3707],
        [-0.9341, -0.7854],
        [-2.1101, -2.5051],
        [-0.9357, -0.7549],
        [-1.6173, -1.5678],
        [-0.2961, -1.8629],
        [-0.3229, -1.3561],
        [-0.4660, -1.0942],
        [ 0.4576, -2.0993],
        [-0.7107, -1.5179],
        [-0.9181, -0.7010],
        [-1.1975, -1.0513],
        [-0.3962, -1.2648],
        [ 0.2771, -1.7943],
        [-1.0913, -1.0165],
        [-0.5544, -2.6878],
        [-0.7623, -0.8211],
        [-1.0190, -1.2200],
        [-1.2732, -3.2726],
        [-1.2777, -0.2571],
        [-0.2280, -1.1656],
        [-0.3275, -1.4063],
        [-1.3839, -1.0078],
        [ 0.0921, -1.8155]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5096, 0.4904],
        [0.7608, 0.2392]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6368, 0.3632], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.5760e-01, 1.3153e-01],
         [8.8961e-01, 1.0286e-01]],

        [[4.6091e-01, 1.2773e-01],
         [9.6850e-01, 9.7387e-01]],

        [[5.0875e-01, 1.2340e-01],
         [1.0698e-01, 6.3839e-01]],

        [[2.6673e-01, 1.2770e-01],
         [2.9781e-01, 6.2356e-01]],

        [[7.2652e-04, 1.2726e-01],
         [8.7020e-01, 9.3130e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.016420882005743785
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.03115331411624301
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009099092785950311
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.05910341875114672
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.04015674982284218
Global Adjusted Rand Index: 0.036441564817859526
Average Adjusted Rand Index: 0.0311866914963852
Iteration 0: Loss = -34042.27561892991
Iteration 10: Loss = -9799.052835007178
Iteration 20: Loss = -9799.052839195601
1
Iteration 30: Loss = -9799.052863075674
2
Iteration 40: Loss = -9799.052928198622
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[8.8262e-12, 1.0000e+00],
        [1.5310e-08, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.4743e-08, 1.0000e+00])
beta: tensor([[[0.2164, 0.1632],
         [0.7768, 0.1337]],

        [[0.7225, 0.1734],
         [0.6411, 0.3099]],

        [[0.7858, 0.0881],
         [0.8896, 0.1968]],

        [[0.9717, 0.1805],
         [0.6880, 0.3387]],

        [[0.6470, 0.2312],
         [0.0134, 0.0376]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34042.073221485734
Iteration 100: Loss = -9862.819596113333
Iteration 200: Loss = -9835.528171396616
Iteration 300: Loss = -9819.27129539006
Iteration 400: Loss = -9804.44233076961
Iteration 500: Loss = -9802.530940356546
Iteration 600: Loss = -9801.582201116038
Iteration 700: Loss = -9800.984251261643
Iteration 800: Loss = -9800.572351583109
Iteration 900: Loss = -9800.27317150328
Iteration 1000: Loss = -9800.047696926675
Iteration 1100: Loss = -9799.87280503315
Iteration 1200: Loss = -9799.734001090075
Iteration 1300: Loss = -9799.62159594779
Iteration 1400: Loss = -9799.529091414854
Iteration 1500: Loss = -9799.451866548654
Iteration 1600: Loss = -9799.386604170973
Iteration 1700: Loss = -9799.330213304958
Iteration 1800: Loss = -9799.278356845374
Iteration 1900: Loss = -9799.22357083539
Iteration 2000: Loss = -9799.183381076018
Iteration 2100: Loss = -9799.150503364042
Iteration 2200: Loss = -9799.12141807826
Iteration 2300: Loss = -9799.095602903977
Iteration 2400: Loss = -9799.072531663875
Iteration 2500: Loss = -9799.051759061178
Iteration 2600: Loss = -9799.03293099627
Iteration 2700: Loss = -9799.015701451292
Iteration 2800: Loss = -9798.999813532531
Iteration 2900: Loss = -9798.984907127187
Iteration 3000: Loss = -9798.970872073493
Iteration 3100: Loss = -9798.95739364149
Iteration 3200: Loss = -9798.94404033725
Iteration 3300: Loss = -9798.930985732213
Iteration 3400: Loss = -9798.919085630909
Iteration 3500: Loss = -9798.907827016967
Iteration 3600: Loss = -9798.896516456889
Iteration 3700: Loss = -9798.88491215503
Iteration 3800: Loss = -9798.872693961852
Iteration 3900: Loss = -9798.859405833085
Iteration 4000: Loss = -9798.844378307305
Iteration 4100: Loss = -9798.82680184435
Iteration 4200: Loss = -9798.805760544303
Iteration 4300: Loss = -9798.780677744835
Iteration 4400: Loss = -9798.752319854419
Iteration 4500: Loss = -9798.724050166471
Iteration 4600: Loss = -9798.699254332912
Iteration 4700: Loss = -9798.67849531571
Iteration 4800: Loss = -9798.656839376406
Iteration 4900: Loss = -9798.636899280082
Iteration 5000: Loss = -9798.625052302232
Iteration 5100: Loss = -9798.616256285253
Iteration 5200: Loss = -9798.610791027142
Iteration 5300: Loss = -9798.606887383448
Iteration 5400: Loss = -9798.603397213827
Iteration 5500: Loss = -9798.60010918077
Iteration 5600: Loss = -9798.596944615847
Iteration 5700: Loss = -9798.593900224394
Iteration 5800: Loss = -9798.59095461329
Iteration 5900: Loss = -9798.588050975612
Iteration 6000: Loss = -9798.584937645008
Iteration 6100: Loss = -9798.581221467088
Iteration 6200: Loss = -9798.57607605629
Iteration 6300: Loss = -9798.56575361414
Iteration 6400: Loss = -9798.511597389353
Iteration 6500: Loss = -9798.41751377843
Iteration 6600: Loss = -9798.401441667942
Iteration 6700: Loss = -9798.395708353462
Iteration 6800: Loss = -9798.391968868402
Iteration 6900: Loss = -9798.389075890032
Iteration 7000: Loss = -9798.386680502448
Iteration 7100: Loss = -9798.384648055302
Iteration 7200: Loss = -9798.38289012099
Iteration 7300: Loss = -9798.381377085483
Iteration 7400: Loss = -9798.37999054984
Iteration 7500: Loss = -9798.378735619917
Iteration 7600: Loss = -9798.37757935969
Iteration 7700: Loss = -9798.376460273872
Iteration 7800: Loss = -9798.37544083568
Iteration 7900: Loss = -9798.374481295381
Iteration 8000: Loss = -9798.373613891412
Iteration 8100: Loss = -9798.37230192102
Iteration 8200: Loss = -9798.371008888464
Iteration 8300: Loss = -9798.375146948736
1
Iteration 8400: Loss = -9798.369522523508
Iteration 8500: Loss = -9798.399992139757
1
Iteration 8600: Loss = -9798.368860329836
Iteration 8700: Loss = -9798.368447203298
Iteration 8800: Loss = -9798.367753663935
Iteration 8900: Loss = -9798.366112252037
Iteration 9000: Loss = -9798.33674638817
Iteration 9100: Loss = -9797.383060116521
Iteration 9200: Loss = -9797.330500895652
Iteration 9300: Loss = -9797.323834728306
Iteration 9400: Loss = -9797.312797172663
Iteration 9500: Loss = -9797.30934448789
Iteration 9600: Loss = -9797.306946234834
Iteration 9700: Loss = -9797.30516679553
Iteration 9800: Loss = -9797.303832143765
Iteration 9900: Loss = -9797.302765183069
Iteration 10000: Loss = -9797.30181334467
Iteration 10100: Loss = -9797.301090004814
Iteration 10200: Loss = -9797.300953933323
Iteration 10300: Loss = -9797.299932453006
Iteration 10400: Loss = -9797.29945135153
Iteration 10500: Loss = -9797.298999948054
Iteration 10600: Loss = -9797.41547701388
1
Iteration 10700: Loss = -9797.29834383992
Iteration 10800: Loss = -9797.298019527118
Iteration 10900: Loss = -9797.297771954285
Iteration 11000: Loss = -9797.560142032846
1
Iteration 11100: Loss = -9797.297299930753
Iteration 11200: Loss = -9797.297104188094
Iteration 11300: Loss = -9797.296908730417
Iteration 11400: Loss = -9797.311807714937
1
Iteration 11500: Loss = -9797.29659831206
Iteration 11600: Loss = -9797.296470642328
Iteration 11700: Loss = -9797.296318820847
Iteration 11800: Loss = -9797.296704604993
1
Iteration 11900: Loss = -9797.29610556854
Iteration 12000: Loss = -9797.295991865762
Iteration 12100: Loss = -9797.295871461689
Iteration 12200: Loss = -9797.29592561685
1
Iteration 12300: Loss = -9797.29568295091
Iteration 12400: Loss = -9797.295602298465
Iteration 12500: Loss = -9797.311224762203
1
Iteration 12600: Loss = -9797.295446419876
Iteration 12700: Loss = -9797.295391113503
Iteration 12800: Loss = -9797.295357424784
Iteration 12900: Loss = -9797.29581483915
1
Iteration 13000: Loss = -9797.295189420143
Iteration 13100: Loss = -9797.29515605069
Iteration 13200: Loss = -9797.295103917171
Iteration 13300: Loss = -9797.295816899234
1
Iteration 13400: Loss = -9797.295011045364
Iteration 13500: Loss = -9797.294932950896
Iteration 13600: Loss = -9797.295069429809
1
Iteration 13700: Loss = -9797.294910068647
Iteration 13800: Loss = -9797.294817524069
Iteration 13900: Loss = -9797.294785950031
Iteration 14000: Loss = -9797.29473832327
Iteration 14100: Loss = -9797.29484768123
1
Iteration 14200: Loss = -9797.294709019532
Iteration 14300: Loss = -9797.294682085832
Iteration 14400: Loss = -9797.294644033016
Iteration 14500: Loss = -9797.29542269849
1
Iteration 14600: Loss = -9797.294600981644
Iteration 14700: Loss = -9797.294561881119
Iteration 14800: Loss = -9797.294537394062
Iteration 14900: Loss = -9797.294608594557
1
Iteration 15000: Loss = -9797.294498073736
Iteration 15100: Loss = -9797.294485295088
Iteration 15200: Loss = -9797.294462994203
Iteration 15300: Loss = -9797.294555545426
1
Iteration 15400: Loss = -9797.294437198245
Iteration 15500: Loss = -9797.294388521877
Iteration 15600: Loss = -9797.294427086856
1
Iteration 15700: Loss = -9797.294453854374
2
Iteration 15800: Loss = -9797.294364456196
Iteration 15900: Loss = -9797.294695740364
1
Iteration 16000: Loss = -9797.294675506799
2
Iteration 16100: Loss = -9797.294303231902
Iteration 16200: Loss = -9797.294328528764
1
Iteration 16300: Loss = -9797.294429135398
2
Iteration 16400: Loss = -9797.294408471125
3
Iteration 16500: Loss = -9797.294282888855
Iteration 16600: Loss = -9797.294275448106
Iteration 16700: Loss = -9797.306948066229
1
Iteration 16800: Loss = -9797.294268457883
Iteration 16900: Loss = -9797.294253368284
Iteration 17000: Loss = -9797.29859406337
1
Iteration 17100: Loss = -9797.294254883196
2
Iteration 17200: Loss = -9797.294215732298
Iteration 17300: Loss = -9797.295885079251
1
Iteration 17400: Loss = -9797.29422128504
2
Iteration 17500: Loss = -9797.294243791957
3
Iteration 17600: Loss = -9797.294220320091
4
Iteration 17700: Loss = -9797.29594537664
5
Iteration 17800: Loss = -9797.294211259516
Iteration 17900: Loss = -9797.294205942435
Iteration 18000: Loss = -9797.296494176546
1
Iteration 18100: Loss = -9797.295385295922
2
Iteration 18200: Loss = -9797.294211693516
3
Iteration 18300: Loss = -9797.294175265317
Iteration 18400: Loss = -9797.56911969456
1
Iteration 18500: Loss = -9797.294227697586
2
Iteration 18600: Loss = -9797.294313198057
3
Iteration 18700: Loss = -9797.294250271272
4
Iteration 18800: Loss = -9797.294249963546
5
Iteration 18900: Loss = -9797.294203957848
6
Iteration 19000: Loss = -9797.294321712588
7
Iteration 19100: Loss = -9797.29912031072
8
Iteration 19200: Loss = -9797.294210800528
9
Iteration 19300: Loss = -9797.294188270484
10
Stopping early at iteration 19300 due to no improvement.
tensor([[ -9.0091,   7.5339],
        [ -9.1121,   7.3913],
        [ -8.8443,   6.9579],
        [-10.2777,   6.7297],
        [ -9.5667,   6.9838],
        [ -8.9688,   7.5491],
        [ -8.7740,   6.8051],
        [ -8.8214,   7.2806],
        [ -8.3352,   6.7180],
        [-10.0677,   5.5320],
        [ -9.6362,   5.0209],
        [-11.2008,   7.0479],
        [ -9.1084,   7.2950],
        [ -8.6790,   6.8003],
        [ -8.2285,   6.7515],
        [ -7.7250,   6.3277],
        [ -9.9262,   6.7207],
        [ -8.7036,   7.1675],
        [ -8.8188,   7.4111],
        [ -8.9608,   6.7825],
        [-10.1879,   7.6906],
        [ -8.9178,   7.5196],
        [ -9.1695,   7.5232],
        [ -9.1831,   7.4412],
        [-10.2485,   6.6620],
        [ -8.4463,   6.8788],
        [ -9.3623,   6.7951],
        [ -8.9938,   7.5500],
        [-11.8396,   7.3337],
        [ -8.0415,   6.5161],
        [ -9.1128,   6.5484],
        [ -9.3537,   6.3026],
        [ -8.5734,   5.8832],
        [ -7.9460,   6.4580],
        [ -8.5636,   7.1617],
        [ -8.8121,   6.7318],
        [ -8.7106,   7.2725],
        [ -9.1243,   7.6047],
        [-10.6060,   5.9908],
        [ -9.3233,   7.5383],
        [ -9.3002,   7.4939],
        [ -8.0454,   6.5278],
        [ -8.2757,   6.8893],
        [ -7.9124,   6.5056],
        [ -8.2567,   5.6284],
        [-10.2235,   7.1861],
        [ -8.1127,   6.5000],
        [ -8.7326,   7.0968],
        [-10.4671,   7.0092],
        [ -9.0430,   7.2662],
        [ -8.4712,   6.2242],
        [-10.0187,   5.9555],
        [-10.1565,   7.4495],
        [ -9.2984,   7.6374],
        [ -8.7494,   5.2414],
        [ -9.2449,   7.4180],
        [ -7.8115,   6.4042],
        [-10.2266,   7.0056],
        [ -9.8376,   6.5738],
        [ -7.3555,   5.9632],
        [ -9.0580,   7.5181],
        [ -8.9487,   6.8062],
        [ -7.9035,   6.2343],
        [ -9.6291,   7.0153],
        [ -7.6472,   5.5881],
        [ -8.9778,   6.6489],
        [  0.3788,  -2.0243],
        [ -8.3507,   6.9600],
        [ -7.1438,   5.7323],
        [ -9.3293,   7.6328],
        [ -8.8907,   5.9325],
        [ -8.7046,   7.0350],
        [ -8.7997,   7.2768],
        [ -7.8724,   6.4599],
        [ -8.7465,   7.0934],
        [ -8.0918,   6.6965],
        [ -9.3483,   7.7265],
        [ -9.8291,   7.6772],
        [ -9.1291,   7.6002],
        [ -9.6212,   7.5665],
        [ -9.2680,   7.2359],
        [ -8.1569,   6.7492],
        [ -9.2898,   6.6808],
        [ -8.6799,   7.2767],
        [ -7.0345,   5.4514],
        [ -8.7888,   7.3147],
        [ -9.0675,   7.6783],
        [ -8.7609,   7.3690],
        [ -8.7322,   7.2548],
        [ -7.3069,   5.8238],
        [-10.2795,   7.3698],
        [ -7.4209,   6.0156],
        [-10.0694,   7.2696],
        [ -8.7026,   7.3063],
        [ -8.4811,   5.6691],
        [ -9.1200,   7.5309],
        [ -9.5375,   6.1691],
        [ -8.0993,   5.8935],
        [-10.0055,   7.8853],
        [ -8.5405,   6.1273]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 1.1062e-05],
        [5.1676e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0092, 0.9908], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1461, 0.2020],
         [0.7768, 0.1349]],

        [[0.7225, 0.1010],
         [0.6411, 0.3099]],

        [[0.7858, 0.0909],
         [0.8896, 0.1968]],

        [[0.9717, 0.2322],
         [0.6880, 0.3387]],

        [[0.6470, 0.1010],
         [0.0134, 0.0376]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.0003567551354141363
Average Adjusted Rand Index: 0.0005525063461823855
9972.982863569872
new:  [0.041296639816264, -0.0003567551354141363, 0.036441564817859526, -0.0003567551354141363] [0.03665590942975774, 0.0005525063461823855, 0.0311866914963852, 0.0005525063461823855] [9795.942502353977, 9797.294203474306, 9795.942031102715, 9797.294188270484]
prior:  [0.0, 0.02893078709592487, 0.02893078709592487, 0.0] [0.0, 0.030567647759473137, 0.030567647759473137, 0.0] [nan, 9796.575460130849, 9796.575588169791, 9799.052928198622]
-----------------------------------------------------------------------------------------
This iteration is 18
True Objective function: Loss = -9998.446041528518
Iteration 0: Loss = -11878.848632125746
Iteration 10: Loss = -9869.364641010683
Iteration 20: Loss = -9869.361231351051
Iteration 30: Loss = -9869.289017510602
Iteration 40: Loss = -9868.707067266347
Iteration 50: Loss = -9867.892146918146
Iteration 60: Loss = -9867.421012958393
Iteration 70: Loss = -9867.18088854717
Iteration 80: Loss = -9867.074088452877
Iteration 90: Loss = -9867.030892673807
Iteration 100: Loss = -9867.017298476569
Iteration 110: Loss = -9867.014742194446
Iteration 120: Loss = -9867.015482629795
1
Iteration 130: Loss = -9867.01687008389
2
Iteration 140: Loss = -9867.01820157606
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[9.3277e-01, 6.7230e-02],
        [1.0000e+00, 2.6516e-07]], dtype=torch.float64)
alpha: tensor([0.9372, 0.0628])
beta: tensor([[[0.1370, 0.1598],
         [0.9753, 0.1199]],

        [[0.0424, 0.0780],
         [0.9651, 0.6529]],

        [[0.2852, 0.1379],
         [0.7932, 0.4798]],

        [[0.5354, 0.1279],
         [0.4377, 0.3319]],

        [[0.7807, 0.1091],
         [0.7734, 0.5734]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 9.716743485269148e-05
Average Adjusted Rand Index: 0.00033344555058593435
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -12656.931077242814
Iteration 100: Loss = -9871.74650892909
Iteration 200: Loss = -9870.272051442724
Iteration 300: Loss = -9869.78552964178
Iteration 400: Loss = -9869.549954215368
Iteration 500: Loss = -9869.415562119033
Iteration 600: Loss = -9869.330048951242
Iteration 700: Loss = -9869.27159522448
Iteration 800: Loss = -9869.231161544669
Iteration 900: Loss = -9869.204869958572
Iteration 1000: Loss = -9869.186785651698
Iteration 1100: Loss = -9869.17339192534
Iteration 1200: Loss = -9869.162843176324
Iteration 1300: Loss = -9869.153518666752
Iteration 1400: Loss = -9869.144447192806
Iteration 1500: Loss = -9869.134874104258
Iteration 1600: Loss = -9869.124129037387
Iteration 1700: Loss = -9869.11134954977
Iteration 1800: Loss = -9869.095265832028
Iteration 1900: Loss = -9869.0738533051
Iteration 2000: Loss = -9869.043842658284
Iteration 2100: Loss = -9868.99973232979
Iteration 2200: Loss = -9868.930999700335
Iteration 2300: Loss = -9868.803990986455
Iteration 2400: Loss = -9867.392223954792
Iteration 2500: Loss = -9865.483413855678
Iteration 2600: Loss = -9865.243166214113
Iteration 2700: Loss = -9865.163083673058
Iteration 2800: Loss = -9865.120810862738
Iteration 2900: Loss = -9865.096425319887
Iteration 3000: Loss = -9865.083540340876
Iteration 3100: Loss = -9865.076891947561
Iteration 3200: Loss = -9865.072097731121
Iteration 3300: Loss = -9865.065818380923
Iteration 3400: Loss = -9865.061119795324
Iteration 3500: Loss = -9865.056655024804
Iteration 3600: Loss = -9865.053706962053
Iteration 3700: Loss = -9865.050938469441
Iteration 3800: Loss = -9865.048343318429
Iteration 3900: Loss = -9865.046044082337
Iteration 4000: Loss = -9865.04426438711
Iteration 4100: Loss = -9865.042752105846
Iteration 4200: Loss = -9865.041456578512
Iteration 4300: Loss = -9865.04016230828
Iteration 4400: Loss = -9865.18550150783
1
Iteration 4500: Loss = -9865.038146616656
Iteration 4600: Loss = -9865.037255084717
Iteration 4700: Loss = -9865.038580650587
1
Iteration 4800: Loss = -9865.035893173103
Iteration 4900: Loss = -9865.035292026228
Iteration 5000: Loss = -9865.034811308215
Iteration 5100: Loss = -9865.034386299398
Iteration 5200: Loss = -9865.033948341812
Iteration 5300: Loss = -9865.033593773624
Iteration 5400: Loss = -9865.045181722846
1
Iteration 5500: Loss = -9865.032970920047
Iteration 5600: Loss = -9865.032650470155
Iteration 5700: Loss = -9865.462387920683
1
Iteration 5800: Loss = -9865.03218471477
Iteration 5900: Loss = -9865.031963031835
Iteration 6000: Loss = -9865.031722222247
Iteration 6100: Loss = -9865.031612267723
Iteration 6200: Loss = -9865.031352782384
Iteration 6300: Loss = -9865.03116788094
Iteration 6400: Loss = -9865.038032213364
1
Iteration 6500: Loss = -9865.030847196755
Iteration 6600: Loss = -9865.030721369536
Iteration 6700: Loss = -9865.053355349837
1
Iteration 6800: Loss = -9865.030453053405
Iteration 6900: Loss = -9865.030287672122
Iteration 7000: Loss = -9865.459102165263
1
Iteration 7100: Loss = -9865.03009270867
Iteration 7200: Loss = -9865.029970950156
Iteration 7300: Loss = -9865.029922349497
Iteration 7400: Loss = -9865.029802317988
Iteration 7500: Loss = -9865.029695246323
Iteration 7600: Loss = -9865.029592976454
Iteration 7700: Loss = -9865.030216045507
1
Iteration 7800: Loss = -9865.029434493063
Iteration 7900: Loss = -9865.029353642976
Iteration 8000: Loss = -9865.13401179024
1
Iteration 8100: Loss = -9865.02922433044
Iteration 8200: Loss = -9865.029135005076
Iteration 8300: Loss = -9865.029067861347
Iteration 8400: Loss = -9865.029349646138
1
Iteration 8500: Loss = -9865.0289918874
Iteration 8600: Loss = -9865.02891885192
Iteration 8700: Loss = -9865.051125762388
1
Iteration 8800: Loss = -9865.028824434163
Iteration 8900: Loss = -9865.028776467865
Iteration 9000: Loss = -9865.193061430995
1
Iteration 9100: Loss = -9865.028661745493
Iteration 9200: Loss = -9865.028624489742
Iteration 9300: Loss = -9865.028584106392
Iteration 9400: Loss = -9865.028564252985
Iteration 9500: Loss = -9865.028525463673
Iteration 9600: Loss = -9865.028454711619
Iteration 9700: Loss = -9865.028519502888
1
Iteration 9800: Loss = -9865.02839819427
Iteration 9900: Loss = -9865.028375173832
Iteration 10000: Loss = -9865.02851460672
1
Iteration 10100: Loss = -9865.028337392425
Iteration 10200: Loss = -9865.029820695048
1
Iteration 10300: Loss = -9865.028288717931
Iteration 10400: Loss = -9865.028584511278
1
Iteration 10500: Loss = -9865.028234804906
Iteration 10600: Loss = -9865.028191855632
Iteration 10700: Loss = -9865.028180969466
Iteration 10800: Loss = -9865.042510012052
1
Iteration 10900: Loss = -9865.028158902234
Iteration 11000: Loss = -9865.028143213282
Iteration 11100: Loss = -9865.030884816899
1
Iteration 11200: Loss = -9865.028082893854
Iteration 11300: Loss = -9865.0280576367
Iteration 11400: Loss = -9865.028373714445
1
Iteration 11500: Loss = -9865.028071271503
2
Iteration 11600: Loss = -9865.028059957831
3
Iteration 11700: Loss = -9865.032270854756
4
Iteration 11800: Loss = -9865.028019850652
Iteration 11900: Loss = -9865.028158890456
1
Iteration 12000: Loss = -9865.03105603654
2
Iteration 12100: Loss = -9865.028063245736
3
Iteration 12200: Loss = -9865.027989022885
Iteration 12300: Loss = -9865.027979779004
Iteration 12400: Loss = -9865.027993051115
1
Iteration 12500: Loss = -9865.034101643065
2
Iteration 12600: Loss = -9865.027930696759
Iteration 12700: Loss = -9865.02960402562
1
Iteration 12800: Loss = -9865.027953100422
2
Iteration 12900: Loss = -9865.037039159275
3
Iteration 13000: Loss = -9865.028308825948
4
Iteration 13100: Loss = -9865.029789812896
5
Iteration 13200: Loss = -9865.027898181894
Iteration 13300: Loss = -9865.028014793286
1
Iteration 13400: Loss = -9865.02820220397
2
Iteration 13500: Loss = -9865.027919333483
3
Iteration 13600: Loss = -9865.027971051015
4
Iteration 13700: Loss = -9865.027932397634
5
Iteration 13800: Loss = -9865.055226123677
6
Iteration 13900: Loss = -9865.027894172721
Iteration 14000: Loss = -9865.029538852823
1
Iteration 14100: Loss = -9865.027900207064
2
Iteration 14200: Loss = -9865.034715582915
3
Iteration 14300: Loss = -9865.027898238102
4
Iteration 14400: Loss = -9865.048264510477
5
Iteration 14500: Loss = -9865.027860475457
Iteration 14600: Loss = -9865.027831917017
Iteration 14700: Loss = -9865.028126745094
1
Iteration 14800: Loss = -9865.028409509445
2
Iteration 14900: Loss = -9865.05513375133
3
Iteration 15000: Loss = -9865.028696562971
4
Iteration 15100: Loss = -9865.027906921185
5
Iteration 15200: Loss = -9865.034327878482
6
Iteration 15300: Loss = -9865.027910946079
7
Iteration 15400: Loss = -9865.028336262341
8
Iteration 15500: Loss = -9865.027807814868
Iteration 15600: Loss = -9865.033960804001
1
Iteration 15700: Loss = -9865.029236560604
2
Iteration 15800: Loss = -9865.028085753976
3
Iteration 15900: Loss = -9865.027967310534
4
Iteration 16000: Loss = -9865.027957510723
5
Iteration 16100: Loss = -9865.288929827468
6
Iteration 16200: Loss = -9865.02789954807
7
Iteration 16300: Loss = -9865.028742263377
8
Iteration 16400: Loss = -9865.029009737498
9
Iteration 16500: Loss = -9865.028424037631
10
Stopping early at iteration 16500 due to no improvement.
tensor([[ 1.8053e+00, -6.4205e+00],
        [-3.6878e-01, -4.2464e+00],
        [ 2.2256e+00, -6.8408e+00],
        [ 1.5599e+00, -6.1751e+00],
        [ 3.5826e+00, -8.1979e+00],
        [ 1.8032e+00, -6.4184e+00],
        [ 1.6491e+00, -6.2644e+00],
        [ 3.0459e+00, -7.6611e+00],
        [-3.8055e+00, -8.0968e-01],
        [ 1.2042e+00, -5.8194e+00],
        [ 8.9436e-01, -5.5096e+00],
        [-2.0087e-01, -4.4143e+00],
        [ 4.4653e-01, -5.0618e+00],
        [ 1.6242e+00, -6.2394e+00],
        [ 9.6767e-01, -5.5829e+00],
        [ 3.7298e+00, -8.3450e+00],
        [ 1.3403e+00, -5.9555e+00],
        [ 4.6241e+00, -9.2393e+00],
        [ 2.4838e+00, -7.0990e+00],
        [ 3.7579e+00, -8.3732e+00],
        [ 6.7073e-01, -5.2859e+00],
        [ 4.0229e+00, -8.6381e+00],
        [ 1.0703e+00, -5.6855e+00],
        [ 4.6045e+00, -9.2197e+00],
        [ 1.2093e+00, -5.8245e+00],
        [ 7.5396e-01, -5.3692e+00],
        [ 5.2231e+00, -9.8384e+00],
        [-2.8142e-01, -4.3338e+00],
        [ 1.8176e+00, -6.4328e+00],
        [ 7.2925e-01, -5.3445e+00],
        [ 1.3377e+00, -5.9529e+00],
        [ 1.3773e-01, -4.7529e+00],
        [-3.4041e-01, -4.2748e+00],
        [ 4.2094e-01, -5.0362e+00],
        [ 3.8526e-01, -5.0005e+00],
        [ 1.9528e+00, -6.5680e+00],
        [-2.8149e+00, -1.8003e+00],
        [ 3.4510e-01, -4.9603e+00],
        [ 4.0238e+00, -8.6390e+00],
        [ 1.0737e+00, -5.6889e+00],
        [ 1.7378e+00, -6.3530e+00],
        [ 1.0725e+00, -5.6877e+00],
        [-7.8650e-02, -4.5366e+00],
        [ 3.7387e+00, -8.3539e+00],
        [ 2.9749e+00, -7.5902e+00],
        [-3.1740e+00, -1.4412e+00],
        [-7.7864e-01, -3.8366e+00],
        [ 3.4770e+00, -8.0922e+00],
        [ 1.3898e+00, -6.0050e+00],
        [ 9.3160e-01, -5.5468e+00],
        [ 2.1154e+00, -6.7306e+00],
        [ 4.8369e-01, -5.0989e+00],
        [ 3.9927e+00, -8.6080e+00],
        [ 3.0901e-01, -4.9242e+00],
        [-1.8498e-01, -4.4302e+00],
        [ 1.2834e+00, -5.8986e+00],
        [ 2.9788e+00, -7.5940e+00],
        [-1.3170e+00, -3.2982e+00],
        [-1.4401e-01, -4.4712e+00],
        [ 1.5719e+00, -6.1871e+00],
        [ 1.6509e+00, -6.2661e+00],
        [ 8.7712e-03, -4.6240e+00],
        [ 2.6517e+00, -7.2669e+00],
        [ 4.5773e+00, -9.1925e+00],
        [ 1.9478e+00, -6.5630e+00],
        [ 1.4954e+00, -6.1106e+00],
        [ 3.7233e-01, -4.9876e+00],
        [-1.6890e+00, -2.9262e+00],
        [-1.7983e+00, -2.8169e+00],
        [ 1.0650e+00, -5.6802e+00],
        [ 1.5386e+00, -6.1538e+00],
        [ 4.4524e+00, -9.0676e+00],
        [-3.2935e-01, -4.2859e+00],
        [-3.2800e+00, -1.3353e+00],
        [-1.2743e+00, -3.3409e+00],
        [ 3.3586e-01, -4.9511e+00],
        [ 2.7887e+00, -7.4039e+00],
        [-2.8454e-01, -4.3307e+00],
        [ 6.5310e-01, -5.2683e+00],
        [-1.3815e+00, -3.2337e+00],
        [ 1.5770e-01, -4.7729e+00],
        [ 1.1747e+00, -5.7899e+00],
        [ 2.3710e+00, -6.9863e+00],
        [ 2.0870e+00, -6.7022e+00],
        [ 2.0639e+00, -6.6792e+00],
        [-1.2346e+00, -3.3807e+00],
        [ 3.8864e+00, -8.5016e+00],
        [ 4.8313e+00, -9.4465e+00],
        [ 1.7915e+00, -6.4067e+00],
        [ 1.0988e+00, -5.7140e+00],
        [ 1.7617e+00, -6.3770e+00],
        [ 8.3939e-01, -5.4546e+00],
        [ 9.0068e-01, -5.5159e+00],
        [-3.0685e+00, -1.5467e+00],
        [-6.1340e-01, -4.0018e+00],
        [ 4.4648e-01, -5.0617e+00],
        [-1.8981e+00, -2.7171e+00],
        [ 1.7686e+00, -6.3838e+00],
        [-6.0226e-01, -4.0130e+00],
        [ 1.7700e+00, -6.3852e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.5535e-07],
        [4.8986e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9419, 0.0581], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1383, 0.1658],
         [0.9753, 0.1431]],

        [[0.0424, 0.0648],
         [0.9651, 0.6529]],

        [[0.2852, 0.1243],
         [0.7932, 0.4798]],

        [[0.5354, 0.1062],
         [0.4377, 0.3319]],

        [[0.7807, 0.1432],
         [0.7734, 0.5734]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006438680677883739
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.006809752538456861
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.009618156350865796
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0015221131464426677
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
Global Adjusted Rand Index: 0.0018075522867661733
Average Adjusted Rand Index: 0.001882043885437519
Iteration 0: Loss = -24459.461673476544
Iteration 10: Loss = -9868.305724046922
Iteration 20: Loss = -9867.453148019635
Iteration 30: Loss = -9867.000205223281
Iteration 40: Loss = -9866.92517522876
Iteration 50: Loss = -9866.885501073837
Iteration 60: Loss = -9866.87306023399
Iteration 70: Loss = -9866.870838370038
Iteration 80: Loss = -9866.871289099528
1
Iteration 90: Loss = -9866.872360018388
2
Iteration 100: Loss = -9866.873483321835
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.9292, 0.0708],
        [0.9222, 0.0778]], dtype=torch.float64)
alpha: tensor([0.9292, 0.0708])
beta: tensor([[[0.1376, 0.1556],
         [0.9243, 0.1169]],

        [[0.1313, 0.0777],
         [0.9068, 0.9114]],

        [[0.5853, 0.1344],
         [0.8255, 0.4542]],

        [[0.9878, 0.1268],
         [0.3987, 0.4671]],

        [[0.3135, 0.1094],
         [0.9031, 0.6909]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00038770756378241793
Average Adjusted Rand Index: 0.0014863187797952812
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24459.17661292739
Iteration 100: Loss = -9905.989678401245
Iteration 200: Loss = -9881.32344100226
Iteration 300: Loss = -9872.76283839598
Iteration 400: Loss = -9871.307106016087
Iteration 500: Loss = -9870.575438514214
Iteration 600: Loss = -9870.144982265687
Iteration 700: Loss = -9869.873437924778
Iteration 800: Loss = -9869.687560181144
Iteration 900: Loss = -9869.555903503302
Iteration 1000: Loss = -9869.459677857867
Iteration 1100: Loss = -9869.38614589133
Iteration 1200: Loss = -9869.327604299047
Iteration 1300: Loss = -9869.279189468329
Iteration 1400: Loss = -9869.238217811606
Iteration 1500: Loss = -9869.20249326789
Iteration 1600: Loss = -9869.170105057417
Iteration 1700: Loss = -9869.140127211187
Iteration 1800: Loss = -9869.111446240935
Iteration 1900: Loss = -9869.08280032939
Iteration 2000: Loss = -9869.052952765727
Iteration 2100: Loss = -9869.022136326137
Iteration 2200: Loss = -9868.989598848282
Iteration 2300: Loss = -9868.955587119819
Iteration 2400: Loss = -9868.92025292061
Iteration 2500: Loss = -9868.881545088501
Iteration 2600: Loss = -9868.833337237718
Iteration 2700: Loss = -9868.76346217838
Iteration 2800: Loss = -9868.63497253839
Iteration 2900: Loss = -9868.434920627285
Iteration 3000: Loss = -9868.282273824901
Iteration 3100: Loss = -9868.064678686009
Iteration 3200: Loss = -9866.558674535032
Iteration 3300: Loss = -9866.009809042345
Iteration 3400: Loss = -9865.863085516896
Iteration 3500: Loss = -9865.796426872428
Iteration 3600: Loss = -9865.74977944284
Iteration 3700: Loss = -9865.687117803078
Iteration 3800: Loss = -9865.640150048674
Iteration 3900: Loss = -9865.59308856159
Iteration 4000: Loss = -9865.568276864033
Iteration 4100: Loss = -9865.5490031259
Iteration 4200: Loss = -9865.50966230713
Iteration 4300: Loss = -9865.472862181825
Iteration 4400: Loss = -9865.447299721223
Iteration 4500: Loss = -9865.399099038654
Iteration 4600: Loss = -9865.385941260472
Iteration 4700: Loss = -9865.374769330852
Iteration 4800: Loss = -9865.371009745495
Iteration 4900: Loss = -9865.365835022665
Iteration 5000: Loss = -9865.360372763758
Iteration 5100: Loss = -9865.265628955256
Iteration 5200: Loss = -9865.2537829694
Iteration 5300: Loss = -9865.23881011105
Iteration 5400: Loss = -9865.178430893851
Iteration 5500: Loss = -9865.1755908825
Iteration 5600: Loss = -9865.173494090528
Iteration 5700: Loss = -9865.171677896007
Iteration 5800: Loss = -9865.1689871764
Iteration 5900: Loss = -9865.166122420855
Iteration 6000: Loss = -9865.16353586286
Iteration 6100: Loss = -9865.160732088183
Iteration 6200: Loss = -9865.156936808073
Iteration 6300: Loss = -9865.152314153209
Iteration 6400: Loss = -9865.145799116879
Iteration 6500: Loss = -9865.13805480564
Iteration 6600: Loss = -9865.130573969931
Iteration 6700: Loss = -9865.124046505925
Iteration 6800: Loss = -9865.11880471041
Iteration 6900: Loss = -9865.113589231447
Iteration 7000: Loss = -9865.108128483804
Iteration 7100: Loss = -9865.10478868304
Iteration 7200: Loss = -9865.12143567191
1
Iteration 7300: Loss = -9865.100118157474
Iteration 7400: Loss = -9865.212266893283
1
Iteration 7500: Loss = -9865.097036606297
Iteration 7600: Loss = -9865.095792906759
Iteration 7700: Loss = -9865.095481257662
Iteration 7800: Loss = -9865.093783418935
Iteration 7900: Loss = -9865.093002172076
Iteration 8000: Loss = -9865.092455233796
Iteration 8100: Loss = -9865.09162498611
Iteration 8200: Loss = -9865.091066615429
Iteration 8300: Loss = -9865.090641965102
Iteration 8400: Loss = -9865.090040705007
Iteration 8500: Loss = -9865.089596209762
Iteration 8600: Loss = -9865.090500943836
1
Iteration 8700: Loss = -9865.088807461518
Iteration 8800: Loss = -9865.088436710264
Iteration 8900: Loss = -9865.138226443667
1
Iteration 9000: Loss = -9865.087720897665
Iteration 9100: Loss = -9865.087329960053
Iteration 9200: Loss = -9865.08683051944
Iteration 9300: Loss = -9865.086451886691
Iteration 9400: Loss = -9865.086087386117
Iteration 9500: Loss = -9865.085875646431
Iteration 9600: Loss = -9865.086448520284
1
Iteration 9700: Loss = -9865.085545215297
Iteration 9800: Loss = -9865.085399895534
Iteration 9900: Loss = -9865.08918608523
1
Iteration 10000: Loss = -9865.085124357725
Iteration 10100: Loss = -9865.085025555773
Iteration 10200: Loss = -9865.087989868338
1
Iteration 10300: Loss = -9865.084780370582
Iteration 10400: Loss = -9865.084700258023
Iteration 10500: Loss = -9865.084606100361
Iteration 10600: Loss = -9865.084547746103
Iteration 10700: Loss = -9865.08428039003
Iteration 10800: Loss = -9865.091470149531
1
Iteration 10900: Loss = -9865.083919307777
Iteration 11000: Loss = -9865.083510409026
Iteration 11100: Loss = -9865.083355286639
Iteration 11200: Loss = -9865.27182211489
1
Iteration 11300: Loss = -9865.083206641639
Iteration 11400: Loss = -9865.205893175778
1
Iteration 11500: Loss = -9865.082983095672
Iteration 11600: Loss = -9865.082941385732
Iteration 11700: Loss = -9865.082950028183
1
Iteration 11800: Loss = -9865.082818707353
Iteration 11900: Loss = -9865.082772043825
Iteration 12000: Loss = -9865.083155190376
1
Iteration 12100: Loss = -9865.082698051381
Iteration 12200: Loss = -9865.082674727188
Iteration 12300: Loss = -9865.083703260972
1
Iteration 12400: Loss = -9865.082594235513
Iteration 12500: Loss = -9865.082549139428
Iteration 12600: Loss = -9865.114154824836
1
Iteration 12700: Loss = -9865.082501330153
Iteration 12800: Loss = -9865.082450794407
Iteration 12900: Loss = -9865.327936763657
1
Iteration 13000: Loss = -9865.078961249772
Iteration 13100: Loss = -9865.078938005
Iteration 13200: Loss = -9865.078899883942
Iteration 13300: Loss = -9865.078938046494
1
Iteration 13400: Loss = -9865.078851740367
Iteration 13500: Loss = -9865.07883505168
Iteration 13600: Loss = -9865.079472461754
1
Iteration 13700: Loss = -9865.07879984387
Iteration 13800: Loss = -9865.078801158392
1
Iteration 13900: Loss = -9865.078998048346
2
Iteration 14000: Loss = -9865.079119022812
3
Iteration 14100: Loss = -9865.078755502758
Iteration 14200: Loss = -9865.07853354413
Iteration 14300: Loss = -9865.078764470198
1
Iteration 14400: Loss = -9865.07851819465
Iteration 14500: Loss = -9865.080210617767
1
Iteration 14600: Loss = -9865.078481139848
Iteration 14700: Loss = -9865.204323132308
1
Iteration 14800: Loss = -9865.07846235411
Iteration 14900: Loss = -9865.07884973844
1
Iteration 15000: Loss = -9865.078444027902
Iteration 15100: Loss = -9865.07843376082
Iteration 15200: Loss = -9865.078577716127
1
Iteration 15300: Loss = -9865.078417938557
Iteration 15400: Loss = -9865.10380673801
1
Iteration 15500: Loss = -9865.078377477499
Iteration 15600: Loss = -9865.072865908802
Iteration 15700: Loss = -9865.073712933427
1
Iteration 15800: Loss = -9865.07283956626
Iteration 15900: Loss = -9865.073778055084
1
Iteration 16000: Loss = -9865.072881980084
2
Iteration 16100: Loss = -9865.072816352342
Iteration 16200: Loss = -9865.079048260011
1
Iteration 16300: Loss = -9865.073084272834
2
Iteration 16400: Loss = -9865.073216600236
3
Iteration 16500: Loss = -9865.097686813824
4
Iteration 16600: Loss = -9865.072962551725
5
Iteration 16700: Loss = -9865.318195419353
6
Iteration 16800: Loss = -9865.072804855108
Iteration 16900: Loss = -9865.072798237548
Iteration 17000: Loss = -9865.073218443276
1
Iteration 17100: Loss = -9865.072772739852
Iteration 17200: Loss = -9865.072794235388
1
Iteration 17300: Loss = -9865.073936208457
2
Iteration 17400: Loss = -9865.072849586315
3
Iteration 17500: Loss = -9865.074477528677
4
Iteration 17600: Loss = -9865.072803314863
5
Iteration 17700: Loss = -9865.07355369908
6
Iteration 17800: Loss = -9865.072781703304
7
Iteration 17900: Loss = -9865.081002190647
8
Iteration 18000: Loss = -9865.072762454563
Iteration 18100: Loss = -9865.117420539373
1
Iteration 18200: Loss = -9865.072773380578
2
Iteration 18300: Loss = -9865.087451011186
3
Iteration 18400: Loss = -9865.069155204987
Iteration 18500: Loss = -9865.0711615709
1
Iteration 18600: Loss = -9865.069318862248
2
Iteration 18700: Loss = -9865.072084907928
3
Iteration 18800: Loss = -9865.069257900468
4
Iteration 18900: Loss = -9865.069680482979
5
Iteration 19000: Loss = -9865.069108916941
Iteration 19100: Loss = -9865.070237452312
1
Iteration 19200: Loss = -9865.069135839865
2
Iteration 19300: Loss = -9865.085685719261
3
Iteration 19400: Loss = -9865.068215390613
Iteration 19500: Loss = -9865.069068533365
1
Iteration 19600: Loss = -9865.083966883589
2
Iteration 19700: Loss = -9865.068225062774
3
Iteration 19800: Loss = -9865.080046835248
4
Iteration 19900: Loss = -9865.068271588878
5
tensor([[ 3.4882, -4.8777],
        [ 0.7913, -3.2596],
        [ 5.3044, -8.1786],
        [ 6.2609, -8.3688],
        [ 6.2458, -8.1216],
        [ 6.0718, -8.8281],
        [ 6.6126, -8.0827],
        [ 5.9191, -8.5367],
        [-2.2378,  0.7829],
        [ 6.3505, -7.7507],
        [ 5.7980, -7.8385],
        [ 6.6975, -8.1018],
        [ 1.7784, -3.8470],
        [ 6.8312, -8.2368],
        [ 2.5960, -4.1355],
        [ 6.8409, -8.4250],
        [ 6.3557, -7.7493],
        [ 6.1085, -8.2226],
        [ 6.0175, -7.4521],
        [ 6.7858, -8.1753],
        [ 2.2486, -3.8900],
        [ 6.7209, -8.1084],
        [ 1.1505, -5.7657],
        [ 6.2403, -7.6266],
        [ 6.1649, -7.7697],
        [ 0.8140, -5.4293],
        [ 5.4955, -9.8653],
        [ 0.9810, -3.1441],
        [ 3.4647, -4.9184],
        [ 5.1992, -9.8144],
        [ 6.2878, -7.6825],
        [ 1.1599, -3.8759],
        [ 1.2469, -2.7232],
        [ 1.9620, -3.6426],
        [ 2.0625, -3.4679],
        [ 3.3152, -5.3448],
        [-1.1917, -0.1997],
        [ 0.6939, -4.7690],
        [ 6.4117, -7.8566],
        [ 2.8100, -4.1965],
        [ 5.7922, -8.7822],
        [ 2.6937, -4.1963],
        [ 6.0616, -7.4637],
        [ 6.1584, -8.4327],
        [ 6.7415, -8.9747],
        [-1.9684, -0.2437],
        [ 0.7391, -2.3660],
        [ 5.2239, -9.5357],
        [ 6.9205, -8.3320],
        [ 2.5823, -4.1336],
        [ 5.9391, -8.7971],
        [ 2.1067, -3.6493],
        [ 6.4637, -8.6332],
        [ 1.8699, -3.4961],
        [ 1.3283, -3.0410],
        [ 5.7485, -9.0987],
        [ 7.2029, -8.6641],
        [ 0.3083, -1.7080],
        [ 0.5859, -3.9230],
        [ 6.2066, -7.5930],
        [ 6.3592, -8.7971],
        [ 1.4149, -3.3838],
        [ 6.3340, -8.7841],
        [ 6.1512, -7.7187],
        [ 5.3287, -9.9440],
        [ 3.1685, -4.5585],
        [ 2.0677, -3.4654],
        [-0.2350, -1.5266],
        [-1.2556, -2.4086],
        [ 2.5854, -4.4386],
        [ 2.7616, -5.0795],
        [ 1.4589, -3.9863],
        [ 0.2020, -3.8533],
        [-1.8196,  0.1316],
        [ 0.3475, -1.7965],
        [ 6.1129, -8.4530],
        [ 6.2841, -7.8324],
        [ 1.3421, -2.7285],
        [ 2.2002, -3.9280],
        [-0.0712, -1.9755],
        [ 1.5752, -3.4340],
        [ 6.3105, -8.0809],
        [ 5.2767, -9.1692],
        [ 6.9392, -8.5552],
        [ 5.4483, -8.6493],
        [ 0.3822, -1.9083],
        [ 6.6957, -8.1536],
        [ 6.1183, -8.7055],
        [ 7.0770, -8.4633],
        [ 2.7639, -4.1517],
        [ 3.1625, -5.2061],
        [ 5.6136, -8.7523],
        [ 6.2423, -8.1465],
        [-1.6934, -0.1029],
        [ 0.6050, -2.8737],
        [ 1.8678, -3.7803],
        [-0.2211, -1.1689],
        [ 6.7397, -8.6535],
        [ 0.9884, -2.5153],
        [ 6.5515, -8.4019]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.4632e-07],
        [8.3222e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9431, 0.0569], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1385, 0.1664],
         [0.9243, 0.1457]],

        [[0.1313, 0.0641],
         [0.9068, 0.9114]],

        [[0.5853, 0.1238],
         [0.8255, 0.4542]],

        [[0.9878, 0.1062],
         [0.3987, 0.4671]],

        [[0.3135, 0.1435],
         [0.9031, 0.6909]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006438680677883739
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.006809752538456861
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.009618156350865796
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0015221131464426677
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
Global Adjusted Rand Index: 0.0018075522867661733
Average Adjusted Rand Index: 0.001882043885437519
Iteration 0: Loss = -18485.86119095477
Iteration 10: Loss = -9868.057670478815
Iteration 20: Loss = -9867.955278896445
Iteration 30: Loss = -9867.895529677688
Iteration 40: Loss = -9867.844516485438
Iteration 50: Loss = -9867.79502975565
Iteration 60: Loss = -9867.746596632902
Iteration 70: Loss = -9867.702776063103
Iteration 80: Loss = -9867.668594636702
Iteration 90: Loss = -9867.647475106935
Iteration 100: Loss = -9867.639686641096
Iteration 110: Loss = -9867.642379575811
1
Iteration 120: Loss = -9867.652242723367
2
Iteration 130: Loss = -9867.666327068258
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[0.1984, 0.8016],
        [0.1922, 0.8078]], dtype=torch.float64)
alpha: tensor([0.1916, 0.8084])
beta: tensor([[[0.1159, 0.1411],
         [0.9404, 0.1404]],

        [[0.3205, 0.1122],
         [0.3958, 0.7429]],

        [[0.5319, 0.1322],
         [0.6113, 0.8343]],

        [[0.0407, 0.1283],
         [0.9732, 0.9554]],

        [[0.5511, 0.1204],
         [0.6737, 0.8415]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 9.716743485269148e-05
Average Adjusted Rand Index: 0.00033344555058593435
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18485.448855851315
Iteration 100: Loss = -9950.711942048614
Iteration 200: Loss = -9875.877216883913
Iteration 300: Loss = -9872.037303804042
Iteration 400: Loss = -9870.76870068016
Iteration 500: Loss = -9870.124122818659
Iteration 600: Loss = -9869.74333001659
Iteration 700: Loss = -9869.495329010613
Iteration 800: Loss = -9869.32217066202
Iteration 900: Loss = -9869.195600850715
Iteration 1000: Loss = -9869.100509318421
Iteration 1100: Loss = -9869.027062758973
Iteration 1200: Loss = -9868.968512092564
Iteration 1300: Loss = -9868.920853778614
Iteration 1400: Loss = -9868.880865551972
Iteration 1500: Loss = -9868.846843923504
Iteration 1600: Loss = -9868.817913017368
Iteration 1700: Loss = -9868.793699385222
Iteration 1800: Loss = -9868.7732171579
Iteration 1900: Loss = -9868.7556384932
Iteration 2000: Loss = -9868.740426825441
Iteration 2100: Loss = -9868.727125934882
Iteration 2200: Loss = -9868.715378902145
Iteration 2300: Loss = -9868.704937141896
Iteration 2400: Loss = -9868.695595639327
Iteration 2500: Loss = -9868.68719821771
Iteration 2600: Loss = -9868.679602260261
Iteration 2700: Loss = -9868.672743994666
Iteration 2800: Loss = -9868.666476350283
Iteration 2900: Loss = -9868.66085668232
Iteration 3000: Loss = -9868.655719205131
Iteration 3100: Loss = -9868.650931614395
Iteration 3200: Loss = -9868.646436899451
Iteration 3300: Loss = -9868.641874151192
Iteration 3400: Loss = -9868.6355424128
Iteration 3500: Loss = -9868.628913535658
Iteration 3600: Loss = -9868.623836111328
Iteration 3700: Loss = -9868.617735795122
Iteration 3800: Loss = -9868.60808865074
Iteration 3900: Loss = -9868.583596577228
Iteration 4000: Loss = -9868.529833709748
Iteration 4100: Loss = -9868.501592062416
Iteration 4200: Loss = -9868.468303223459
Iteration 4300: Loss = -9868.458738286117
Iteration 4400: Loss = -9868.454746455798
Iteration 4500: Loss = -9868.451939522132
Iteration 4600: Loss = -9868.449577956668
Iteration 4700: Loss = -9868.447462893519
Iteration 4800: Loss = -9868.44543615921
Iteration 4900: Loss = -9868.443431992786
Iteration 5000: Loss = -9868.441454115526
Iteration 5100: Loss = -9868.439130231582
Iteration 5200: Loss = -9868.43745140017
Iteration 5300: Loss = -9868.436342480505
Iteration 5400: Loss = -9868.435352969835
Iteration 5500: Loss = -9868.43450225683
Iteration 5600: Loss = -9868.433695716292
Iteration 5700: Loss = -9868.432844648698
Iteration 5800: Loss = -9868.431855827424
Iteration 5900: Loss = -9868.480797411328
1
Iteration 6000: Loss = -9868.092990797899
Iteration 6100: Loss = -9868.083625183748
Iteration 6200: Loss = -9868.128793917049
1
Iteration 6300: Loss = -9868.081424019172
Iteration 6400: Loss = -9868.080613354296
Iteration 6500: Loss = -9868.081937252226
1
Iteration 6600: Loss = -9868.079278669604
Iteration 6700: Loss = -9868.078545863844
Iteration 6800: Loss = -9868.078705742979
1
Iteration 6900: Loss = -9868.073787216103
Iteration 7000: Loss = -9868.072903485421
Iteration 7100: Loss = -9868.066254132078
Iteration 7200: Loss = -9868.060323839156
Iteration 7300: Loss = -9868.059308488597
Iteration 7400: Loss = -9868.05865153427
Iteration 7500: Loss = -9868.059719933131
1
Iteration 7600: Loss = -9867.951258357161
Iteration 7700: Loss = -9867.950464765736
Iteration 7800: Loss = -9868.158988760895
1
Iteration 7900: Loss = -9867.949725593775
Iteration 8000: Loss = -9867.949438046859
Iteration 8100: Loss = -9867.958961526012
1
Iteration 8200: Loss = -9867.948938809872
Iteration 8300: Loss = -9867.949426405881
1
Iteration 8400: Loss = -9867.948797604455
Iteration 8500: Loss = -9867.948205673874
Iteration 8600: Loss = -9867.97003728902
1
Iteration 8700: Loss = -9867.947778047735
Iteration 8800: Loss = -9867.947580112475
Iteration 8900: Loss = -9867.947397986345
Iteration 9000: Loss = -9867.956186989883
1
Iteration 9100: Loss = -9867.947056476527
Iteration 9200: Loss = -9867.951636122301
1
Iteration 9300: Loss = -9867.929533537788
Iteration 9400: Loss = -9867.92858612146
Iteration 9500: Loss = -9867.9468140347
1
Iteration 9600: Loss = -9867.92843827901
Iteration 9700: Loss = -9867.928384747154
Iteration 9800: Loss = -9867.929696885863
1
Iteration 9900: Loss = -9867.930100072013
2
Iteration 10000: Loss = -9867.928342401707
Iteration 10100: Loss = -9867.928200348511
Iteration 10200: Loss = -9867.938706352325
1
Iteration 10300: Loss = -9867.92802023688
Iteration 10400: Loss = -9867.929397266787
1
Iteration 10500: Loss = -9867.92822898694
2
Iteration 10600: Loss = -9867.92790836249
Iteration 10700: Loss = -9867.9278163331
Iteration 10800: Loss = -9867.900495918724
Iteration 10900: Loss = -9867.949880581287
1
Iteration 11000: Loss = -9867.916101571329
2
Iteration 11100: Loss = -9867.900280302889
Iteration 11200: Loss = -9867.900501159247
1
Iteration 11300: Loss = -9867.91308733633
2
Iteration 11400: Loss = -9867.901401634243
3
Iteration 11500: Loss = -9867.964599599009
4
Iteration 11600: Loss = -9867.902624347642
5
Iteration 11700: Loss = -9867.900013537605
Iteration 11800: Loss = -9867.93821332852
1
Iteration 11900: Loss = -9867.89989675381
Iteration 12000: Loss = -9867.899903942101
1
Iteration 12100: Loss = -9867.90153432359
2
Iteration 12200: Loss = -9867.899848272209
Iteration 12300: Loss = -9867.899877402193
1
Iteration 12400: Loss = -9867.899820391542
Iteration 12500: Loss = -9867.899782643944
Iteration 12600: Loss = -9867.900356708751
1
Iteration 12700: Loss = -9867.974310342435
2
Iteration 12800: Loss = -9867.900385435525
3
Iteration 12900: Loss = -9867.905515658105
4
Iteration 13000: Loss = -9867.889745513035
Iteration 13100: Loss = -9867.889813861251
1
Iteration 13200: Loss = -9867.894940948896
2
Iteration 13300: Loss = -9867.967119629924
3
Iteration 13400: Loss = -9867.895143122747
4
Iteration 13500: Loss = -9867.936203234889
5
Iteration 13600: Loss = -9867.912686506106
6
Iteration 13700: Loss = -9867.891355977212
7
Iteration 13800: Loss = -9867.901698571564
8
Iteration 13900: Loss = -9867.8900441332
9
Iteration 14000: Loss = -9867.889316909397
Iteration 14100: Loss = -9867.889501057254
1
Iteration 14200: Loss = -9867.89324286181
2
Iteration 14300: Loss = -9867.913941755696
3
Iteration 14400: Loss = -9867.901219873424
4
Iteration 14500: Loss = -9867.889341819391
5
Iteration 14600: Loss = -9867.88930816295
Iteration 14700: Loss = -9867.928603881666
1
Iteration 14800: Loss = -9867.889286126765
Iteration 14900: Loss = -9867.898992139775
1
Iteration 15000: Loss = -9867.889279976445
Iteration 15100: Loss = -9867.895446006425
1
Iteration 15200: Loss = -9867.894190222396
2
Iteration 15300: Loss = -9867.911700315204
3
Iteration 15400: Loss = -9867.892780097229
4
Iteration 15500: Loss = -9867.889421349906
5
Iteration 15600: Loss = -9867.903024754467
6
Iteration 15700: Loss = -9867.90331718995
7
Iteration 15800: Loss = -9867.890630466414
8
Iteration 15900: Loss = -9867.901463188826
9
Iteration 16000: Loss = -9867.88625740777
Iteration 16100: Loss = -9867.88783837326
1
Iteration 16200: Loss = -9867.905959229285
2
Iteration 16300: Loss = -9867.886211751384
Iteration 16400: Loss = -9867.88669354482
1
Iteration 16500: Loss = -9868.006638056817
2
Iteration 16600: Loss = -9867.886127059017
Iteration 16700: Loss = -9867.930529156401
1
Iteration 16800: Loss = -9867.8861487119
2
Iteration 16900: Loss = -9868.006926036396
3
Iteration 17000: Loss = -9867.886123993196
Iteration 17100: Loss = -9867.888725634288
1
Iteration 17200: Loss = -9867.88617083668
2
Iteration 17300: Loss = -9867.887257664948
3
Iteration 17400: Loss = -9867.886159408012
4
Iteration 17500: Loss = -9867.886111624653
Iteration 17600: Loss = -9867.886245558797
1
Iteration 17700: Loss = -9867.886469884606
2
Iteration 17800: Loss = -9867.886778963162
3
Iteration 17900: Loss = -9867.906984534558
4
Iteration 18000: Loss = -9867.887526962673
5
Iteration 18100: Loss = -9867.886206943826
6
Iteration 18200: Loss = -9867.886156305098
7
Iteration 18300: Loss = -9867.938890692809
8
Iteration 18400: Loss = -9867.886129930484
9
Iteration 18500: Loss = -9867.901651791826
10
Stopping early at iteration 18500 due to no improvement.
tensor([[ -9.5582,   8.1719],
        [ -9.7775,   8.2841],
        [-10.3733,   8.6892],
        [-10.8952,   9.1110],
        [ -9.8507,   8.2778],
        [-10.7482,   7.6226],
        [-10.5647,   9.1662],
        [ -9.9012,   8.5079],
        [ -9.3827,   7.9459],
        [-10.5305,   8.0876],
        [-10.0751,   8.6685],
        [-10.2026,   8.3012],
        [-10.2288,   8.5144],
        [ -9.9204,   7.7231],
        [-10.2291,   8.2602],
        [ -9.6493,   8.2241],
        [-10.4041,   9.0010],
        [ -9.9281,   8.3368],
        [-10.2645,   8.2771],
        [ -9.2961,   7.9098],
        [-10.0700,   7.5114],
        [-10.7737,   8.7005],
        [-10.5034,   8.0778],
        [ -9.9953,   8.3298],
        [ -9.5189,   8.0068],
        [ -9.8890,   8.4557],
        [-10.6621,   7.8394],
        [ -9.9379,   8.3316],
        [-10.0155,   8.4324],
        [-10.3360,   8.2322],
        [-10.0582,   8.5514],
        [ -9.4945,   8.0818],
        [-10.3133,   8.2499],
        [ -9.9370,   8.4793],
        [ -9.7981,   8.3155],
        [ -9.4664,   8.0469],
        [-10.3222,   8.6432],
        [-10.2628,   8.4213],
        [-10.3026,   8.6685],
        [-10.2426,   8.8210],
        [-10.3938,   8.9842],
        [-10.5458,   8.3317],
        [ -9.9941,   8.4442],
        [ -9.3122,   7.6131],
        [ -9.6271,   8.1093],
        [-10.0534,   8.5185],
        [-10.3302,   8.1495],
        [-10.6322,   8.9785],
        [-10.0537,   8.2715],
        [-11.0854,   8.6771],
        [-10.1589,   8.7697],
        [-10.7381,   8.7464],
        [ -9.7211,   8.1929],
        [-10.4907,   8.8959],
        [ -9.9020,   8.3262],
        [ -9.8066,   8.1728],
        [-10.5407,   8.4225],
        [-10.1814,   8.7941],
        [-10.1148,   8.1317],
        [ -9.6242,   8.1648],
        [-10.0630,   8.1699],
        [ -9.8530,   8.4628],
        [-10.0764,   8.3869],
        [ -9.9125,   8.4193],
        [ -9.6556,   7.5650],
        [-10.6735,   8.7973],
        [-10.5041,   7.7997],
        [ -9.9146,   7.9910],
        [-10.4310,   8.5972],
        [-10.5987,   8.5542],
        [-10.0011,   8.5033],
        [-10.9350,   8.5286],
        [-10.1954,   8.7421],
        [-10.7250,   8.9694],
        [-10.3390,   8.8352],
        [-12.0845,   7.8579],
        [-10.5748,   7.9405],
        [ -9.5560,   8.0361],
        [ -9.6508,   8.1840],
        [ -9.8018,   8.4156],
        [ -9.8866,   7.8942],
        [-10.2782,   8.7883],
        [-10.4608,   7.4936],
        [ -9.8021,   8.3795],
        [ -9.8710,   8.4847],
        [-10.6873,   8.2606],
        [-11.8879,   8.2826],
        [-10.0305,   8.6440],
        [ -9.8963,   8.3183],
        [ -9.4088,   7.9280],
        [ -9.8787,   8.4830],
        [ -9.7619,   8.3347],
        [-10.1478,   8.4084],
        [-10.4918,   8.5837],
        [-10.6216,   8.5470],
        [-11.1737,   8.1659],
        [-10.9227,   7.8412],
        [-10.5554,   9.0155],
        [-10.4535,   8.4918],
        [-11.2199,   8.1654]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.5514e-02, 9.8449e-01],
        [1.0000e+00, 8.6750e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.1575e-08, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1336, 0.1476],
         [0.9404, 0.1396]],

        [[0.3205, 0.1511],
         [0.3958, 0.7429]],

        [[0.5319, 0.1163],
         [0.6113, 0.8343]],

        [[0.0407, 0.1349],
         [0.9732, 0.9554]],

        [[0.5511, 0.0754],
         [0.6737, 0.8415]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
Global Adjusted Rand Index: -0.0019228838346359532
Average Adjusted Rand Index: -0.0007444228734768356
Iteration 0: Loss = -22923.922386649807
Iteration 10: Loss = -9867.148654514112
Iteration 20: Loss = -9866.918772368275
Iteration 30: Loss = -9866.893559467115
Iteration 40: Loss = -9866.887798546019
Iteration 50: Loss = -9866.884706622675
Iteration 60: Loss = -9866.88256949459
Iteration 70: Loss = -9866.881087257876
Iteration 80: Loss = -9866.880058749886
Iteration 90: Loss = -9866.8793345469
Iteration 100: Loss = -9866.878831233216
Iteration 110: Loss = -9866.878421283192
Iteration 120: Loss = -9866.878143618515
Iteration 130: Loss = -9866.877955228703
Iteration 140: Loss = -9866.877840872268
Iteration 150: Loss = -9866.877708534781
Iteration 160: Loss = -9866.877659084228
Iteration 170: Loss = -9866.87756904721
Iteration 180: Loss = -9866.877550576037
Iteration 190: Loss = -9866.87755505926
1
Iteration 200: Loss = -9866.877490924218
Iteration 210: Loss = -9866.877464201363
Iteration 220: Loss = -9866.877458598505
Iteration 230: Loss = -9866.877476226986
1
Iteration 240: Loss = -9866.877437807529
Iteration 250: Loss = -9866.877438475867
1
Iteration 260: Loss = -9866.877470892321
2
Iteration 270: Loss = -9866.87743876041
3
Stopping early at iteration 269 due to no improvement.
pi: tensor([[0.0799, 0.9201],
        [0.0727, 0.9273]], dtype=torch.float64)
alpha: tensor([0.0727, 0.9273])
beta: tensor([[[0.1164, 0.1548],
         [0.8257, 0.1377]],

        [[0.8557, 0.0781],
         [0.3605, 0.0107]],

        [[0.8241, 0.1341],
         [0.4429, 0.7752]],

        [[0.1393, 0.1266],
         [0.2440, 0.9122]],

        [[0.8347, 0.1097],
         [0.7665, 0.3219]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00038770756378241793
Average Adjusted Rand Index: 0.0014863187797952812
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22924.33674735252
Iteration 100: Loss = -9905.573540532523
Iteration 200: Loss = -9886.440003075582
Iteration 300: Loss = -9872.925615023996
Iteration 400: Loss = -9871.290199344943
Iteration 500: Loss = -9870.628053594122
Iteration 600: Loss = -9870.240663170523
Iteration 700: Loss = -9869.983576828354
Iteration 800: Loss = -9869.79612329247
Iteration 900: Loss = -9869.643208617328
Iteration 1000: Loss = -9869.509614165429
Iteration 1100: Loss = -9869.407560954163
Iteration 1200: Loss = -9869.325880084376
Iteration 1300: Loss = -9869.25999102191
Iteration 1400: Loss = -9869.205695165007
Iteration 1500: Loss = -9869.160550285664
Iteration 1600: Loss = -9869.12418505603
Iteration 1700: Loss = -9869.094437641994
Iteration 1800: Loss = -9869.06934238797
Iteration 1900: Loss = -9869.046813295512
Iteration 2000: Loss = -9869.0246558879
Iteration 2100: Loss = -9869.001175847523
Iteration 2200: Loss = -9868.974711955543
Iteration 2300: Loss = -9868.942576574342
Iteration 2400: Loss = -9868.8998488074
Iteration 2500: Loss = -9868.837076815758
Iteration 2600: Loss = -9868.732556875686
Iteration 2700: Loss = -9868.532973369205
Iteration 2800: Loss = -9868.192408808447
Iteration 2900: Loss = -9867.876400705129
Iteration 3000: Loss = -9867.63708891352
Iteration 3100: Loss = -9867.515658983366
Iteration 3200: Loss = -9867.405470639455
Iteration 3300: Loss = -9867.354475351523
Iteration 3400: Loss = -9867.322312977765
Iteration 3500: Loss = -9867.302819666469
Iteration 3600: Loss = -9867.289369648306
Iteration 3700: Loss = -9867.279342504848
Iteration 3800: Loss = -9867.271568853368
Iteration 3900: Loss = -9867.265205843261
Iteration 4000: Loss = -9867.259640048915
Iteration 4100: Loss = -9867.254581015648
Iteration 4200: Loss = -9867.249621322639
Iteration 4300: Loss = -9867.244345446954
Iteration 4400: Loss = -9867.238353069915
Iteration 4500: Loss = -9867.231031657719
Iteration 4600: Loss = -9867.221999540989
Iteration 4700: Loss = -9867.211182076893
Iteration 4800: Loss = -9867.19818724901
Iteration 4900: Loss = -9867.182235547822
Iteration 5000: Loss = -9867.164190196383
Iteration 5100: Loss = -9867.146879497132
Iteration 5200: Loss = -9867.132483691443
Iteration 5300: Loss = -9867.119058933713
Iteration 5400: Loss = -9867.108055912639
Iteration 5500: Loss = -9867.124527604607
1
Iteration 5600: Loss = -9867.090798858151
Iteration 5700: Loss = -9867.084156955514
Iteration 5800: Loss = -9867.078229278166
Iteration 5900: Loss = -9867.071038829146
Iteration 6000: Loss = -9867.047880189246
Iteration 6100: Loss = -9867.042905727656
Iteration 6200: Loss = -9867.088781506152
1
Iteration 6300: Loss = -9867.036520552265
Iteration 6400: Loss = -9867.03407191483
Iteration 6500: Loss = -9867.032003980958
Iteration 6600: Loss = -9867.031209296922
Iteration 6700: Loss = -9867.02867130436
Iteration 6800: Loss = -9867.027329079765
Iteration 6900: Loss = -9867.13849122486
1
Iteration 7000: Loss = -9867.025059394695
Iteration 7100: Loss = -9867.024076566004
Iteration 7200: Loss = -9867.029656692463
1
Iteration 7300: Loss = -9867.025039843249
2
Iteration 7400: Loss = -9867.021846175012
Iteration 7500: Loss = -9867.021311037974
Iteration 7600: Loss = -9867.021783936794
1
Iteration 7700: Loss = -9867.020239623185
Iteration 7800: Loss = -9867.019675955205
Iteration 7900: Loss = -9867.019417968831
Iteration 8000: Loss = -9867.018840488488
Iteration 8100: Loss = -9867.025686816714
1
Iteration 8200: Loss = -9867.018119530154
Iteration 8300: Loss = -9867.018007408193
Iteration 8400: Loss = -9867.017591314558
Iteration 8500: Loss = -9867.017304906
Iteration 8600: Loss = -9867.017048891217
Iteration 8700: Loss = -9867.016815663092
Iteration 8800: Loss = -9867.024356086013
1
Iteration 8900: Loss = -9867.01748819746
2
Iteration 9000: Loss = -9867.047089395346
3
Iteration 9100: Loss = -9867.016712485842
Iteration 9200: Loss = -9867.016064752093
Iteration 9300: Loss = -9867.015758050715
Iteration 9400: Loss = -9867.015519777924
Iteration 9500: Loss = -9867.016711583161
1
Iteration 9600: Loss = -9867.016956729882
2
Iteration 9700: Loss = -9867.016100606352
3
Iteration 9800: Loss = -9867.015230795132
Iteration 9900: Loss = -9867.015910065855
1
Iteration 10000: Loss = -9867.014859472305
Iteration 10100: Loss = -9867.017787992188
1
Iteration 10200: Loss = -9867.014679243175
Iteration 10300: Loss = -9867.015159987508
1
Iteration 10400: Loss = -9867.014548981218
Iteration 10500: Loss = -9867.031172361672
1
Iteration 10600: Loss = -9867.016756705081
2
Iteration 10700: Loss = -9867.01429441002
Iteration 10800: Loss = -9867.014320717728
1
Iteration 10900: Loss = -9867.018336323743
2
Iteration 11000: Loss = -9867.015005658679
3
Iteration 11100: Loss = -9867.082237586337
4
Iteration 11200: Loss = -9867.014023609823
Iteration 11300: Loss = -9867.042935542195
1
Iteration 11400: Loss = -9867.014106449922
2
Iteration 11500: Loss = -9867.014498731229
3
Iteration 11600: Loss = -9867.014195772206
4
Iteration 11700: Loss = -9867.013841065484
Iteration 11800: Loss = -9867.014110339613
1
Iteration 11900: Loss = -9867.018876816264
2
Iteration 12000: Loss = -9867.013698070698
Iteration 12100: Loss = -9867.014734820968
1
Iteration 12200: Loss = -9867.193205228674
2
Iteration 12300: Loss = -9867.016731176263
3
Iteration 12400: Loss = -9867.022319700562
4
Iteration 12500: Loss = -9867.013961336936
5
Iteration 12600: Loss = -9867.015722146263
6
Iteration 12700: Loss = -9867.013494545654
Iteration 12800: Loss = -9867.013510541612
1
Iteration 12900: Loss = -9867.017866284712
2
Iteration 13000: Loss = -9867.013616375221
3
Iteration 13100: Loss = -9867.013569316223
4
Iteration 13200: Loss = -9867.023260714424
5
Iteration 13300: Loss = -9867.013392545223
Iteration 13400: Loss = -9867.014088303942
1
Iteration 13500: Loss = -9867.02323451857
2
Iteration 13600: Loss = -9867.01353948817
3
Iteration 13700: Loss = -9867.01594096765
4
Iteration 13800: Loss = -9867.013352677384
Iteration 13900: Loss = -9867.032367677526
1
Iteration 14000: Loss = -9867.013283661656
Iteration 14100: Loss = -9867.047646398436
1
Iteration 14200: Loss = -9867.013284206514
2
Iteration 14300: Loss = -9867.036559551494
3
Iteration 14400: Loss = -9867.12335954695
4
Iteration 14500: Loss = -9867.0255353282
5
Iteration 14600: Loss = -9867.0139012001
6
Iteration 14700: Loss = -9867.146580682884
7
Iteration 14800: Loss = -9867.016564756063
8
Iteration 14900: Loss = -9867.013440292787
9
Iteration 15000: Loss = -9867.01462220906
10
Stopping early at iteration 15000 due to no improvement.
tensor([[ 2.9528, -4.3449],
        [ 2.7364, -4.6030],
        [ 1.5612, -5.7317],
        [ 2.9695, -4.3560],
        [ 1.3409, -5.9561],
        [ 2.9302, -4.4043],
        [ 2.9144, -4.4200],
        [ 2.8854, -4.4405],
        [ 2.9522, -4.3819],
        [ 2.5928, -4.7223],
        [ 2.6161, -4.6804],
        [ 2.5149, -4.8398],
        [ 2.5197, -4.8027],
        [ 2.9677, -4.3564],
        [ 2.9648, -4.3912],
        [ 2.9802, -4.3673],
        [ 2.9597, -4.3467],
        [ 2.3518, -4.9839],
        [ 2.8538, -4.4340],
        [ 2.8974, -4.4310],
        [ 2.8934, -4.4237],
        [ 2.9251, -4.3531],
        [ 2.3810, -4.9467],
        [ 2.5890, -4.7142],
        [ 2.2879, -5.0323],
        [ 2.8668, -4.4482],
        [ 2.8712, -4.3217],
        [ 2.9140, -4.3785],
        [ 2.9651, -4.3516],
        [ 1.3596, -5.9748],
        [ 2.6992, -4.6087],
        [ 2.9655, -4.3529],
        [ 2.6011, -4.7115],
        [ 2.5681, -4.7820],
        [ 2.7355, -4.6051],
        [ 2.6237, -4.6943],
        [ 2.9653, -4.3518],
        [ 2.9120, -4.3995],
        [ 2.7445, -4.4740],
        [ 1.8243, -5.5146],
        [ 2.9761, -4.3624],
        [ 1.3670, -5.9822],
        [ 2.6188, -4.6848],
        [ 2.2409, -5.0336],
        [ 2.6483, -4.6869],
        [ 2.6534, -4.6772],
        [ 2.8682, -4.4477],
        [ 2.7433, -4.5798],
        [ 2.8818, -4.4728],
        [ 2.9054, -4.4632],
        [ 2.5827, -4.7383],
        [ 2.3848, -4.9624],
        [ 2.6782, -4.6519],
        [ 2.5296, -4.8094],
        [ 2.8797, -4.4449],
        [ 2.3735, -4.9550],
        [ 2.2864, -5.0586],
        [ 2.7886, -4.5247],
        [ 2.9356, -4.4123],
        [ 2.9441, -4.3688],
        [ 2.9600, -4.3691],
        [ 2.9735, -4.3629],
        [ 2.9259, -4.4092],
        [ 2.4911, -4.7471],
        [ 2.9155, -4.4191],
        [ 2.9822, -4.3696],
        [ 2.9119, -4.4261],
        [ 2.9641, -4.3602],
        [ 2.8657, -4.4830],
        [ 2.8897, -4.4576],
        [ 2.9481, -4.3663],
        [ 2.8667, -4.4262],
        [ 2.5989, -4.7404],
        [ 2.8625, -4.4675],
        [ 2.5379, -4.8092],
        [ 2.5243, -4.8167],
        [ 2.3057, -4.9872],
        [ 2.4042, -4.9176],
        [ 2.8655, -4.4888],
        [ 2.7846, -4.5450],
        [ 2.9085, -4.4242],
        [ 1.3611, -5.9763],
        [ 2.8731, -4.4472],
        [ 2.4831, -4.8459],
        [ 2.9633, -4.3519],
        [ 2.9670, -4.3746],
        [ 2.9622, -4.3658],
        [ 2.9763, -4.3627],
        [ 2.9311, -4.4355],
        [ 2.7531, -4.5679],
        [ 2.8634, -4.4756],
        [ 2.5300, -4.8062],
        [ 2.8383, -4.4857],
        [ 2.8714, -4.4416],
        [ 2.8574, -4.4811],
        [ 2.6071, -4.7212],
        [ 2.9843, -4.3730],
        [ 2.5554, -4.7829],
        [ 2.9819, -4.3790],
        [ 2.9642, -4.3811]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.2466e-05, 9.9999e-01],
        [1.4876e-02, 9.8512e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9934e-01, 6.5908e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1453, 0.1458],
         [0.8257, 0.1345]],

        [[0.8557, 0.1735],
         [0.3605, 0.0107]],

        [[0.8241, 0.1579],
         [0.4429, 0.7752]],

        [[0.1393, 0.1746],
         [0.2440, 0.9122]],

        [[0.8347, 0.0769],
         [0.7665, 0.3219]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013199815872971228
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -33663.39522130998
Iteration 10: Loss = -9869.366724955578
Iteration 20: Loss = -9869.36662502801
Iteration 30: Loss = -9869.363974557657
Iteration 40: Loss = -9869.324558241444
Iteration 50: Loss = -9868.916920961812
Iteration 60: Loss = -9868.040727530899
Iteration 70: Loss = -9867.508986972698
Iteration 80: Loss = -9867.220043902864
Iteration 90: Loss = -9867.08601532121
Iteration 100: Loss = -9867.007131816772
Iteration 110: Loss = -9866.934631431304
Iteration 120: Loss = -9866.88935254722
Iteration 130: Loss = -9866.873922983024
Iteration 140: Loss = -9866.870489349842
Iteration 150: Loss = -9866.870589688891
1
Iteration 160: Loss = -9866.871578866068
2
Iteration 170: Loss = -9866.872717183329
3
Stopping early at iteration 169 due to no improvement.
pi: tensor([[0.0774, 0.9226],
        [0.0704, 0.9296]], dtype=torch.float64)
alpha: tensor([0.0704, 0.9296])
beta: tensor([[[0.1170, 0.1558],
         [0.5447, 0.1375]],

        [[0.4533, 0.0776],
         [0.7118, 0.1614]],

        [[0.9843, 0.1345],
         [0.7538, 0.9097]],

        [[0.8719, 0.1268],
         [0.3452, 0.1384]],

        [[0.2562, 0.1093],
         [0.8865, 0.7237]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00038770756378241793
Average Adjusted Rand Index: 0.0014863187797952812
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33662.932382594256
Iteration 100: Loss = -9879.415949583785
Iteration 200: Loss = -9873.381491851782
Iteration 300: Loss = -9871.596836928575
Iteration 400: Loss = -9870.697940915426
Iteration 500: Loss = -9870.172443593565
Iteration 600: Loss = -9869.848563247726
Iteration 700: Loss = -9869.639648535856
Iteration 800: Loss = -9869.502702046097
Iteration 900: Loss = -9869.408461626883
Iteration 1000: Loss = -9869.339016237167
Iteration 1100: Loss = -9869.28499177694
Iteration 1200: Loss = -9869.240776082852
Iteration 1300: Loss = -9869.202499286264
Iteration 1400: Loss = -9869.16664619132
Iteration 1500: Loss = -9869.130337792609
Iteration 1600: Loss = -9869.09290336977
Iteration 1700: Loss = -9869.052893344253
Iteration 1800: Loss = -9869.009187582762
Iteration 1900: Loss = -9868.961086652873
Iteration 2000: Loss = -9868.906737926673
Iteration 2100: Loss = -9868.841872755915
Iteration 2200: Loss = -9868.759319223693
Iteration 2300: Loss = -9868.643767629545
Iteration 2400: Loss = -9868.473249546083
Iteration 2500: Loss = -9868.134540800773
Iteration 2600: Loss = -9867.720133606357
Iteration 2700: Loss = -9867.549062119642
Iteration 2800: Loss = -9867.436798810551
Iteration 2900: Loss = -9867.331283827329
Iteration 3000: Loss = -9867.22636581609
Iteration 3100: Loss = -9867.089105812962
Iteration 3200: Loss = -9866.906617788693
Iteration 3300: Loss = -9865.737737805051
Iteration 3400: Loss = -9865.47135143031
Iteration 3500: Loss = -9865.363333607174
Iteration 3600: Loss = -9865.256477025074
Iteration 3700: Loss = -9865.229334331338
Iteration 3800: Loss = -9865.213746221672
Iteration 3900: Loss = -9865.201288703794
Iteration 4000: Loss = -9865.189704270459
Iteration 4100: Loss = -9865.17569149671
Iteration 4200: Loss = -9865.166190794263
Iteration 4300: Loss = -9865.15732722644
Iteration 4400: Loss = -9865.15348053574
Iteration 4500: Loss = -9865.150765421151
Iteration 4600: Loss = -9865.148408480502
Iteration 4700: Loss = -9865.145889621239
Iteration 4800: Loss = -9865.142953375573
Iteration 4900: Loss = -9865.139606213183
Iteration 5000: Loss = -9865.136864481725
Iteration 5100: Loss = -9865.133439827674
Iteration 5200: Loss = -9865.130019492815
Iteration 5300: Loss = -9865.128933513259
Iteration 5400: Loss = -9865.128082301848
Iteration 5500: Loss = -9865.127270869716
Iteration 5600: Loss = -9865.126465042396
Iteration 5700: Loss = -9865.125707383299
Iteration 5800: Loss = -9865.124869121952
Iteration 5900: Loss = -9865.124115247289
Iteration 6000: Loss = -9865.123322331388
Iteration 6100: Loss = -9865.122487037217
Iteration 6200: Loss = -9865.121078899618
Iteration 6300: Loss = -9865.11880974369
Iteration 6400: Loss = -9865.116633499376
Iteration 6500: Loss = -9865.11397320375
Iteration 6600: Loss = -9865.110982095548
Iteration 6700: Loss = -9865.110091807452
Iteration 6800: Loss = -9865.109419370856
Iteration 6900: Loss = -9865.10880881662
Iteration 7000: Loss = -9865.10842849155
Iteration 7100: Loss = -9865.108045885894
Iteration 7200: Loss = -9865.107662593555
Iteration 7300: Loss = -9865.107231443424
Iteration 7400: Loss = -9865.106665751711
Iteration 7500: Loss = -9865.105177992316
Iteration 7600: Loss = -9865.100853234793
Iteration 7700: Loss = -9865.100308886298
Iteration 7800: Loss = -9865.099649636439
Iteration 7900: Loss = -9865.098531052798
Iteration 8000: Loss = -9865.097552345323
Iteration 8100: Loss = -9865.095637823257
Iteration 8200: Loss = -9865.094296641215
Iteration 8300: Loss = -9865.091620776067
Iteration 8400: Loss = -9865.090907017193
Iteration 8500: Loss = -9865.090045895924
Iteration 8600: Loss = -9865.088604557955
Iteration 8700: Loss = -9865.08656962339
Iteration 8800: Loss = -9865.083018619853
Iteration 8900: Loss = -9865.072889725758
Iteration 9000: Loss = -9865.062764725893
Iteration 9100: Loss = -9865.055803451323
Iteration 9200: Loss = -9865.051210445368
Iteration 9300: Loss = -9865.051013191402
Iteration 9400: Loss = -9865.092037254082
1
Iteration 9500: Loss = -9865.062342703135
2
Iteration 9600: Loss = -9865.097304583687
3
Iteration 9700: Loss = -9865.042693189125
Iteration 9800: Loss = -9865.04634045809
1
Iteration 9900: Loss = -9865.041581345022
Iteration 10000: Loss = -9865.042223340199
1
Iteration 10100: Loss = -9865.040564055405
Iteration 10200: Loss = -9865.042064565207
1
Iteration 10300: Loss = -9865.039933422117
Iteration 10400: Loss = -9865.0396339839
Iteration 10500: Loss = -9865.056835689966
1
Iteration 10600: Loss = -9865.039126534459
Iteration 10700: Loss = -9865.03890287428
Iteration 10800: Loss = -9865.03907940264
1
Iteration 10900: Loss = -9865.037859169594
Iteration 11000: Loss = -9865.037545383793
Iteration 11100: Loss = -9865.037400495254
Iteration 11200: Loss = -9865.037681148786
1
Iteration 11300: Loss = -9865.03728445909
Iteration 11400: Loss = -9865.037170564125
Iteration 11500: Loss = -9865.036772554267
Iteration 11600: Loss = -9865.035650400196
Iteration 11700: Loss = -9865.035544320848
Iteration 11800: Loss = -9865.036183322569
1
Iteration 11900: Loss = -9865.035426320042
Iteration 12000: Loss = -9865.035368495299
Iteration 12100: Loss = -9865.030436639307
Iteration 12200: Loss = -9865.030244825773
Iteration 12300: Loss = -9865.031552865119
1
Iteration 12400: Loss = -9865.028610004172
Iteration 12500: Loss = -9865.029660829754
1
Iteration 12600: Loss = -9865.02888374349
2
Iteration 12700: Loss = -9865.028484328393
Iteration 12800: Loss = -9865.039784569368
1
Iteration 12900: Loss = -9865.028436850786
Iteration 13000: Loss = -9865.029777427633
1
Iteration 13100: Loss = -9865.028366768
Iteration 13200: Loss = -9865.02834078544
Iteration 13300: Loss = -9865.029479182525
1
Iteration 13400: Loss = -9865.033706135364
2
Iteration 13500: Loss = -9865.028263941958
Iteration 13600: Loss = -9865.030517416419
1
Iteration 13700: Loss = -9865.181604440839
2
Iteration 13800: Loss = -9865.028274990967
3
Iteration 13900: Loss = -9865.028662731313
4
Iteration 14000: Loss = -9865.02779237318
Iteration 14100: Loss = -9865.029450564574
1
Iteration 14200: Loss = -9865.027795819049
2
Iteration 14300: Loss = -9865.02814173897
3
Iteration 14400: Loss = -9865.02791786629
4
Iteration 14500: Loss = -9865.034815344416
5
Iteration 14600: Loss = -9865.02871472405
6
Iteration 14700: Loss = -9865.151298161263
7
Iteration 14800: Loss = -9865.033207819664
8
Iteration 14900: Loss = -9865.027631278415
Iteration 15000: Loss = -9865.030814054971
1
Iteration 15100: Loss = -9865.027626763627
Iteration 15200: Loss = -9865.029559660232
1
Iteration 15300: Loss = -9865.027626753612
Iteration 15400: Loss = -9865.040733090267
1
Iteration 15500: Loss = -9865.027683776507
2
Iteration 15600: Loss = -9865.028511133629
3
Iteration 15700: Loss = -9865.028825556137
4
Iteration 15800: Loss = -9865.08318878465
5
Iteration 15900: Loss = -9865.028249029132
6
Iteration 16000: Loss = -9865.027204485665
Iteration 16100: Loss = -9865.027209208785
1
Iteration 16200: Loss = -9865.027203888676
Iteration 16300: Loss = -9865.028211071689
1
Iteration 16400: Loss = -9865.047750145002
2
Iteration 16500: Loss = -9865.031143705937
3
Iteration 16600: Loss = -9865.027295267479
4
Iteration 16700: Loss = -9865.055253403109
5
Iteration 16800: Loss = -9865.027571919216
6
Iteration 16900: Loss = -9865.028216664168
7
Iteration 17000: Loss = -9865.027143910072
Iteration 17100: Loss = -9865.02784449163
1
Iteration 17200: Loss = -9865.02842771694
2
Iteration 17300: Loss = -9865.033528395537
3
Iteration 17400: Loss = -9865.02730787684
4
Iteration 17500: Loss = -9865.092466999715
5
Iteration 17600: Loss = -9865.027316466168
6
Iteration 17700: Loss = -9865.02716458229
7
Iteration 17800: Loss = -9865.027217209576
8
Iteration 17900: Loss = -9865.063753145632
9
Iteration 18000: Loss = -9865.027137202158
Iteration 18100: Loss = -9865.093962184957
1
Iteration 18200: Loss = -9865.027124539256
Iteration 18300: Loss = -9865.027120351811
Iteration 18400: Loss = -9865.027117763853
Iteration 18500: Loss = -9865.027199014843
1
Iteration 18600: Loss = -9865.027861193188
2
Iteration 18700: Loss = -9865.027107261361
Iteration 18800: Loss = -9865.030005919547
1
Iteration 18900: Loss = -9865.026590699805
Iteration 19000: Loss = -9865.026452675624
Iteration 19100: Loss = -9865.026602638096
1
Iteration 19200: Loss = -9865.026441719938
Iteration 19300: Loss = -9865.083577215066
1
Iteration 19400: Loss = -9865.024805738392
Iteration 19500: Loss = -9865.024763393456
Iteration 19600: Loss = -9865.036953943169
1
Iteration 19700: Loss = -9865.02475220178
Iteration 19800: Loss = -9865.025413255147
1
Iteration 19900: Loss = -9865.023868342509
tensor([[-4.8848e+00,  3.3376e+00],
        [-2.9256e+00,  9.7408e-01],
        [-7.5217e+00,  6.0005e+00],
        [-8.1029e+00,  6.5255e+00],
        [-8.1079e+00,  6.2320e+00],
        [-4.9201e+00,  3.2853e+00],
        [-5.9893e+00,  1.9238e+00],
        [-8.4621e+00,  6.0888e+00],
        [-7.4715e-01, -3.7429e+00],
        [-4.4374e+00,  2.5992e+00],
        [-4.0302e+00,  2.3587e+00],
        [-2.8125e+00,  1.4229e+00],
        [-3.8117e+00,  1.6966e+00],
        [-4.6299e+00,  3.2161e+00],
        [-4.5451e+00,  2.0351e+00],
        [-8.7315e+00,  6.5026e+00],
        [-7.8420e+00,  6.2849e+00],
        [-4.0975e+00,  2.0547e+00],
        [-7.4813e+00,  6.0949e+00],
        [-6.8037e+00,  5.3214e+00],
        [-3.8056e+00,  2.1705e+00],
        [-8.1339e+00,  6.7356e+00],
        [-4.0800e+00,  2.6635e+00],
        [-4.6871e+00,  3.2606e+00],
        [-4.5787e+00,  2.4635e+00],
        [-3.9808e+00,  2.1198e+00],
        [-8.3684e+00,  6.8742e+00],
        [-3.9547e+00,  1.1324e-01],
        [-7.6661e+00,  5.9612e+00],
        [-4.0998e+00,  1.9846e+00],
        [-4.7623e+00,  2.5446e+00],
        [-3.4998e+00,  1.3990e+00],
        [-2.7496e+00,  1.1819e+00],
        [-4.4805e+00,  9.9811e-01],
        [-3.7178e+00,  1.6894e+00],
        [-7.9741e+00,  5.9917e+00],
        [-2.8457e-01, -1.2828e+00],
        [-3.3661e+00,  1.9611e+00],
        [-8.0290e+00,  6.1972e+00],
        [-4.2248e+00,  2.5593e+00],
        [-4.7598e+00,  3.3436e+00],
        [-4.3472e+00,  2.4283e+00],
        [-3.2490e+00,  1.2133e+00],
        [-8.2004e+00,  6.4688e+00],
        [-8.4625e+00,  7.0633e+00],
        [ 9.6029e-02, -1.6357e+00],
        [-2.2315e+00,  8.1617e-01],
        [-8.6260e+00,  6.5050e+00],
        [-4.3940e+00,  3.0077e+00],
        [-4.7352e+00,  1.7719e+00],
        [-8.2613e+00,  6.4823e+00],
        [-3.5287e+00,  2.0679e+00],
        [-8.5611e+00,  6.5887e+00],
        [-3.3517e+00,  1.9121e+00],
        [-2.8333e+00,  1.4195e+00],
        [-4.3361e+00,  2.8644e+00],
        [-8.6485e+00,  7.2419e+00],
        [-1.9693e+00,  1.9527e-03],
        [-3.0845e+00,  1.2619e+00],
        [-5.9299e+00,  1.8386e+00],
        [-5.8217e+00,  2.0965e+00],
        [-3.1379e+00,  1.5162e+00],
        [-6.2411e+00,  3.6805e+00],
        [-8.7413e+00,  5.2619e+00],
        [-8.6906e+00,  6.2912e+00],
        [-8.7431e+00,  6.6964e+00],
        [-3.4001e+00,  1.9829e+00],
        [-1.3191e+00, -6.8935e-02],
        [-2.8337e+00, -1.7815e+00],
        [-4.1861e+00,  2.6045e+00],
        [-7.8219e+00,  6.0281e+00],
        [-3.6824e+00,  1.7013e+00],
        [-2.9398e+00,  1.0376e+00],
        [ 8.0879e-02, -1.8458e+00],
        [-2.7971e+00, -7.1416e-01],
        [-3.7940e+00,  1.5070e+00],
        [-7.8134e+00,  6.3812e+00],
        [-2.7343e+00,  1.3260e+00],
        [-4.3066e+00,  1.6383e+00],
        [-1.8932e+00, -2.7336e-02],
        [-3.2879e+00,  1.6503e+00],
        [-4.8167e+00,  2.1587e+00],
        [-7.9912e+00,  6.4483e+00],
        [-8.4998e+00,  7.1090e+00],
        [-7.8239e+00,  6.4162e+00],
        [-1.7962e+00,  3.6805e-01],
        [-8.7719e+00,  6.3077e+00],
        [-5.5293e+00,  1.1138e+00],
        [-8.6670e+00,  6.7824e+00],
        [-5.3064e+00,  1.4928e+00],
        [-8.2656e+00,  6.4139e+00],
        [-4.2866e+00,  2.0197e+00],
        [-3.9097e+00,  2.5209e+00],
        [ 7.0464e-02, -1.4859e+00],
        [-2.4383e+00,  9.6865e-01],
        [-3.4582e+00,  2.0677e+00],
        [-1.2457e+00, -3.8967e-01],
        [-5.6147e+00,  2.5193e+00],
        [-2.4139e+00,  1.0168e+00],
        [-8.2977e+00,  6.8562e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 7.4044e-06],
        [7.4242e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0590, 0.9410], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1429, 0.1650],
         [0.5447, 0.1390]],

        [[0.4533, 0.0655],
         [0.7118, 0.1614]],

        [[0.9843, 0.1241],
         [0.7538, 0.9097]],

        [[0.8719, 0.1062],
         [0.3452, 0.1384]],

        [[0.2562, 0.1423],
         [0.8865, 0.7237]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0006438680677883739
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.006809752538456861
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.009618156350865796
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0015221131464426677
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
Global Adjusted Rand Index: 0.0018075522867661733
Average Adjusted Rand Index: 0.001882043885437519
9998.446041528518
new:  [0.0018075522867661733, -0.0019228838346359532, -0.0013199815872971228, 0.0018075522867661733] [0.001882043885437519, -0.0007444228734768356, 0.0, 0.001882043885437519] [9865.068324698572, 9867.901651791826, 9867.01462220906, 9865.032282359527]
prior:  [0.00038770756378241793, 9.716743485269148e-05, 0.00038770756378241793, 0.00038770756378241793] [0.0014863187797952812, 0.00033344555058593435, 0.0014863187797952812, 0.0014863187797952812] [9866.873483321835, 9867.666327068258, 9866.87743876041, 9866.872717183329]
-----------------------------------------------------------------------------------------
This iteration is 19
True Objective function: Loss = -10172.9287096895
Iteration 0: Loss = -25218.339354556392
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.7561,    nan]],

        [[0.4290,    nan],
         [0.7994, 0.4979]],

        [[0.9340,    nan],
         [0.0683, 0.1848]],

        [[0.8408,    nan],
         [0.7177, 0.6265]],

        [[0.6385,    nan],
         [0.2615, 0.2617]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25486.40941040878
Iteration 100: Loss = -10095.355046565768
Iteration 200: Loss = -10093.630335764876
Iteration 300: Loss = -10092.939093601506
Iteration 400: Loss = -10092.569052056182
Iteration 500: Loss = -10092.336294564695
Iteration 600: Loss = -10092.17609106648
Iteration 700: Loss = -10092.059102688543
Iteration 800: Loss = -10091.969775492176
Iteration 900: Loss = -10091.899207412042
Iteration 1000: Loss = -10091.841816020042
Iteration 1100: Loss = -10091.793998753577
Iteration 1200: Loss = -10091.75321688746
Iteration 1300: Loss = -10091.717824509818
Iteration 1400: Loss = -10091.686819452538
Iteration 1500: Loss = -10091.659691060186
Iteration 1600: Loss = -10091.636510240565
Iteration 1700: Loss = -10091.617149912287
Iteration 1800: Loss = -10091.600883028494
Iteration 1900: Loss = -10091.586397618606
Iteration 2000: Loss = -10091.571031346748
Iteration 2100: Loss = -10091.548684801211
Iteration 2200: Loss = -10091.490929858284
Iteration 2300: Loss = -10091.10289776806
Iteration 2400: Loss = -10090.613544036009
Iteration 2500: Loss = -10090.304873274057
Iteration 2600: Loss = -10090.169100901627
Iteration 2700: Loss = -10090.09214910817
Iteration 2800: Loss = -10090.034354798156
Iteration 2900: Loss = -10090.000867269797
Iteration 3000: Loss = -10089.973606076503
Iteration 3100: Loss = -10089.94825214361
Iteration 3200: Loss = -10089.923379313157
Iteration 3300: Loss = -10089.897909366788
Iteration 3400: Loss = -10089.871087034
Iteration 3500: Loss = -10089.84207066512
Iteration 3600: Loss = -10089.810209637953
Iteration 3700: Loss = -10089.773660052388
Iteration 3800: Loss = -10089.728367242822
Iteration 3900: Loss = -10089.674442721967
Iteration 4000: Loss = -10089.615079354482
Iteration 4100: Loss = -10089.508553235071
Iteration 4200: Loss = -10089.274102122892
Iteration 4300: Loss = -10088.852800542607
Iteration 4400: Loss = -10087.482250572093
Iteration 4500: Loss = -10087.746163791802
1
Iteration 4600: Loss = -10087.315640354424
Iteration 4700: Loss = -10087.276291301641
Iteration 4800: Loss = -10087.247498521656
Iteration 4900: Loss = -10087.212990490774
Iteration 5000: Loss = -10087.179094652689
Iteration 5100: Loss = -10087.143167486463
Iteration 5200: Loss = -10087.235620083726
1
Iteration 5300: Loss = -10087.076138876402
Iteration 5400: Loss = -10087.049737552717
Iteration 5500: Loss = -10087.027614249695
Iteration 5600: Loss = -10087.009476053952
Iteration 5700: Loss = -10086.994864339465
Iteration 5800: Loss = -10087.18869205922
1
Iteration 5900: Loss = -10086.973698322168
Iteration 6000: Loss = -10086.966176814054
Iteration 6100: Loss = -10086.960277590564
Iteration 6200: Loss = -10086.955319267432
Iteration 6300: Loss = -10086.951338894502
Iteration 6400: Loss = -10086.948138725853
Iteration 6500: Loss = -10086.957074407117
1
Iteration 6600: Loss = -10086.94335305452
Iteration 6700: Loss = -10086.941555176803
Iteration 6800: Loss = -10086.942977525463
1
Iteration 6900: Loss = -10086.938832560289
Iteration 7000: Loss = -10086.93776401546
Iteration 7100: Loss = -10086.936846142322
Iteration 7200: Loss = -10086.938129095048
1
Iteration 7300: Loss = -10086.935426806249
Iteration 7400: Loss = -10086.934869128192
Iteration 7500: Loss = -10086.958937430758
1
Iteration 7600: Loss = -10086.933894102443
Iteration 7700: Loss = -10086.933457627047
Iteration 7800: Loss = -10086.933047299623
Iteration 7900: Loss = -10086.934960820905
1
Iteration 8000: Loss = -10086.932110854521
Iteration 8100: Loss = -10086.931574610991
Iteration 8200: Loss = -10087.041656818014
1
Iteration 8300: Loss = -10086.930154204369
Iteration 8400: Loss = -10086.929755565947
Iteration 8500: Loss = -10086.938893360217
1
Iteration 8600: Loss = -10086.92944408644
Iteration 8700: Loss = -10086.92928189354
Iteration 8800: Loss = -10086.929208349278
Iteration 8900: Loss = -10086.929338717036
1
Iteration 9000: Loss = -10086.929085659542
Iteration 9100: Loss = -10086.929005857332
Iteration 9200: Loss = -10086.937656880316
1
Iteration 9300: Loss = -10086.928863738183
Iteration 9400: Loss = -10086.92886217667
Iteration 9500: Loss = -10086.92879101009
Iteration 9600: Loss = -10086.932449422897
1
Iteration 9700: Loss = -10086.92870351263
Iteration 9800: Loss = -10086.928649199368
Iteration 9900: Loss = -10087.045927248055
1
Iteration 10000: Loss = -10086.928612677379
Iteration 10100: Loss = -10086.928599936295
Iteration 10200: Loss = -10086.928562112962
Iteration 10300: Loss = -10086.928766871564
1
Iteration 10400: Loss = -10086.928563979245
2
Iteration 10500: Loss = -10086.928525829417
Iteration 10600: Loss = -10086.987370685434
1
Iteration 10700: Loss = -10086.928516721598
Iteration 10800: Loss = -10086.92850997581
Iteration 10900: Loss = -10086.928506196013
Iteration 11000: Loss = -10086.928586209422
1
Iteration 11100: Loss = -10086.928473614093
Iteration 11200: Loss = -10086.928863694735
1
Iteration 11300: Loss = -10086.928502273408
2
Iteration 11400: Loss = -10086.928882582872
3
Iteration 11500: Loss = -10086.928513706076
4
Iteration 11600: Loss = -10086.929397070064
5
Iteration 11700: Loss = -10086.928448254686
Iteration 11800: Loss = -10086.952815576797
1
Iteration 11900: Loss = -10086.928445035417
Iteration 12000: Loss = -10086.933035446269
1
Iteration 12100: Loss = -10086.928711570035
2
Iteration 12200: Loss = -10086.961812799447
3
Iteration 12300: Loss = -10086.928450773466
4
Iteration 12400: Loss = -10086.92843310707
Iteration 12500: Loss = -10086.929154070947
1
Iteration 12600: Loss = -10086.928425822876
Iteration 12700: Loss = -10086.92847940987
1
Iteration 12800: Loss = -10086.928427229308
2
Iteration 12900: Loss = -10086.937359911726
3
Iteration 13000: Loss = -10086.928402892096
Iteration 13100: Loss = -10086.952014518274
1
Iteration 13200: Loss = -10086.928397194191
Iteration 13300: Loss = -10086.928396465435
Iteration 13400: Loss = -10086.929398311622
1
Iteration 13500: Loss = -10086.928413950835
2
Iteration 13600: Loss = -10086.935047520547
3
Iteration 13700: Loss = -10086.928405670225
4
Iteration 13800: Loss = -10086.93211609592
5
Iteration 13900: Loss = -10086.928485605327
6
Iteration 14000: Loss = -10086.928703448502
7
Iteration 14100: Loss = -10086.928459079612
8
Iteration 14200: Loss = -10086.928845089304
9
Iteration 14300: Loss = -10086.92843795293
10
Stopping early at iteration 14300 due to no improvement.
tensor([[-5.6193,  1.0041],
        [-6.4686,  1.8533],
        [-6.7888,  2.1736],
        [-4.4077, -0.2075],
        [-6.1874,  1.5722],
        [-6.6435,  2.0283],
        [-3.8683, -0.7469],
        [-5.5662,  0.9509],
        [-5.6197,  1.0045],
        [-5.0182,  0.4030],
        [-5.4396,  0.8243],
        [-7.3534,  2.7382],
        [-6.9627,  2.3475],
        [-5.1405,  0.5253],
        [-7.8781,  3.2629],
        [-6.6186,  2.0034],
        [-4.9369,  0.3217],
        [-7.8950,  3.2798],
        [-6.9650,  2.3498],
        [-9.0739,  4.4586],
        [-6.0347,  1.4195],
        [-7.3269,  2.7117],
        [-4.9112,  0.2960],
        [-6.2691,  1.6538],
        [-5.7445,  1.1293],
        [-8.7011,  4.0859],
        [-4.4526, -0.1626],
        [-6.2452,  1.6299],
        [-7.0784,  2.4631],
        [-5.5183,  0.9031],
        [-6.2606,  1.6454],
        [-5.6000,  0.9848],
        [-4.6651,  0.0498],
        [-6.0902,  1.4749],
        [-5.1334,  0.5181],
        [-7.5314,  2.9162],
        [-5.4122,  0.7969],
        [-5.9052,  1.2899],
        [-4.5712, -0.0441],
        [-7.4158,  2.8006],
        [-5.1452,  0.5300],
        [-7.2982,  2.6830],
        [-5.7566,  1.1414],
        [-5.4099,  0.7947],
        [-7.2770,  2.6617],
        [-9.0955,  4.4802],
        [-5.3287,  0.7135],
        [-7.8464,  3.2312],
        [-8.3798,  3.7645],
        [-5.4306,  0.8154],
        [-5.1150,  0.4998],
        [-6.0119,  1.3967],
        [-7.6314,  3.0162],
        [-7.3940,  2.7788],
        [-5.7991,  1.1839],
        [-6.6610,  2.0457],
        [-5.7500,  1.1348],
        [-7.6905,  3.0753],
        [-5.8236,  1.2084],
        [-5.9260,  1.3108],
        [-6.1775,  1.5623],
        [-5.0291,  0.4139],
        [-6.4903,  1.8751],
        [-6.5928,  1.9776],
        [-4.8195,  0.2043],
        [-6.3804,  1.7652],
        [-6.3852,  1.7700],
        [-5.7846,  1.1694],
        [-6.7197,  2.1044],
        [-6.2901,  1.6749],
        [-5.1783,  0.5630],
        [-6.8707,  2.2555],
        [-7.7444,  3.1292],
        [-6.8490,  2.2338],
        [-5.2025,  0.5873],
        [-7.1788,  2.5635],
        [-6.6072,  1.9920],
        [-7.6575,  3.0423],
        [-8.0509,  3.4357],
        [-7.6411,  3.0259],
        [-4.6245,  0.0093],
        [-7.6397,  3.0245],
        [-5.4319,  0.8167],
        [-7.7653,  3.1501],
        [-2.8173, -1.7979],
        [-6.1395,  1.5242],
        [-6.4779,  1.8626],
        [-8.1496,  3.5344],
        [-7.5449,  2.9297],
        [-5.1423,  0.5271],
        [-8.4452,  3.8300],
        [-8.0195,  3.4043],
        [-6.5942,  1.9790],
        [-7.5308,  2.9156],
        [-6.9241,  2.3089],
        [-6.4004,  1.7852],
        [-0.5732, -4.0420],
        [-5.7866,  1.1713],
        [-5.0087,  0.3935],
        [-5.7913,  1.1761]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6936, 0.3064],
        [0.0057, 0.9943]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0143, 0.9857], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0696, 0.2493],
         [0.7561, 0.1408]],

        [[0.4290, 0.1282],
         [0.7994, 0.4979]],

        [[0.9340, 0.1998],
         [0.0683, 0.1848]],

        [[0.8408, 0.0614],
         [0.7177, 0.6265]],

        [[0.6385, 0.2500],
         [0.2615, 0.2617]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.005797822571485746
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -5.716875842775222e-05
Average Adjusted Rand Index: 0.0007444904455929949
Iteration 0: Loss = -15563.542945958206
Iteration 10: Loss = -10089.393421252164
Iteration 20: Loss = -10089.295260619923
Iteration 30: Loss = -10089.264043576615
Iteration 40: Loss = -10089.249199580223
Iteration 50: Loss = -10089.24020622527
Iteration 60: Loss = -10089.234244926543
Iteration 70: Loss = -10089.230138313871
Iteration 80: Loss = -10089.22730678124
Iteration 90: Loss = -10089.22531696289
Iteration 100: Loss = -10089.223955418427
Iteration 110: Loss = -10089.222936472483
Iteration 120: Loss = -10089.222240098203
Iteration 130: Loss = -10089.221776855891
Iteration 140: Loss = -10089.221420299356
Iteration 150: Loss = -10089.221126375161
Iteration 160: Loss = -10089.220945318852
Iteration 170: Loss = -10089.220826381796
Iteration 180: Loss = -10089.220787705544
Iteration 190: Loss = -10089.2206456566
Iteration 200: Loss = -10089.220593612821
Iteration 210: Loss = -10089.220604651246
1
Iteration 220: Loss = -10089.220553408379
Iteration 230: Loss = -10089.220527472626
Iteration 240: Loss = -10089.220534863649
1
Iteration 250: Loss = -10089.220489094849
Iteration 260: Loss = -10089.220513764309
1
Iteration 270: Loss = -10089.22052016946
2
Iteration 280: Loss = -10089.220514668206
3
Stopping early at iteration 279 due to no improvement.
pi: tensor([[0.9614, 0.0386],
        [0.9759, 0.0241]], dtype=torch.float64)
alpha: tensor([0.9614, 0.0386])
beta: tensor([[[0.1360, 0.1980],
         [0.2466, 0.2506]],

        [[0.1922, 0.1979],
         [0.5897, 0.4140]],

        [[0.7915, 0.1509],
         [0.0812, 0.3957]],

        [[0.2147, 0.1903],
         [0.0894, 0.6666]],

        [[0.0352, 0.2054],
         [0.1315, 0.6140]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.0004200686716773755
Average Adjusted Rand Index: -0.001175967141154427
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15563.277232798335
Iteration 100: Loss = -10095.694784840567
Iteration 200: Loss = -10091.175346304562
Iteration 300: Loss = -10090.348574842037
Iteration 400: Loss = -10089.770984359355
Iteration 500: Loss = -10089.371141455566
Iteration 600: Loss = -10089.070156415331
Iteration 700: Loss = -10088.858076985252
Iteration 800: Loss = -10088.792920423422
Iteration 900: Loss = -10088.74585446485
Iteration 1000: Loss = -10088.696048442205
Iteration 1100: Loss = -10088.641656215439
Iteration 1200: Loss = -10088.580927308134
Iteration 1300: Loss = -10088.522852815171
Iteration 1400: Loss = -10088.479237210371
Iteration 1500: Loss = -10088.447699415337
Iteration 1600: Loss = -10088.416118901509
Iteration 1700: Loss = -10088.403712706619
Iteration 1800: Loss = -10088.393565457092
Iteration 1900: Loss = -10088.386296834211
Iteration 2000: Loss = -10088.386132983182
Iteration 2100: Loss = -10088.381378121767
Iteration 2200: Loss = -10088.379947570624
Iteration 2300: Loss = -10088.37888391563
Iteration 2400: Loss = -10088.37811479022
Iteration 2500: Loss = -10088.377627508638
Iteration 2600: Loss = -10088.37707163
Iteration 2700: Loss = -10088.37836833618
1
Iteration 2800: Loss = -10088.376093451232
Iteration 2900: Loss = -10088.375754302526
Iteration 3000: Loss = -10088.375494171878
Iteration 3100: Loss = -10088.375224712681
Iteration 3200: Loss = -10088.375056361821
Iteration 3300: Loss = -10088.37487015018
Iteration 3400: Loss = -10088.374718189669
Iteration 3500: Loss = -10088.374624939086
Iteration 3600: Loss = -10088.374477541614
Iteration 3700: Loss = -10088.404911517388
1
Iteration 3800: Loss = -10088.374305749396
Iteration 3900: Loss = -10088.374700615328
1
Iteration 4000: Loss = -10088.374179932393
Iteration 4100: Loss = -10088.408788646919
1
Iteration 4200: Loss = -10088.374135586757
Iteration 4300: Loss = -10088.374070349408
Iteration 4400: Loss = -10088.374697501458
1
Iteration 4500: Loss = -10088.374044276403
Iteration 4600: Loss = -10088.374004970734
Iteration 4700: Loss = -10088.374279637936
1
Iteration 4800: Loss = -10088.37398245444
Iteration 4900: Loss = -10088.373991969538
1
Iteration 5000: Loss = -10088.374051510968
2
Iteration 5100: Loss = -10088.373978271346
Iteration 5200: Loss = -10088.373974356009
Iteration 5300: Loss = -10088.374261341865
1
Iteration 5400: Loss = -10088.373942480255
Iteration 5500: Loss = -10088.373951010706
1
Iteration 5600: Loss = -10088.374308136981
2
Iteration 5700: Loss = -10088.37395808616
3
Iteration 5800: Loss = -10088.373931286314
Iteration 5900: Loss = -10088.374011340999
1
Iteration 6000: Loss = -10088.373949945782
2
Iteration 6100: Loss = -10088.374139321442
3
Iteration 6200: Loss = -10088.373936349211
4
Iteration 6300: Loss = -10088.373944961064
5
Iteration 6400: Loss = -10088.373957416423
6
Iteration 6500: Loss = -10088.374164743958
7
Iteration 6600: Loss = -10088.373943589551
8
Iteration 6700: Loss = -10088.373946110503
9
Iteration 6800: Loss = -10088.37452768976
10
Stopping early at iteration 6800 due to no improvement.
tensor([[-0.6458, -0.7976],
        [-1.1901, -2.2007],
        [-0.6186, -0.9443],
        [-0.9410, -1.7252],
        [-0.5573, -1.5507],
        [-0.4431, -1.0538],
        [-1.4533, -1.5920],
        [-0.5637, -0.8277],
        [-1.1766, -1.3584],
        [-1.4758, -1.4236],
        [-0.5052, -1.0055],
        [-0.3147, -1.0822],
        [-0.2669, -1.1276],
        [-2.2670, -2.3482],
        [-1.5263, -2.6760],
        [-0.4625, -1.5656],
        [-0.7901, -0.9390],
        [-0.1705, -1.5010],
        [-0.6356, -1.3810],
        [ 0.0472, -1.4740],
        [-0.5884, -1.2524],
        [-0.3455, -1.3052],
        [-1.0878, -1.3222],
        [-0.2646, -1.1285],
        [-0.7408, -1.0561],
        [-0.0533, -1.6808],
        [-0.7659, -0.9874],
        [-0.2774, -1.2111],
        [-0.6521, -0.7748],
        [-0.4106, -0.9897],
        [-0.8490, -1.2312],
        [-0.4197, -0.9966],
        [-1.4901, -1.2107],
        [-0.7306, -1.7309],
        [-0.7077, -1.0536],
        [-0.1180, -1.7506],
        [-0.4673, -1.0072],
        [-0.3483, -1.0996],
        [-0.7661, -0.7047],
        [-0.1090, -1.2794],
        [-0.4772, -0.9708],
        [-0.2321, -1.1632],
        [-0.3152, -1.1991],
        [-0.4882, -0.9834],
        [-0.4903, -1.9110],
        [ 0.4942, -1.9076],
        [-0.7408, -0.6889],
        [-1.1729, -2.7840],
        [-0.7761, -1.7540],
        [-0.6633, -0.7609],
        [-0.5252, -1.0686],
        [-1.1164, -1.4304],
        [-0.5675, -2.1095],
        [-0.0432, -1.6593],
        [-0.5236, -0.8928],
        [-0.2739, -1.1124],
        [-0.2973, -1.0938],
        [-0.0917, -1.4330],
        [-0.5126, -1.3863],
        [-0.4493, -1.2005],
        [-1.8425, -1.3350],
        [-0.8671, -0.5193],
        [-0.3461, -1.0689],
        [-0.2334, -1.3203],
        [-0.5832, -0.8969],
        [-0.3508, -1.5711],
        [-0.3486, -1.0412],
        [-0.8593, -1.0030],
        [-0.5271, -1.1151],
        [-0.2068, -1.1867],
        [-0.7186, -1.2156],
        [-0.1947, -1.2639],
        [-0.6228, -1.4061],
        [-0.2408, -1.1532],
        [-1.0254, -1.2494],
        [-0.3826, -1.7514],
        [-0.3274, -1.3947],
        [-0.1406, -1.5886],
        [-0.4597, -0.9311],
        [-0.3608, -1.0332],
        [-0.5888, -1.0167],
        [-1.7859, -2.6922],
        [-1.3032, -1.8006],
        [-0.2251, -2.0402],
        [-1.0800, -0.3764],
        [-0.5835, -0.8565],
        [-0.3694, -1.0411],
        [-1.6923, -2.3390],
        [ 0.0787, -1.5386],
        [-0.1907, -1.5858],
        [-0.1270, -1.5673],
        [-0.2063, -1.6551],
        [-0.8387, -1.9240],
        [-0.8389, -2.4329],
        [-0.4174, -1.3814],
        [-1.0445, -1.5794],
        [-1.1892, -0.3458],
        [-0.3949, -1.2471],
        [-2.1901, -1.8909],
        [-0.5163, -1.3278]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9724, 0.0276],
        [0.7493, 0.2507]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6615, 0.3385], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1355, 0.1531],
         [0.2466, 0.1762]],

        [[0.1922, 0.1727],
         [0.5897, 0.4140]],

        [[0.7915, 0.1428],
         [0.0812, 0.3957]],

        [[0.2147, 0.1895],
         [0.0894, 0.6666]],

        [[0.0352, 0.2091],
         [0.1315, 0.6140]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.006713123426611697
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.0013545739247499873
Average Adjusted Rand Index: 0.0004820899368136674
Iteration 0: Loss = -13734.31853602495
Iteration 10: Loss = -10088.948678396599
Iteration 20: Loss = -10088.979797832753
1
Iteration 30: Loss = -10089.01426132213
2
Iteration 40: Loss = -10089.04625412933
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.4356, 0.5644],
        [0.3841, 0.6159]], dtype=torch.float64)
alpha: tensor([0.4088, 0.5912])
beta: tensor([[[0.1719, 0.1502],
         [0.0097, 0.1212]],

        [[0.0928, 0.1452],
         [0.7028, 0.7462]],

        [[0.8041, 0.1382],
         [0.7264, 0.6725]],

        [[0.7483, 0.1413],
         [0.2577, 0.4781]],

        [[0.3516, 0.1390],
         [0.1963, 0.2317]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 34
Adjusted Rand Index: 0.09443837537297614
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 28
Adjusted Rand Index: 0.18652543355230433
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 32
Adjusted Rand Index: 0.11960813930906501
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.002154149743706027
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 29
Adjusted Rand Index: 0.16858896894606934
Global Adjusted Rand Index: 0.10612849200593943
Average Adjusted Rand Index: 0.11426301338482418
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -13734.086486710667
Iteration 100: Loss = -10096.193570539268
Iteration 200: Loss = -10088.272234245345
Iteration 300: Loss = -10087.844852963655
Iteration 400: Loss = -10087.586890181885
Iteration 500: Loss = -10087.362149445233
Iteration 600: Loss = -10087.204689788357
Iteration 700: Loss = -10087.081000981838
Iteration 800: Loss = -10086.955041047564
Iteration 900: Loss = -10086.867578026015
Iteration 1000: Loss = -10083.622963864864
Iteration 1100: Loss = -10067.91031819604
Iteration 1200: Loss = -10067.876182327063
Iteration 1300: Loss = -10067.868067498137
Iteration 1400: Loss = -10067.85843950785
Iteration 1500: Loss = -10067.85214173637
Iteration 1600: Loss = -10067.850531549324
Iteration 1700: Loss = -10067.849791535671
Iteration 1800: Loss = -10067.849220227898
Iteration 1900: Loss = -10067.848717619492
Iteration 2000: Loss = -10067.848397936106
Iteration 2100: Loss = -10067.84815113053
Iteration 2200: Loss = -10067.847926214576
Iteration 2300: Loss = -10067.847824110124
Iteration 2400: Loss = -10067.847715035237
Iteration 2500: Loss = -10067.847602118212
Iteration 2600: Loss = -10067.847553788873
Iteration 2700: Loss = -10067.847483035126
Iteration 2800: Loss = -10067.847431352742
Iteration 2900: Loss = -10067.84743389194
1
Iteration 3000: Loss = -10067.847452844493
2
Iteration 3100: Loss = -10067.847335412418
Iteration 3200: Loss = -10067.847296895177
Iteration 3300: Loss = -10067.847270782491
Iteration 3400: Loss = -10067.847274761416
1
Iteration 3500: Loss = -10067.847769703598
2
Iteration 3600: Loss = -10067.847226646241
Iteration 3700: Loss = -10067.847275683074
1
Iteration 3800: Loss = -10067.84972116067
2
Iteration 3900: Loss = -10067.874418254693
3
Iteration 4000: Loss = -10067.873840431466
4
Iteration 4100: Loss = -10067.847167839747
Iteration 4200: Loss = -10067.84807751289
1
Iteration 4300: Loss = -10067.847133256235
Iteration 4400: Loss = -10067.847216028704
1
Iteration 4500: Loss = -10067.847130859012
Iteration 4600: Loss = -10067.84715385892
1
Iteration 4700: Loss = -10067.847092037924
Iteration 4800: Loss = -10067.847602185991
1
Iteration 4900: Loss = -10067.847091547641
Iteration 5000: Loss = -10067.866797868113
1
Iteration 5100: Loss = -10067.847123500263
2
Iteration 5200: Loss = -10067.848282273675
3
Iteration 5300: Loss = -10067.847114145832
4
Iteration 5400: Loss = -10067.847091418438
Iteration 5500: Loss = -10067.855311199244
1
Iteration 5600: Loss = -10067.847077440621
Iteration 5700: Loss = -10067.847118334554
1
Iteration 5800: Loss = -10067.847102555961
2
Iteration 5900: Loss = -10067.847090286075
3
Iteration 6000: Loss = -10067.901596957705
4
Iteration 6100: Loss = -10067.847102786278
5
Iteration 6200: Loss = -10067.847072769098
Iteration 6300: Loss = -10067.853236064097
1
Iteration 6400: Loss = -10067.847069977712
Iteration 6500: Loss = -10067.84707126611
1
Iteration 6600: Loss = -10067.85319607557
2
Iteration 6700: Loss = -10067.847099477265
3
Iteration 6800: Loss = -10067.8470901903
4
Iteration 6900: Loss = -10067.84712450706
5
Iteration 7000: Loss = -10067.847437451897
6
Iteration 7100: Loss = -10067.847074700072
7
Iteration 7200: Loss = -10067.847646622804
8
Iteration 7300: Loss = -10067.847067538309
Iteration 7400: Loss = -10067.85099745202
1
Iteration 7500: Loss = -10067.847084794246
2
Iteration 7600: Loss = -10067.849187901174
3
Iteration 7700: Loss = -10067.847063566
Iteration 7800: Loss = -10067.847092541071
1
Iteration 7900: Loss = -10067.847193098958
2
Iteration 8000: Loss = -10067.84709460527
3
Iteration 8100: Loss = -10067.84708523703
4
Iteration 8200: Loss = -10067.847141477101
5
Iteration 8300: Loss = -10067.847073402156
6
Iteration 8400: Loss = -10067.863297511643
7
Iteration 8500: Loss = -10067.847050215696
Iteration 8600: Loss = -10067.847075947582
1
Iteration 8700: Loss = -10067.847540873292
2
Iteration 8800: Loss = -10067.847083024231
3
Iteration 8900: Loss = -10068.244460819582
4
Iteration 9000: Loss = -10067.847067447796
5
Iteration 9100: Loss = -10067.847120844963
6
Iteration 9200: Loss = -10067.850628774106
7
Iteration 9300: Loss = -10067.847091466005
8
Iteration 9400: Loss = -10067.861780814472
9
Iteration 9500: Loss = -10067.847077134926
10
Stopping early at iteration 9500 due to no improvement.
tensor([[-0.6025, -0.8765],
        [-3.7194,  1.4042],
        [-0.0281, -2.0988],
        [-0.5455, -2.9561],
        [-1.5530,  0.1629],
        [-1.1287, -0.5849],
        [ 0.5791, -1.9853],
        [ 0.5503, -3.1993],
        [-1.0746, -0.3370],
        [ 1.8324, -3.7150],
        [-4.3268,  1.4003],
        [-1.9262, -1.1275],
        [ 0.0112, -2.1228],
        [-3.4976,  1.1930],
        [-1.5733, -1.7684],
        [-2.4058,  0.0975],
        [ 0.4597, -1.8464],
        [-3.2166,  1.7965],
        [-1.4675, -0.8678],
        [-3.3381,  1.7575],
        [ 0.6712, -3.8885],
        [-1.9325, -2.3244],
        [ 0.7759, -2.1762],
        [-1.9208,  0.5111],
        [ 0.6089, -1.9973],
        [-1.5319,  0.1249],
        [ 0.7447, -2.1395],
        [-0.3580, -2.0912],
        [-2.8603,  0.0338],
        [-2.6787,  1.1919],
        [-3.5712,  1.5055],
        [-1.2172, -1.0370],
        [-0.3436, -1.3469],
        [-1.6748,  0.0790],
        [ 0.4169, -2.0637],
        [-3.5233,  0.4257],
        [ 0.8202, -2.4259],
        [-3.2639,  0.7519],
        [-1.0117, -3.6035],
        [-0.8108, -1.3767],
        [ 0.7128, -2.6213],
        [-2.6357,  1.2466],
        [-2.3694,  0.2157],
        [ 0.9767, -2.5607],
        [-2.8669,  1.1906],
        [-3.2809,  1.8682],
        [ 1.8555, -3.2423],
        [-2.2729,  0.7408],
        [-2.4468,  1.0372],
        [ 1.1344, -2.5212],
        [-2.0455, -0.7491],
        [ 1.3805, -3.6941],
        [-2.4762,  1.0820],
        [-2.9811,  1.3690],
        [-2.1896,  0.2114],
        [-1.3405, -0.3137],
        [-0.0264, -1.7453],
        [-2.0732,  0.6848],
        [-1.7243,  0.0897],
        [-0.5554, -2.2859],
        [-0.7474, -0.6391],
        [-1.0937, -1.0431],
        [-0.0334, -2.4629],
        [-2.6006,  0.6607],
        [-2.2488,  0.8542],
        [-2.7570, -0.0503],
        [-1.5924, -0.7966],
        [ 0.0246, -2.3598],
        [-1.3147, -0.2199],
        [-3.4826,  1.2872],
        [-2.9895,  1.5716],
        [-2.4783,  0.6880],
        [-1.8550,  0.4534],
        [-0.3466, -2.1371],
        [ 1.1743, -3.1363],
        [-3.6138,  1.3992],
        [-2.0488,  0.0267],
        [-3.9698,  2.5821],
        [-0.6031, -0.9482],
        [ 0.8482, -2.2421],
        [ 0.3397, -1.9064],
        [ 0.1426, -1.7240],
        [-0.9640, -1.6186],
        [-2.8340,  1.3364],
        [ 2.0259, -4.2294],
        [-2.7514,  1.3651],
        [-2.2696,  0.8785],
        [ 0.4420, -2.0853],
        [-2.8754,  0.2323],
        [-1.5969, -0.5894],
        [-3.9948, -0.6204],
        [-1.9658,  0.2900],
        [-3.2334,  1.7610],
        [-2.7400,  0.4814],
        [-3.7578,  0.1708],
        [ 0.6354, -2.0341],
        [-1.3946, -0.3489],
        [-2.0037,  0.6128],
        [ 1.3587, -2.8968],
        [-2.9001,  0.7296]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7675, 0.2325],
        [0.1932, 0.8068]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4160, 0.5840], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2334, 0.1210],
         [0.0097, 0.1416]],

        [[0.0928, 0.1047],
         [0.7028, 0.7462]],

        [[0.8041, 0.1004],
         [0.7264, 0.6725]],

        [[0.7483, 0.1081],
         [0.2577, 0.4781]],

        [[0.3516, 0.1013],
         [0.1963, 0.2317]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 14
Adjusted Rand Index: 0.5136842105263157
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7026262626262626
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 16
Adjusted Rand Index: 0.45689655172413796
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 21
Adjusted Rand Index: 0.3298938787500459
time is 4
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 8
Adjusted Rand Index: 0.7026170871755089
Global Adjusted Rand Index: 0.534911781254286
Average Adjusted Rand Index: 0.5411435981604542
Iteration 0: Loss = -31996.325138432116
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.0247,    nan]],

        [[0.7706,    nan],
         [0.4079, 0.1797]],

        [[0.9591,    nan],
         [0.5074, 0.8946]],

        [[0.8315,    nan],
         [0.6945, 0.0530]],

        [[0.8579,    nan],
         [0.4528, 0.6487]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31997.50949439719
Iteration 100: Loss = -10127.99506171278
Iteration 200: Loss = -10108.772453286063
Iteration 300: Loss = -10098.041629004909
Iteration 400: Loss = -10095.710081235839
Iteration 500: Loss = -10094.325178997873
Iteration 600: Loss = -10093.205765847644
Iteration 700: Loss = -10092.430429670016
Iteration 800: Loss = -10091.982908399821
Iteration 900: Loss = -10091.510968734203
Iteration 1000: Loss = -10091.211990110072
Iteration 1100: Loss = -10090.993312176046
Iteration 1200: Loss = -10090.75736558599
Iteration 1300: Loss = -10090.550153599721
Iteration 1400: Loss = -10090.40838571341
Iteration 1500: Loss = -10090.263676838078
Iteration 1600: Loss = -10090.118885312544
Iteration 1700: Loss = -10090.006594235907
Iteration 1800: Loss = -10089.904244570474
Iteration 1900: Loss = -10089.825291353587
Iteration 2000: Loss = -10089.76019554739
Iteration 2100: Loss = -10089.698805642183
Iteration 2200: Loss = -10089.641175406025
Iteration 2300: Loss = -10089.588308476348
Iteration 2400: Loss = -10089.542189279078
Iteration 2500: Loss = -10089.5010954074
Iteration 2600: Loss = -10089.462766716455
Iteration 2700: Loss = -10089.426592798629
Iteration 2800: Loss = -10089.39268141488
Iteration 2900: Loss = -10089.36238539308
Iteration 3000: Loss = -10089.3314062778
Iteration 3100: Loss = -10089.303565517896
Iteration 3200: Loss = -10089.353762094448
1
Iteration 3300: Loss = -10089.248453417227
Iteration 3400: Loss = -10089.224523990046
Iteration 3500: Loss = -10089.203757569983
Iteration 3600: Loss = -10089.220347024077
1
Iteration 3700: Loss = -10089.166608110112
Iteration 3800: Loss = -10089.150015696274
Iteration 3900: Loss = -10089.13445218006
Iteration 4000: Loss = -10089.122181644982
Iteration 4100: Loss = -10089.105536709652
Iteration 4200: Loss = -10089.091786506053
Iteration 4300: Loss = -10089.117986079369
1
Iteration 4400: Loss = -10089.065078260423
Iteration 4500: Loss = -10089.05205474526
Iteration 4600: Loss = -10089.03922930204
Iteration 4700: Loss = -10089.032565486139
Iteration 4800: Loss = -10089.013860357767
Iteration 4900: Loss = -10089.001085105914
Iteration 5000: Loss = -10089.093642059996
1
Iteration 5100: Loss = -10088.974995181934
Iteration 5200: Loss = -10088.96148220649
Iteration 5300: Loss = -10088.947527627894
Iteration 5400: Loss = -10088.934156997919
Iteration 5500: Loss = -10088.917899763976
Iteration 5600: Loss = -10088.902559850587
Iteration 5700: Loss = -10088.887161719018
Iteration 5800: Loss = -10088.873772399851
Iteration 5900: Loss = -10088.859047619735
Iteration 6000: Loss = -10088.847058564355
Iteration 6100: Loss = -10088.89166958301
1
Iteration 6200: Loss = -10088.829397430085
Iteration 6300: Loss = -10088.85147408266
1
Iteration 6400: Loss = -10088.894882227045
2
Iteration 6500: Loss = -10088.83292946375
3
Iteration 6600: Loss = -10088.795717510253
Iteration 6700: Loss = -10088.900683537759
1
Iteration 6800: Loss = -10088.780074275584
Iteration 6900: Loss = -10088.776520227462
Iteration 7000: Loss = -10088.774044469186
Iteration 7100: Loss = -10088.776023214006
1
Iteration 7200: Loss = -10088.770497671261
Iteration 7300: Loss = -10088.773135525536
1
Iteration 7400: Loss = -10088.768237554865
Iteration 7500: Loss = -10088.78506337672
1
Iteration 7600: Loss = -10088.76629478169
Iteration 7700: Loss = -10088.765360935187
Iteration 7800: Loss = -10088.764268778148
Iteration 7900: Loss = -10088.762953759884
Iteration 8000: Loss = -10088.76130008087
Iteration 8100: Loss = -10088.759030571338
Iteration 8200: Loss = -10088.755485178364
Iteration 8300: Loss = -10088.856986566125
1
Iteration 8400: Loss = -10088.73604929563
Iteration 8500: Loss = -10088.686773205322
Iteration 8600: Loss = -10088.488018386764
Iteration 8700: Loss = -10088.426231259546
Iteration 8800: Loss = -10088.384958165967
Iteration 8900: Loss = -10088.379057903245
Iteration 9000: Loss = -10088.503907942242
1
Iteration 9100: Loss = -10088.374723925263
Iteration 9200: Loss = -10088.384606068841
1
Iteration 9300: Loss = -10088.384959363026
2
Iteration 9400: Loss = -10088.374231467438
Iteration 9500: Loss = -10088.374894527458
1
Iteration 9600: Loss = -10088.375246507858
2
Iteration 9700: Loss = -10088.375633637519
3
Iteration 9800: Loss = -10088.415355709083
4
Iteration 9900: Loss = -10088.376913453461
5
Iteration 10000: Loss = -10088.384678478296
6
Iteration 10100: Loss = -10088.374144833435
Iteration 10200: Loss = -10088.374020782683
Iteration 10300: Loss = -10088.37734030721
1
Iteration 10400: Loss = -10088.414665853059
2
Iteration 10500: Loss = -10088.37674159418
3
Iteration 10600: Loss = -10088.374408601516
4
Iteration 10700: Loss = -10088.394286779423
5
Iteration 10800: Loss = -10088.383647354633
6
Iteration 10900: Loss = -10088.408439991126
7
Iteration 11000: Loss = -10088.42921954935
8
Iteration 11100: Loss = -10088.38580354935
9
Iteration 11200: Loss = -10088.386920386667
10
Stopping early at iteration 11200 due to no improvement.
tensor([[-0.9839, -0.8270],
        [-1.6622, -0.6465],
        [-1.1312, -0.7993],
        [-1.0937, -0.3042],
        [-1.6592, -0.6625],
        [-1.0033, -0.3855],
        [-1.0697, -0.9279],
        [-0.8278, -0.5585],
        [-0.9328, -0.7453],
        [-0.6943, -0.7413],
        [-1.2655, -0.7616],
        [-1.2375, -0.4628],
        [-1.4713, -0.6026],
        [-0.7476, -0.6636],
        [-1.8194, -0.6649],
        [-1.4184, -0.3109],
        [-0.8311, -0.6780],
        [-1.3674, -0.0343],
        [-1.2054, -0.4542],
        [-1.8334, -0.3057],
        [-2.0838, -1.4113],
        [-1.9736, -1.0069],
        [-1.4357, -1.1958],
        [-2.4406, -1.5702],
        [-0.8951, -0.5744],
        [-1.7051, -0.0722],
        [-0.8809, -0.6539],
        [-1.2185, -0.2811],
        [-0.8419, -0.7138],
        [-1.3912, -0.8066],
        [-0.8863, -0.5002],
        [-0.9983, -0.4184],
        [-0.7870, -1.0617],
        [-1.9234, -0.9188],
        [-1.0680, -0.7155],
        [-2.0150, -0.3778],
        [-0.9787, -0.4349],
        [-1.1197, -0.3652],
        [-0.9609, -1.0172],
        [-1.6600, -0.4803],
        [-1.3367, -0.8364],
        [-1.1643, -0.2282],
        [-2.7526, -1.8626],
        [-1.0111, -0.5102],
        [-1.4141,  0.0101],
        [-2.0936,  0.3098],
        [-1.0974, -1.1455],
        [-3.1167, -1.4985],
        [-1.9184, -0.9336],
        [-0.7633, -0.6599],
        [-1.2214, -0.6745],
        [-1.5132, -1.1912],
        [-1.7113, -0.1640],
        [-1.5192,  0.1015],
        [-0.8803, -0.5072],
        [-1.5592, -0.7143],
        [-1.1114, -0.3105],
        [-1.4588, -0.1121],
        [-1.2802, -0.4011],
        [-1.0766, -0.3215],
        [-0.4962, -1.0022],
        [-0.5226, -0.8666],
        [-1.1604, -0.4321],
        [-1.3573, -0.2656],
        [-0.8833, -0.5658],
        [-2.9001, -1.6758],
        [-1.0453, -0.3446],
        [-0.8558, -0.7066],
        [-1.2524, -0.6592],
        [-1.2068, -0.2209],
        [-1.9210, -1.4190],
        [-1.6703, -0.5940],
        [-1.1511, -0.3636],
        [-1.8794, -0.9546],
        [-1.0235, -0.7954],
        [-1.4248, -0.0498],
        [-1.2967, -0.2251],
        [-1.4207,  0.0311],
        [-1.0198, -0.5423],
        [-1.4338, -0.7520],
        [-1.0890, -0.6572],
        [-1.8284, -0.9145],
        [-0.9504, -0.4457],
        [-1.6672,  0.1503],
        [-0.3552, -1.0541],
        [-1.0501, -0.7725],
        [-1.5512, -0.8742],
        [-1.4604, -0.8078],
        [-2.0425, -0.4206],
        [-1.4704, -0.0694],
        [-1.4568, -0.0119],
        [-1.7266, -0.2721],
        [-1.4176, -0.3287],
        [-2.0226, -0.4226],
        [-1.6297, -0.6586],
        [-1.1642, -0.6229],
        [-0.4077, -1.2476],
        [-1.1224, -0.2673],
        [-0.7349, -1.0293],
        [-1.1240, -0.3074]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.2496, 0.7504],
        [0.0277, 0.9723]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3387, 0.6613], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1762, 0.1528],
         [0.0247, 0.1354]],

        [[0.7706, 0.1725],
         [0.4079, 0.1797]],

        [[0.9591, 0.1427],
         [0.5074, 0.8946]],

        [[0.8315, 0.1897],
         [0.6945, 0.0530]],

        [[0.8579, 0.2092],
         [0.4528, 0.6487]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.006713123426611697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.0013545739247499873
Average Adjusted Rand Index: 0.0004820899368136674
Iteration 0: Loss = -29849.575337871862
Iteration 10: Loss = -10091.770533671594
Iteration 20: Loss = -10090.673249618623
Iteration 30: Loss = -10089.601877952526
Iteration 40: Loss = -10089.411813288083
Iteration 50: Loss = -10089.344898919036
Iteration 60: Loss = -10089.31267592876
Iteration 70: Loss = -10089.294636026007
Iteration 80: Loss = -10089.28352594371
Iteration 90: Loss = -10089.273782982988
Iteration 100: Loss = -10089.254144623745
Iteration 110: Loss = -10089.234548084478
Iteration 120: Loss = -10089.22818872078
Iteration 130: Loss = -10089.225723278998
Iteration 140: Loss = -10089.224158851954
Iteration 150: Loss = -10089.22311851097
Iteration 160: Loss = -10089.222374825671
Iteration 170: Loss = -10089.22180268788
Iteration 180: Loss = -10089.221445512141
Iteration 190: Loss = -10089.221159856137
Iteration 200: Loss = -10089.220985567128
Iteration 210: Loss = -10089.220847221512
Iteration 220: Loss = -10089.220739310458
Iteration 230: Loss = -10089.220675198818
Iteration 240: Loss = -10089.220637044713
Iteration 250: Loss = -10089.22058767644
Iteration 260: Loss = -10089.220569510611
Iteration 270: Loss = -10089.2205274376
Iteration 280: Loss = -10089.220539779391
1
Iteration 290: Loss = -10089.220533265385
2
Iteration 300: Loss = -10089.220511136027
Iteration 310: Loss = -10089.220497569173
Iteration 320: Loss = -10089.220503717243
1
Iteration 330: Loss = -10089.220505436238
2
Iteration 340: Loss = -10089.220513453767
3
Stopping early at iteration 339 due to no improvement.
pi: tensor([[0.0241, 0.9759],
        [0.0386, 0.9614]], dtype=torch.float64)
alpha: tensor([0.0386, 0.9614])
beta: tensor([[[0.2506, 0.1980],
         [0.6369, 0.1360]],

        [[0.9973, 0.1979],
         [0.5003, 0.2626]],

        [[0.5767, 0.1509],
         [0.7660, 0.4060]],

        [[0.2183, 0.1903],
         [0.2513, 0.9149]],

        [[0.2864, 0.2054],
         [0.8536, 0.1071]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.0004200686716773755
Average Adjusted Rand Index: -0.001175967141154427
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29849.73973038376
Iteration 100: Loss = -10116.710427593507
Iteration 200: Loss = -10098.744824761692
Iteration 300: Loss = -10095.568950153092
Iteration 400: Loss = -10094.093685595497
Iteration 500: Loss = -10093.211462606232
Iteration 600: Loss = -10092.542815868645
Iteration 700: Loss = -10092.12917938699
Iteration 800: Loss = -10091.908310265873
Iteration 900: Loss = -10091.753339893572
Iteration 1000: Loss = -10091.637794722152
Iteration 1100: Loss = -10091.547978234534
Iteration 1200: Loss = -10091.476156054006
Iteration 1300: Loss = -10091.417660640496
Iteration 1400: Loss = -10091.369012189252
Iteration 1500: Loss = -10091.327515918374
Iteration 1600: Loss = -10091.289749515794
Iteration 1700: Loss = -10091.246502698194
Iteration 1800: Loss = -10091.209943905505
Iteration 1900: Loss = -10091.174389193302
Iteration 2000: Loss = -10091.137865813283
Iteration 2100: Loss = -10091.095320259945
Iteration 2200: Loss = -10091.055190337345
Iteration 2300: Loss = -10091.012520967019
Iteration 2400: Loss = -10090.957267538639
Iteration 2500: Loss = -10090.883607612512
Iteration 2600: Loss = -10090.825575115021
Iteration 2700: Loss = -10090.777310001558
Iteration 2800: Loss = -10090.725201678753
Iteration 2900: Loss = -10090.659074842239
Iteration 3000: Loss = -10090.572529176046
Iteration 3100: Loss = -10090.399621976077
Iteration 3200: Loss = -10090.05800228023
Iteration 3300: Loss = -10089.758302820845
Iteration 3400: Loss = -10089.637912968437
Iteration 3500: Loss = -10089.587089542192
Iteration 3600: Loss = -10089.555491809466
Iteration 3700: Loss = -10089.53068163986
Iteration 3800: Loss = -10089.50872681756
Iteration 3900: Loss = -10089.487992303784
Iteration 4000: Loss = -10089.468076875179
Iteration 4100: Loss = -10089.44843168866
Iteration 4200: Loss = -10089.428979896556
Iteration 4300: Loss = -10089.409694436936
Iteration 4400: Loss = -10089.39025024582
Iteration 4500: Loss = -10089.371743454394
Iteration 4600: Loss = -10089.353306973831
Iteration 4700: Loss = -10089.331240047399
Iteration 4800: Loss = -10089.316962617053
Iteration 4900: Loss = -10089.306628008668
Iteration 5000: Loss = -10089.29775030914
Iteration 5100: Loss = -10089.28952381642
Iteration 5200: Loss = -10089.28183254231
Iteration 5300: Loss = -10089.274977403045
Iteration 5400: Loss = -10089.269115156041
Iteration 5500: Loss = -10089.263585877941
Iteration 5600: Loss = -10089.258578750088
Iteration 5700: Loss = -10089.253775370024
Iteration 5800: Loss = -10089.249038975091
Iteration 5900: Loss = -10089.244878649257
Iteration 6000: Loss = -10089.241231057244
Iteration 6100: Loss = -10089.238133370834
Iteration 6200: Loss = -10089.235428401573
Iteration 6300: Loss = -10089.232975438817
Iteration 6400: Loss = -10089.230948492757
Iteration 6500: Loss = -10089.228690615726
Iteration 6600: Loss = -10089.226877754365
Iteration 6700: Loss = -10089.225285799746
Iteration 6800: Loss = -10089.22398570179
Iteration 6900: Loss = -10089.222941491642
Iteration 7000: Loss = -10089.222131105424
Iteration 7100: Loss = -10089.221449377716
Iteration 7200: Loss = -10089.221062826473
Iteration 7300: Loss = -10089.220486703302
Iteration 7400: Loss = -10089.220140688549
Iteration 7500: Loss = -10089.219822078712
Iteration 7600: Loss = -10089.219638799788
Iteration 7700: Loss = -10089.220483796842
1
Iteration 7800: Loss = -10089.21913470848
Iteration 7900: Loss = -10089.218985894544
Iteration 8000: Loss = -10089.218933246662
Iteration 8100: Loss = -10089.21875810195
Iteration 8200: Loss = -10089.21876387822
1
Iteration 8300: Loss = -10089.219837319368
2
Iteration 8400: Loss = -10089.218698702605
Iteration 8500: Loss = -10089.2185602385
Iteration 8600: Loss = -10089.21884842148
1
Iteration 8700: Loss = -10089.219381186991
2
Iteration 8800: Loss = -10089.218450991946
Iteration 8900: Loss = -10089.218360701214
Iteration 9000: Loss = -10089.223785256447
1
Iteration 9100: Loss = -10089.218411991684
2
Iteration 9200: Loss = -10089.218251095497
Iteration 9300: Loss = -10089.218195964782
Iteration 9400: Loss = -10089.218626817672
1
Iteration 9500: Loss = -10089.21815519116
Iteration 9600: Loss = -10089.218438312595
1
Iteration 9700: Loss = -10089.218163010486
2
Iteration 9800: Loss = -10089.21813845145
Iteration 9900: Loss = -10089.218100220738
Iteration 10000: Loss = -10089.218253909828
1
Iteration 10100: Loss = -10089.218094097443
Iteration 10200: Loss = -10089.218755340642
1
Iteration 10300: Loss = -10089.218124084638
2
Iteration 10400: Loss = -10089.219418748378
3
Iteration 10500: Loss = -10089.218184296265
4
Iteration 10600: Loss = -10089.218601546283
5
Iteration 10700: Loss = -10089.220955140041
6
Iteration 10800: Loss = -10089.22065961083
7
Iteration 10900: Loss = -10089.21815835618
8
Iteration 11000: Loss = -10089.218161796402
9
Iteration 11100: Loss = -10089.220567691724
10
Stopping early at iteration 11100 due to no improvement.
tensor([[ 1.5106, -2.9515],
        [ 1.4720, -2.8857],
        [ 0.6906, -3.7329],
        [ 1.0559, -3.3231],
        [ 1.4811, -2.8771],
        [ 1.0928, -3.3038],
        [ 1.5391, -2.9302],
        [ 1.5088, -2.9197],
        [ 1.2538, -3.1951],
        [ 1.5456, -2.9327],
        [ 1.4973, -2.9185],
        [ 0.5650, -3.8067],
        [ 1.3501, -2.9982],
        [ 1.4465, -3.0243],
        [ 1.2013, -3.1388],
        [ 0.8749, -3.4641],
        [ 1.5169, -2.9358],
        [ 1.3901, -2.9285],
        [ 1.1642, -3.2157],
        [ 1.3179, -2.9611],
        [ 1.1175, -3.2643],
        [ 1.4757, -2.8824],
        [ 1.3684, -3.0708],
        [ 1.2142, -3.1498],
        [ 1.5195, -2.9104],
        [ 1.3967, -2.8711],
        [ 1.3313, -3.1064],
        [ 1.1550, -3.2049],
        [ 1.5250, -2.9697],
        [ 1.1860, -3.2137],
        [ 1.2257, -3.1954],
        [ 1.1549, -3.2471],
        [ 1.5082, -3.0023],
        [ 1.4636, -2.8935],
        [ 0.7808, -3.6424],
        [ 1.4398, -2.8429],
        [ 1.5056, -2.9076],
        [ 1.2827, -3.0989],
        [ 1.4509, -3.0170],
        [ 1.4216, -2.8941],
        [ 1.1471, -3.2666],
        [ 0.8490, -3.5122],
        [ 1.3771, -2.9980],
        [ 0.1238, -4.2909],
        [ 1.3605, -2.9413],
        [ 1.2939, -2.8948],
        [ 1.1995, -3.2795],
        [ 0.3776, -3.9094],
        [ 0.3135, -4.0391],
        [ 1.4267, -3.0274],
        [ 0.5084, -3.9057],
        [ 1.4679, -2.9515],
        [ 1.2317, -3.0517],
        [ 1.4475, -2.8339],
        [ 1.4461, -2.9871],
        [ 0.4722, -3.8830],
        [ 1.4173, -2.9603],
        [ 1.4536, -2.8449],
        [ 1.4784, -2.8972],
        [ 1.4715, -2.9063],
        [ 1.5408, -3.2957],
        [ 1.5216, -3.0229],
        [ 1.0781, -3.3014],
        [ 0.6686, -3.6762],
        [ 0.9640, -3.4716],
        [ 1.4474, -2.8902],
        [ 1.4656, -2.9055],
        [ 1.4242, -3.0307],
        [ 1.5055, -2.8941],
        [ 0.9028, -3.4557],
        [ 0.6622, -3.7531],
        [ 0.8795, -3.4617],
        [ 1.4597, -2.9100],
        [ 1.4701, -2.8616],
        [-0.0897, -4.5255],
        [ 1.2811, -3.0226],
        [ 1.1482, -3.1942],
        [ 1.3976, -2.8943],
        [ 0.6765, -3.7729],
        [ 1.4459, -2.9272],
        [ 1.4798, -2.9520],
        [ 1.1578, -3.1953],
        [ 1.0411, -3.3548],
        [ 1.2363, -3.0252],
        [ 1.5825, -2.9713],
        [ 1.1565, -3.2760],
        [ 0.8240, -3.5715],
        [ 1.4776, -2.9454],
        [ 1.4362, -2.8455],
        [ 1.4173, -2.8841],
        [ 1.1960, -3.1058],
        [ 1.4315, -2.8529],
        [ 1.4599, -2.8820],
        [ 1.2840, -2.9975],
        [ 0.9761, -3.3745],
        [ 1.5039, -2.9076],
        [ 1.4969, -3.0884],
        [ 1.1935, -3.1822],
        [ 0.3272, -4.2133],
        [ 1.4374, -2.9404]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0356, 0.9644],
        [0.0173, 0.9827]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9876, 0.0124], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1487, 0.1458],
         [0.6369, 0.1388]],

        [[0.9973, 0.2034],
         [0.5003, 0.2626]],

        [[0.5767, 0.1278],
         [0.7660, 0.4060]],

        [[0.2183, 0.2065],
         [0.2513, 0.9149]],

        [[0.2864, 0.0838],
         [0.8536, 0.1071]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011723448338714507
Average Adjusted Rand Index: 0.0001633280491905654
10172.9287096895
new:  [0.0013545739247499873, 0.534911781254286, 0.0013545739247499873, -0.0011723448338714507] [0.0004820899368136674, 0.5411435981604542, 0.0004820899368136674, 0.0001633280491905654] [10088.37452768976, 10067.847077134926, 10088.386920386667, 10089.220567691724]
prior:  [0.0004200686716773755, 0.10612849200593943, 0.0, 0.0004200686716773755] [-0.001175967141154427, 0.11426301338482418, 0.0, -0.001175967141154427] [10089.220514668206, 10089.04625412933, nan, 10089.220513453767]
-----------------------------------------------------------------------------------------
This iteration is 20
True Objective function: Loss = -9880.352024718703
Iteration 0: Loss = -13329.205890165016
Iteration 10: Loss = -9745.487396543718
Iteration 20: Loss = -9745.58109453541
1
Iteration 30: Loss = -9745.689289642738
2
Iteration 40: Loss = -9745.818981919892
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.1727, 0.8273],
        [0.0986, 0.9014]], dtype=torch.float64)
alpha: tensor([0.1114, 0.8886])
beta: tensor([[[0.1954, 0.1772],
         [0.3876, 0.1252]],

        [[0.2870, 0.1452],
         [0.1155, 0.1948]],

        [[0.2422, 0.1571],
         [0.5819, 0.9103]],

        [[0.7000, 0.1608],
         [0.3031, 0.5740]],

        [[0.9898, 0.1479],
         [0.7235, 0.4228]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.035323271006983556
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.005278298185760157
Average Adjusted Rand Index: 0.008271176736376243
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -13330.643165173242
Iteration 100: Loss = -9748.498565249625
Iteration 200: Loss = -9745.21426995943
Iteration 300: Loss = -9744.049937535077
Iteration 400: Loss = -9743.454560469694
Iteration 500: Loss = -9742.642549436501
Iteration 600: Loss = -9741.86156252564
Iteration 700: Loss = -9741.436399840555
Iteration 800: Loss = -9741.29927214783
Iteration 900: Loss = -9741.25665301053
Iteration 1000: Loss = -9741.236736119012
Iteration 1100: Loss = -9741.223781588453
Iteration 1200: Loss = -9741.214004813108
Iteration 1300: Loss = -9741.206626324984
Iteration 1400: Loss = -9741.201022887037
Iteration 1500: Loss = -9741.19642672648
Iteration 1600: Loss = -9741.192514410668
Iteration 1700: Loss = -9741.189228011472
Iteration 1800: Loss = -9741.18661697508
Iteration 1900: Loss = -9741.184497389215
Iteration 2000: Loss = -9741.1827084175
Iteration 2100: Loss = -9741.180987714288
Iteration 2200: Loss = -9741.179295476157
Iteration 2300: Loss = -9741.177644867963
Iteration 2400: Loss = -9741.176325462915
Iteration 2500: Loss = -9741.17512035538
Iteration 2600: Loss = -9741.173843067814
Iteration 2700: Loss = -9741.172105088457
Iteration 2800: Loss = -9741.169679374507
Iteration 2900: Loss = -9741.167795002591
Iteration 3000: Loss = -9741.166960646842
Iteration 3100: Loss = -9741.166454415647
Iteration 3200: Loss = -9741.166047461536
Iteration 3300: Loss = -9741.165734504386
Iteration 3400: Loss = -9741.165430575666
Iteration 3500: Loss = -9741.165118811017
Iteration 3600: Loss = -9741.164860966834
Iteration 3700: Loss = -9741.164621742992
Iteration 3800: Loss = -9741.164364754079
Iteration 3900: Loss = -9741.164135249164
Iteration 4000: Loss = -9741.163928262698
Iteration 4100: Loss = -9741.16372582323
Iteration 4200: Loss = -9741.163627568381
Iteration 4300: Loss = -9741.16339495401
Iteration 4400: Loss = -9741.163235705755
Iteration 4500: Loss = -9741.163140437127
Iteration 4600: Loss = -9741.163050432708
Iteration 4700: Loss = -9741.162890426504
Iteration 4800: Loss = -9741.162769029293
Iteration 4900: Loss = -9741.162691733356
Iteration 5000: Loss = -9741.172385635395
1
Iteration 5100: Loss = -9741.163710681069
2
Iteration 5200: Loss = -9741.1636647442
3
Iteration 5300: Loss = -9741.162429950007
Iteration 5400: Loss = -9741.162290668677
Iteration 5500: Loss = -9741.211503785664
1
Iteration 5600: Loss = -9741.162141142066
Iteration 5700: Loss = -9741.162073946032
Iteration 5800: Loss = -9741.162455429523
1
Iteration 5900: Loss = -9741.161997472866
Iteration 6000: Loss = -9741.162795593431
1
Iteration 6100: Loss = -9741.161881397315
Iteration 6200: Loss = -9741.16184514889
Iteration 6300: Loss = -9741.163293377416
1
Iteration 6400: Loss = -9741.161823619626
Iteration 6500: Loss = -9741.161757158385
Iteration 6600: Loss = -9741.162023355364
1
Iteration 6700: Loss = -9741.161718100353
Iteration 6800: Loss = -9741.232907305739
1
Iteration 6900: Loss = -9741.161622368301
Iteration 7000: Loss = -9741.161581741408
Iteration 7100: Loss = -9741.161967296972
1
Iteration 7200: Loss = -9741.163254019657
2
Iteration 7300: Loss = -9741.171870852431
3
Iteration 7400: Loss = -9741.165547252618
4
Iteration 7500: Loss = -9741.169960428344
5
Iteration 7600: Loss = -9741.161553569544
Iteration 7700: Loss = -9741.161879096699
1
Iteration 7800: Loss = -9741.165719234581
2
Iteration 7900: Loss = -9741.167628871492
3
Iteration 8000: Loss = -9741.168491493308
4
Iteration 8100: Loss = -9741.16151539172
Iteration 8200: Loss = -9741.165620958262
1
Iteration 8300: Loss = -9741.176887877318
2
Iteration 8400: Loss = -9741.185921278613
3
Iteration 8500: Loss = -9741.16155064211
4
Iteration 8600: Loss = -9741.1620167791
5
Iteration 8700: Loss = -9741.2146839196
6
Iteration 8800: Loss = -9741.161421825536
Iteration 8900: Loss = -9741.162125120021
1
Iteration 9000: Loss = -9741.161269262751
Iteration 9100: Loss = -9741.162104566987
1
Iteration 9200: Loss = -9741.169539765393
2
Iteration 9300: Loss = -9741.161877934868
3
Iteration 9400: Loss = -9741.161239833327
Iteration 9500: Loss = -9741.161382257376
1
Iteration 9600: Loss = -9741.184543930287
2
Iteration 9700: Loss = -9741.161204003713
Iteration 9800: Loss = -9741.16368164163
1
Iteration 9900: Loss = -9741.161473097201
2
Iteration 10000: Loss = -9741.161110588324
Iteration 10100: Loss = -9741.16493335812
1
Iteration 10200: Loss = -9741.161597847333
2
Iteration 10300: Loss = -9741.17306133684
3
Iteration 10400: Loss = -9741.161100344676
Iteration 10500: Loss = -9741.162836906915
1
Iteration 10600: Loss = -9741.183476396112
2
Iteration 10700: Loss = -9741.173141779866
3
Iteration 10800: Loss = -9741.161158983954
4
Iteration 10900: Loss = -9741.174468399782
5
Iteration 11000: Loss = -9741.16114899323
6
Iteration 11100: Loss = -9741.16168566502
7
Iteration 11200: Loss = -9741.162601334361
8
Iteration 11300: Loss = -9741.198643719006
9
Iteration 11400: Loss = -9741.161120324978
10
Stopping early at iteration 11400 due to no improvement.
tensor([[-1.8388, -2.7764],
        [-3.0593, -1.5559],
        [-4.6245,  0.0093],
        [-7.2212,  2.6059],
        [-5.2322,  0.6170],
        [-6.9291,  2.3139],
        [-5.5186,  0.9034],
        [-4.8970,  0.2817],
        [-6.4006,  1.7854],
        [-8.1663,  3.5511],
        [-4.4123, -0.2029],
        [-7.1534,  2.5381],
        [-5.9065,  1.2912],
        [-5.5787,  0.9635],
        [-5.1713,  0.5561],
        [-4.1328, -0.4824],
        [-8.4221,  3.8069],
        [-3.6779, -0.9373],
        [-4.5238, -0.0915],
        [-5.6206,  1.0054],
        [-5.5050,  0.8898],
        [ 0.9518, -5.5670],
        [-6.7003,  2.0850],
        [-7.1494,  2.5342],
        [-6.0866,  1.4714],
        [-6.5607,  1.9455],
        [-6.5362,  1.9210],
        [-3.7322, -0.8831],
        [-4.4316, -0.1836],
        [-6.7359,  2.1206],
        [-6.7998,  2.1846],
        [-5.1934,  0.5782],
        [-6.0621,  1.4469],
        [-7.5961,  2.9809],
        [-5.5605,  0.9453],
        [-4.7371,  0.1218],
        [-7.4162,  2.8010],
        [-4.4253, -0.1899],
        [-5.7215,  1.1063],
        [-5.7582,  1.1430],
        [-6.8487,  2.2335],
        [-7.1630,  2.5478],
        [-6.9397,  2.3244],
        [-6.1721,  1.5569],
        [-4.9395,  0.3243],
        [-7.1208,  2.5056],
        [-5.6362,  1.0210],
        [-5.2941,  0.6788],
        [-5.6430,  1.0278],
        [-5.6739,  1.0586],
        [-4.8883,  0.2730],
        [ 2.0243, -6.6395],
        [-5.0616,  0.4464],
        [-4.0751, -0.5401],
        [-3.9018, -0.7134],
        [-7.3598,  2.7446],
        [-4.6816,  0.0663],
        [-3.3603, -1.2550],
        [-6.8883,  2.2731],
        [-3.6406, -0.9747],
        [-5.0745,  0.4593],
        [-5.4547,  0.8395],
        [-6.2843,  1.6691],
        [-5.2136,  0.5984],
        [-7.1033,  2.4881],
        [-4.7486,  0.1334],
        [-1.0959, -3.5194],
        [-7.1180,  2.5028],
        [-6.2343,  1.6190],
        [-5.8497,  1.2345],
        [-0.4307, -4.1846],
        [-5.5382,  0.9230],
        [-5.7032,  1.0880],
        [-3.5761, -1.0391],
        [-0.4362, -4.1790],
        [-6.6124,  1.9972],
        [-6.2098,  1.5946],
        [-6.2206,  1.6054],
        [-6.7357,  2.1205],
        [-4.1467, -0.4685],
        [-5.9850,  1.3697],
        [-7.8026,  3.1874],
        [-1.2396, -3.3756],
        [-6.4028,  1.7875],
        [-1.8256, -2.7896],
        [-7.8426,  3.2273],
        [-7.7557,  3.1405],
        [-7.4287,  2.8135],
        [-7.1332,  2.5180],
        [-5.6849,  1.0696],
        [-7.5921,  2.9769],
        [-3.7203, -0.8949],
        [-2.4900, -2.1252],
        [-5.0284,  0.4132],
        [-7.1480,  2.5327],
        [ 0.1134, -4.7286],
        [-4.1274, -0.4878],
        [-6.6546,  2.0394],
        [-3.9966, -0.6186],
        [-4.7522,  0.1370]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.1638e-01, 8.3619e-02],
        [2.6014e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0956, 0.9044], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0544, 0.2099],
         [0.3876, 0.1281]],

        [[0.2870, 0.1348],
         [0.1155, 0.1948]],

        [[0.2422, 0.1719],
         [0.5819, 0.9103]],

        [[0.7000, 0.1765],
         [0.3031, 0.5740]],

        [[0.9898, 0.1529],
         [0.7235, 0.4228]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.035920960918124197
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0029336721668764927
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.012343779535174057
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.020424516829035635
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
Global Adjusted Rand Index: 0.013822564914478525
Average Adjusted Rand Index: 0.016302632174606632
Iteration 0: Loss = -33595.188174500065
Iteration 10: Loss = -9748.78974228384
Iteration 20: Loss = -9748.78974228384
1
Iteration 30: Loss = -9748.78974228384
2
Iteration 40: Loss = -9748.789742283863
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[5.7768e-13, 1.0000e+00],
        [9.8242e-16, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.0456e-15, 1.0000e+00])
beta: tensor([[[0.1819, 0.2109],
         [0.8309, 0.1326]],

        [[0.8768, 0.0489],
         [0.2226, 0.7428]],

        [[0.0379, 0.2166],
         [0.7449, 0.7934]],

        [[0.0405, 0.1906],
         [0.3872, 0.1208]],

        [[0.4869, 0.1013],
         [0.2228, 0.2385]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33597.087686889245
Iteration 100: Loss = -9792.138852349672
Iteration 200: Loss = -9768.053855490583
Iteration 300: Loss = -9757.13462493624
Iteration 400: Loss = -9754.115547184612
Iteration 500: Loss = -9752.50173759611
Iteration 600: Loss = -9751.468866852887
Iteration 700: Loss = -9750.725785753728
Iteration 800: Loss = -9750.16135583043
Iteration 900: Loss = -9749.664173136676
Iteration 1000: Loss = -9749.204671894437
Iteration 1100: Loss = -9748.840850167797
Iteration 1200: Loss = -9748.542316209672
Iteration 1300: Loss = -9748.300784304236
Iteration 1400: Loss = -9748.103934140794
Iteration 1500: Loss = -9747.928258810283
Iteration 1600: Loss = -9747.76364091141
Iteration 1700: Loss = -9747.603374476343
Iteration 1800: Loss = -9747.44355756706
Iteration 1900: Loss = -9747.284407866191
Iteration 2000: Loss = -9747.129984356854
Iteration 2100: Loss = -9746.98608858216
Iteration 2200: Loss = -9746.859386393773
Iteration 2300: Loss = -9746.742597847344
Iteration 2400: Loss = -9746.64400698281
Iteration 2500: Loss = -9746.561731631124
Iteration 2600: Loss = -9746.48418582325
Iteration 2700: Loss = -9746.428921388902
Iteration 2800: Loss = -9746.359103461602
Iteration 2900: Loss = -9746.291288362212
Iteration 3000: Loss = -9746.238183735115
Iteration 3100: Loss = -9746.191003282018
Iteration 3200: Loss = -9746.148861741727
Iteration 3300: Loss = -9746.265459308039
1
Iteration 3400: Loss = -9746.07526936877
Iteration 3500: Loss = -9746.049433262331
Iteration 3600: Loss = -9746.036131861689
Iteration 3700: Loss = -9746.00993043128
Iteration 3800: Loss = -9745.997592131114
Iteration 3900: Loss = -9745.989643337976
Iteration 4000: Loss = -9745.980345462463
Iteration 4100: Loss = -9745.974172994447
Iteration 4200: Loss = -9746.014588558468
1
Iteration 4300: Loss = -9745.964819257155
Iteration 4400: Loss = -9745.96191015565
Iteration 4500: Loss = -9745.957692382557
Iteration 4600: Loss = -9745.954598406397
Iteration 4700: Loss = -9745.955651303444
1
Iteration 4800: Loss = -9745.949796210545
Iteration 4900: Loss = -9745.947766897347
Iteration 5000: Loss = -9745.948476881274
1
Iteration 5100: Loss = -9745.944711506068
Iteration 5200: Loss = -9745.943537652187
Iteration 5300: Loss = -9745.976093558153
1
Iteration 5400: Loss = -9745.941441451545
Iteration 5500: Loss = -9745.940500532211
Iteration 5600: Loss = -9745.939604869885
Iteration 5700: Loss = -9745.938744515715
Iteration 5800: Loss = -9745.937914408414
Iteration 5900: Loss = -9745.937052721441
Iteration 6000: Loss = -9745.93707140693
1
Iteration 6100: Loss = -9745.935379658535
Iteration 6200: Loss = -9745.934575892832
Iteration 6300: Loss = -9745.933744793512
Iteration 6400: Loss = -9745.933077659709
Iteration 6500: Loss = -9745.932222578598
Iteration 6600: Loss = -9745.931485992072
Iteration 6700: Loss = -9745.935418480183
1
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|        | 21/100 [15:27:15<56:19:53, 2567.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|       | 22/100 [16:10:25<55:46:01, 2573.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|       | 23/100 [17:02:38<58:38:30, 2741.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|       | 24/100 [17:43:17<55:57:32, 2650.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|       | 25/100 [18:31:06<56:35:21, 2716.28s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|       | 26/100 [19:14:38<55:11:22, 2684.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|       | 27/100 [20:06:42<57:07:02, 2816.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
