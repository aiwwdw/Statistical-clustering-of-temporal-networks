nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  0%|          | 0/100 [37:31<?, ?it/s]
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
This iteration is 0
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
True Objective function: Loss = -11051.37456775153
prior kmeans ------------------------------------------------------------------------------
Iteration 0: Loss = -24390.480932419123
Iteration 10: Loss = -11284.231404914248
Iteration 20: Loss = -11284.231404897198
Iteration 30: Loss = -11284.23140237784
Iteration 40: Loss = -11284.230929914958
Iteration 50: Loss = -11283.630205040685
Iteration 60: Loss = -11281.648060676682
Iteration 70: Loss = -11281.446868001405
Iteration 80: Loss = -11281.449774943536
1
Iteration 90: Loss = -11281.418405313472
Iteration 100: Loss = -11281.408589596667
Iteration 110: Loss = -11281.407952727479
Iteration 120: Loss = -11281.407870295312
Iteration 130: Loss = -11281.407848735706
Iteration 140: Loss = -11281.407842476412
Iteration 150: Loss = -11281.407840705742
Iteration 160: Loss = -11281.407840219648
Iteration 170: Loss = -11281.407840089127
Iteration 180: Loss = -11281.407840054631
Iteration 190: Loss = -11281.407840045626
Iteration 200: Loss = -11281.407840043294
Iteration 210: Loss = -11281.407840042695
Iteration 220: Loss = -11281.407840042542
Iteration 230: Loss = -11281.407840042504
Iteration 240: Loss = -11281.407840042491
Iteration 250: Loss = -11281.40784004249
Iteration 260: Loss = -11281.407840042488
Iteration 270: Loss = -11281.40784004249
1
Iteration 280: Loss = -11281.407840042488
2
Iteration 290: Loss = -11281.40784004249
3
Stopping early at iteration 289 due to no improvement.
pi: tensor([[0.0306, 0.9694],
        [0.0281, 0.9719]], dtype=torch.float64)
alpha: tensor([0.0279, 0.9721], dtype=torch.float64)
beta: tensor([[[0.2752, 0.1925],
         [0.3575, 0.1672]],

        [[0.9316, 0.2505],
         [0.0429, 0.3325]],

        [[0.1729, 0.1604],
         [0.5164, 0.6463]],

        [[0.7549, 0.0833],
         [0.5128, 0.8218]],

        [[0.5170, 0.1873],
         [0.9375, 0.7828]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002569651861066151
Average Adjusted Rand Index: -0.0015541722481668405
ours kmeans ------------------------------------------------------------------------------
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24390.480932419123
Iteration 100: Loss = -11301.722046897989
Iteration 200: Loss = -11292.110260201527
Iteration 300: Loss = -11288.781775666737
Iteration 400: Loss = -11287.19590352566
Iteration 500: Loss = -11286.296469222938
Iteration 600: Loss = -11285.724458033299
Iteration 700: Loss = -11285.329530872536
Iteration 800: Loss = -11285.039210219793
Iteration 900: Loss = -11284.818864096002
Iteration 1000: Loss = -11284.648264585721
Iteration 1100: Loss = -11284.510329299954
Iteration 1200: Loss = -11284.398175481116
Iteration 1300: Loss = -11284.305639346901
Iteration 1400: Loss = -11284.226135310155
Iteration 1500: Loss = -11284.151073254296
Iteration 1600: Loss = -11284.063977370006
Iteration 1700: Loss = -11283.904894284942
Iteration 1800: Loss = -11283.248599910485
Iteration 1900: Loss = -11282.247419040255
Iteration 2000: Loss = -11282.144696008569
Iteration 2100: Loss = -11282.08350954336
Iteration 2200: Loss = -11282.019876691575
Iteration 2300: Loss = -11281.948813584651
Iteration 2400: Loss = -11281.867598194283
Iteration 2500: Loss = -11281.770868266562
Iteration 2600: Loss = -11281.654275885525
Iteration 2700: Loss = -11281.525306058918
Iteration 2800: Loss = -11281.33434191192
Iteration 2900: Loss = -11280.458360895049
Iteration 3000: Loss = -11278.675418121204
Iteration 3100: Loss = -11274.045339249906
Iteration 3200: Loss = -11269.631164469123
Iteration 3300: Loss = -11268.20883491628
Iteration 3400: Loss = -11267.465542538592
Iteration 3500: Loss = -11267.00206513844
Iteration 3600: Loss = -11266.358950428092
Iteration 3700: Loss = -11263.017877980428
Iteration 3800: Loss = -11261.665454467178
Iteration 3900: Loss = -11261.409498918805
Iteration 4000: Loss = -11260.657857754564
Iteration 4100: Loss = -11259.852055278354
Iteration 4200: Loss = -11259.772813196874
Iteration 4300: Loss = -11259.723432629418
Iteration 4400: Loss = -11259.688004717385
Iteration 4500: Loss = -11259.661638085186
Iteration 4600: Loss = -11259.634639067026
Iteration 4700: Loss = -11259.609705624353
Iteration 4800: Loss = -11259.58732773468
Iteration 4900: Loss = -11259.570125404416
Iteration 5000: Loss = -11259.559714125451
Iteration 5100: Loss = -11259.548005807064
Iteration 5200: Loss = -11259.540425245385
Iteration 5300: Loss = -11259.534184204502
Iteration 5400: Loss = -11259.529102209668
Iteration 5500: Loss = -11259.524506689684
Iteration 5600: Loss = -11259.520693946417
Iteration 5700: Loss = -11259.522015259243
1
Iteration 5800: Loss = -11259.514440820289
Iteration 5900: Loss = -11259.511807179219
Iteration 6000: Loss = -11259.509412729001
Iteration 6100: Loss = -11259.524425120928
1
Iteration 6200: Loss = -11259.505170038823
Iteration 6300: Loss = -11259.50326674102
Iteration 6400: Loss = -11259.50148990264
Iteration 6500: Loss = -11259.509728757206
1
Iteration 6600: Loss = -11259.498305390738
Iteration 6700: Loss = -11259.496897740737
Iteration 6800: Loss = -11259.495613277622
Iteration 6900: Loss = -11259.594371620671
1
Iteration 7000: Loss = -11259.493383568375
Iteration 7100: Loss = -11259.492414895087
Iteration 7200: Loss = -11259.495405891352
1
Iteration 7300: Loss = -11259.51472662231
2
Iteration 7400: Loss = -11259.489971270403
Iteration 7500: Loss = -11259.48928279738
Iteration 7600: Loss = -11259.488641627644
Iteration 7700: Loss = -11259.48804610758
Iteration 7800: Loss = -11259.51618901625
1
Iteration 7900: Loss = -11259.486980306416
Iteration 8000: Loss = -11259.486498093005
Iteration 8100: Loss = -11259.523354003239
1
Iteration 8200: Loss = -11259.485630023768
Iteration 8300: Loss = -11259.485234614314
Iteration 8400: Loss = -11259.488495138065
1
Iteration 8500: Loss = -11259.48452274196
Iteration 8600: Loss = -11259.48419251311
Iteration 8700: Loss = -11259.484330949297
1
Iteration 8800: Loss = -11259.483817484372
Iteration 8900: Loss = -11259.483365550757
Iteration 9000: Loss = -11259.491741544733
1
Iteration 9100: Loss = -11259.482828149932
Iteration 9200: Loss = -11259.482605047055
Iteration 9300: Loss = -11259.482448042323
Iteration 9400: Loss = -11259.482277432893
Iteration 9500: Loss = -11259.481988001526
Iteration 9600: Loss = -11259.481913141795
Iteration 9700: Loss = -11259.481631563516
Iteration 9800: Loss = -11259.481485613145
Iteration 9900: Loss = -11259.48131054286
Iteration 10000: Loss = -11259.493274561732
1
Iteration 10100: Loss = -11259.481022255532
Iteration 10200: Loss = -11259.480888453185
Iteration 10300: Loss = -11259.484809414218
1
Iteration 10400: Loss = -11259.48063993115
Iteration 10500: Loss = -11259.48052387493
Iteration 10600: Loss = -11259.565085865608
1
Iteration 10700: Loss = -11259.480311552765
Iteration 10800: Loss = -11259.480539210133
1
Iteration 10900: Loss = -11259.48010678765
Iteration 11000: Loss = -11259.480182233714
1
Iteration 11100: Loss = -11259.479924416466
Iteration 11200: Loss = -11259.479886451154
Iteration 11300: Loss = -11259.479782227996
Iteration 11400: Loss = -11259.48031992789
1
Iteration 11500: Loss = -11259.479579647681
Iteration 11600: Loss = -11259.480356339765
1
Iteration 11700: Loss = -11259.47956060248
Iteration 11800: Loss = -11259.479341470224
Iteration 11900: Loss = -11259.480569208401
1
Iteration 12000: Loss = -11259.47918779418
Iteration 12100: Loss = -11259.479997212326
1
Iteration 12200: Loss = -11259.47905348684
Iteration 12300: Loss = -11259.498810959285
1
Iteration 12400: Loss = -11259.478934146993
Iteration 12500: Loss = -11259.535856782513
1
Iteration 12600: Loss = -11259.478823800668
Iteration 12700: Loss = -11259.480542208014
1
Iteration 12800: Loss = -11259.480909380922
2
Iteration 12900: Loss = -11259.483613570043
3
Iteration 13000: Loss = -11259.478623721696
Iteration 13100: Loss = -11259.488360467296
1
Iteration 13200: Loss = -11259.47853196556
Iteration 13300: Loss = -11259.478719700628
1
Iteration 13400: Loss = -11259.478447599487
Iteration 13500: Loss = -11259.47872306995
1
Iteration 13600: Loss = -11259.48061095179
2
Iteration 13700: Loss = -11259.478314829183
Iteration 13800: Loss = -11259.47829539854
Iteration 13900: Loss = -11259.478326391962
1
Iteration 14000: Loss = -11259.478201518776
Iteration 14100: Loss = -11259.4781537658
Iteration 14200: Loss = -11259.479164992179
1
Iteration 14300: Loss = -11259.47841567472
2
Iteration 14400: Loss = -11259.47856718071
3
Iteration 14500: Loss = -11259.483429220829
4
Iteration 14600: Loss = -11259.478013305612
Iteration 14700: Loss = -11259.478851104077
1
Iteration 14800: Loss = -11259.534545864686
2
Iteration 14900: Loss = -11259.477943265298
Iteration 15000: Loss = -11259.479668713911
1
Iteration 15100: Loss = -11259.47790360691
Iteration 15200: Loss = -11259.479375747034
1
Iteration 15300: Loss = -11259.477867904716
Iteration 15400: Loss = -11259.480717585378
1
Iteration 15500: Loss = -11259.47786236651
Iteration 15600: Loss = -11259.478218540784
1
Iteration 15700: Loss = -11259.47788949061
2
Iteration 15800: Loss = -11259.477813534359
Iteration 15900: Loss = -11259.504421773336
1
Iteration 16000: Loss = -11259.477780072333
Iteration 16100: Loss = -11259.479007752669
1
Iteration 16200: Loss = -11259.478401859697
2
Iteration 16300: Loss = -11259.480110118742
3
Iteration 16400: Loss = -11259.477752818128
Iteration 16500: Loss = -11259.478078968463
1
Iteration 16600: Loss = -11259.489500543032
2
Iteration 16700: Loss = -11259.509587000195
3
Iteration 16800: Loss = -11259.477717965556
Iteration 16900: Loss = -11259.49026112685
1
Iteration 17000: Loss = -11259.478476569364
2
Iteration 17100: Loss = -11259.477857625036
3
Iteration 17200: Loss = -11259.480305428187
4
Iteration 17300: Loss = -11259.482480746923
5
Iteration 17400: Loss = -11259.487489887624
6
Iteration 17500: Loss = -11259.478545637563
7
Iteration 17600: Loss = -11259.478345856536
8
Iteration 17700: Loss = -11259.491116463714
9
Iteration 17800: Loss = -11259.477665332945
Iteration 17900: Loss = -11259.479200220458
1
Iteration 18000: Loss = -11259.478334525858
2
Iteration 18100: Loss = -11259.477826154643
3
Iteration 18200: Loss = -11259.478748268124
4
Iteration 18300: Loss = -11259.47765688365
Iteration 18400: Loss = -11259.477668862757
1
Iteration 18500: Loss = -11259.477831649203
2
Iteration 18600: Loss = -11259.478824917143
3
Iteration 18700: Loss = -11259.515782011094
4
Iteration 18800: Loss = -11259.477637657468
Iteration 18900: Loss = -11259.47769570437
1
Iteration 19000: Loss = -11259.479926384965
2
Iteration 19100: Loss = -11259.477637648288
Iteration 19200: Loss = -11259.478387294927
1
Iteration 19300: Loss = -11259.4780590528
2
Iteration 19400: Loss = -11259.477634548794
Iteration 19500: Loss = -11259.478231565829
1
Iteration 19600: Loss = -11259.477627107473
Iteration 19700: Loss = -11259.477671705386
1
Iteration 19800: Loss = -11259.495968831112
2
Iteration 19900: Loss = -11259.477626313967
pi: tensor([[3.5728e-01, 6.4272e-01],
        [4.2151e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4411, 0.5589], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2987, 0.1051],
         [0.3575, 0.1688]],

        [[0.9316, 0.1876],
         [0.0429, 0.3325]],

        [[0.1729, 0.1881],
         [0.5164, 0.6463]],

        [[0.7549, 0.8442],
         [0.5128, 0.8218]],

        [[0.5170, 0.2656],
         [0.9375, 0.7828]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 31
Adjusted Rand Index: 0.13912707507365113
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.11259540635922663
Average Adjusted Rand Index: 0.2198254150147302
prior 0 times ------------------------------------------------------------------------------
Iteration 0: Loss = -29918.327602216006
Iteration 10: Loss = -11281.68569360255
Iteration 20: Loss = -11281.268882736551
Iteration 30: Loss = -11281.115524236158
Iteration 40: Loss = -11281.06821472549
Iteration 50: Loss = -11281.05611615923
Iteration 60: Loss = -11281.054446428867
Iteration 70: Loss = -11281.05583353865
1
Iteration 80: Loss = -11281.058094735028
2
Iteration 90: Loss = -11281.060510423004
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.1850, 0.8150],
        [0.0843, 0.9157]], dtype=torch.float64)
alpha: tensor([0.0931, 0.9069], dtype=torch.float64)
beta: tensor([[[0.2524, 0.1933],
         [0.1581, 0.1620]],

        [[0.4307, 0.2204],
         [0.0934, 0.2727]],

        [[0.2977, 0.1744],
         [0.8943, 0.1601]],

        [[0.3162, 0.1911],
         [0.4114, 0.8165]],

        [[0.7193, 0.1885],
         [0.7503, 0.2592]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.008125898971292667
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.00476355833355593
Average Adjusted Rand Index: 0.0025140686831474227
ours 0 times ------------------------------------------------------------------------------
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29918.327602216003
Iteration 100: Loss = -11363.660576730295
Iteration 200: Loss = -11263.39171954137
Iteration 300: Loss = -11170.438251924463
Iteration 400: Loss = -11149.992713829235
Iteration 500: Loss = -11147.051326303392
Iteration 600: Loss = -11145.399548923562
Iteration 700: Loss = -11142.81890720838
Iteration 800: Loss = -11142.154298822463
Iteration 900: Loss = -11141.67465893552
Iteration 1000: Loss = -11141.31627115277
Iteration 1100: Loss = -11141.045589389038
Iteration 1200: Loss = -11140.841857909292
Iteration 1300: Loss = -11140.678404527755
Iteration 1400: Loss = -11140.54510382497
Iteration 1500: Loss = -11140.434677120964
Iteration 1600: Loss = -11140.341350914212
Iteration 1700: Loss = -11140.255899248636
Iteration 1800: Loss = -11140.155893383946
Iteration 1900: Loss = -11140.09795983142
Iteration 2000: Loss = -11140.048358529499
Iteration 2100: Loss = -11140.005138791412
Iteration 2200: Loss = -11139.967220170365
Iteration 2300: Loss = -11139.933766270704
Iteration 2400: Loss = -11139.904105562677
Iteration 2500: Loss = -11139.877690772762
Iteration 2600: Loss = -11139.854071236803
Iteration 2700: Loss = -11139.832872672243
Iteration 2800: Loss = -11139.813781936342
Iteration 2900: Loss = -11139.796535298308
Iteration 3000: Loss = -11139.780909263136
Iteration 3100: Loss = -11139.766713302197
Iteration 3200: Loss = -11139.753784039365
Iteration 3300: Loss = -11139.741980564446
Iteration 3400: Loss = -11139.731180643226
Iteration 3500: Loss = -11139.721277739165
Iteration 3600: Loss = -11139.712179092696
Iteration 3700: Loss = -11139.703805405725
Iteration 3800: Loss = -11139.696090814568
Iteration 3900: Loss = -11139.688978093216
Iteration 4000: Loss = -11139.682410858184
Iteration 4100: Loss = -11139.67633172674
Iteration 4200: Loss = -11139.670693300199
Iteration 4300: Loss = -11139.665477399232
Iteration 4400: Loss = -11139.660654096104
Iteration 4500: Loss = -11139.65617870658
Iteration 4600: Loss = -11139.652014121082
Iteration 4700: Loss = -11139.648129017021
Iteration 4800: Loss = -11139.644495782864
Iteration 4900: Loss = -11139.641092221178
Iteration 5000: Loss = -11139.637907869981
Iteration 5100: Loss = -11139.634947515844
Iteration 5200: Loss = -11139.632210991871
Iteration 5300: Loss = -11139.629674975
Iteration 5400: Loss = -11139.627313519053
Iteration 5500: Loss = -11139.625109064933
Iteration 5600: Loss = -11139.6230485487
Iteration 5700: Loss = -11139.621120754138
Iteration 5800: Loss = -11139.619315980644
Iteration 5900: Loss = -11139.617625331417
Iteration 6000: Loss = -11139.616040761282
Iteration 6100: Loss = -11139.61455639384
Iteration 6200: Loss = -11139.61316113969
Iteration 6300: Loss = -11139.611853226073
Iteration 6400: Loss = -11139.610625507368
Iteration 6500: Loss = -11139.609473748444
Iteration 6600: Loss = -11139.608389708033
Iteration 6700: Loss = -11139.607371745305
Iteration 6800: Loss = -11139.606430742235
Iteration 6900: Loss = -11139.605572355773
Iteration 7000: Loss = -11139.604661817983
Iteration 7100: Loss = -11139.603857957274
Iteration 7200: Loss = -11139.603234723441
Iteration 7300: Loss = -11139.602385264043
Iteration 7400: Loss = -11139.601766636113
Iteration 7500: Loss = -11139.601110153428
Iteration 7600: Loss = -11139.601435932525
1
Iteration 7700: Loss = -11139.600023164772
Iteration 7800: Loss = -11139.599575592249
Iteration 7900: Loss = -11139.59908076701
Iteration 8000: Loss = -11139.598730888927
Iteration 8100: Loss = -11139.599017712579
1
Iteration 8200: Loss = -11139.597998107653
Iteration 8300: Loss = -11139.598884541201
1
Iteration 8400: Loss = -11139.597500820055
Iteration 8500: Loss = -11139.597096594813
Iteration 8600: Loss = -11139.59874271145
1
Iteration 8700: Loss = -11139.597773792217
2
Iteration 8800: Loss = -11139.59700733114
Iteration 8900: Loss = -11139.597894246006
1
Iteration 9000: Loss = -11139.597459787472
2
Iteration 9100: Loss = -11139.595990970894
Iteration 9200: Loss = -11139.595804139602
Iteration 9300: Loss = -11139.59524901303
Iteration 9400: Loss = -11139.60101894878
1
Iteration 9500: Loss = -11139.59595177083
2
Iteration 9600: Loss = -11139.594949229839
Iteration 9700: Loss = -11139.594661183603
Iteration 9800: Loss = -11139.595742046087
1
Iteration 9900: Loss = -11139.594458355918
pi: tensor([[0.2555, 0.7445],
        [0.5803, 0.4197]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.7435e-05, 9.9998e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2562, 0.2114],
         [0.1581, 0.2062]],

        [[0.4307, 0.0940],
         [0.0934, 0.2727]],

        [[0.2977, 0.0905],
         [0.8943, 0.1601]],

        [[0.3162, 0.0933],
         [0.4114, 0.8165]],

        [[0.7193, 0.1003],
         [0.7503, 0.2592]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.772151675588645
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: -0.00018949043915461998
Average Adjusted Rand Index: 0.6619858906732845
prior 1 times ------------------------------------------------------------------------------
Iteration 0: Loss = -29910.454583415143
Iteration 10: Loss = -11284.231404914148
Iteration 20: Loss = -11284.231404914148
1
Iteration 30: Loss = -11284.231404914148
2
Iteration 40: Loss = -11284.231404914148
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0571e-22, 1.0000e+00],
        [2.2260e-15, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([2.0242e-15, 1.0000e+00], dtype=torch.float64)
beta: tensor([[[0.4478, 0.1826],
         [0.9476, 0.1683]],

        [[0.0626, 0.2773],
         [0.1433, 0.5556]],

        [[0.1979, 0.1505],
         [0.7854, 0.7264]],

        [[0.9746, 0.1961],
         [0.9900, 0.1570]],

        [[0.5433, 0.2752],
         [0.0986, 0.8252]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
ours 1 times ------------------------------------------------------------------------------
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29910.454583415147
Iteration 100: Loss = -11389.974206961619
Iteration 200: Loss = -11317.081308111372
Iteration 300: Loss = -11300.985308421365
Iteration 400: Loss = -11294.271786276058
Iteration 500: Loss = -11290.724560785457
Iteration 600: Loss = -11288.580683901779
Iteration 700: Loss = -11287.097660196654
Iteration 800: Loss = -11286.02865244322
Iteration 900: Loss = -11285.067955803128
Iteration 1000: Loss = -11284.155863883447
Iteration 1100: Loss = -11283.583043352815
Iteration 1200: Loss = -11283.244794802258
Iteration 1300: Loss = -11283.03655420481
Iteration 1400: Loss = -11282.875359794694
Iteration 1500: Loss = -11282.739017132979
Iteration 1600: Loss = -11282.624143682056
Iteration 1700: Loss = -11282.526225627696
Iteration 1800: Loss = -11282.435743427835
Iteration 1900: Loss = -11282.348395716801
Iteration 2000: Loss = -11282.262577258029
Iteration 2100: Loss = -11282.177332889567
Iteration 2200: Loss = -11282.090481950067
Iteration 2300: Loss = -11281.999511771199
Iteration 2400: Loss = -11281.90473908152
Iteration 2500: Loss = -11281.81087770621
Iteration 2600: Loss = -11281.722948551807
Iteration 2700: Loss = -11281.642726562512
Iteration 2800: Loss = -11281.570012496542
Iteration 2900: Loss = -11281.503950522718
Iteration 3000: Loss = -11281.443629262429
Iteration 3100: Loss = -11281.38818957784
Iteration 3200: Loss = -11281.336957053616
Iteration 3300: Loss = -11281.289551874075
Iteration 3400: Loss = -11281.245978471272
Iteration 3500: Loss = -11281.206314634182
Iteration 3600: Loss = -11281.170152659395
Iteration 3700: Loss = -11281.137026880555
Iteration 3800: Loss = -11281.106628955024
Iteration 3900: Loss = -11281.0786936886
Iteration 4000: Loss = -11281.05296681578
Iteration 4100: Loss = -11281.029218467047
Iteration 4200: Loss = -11281.007253212389
Iteration 4300: Loss = -11280.986914173754
Iteration 4400: Loss = -11280.968078531434
Iteration 4500: Loss = -11280.950648505894
Iteration 4600: Loss = -11280.934548806326
Iteration 4700: Loss = -11280.919729689183
Iteration 4800: Loss = -11280.906163355234
Iteration 4900: Loss = -11280.89382537736
Iteration 5000: Loss = -11280.882669790468
Iteration 5100: Loss = -11280.872622347126
Iteration 5200: Loss = -11280.863597235846
Iteration 5300: Loss = -11280.855497232704
Iteration 5400: Loss = -11280.848218286714
Iteration 5500: Loss = -11280.841674056835
Iteration 5600: Loss = -11280.864039181184
1
Iteration 5700: Loss = -11280.830514037174
Iteration 5800: Loss = -11280.825814718772
Iteration 5900: Loss = -11280.821633744406
Iteration 6000: Loss = -11280.81792182181
Iteration 6100: Loss = -11280.814625311963
Iteration 6200: Loss = -11280.812445869336
Iteration 6300: Loss = -11280.80906763253
Iteration 6400: Loss = -11280.806687373753
Iteration 6500: Loss = -11280.804543003751
Iteration 6600: Loss = -11280.80258492302
Iteration 6700: Loss = -11280.800784909097
Iteration 6800: Loss = -11280.799698254907
Iteration 6900: Loss = -11280.797565638659
Iteration 7000: Loss = -11280.796116441206
Iteration 7100: Loss = -11280.794754918896
Iteration 7200: Loss = -11280.793470755973
Iteration 7300: Loss = -11280.79225764448
Iteration 7400: Loss = -11280.791132644872
Iteration 7500: Loss = -11280.790010731795
Iteration 7600: Loss = -11280.78897442145
Iteration 7700: Loss = -11280.787994351975
Iteration 7800: Loss = -11280.787169705098
Iteration 7900: Loss = -11280.78617823924
Iteration 8000: Loss = -11280.78532582719
Iteration 8100: Loss = -11280.784554530403
Iteration 8200: Loss = -11280.784223879298
Iteration 8300: Loss = -11280.783051849401
Iteration 8400: Loss = -11280.78252533581
Iteration 8500: Loss = -11280.781724543049
Iteration 8600: Loss = -11280.806616839487
1
Iteration 8700: Loss = -11280.780528603998
Iteration 8800: Loss = -11280.77997462318
Iteration 8900: Loss = -11280.793440807858
1
Iteration 9000: Loss = -11280.778944550006
Iteration 9100: Loss = -11280.778464464309
Iteration 9200: Loss = -11280.780170881184
1
Iteration 9300: Loss = -11280.777568474809
Iteration 9400: Loss = -11280.780099247246
1
Iteration 9500: Loss = -11280.776744255314
Iteration 9600: Loss = -11280.776357922137
Iteration 9700: Loss = -11280.77786329898
1
Iteration 9800: Loss = -11280.775631768145
Iteration 9900: Loss = -11280.77528996243
pi: tensor([[0.0030, 0.9970],
        [0.0586, 0.9414]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0119, 0.9881], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3426, 0.2099],
         [0.9476, 0.1664]],

        [[0.0626, 0.2355],
         [0.1433, 0.5556]],

        [[0.1979, 0.1689],
         [0.7854, 0.7264]],

        [[0.9746, 0.2061],
         [0.9900, 0.1570]],

        [[0.5433, 0.2009],
         [0.0986, 0.8252]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.008125898971292667
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.00476355833355593
Average Adjusted Rand Index: 0.0025140686831474227
prior 2 times ------------------------------------------------------------------------------
Iteration 0: Loss = -20511.46260697839
Iteration 10: Loss = -11281.224368334213
Iteration 20: Loss = -11281.122700808988
Iteration 30: Loss = -11281.079236690777
Iteration 40: Loss = -11281.060255975102
Iteration 50: Loss = -11281.053138919411
Iteration 60: Loss = -11281.051652857144
Iteration 70: Loss = -11281.052729561348
1
Iteration 80: Loss = -11281.0548839244
2
Iteration 90: Loss = -11281.057398150031
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.1813, 0.8187],
        [0.0826, 0.9174]], dtype=torch.float64)
alpha: tensor([0.0909, 0.9091], dtype=torch.float64)
beta: tensor([[[0.2537, 0.1935],
         [0.8743, 0.1621]],

        [[0.3336, 0.2211],
         [0.7227, 0.9740]],

        [[0.0960, 0.1744],
         [0.6099, 0.1275]],

        [[0.2078, 0.1914],
         [0.3203, 0.1416]],

        [[0.7419, 0.1888],
         [0.3628, 0.1945]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.008125898971292667
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.00476355833355593
Average Adjusted Rand Index: 0.0025140686831474227
ours 2 times ------------------------------------------------------------------------------
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20511.462606978395
Iteration 100: Loss = -11407.271038588035
Iteration 200: Loss = -11344.06026854972
Iteration 300: Loss = -11310.671020874714
Iteration 400: Loss = -11290.795727728579
Iteration 500: Loss = -11287.339082837992
Iteration 600: Loss = -11285.695895537894
Iteration 700: Loss = -11284.706157094584
Iteration 800: Loss = -11284.032612368832
Iteration 900: Loss = -11283.527867119767
Iteration 1000: Loss = -11283.138941140141
Iteration 1100: Loss = -11282.827233804119
Iteration 1200: Loss = -11282.575181359924
Iteration 1300: Loss = -11282.36870613335
Iteration 1400: Loss = -11282.195676031588
Iteration 1500: Loss = -11282.04910185925
Iteration 1600: Loss = -11281.922048366525
Iteration 1700: Loss = -11281.806925501249
Iteration 1800: Loss = -11281.693507806493
Iteration 1900: Loss = -11281.52793667613
Iteration 2000: Loss = -11265.529579105982
Iteration 2100: Loss = -11260.072682402733
Iteration 2200: Loss = -11259.923968505704
Iteration 2300: Loss = -11259.833049695233
Iteration 2400: Loss = -11259.680035405394
Iteration 2500: Loss = -11257.438869534344
Iteration 2600: Loss = -11254.770623153465
Iteration 2700: Loss = -11229.4974445065
Iteration 2800: Loss = -11209.74478464063
Iteration 2900: Loss = -11204.951014436048
Iteration 3000: Loss = -11190.608833041351
Iteration 3100: Loss = -11135.012411979982
Iteration 3200: Loss = -11115.010889456129
Iteration 3300: Loss = -11110.275103178048
Iteration 3400: Loss = -11108.5724370403
Iteration 3500: Loss = -11103.886686097872
Iteration 3600: Loss = -11103.790340299505
Iteration 3700: Loss = -11103.760121329731
Iteration 3800: Loss = -11103.720545955062
Iteration 3900: Loss = -11100.48123279155
Iteration 4000: Loss = -11096.537227242437
Iteration 4100: Loss = -11095.12785238246
Iteration 4200: Loss = -11092.507692219273
Iteration 4300: Loss = -11085.997094806486
Iteration 4400: Loss = -11079.62580355979
Iteration 4500: Loss = -11078.021070628987
Iteration 4600: Loss = -11063.385336847603
Iteration 4700: Loss = -11057.383577908866
Iteration 4800: Loss = -11042.749575719989
Iteration 4900: Loss = -11041.362198958659
Iteration 5000: Loss = -11041.3068794753
Iteration 5100: Loss = -11041.214479718627
Iteration 5200: Loss = -11039.725114854698
Iteration 5300: Loss = -11027.860678305637
Iteration 5400: Loss = -11027.85051107055
Iteration 5500: Loss = -11027.843122030383
Iteration 5600: Loss = -11027.837822215823
Iteration 5700: Loss = -11027.795016276012
Iteration 5800: Loss = -11027.787716507486
Iteration 5900: Loss = -11027.784741672014
Iteration 6000: Loss = -11027.782329147716
Iteration 6100: Loss = -11027.780218642394
Iteration 6200: Loss = -11027.778332118027
Iteration 6300: Loss = -11027.776496619561
Iteration 6400: Loss = -11027.77448840968
Iteration 6500: Loss = -11027.772250024713
Iteration 6600: Loss = -11027.770197915319
Iteration 6700: Loss = -11027.76664417472
Iteration 6800: Loss = -11027.606903733853
Iteration 6900: Loss = -11027.605119866768
Iteration 7000: Loss = -11027.604159786817
Iteration 7100: Loss = -11027.60404910808
Iteration 7200: Loss = -11027.6017926135
Iteration 7300: Loss = -11027.59919262727
Iteration 7400: Loss = -11027.584561986449
Iteration 7500: Loss = -11027.575604183716
Iteration 7600: Loss = -11027.571548035789
Iteration 7700: Loss = -11027.570849895861
Iteration 7800: Loss = -11027.570490854918
Iteration 7900: Loss = -11027.569785257241
Iteration 8000: Loss = -11027.569111724984
Iteration 8100: Loss = -11027.567742506099
Iteration 8200: Loss = -11027.562377371356
Iteration 8300: Loss = -11027.552541098692
Iteration 8400: Loss = -11027.552349405369
Iteration 8500: Loss = -11027.552804899642
1
Iteration 8600: Loss = -11027.552042735368
Iteration 8700: Loss = -11027.552905166009
1
Iteration 8800: Loss = -11027.551161131221
Iteration 8900: Loss = -11027.550774005282
Iteration 9000: Loss = -11027.550384372562
Iteration 9100: Loss = -11027.550149288309
Iteration 9200: Loss = -11027.56382896153
1
Iteration 9300: Loss = -11027.549430705049
Iteration 9400: Loss = -11027.54945242549
1
Iteration 9500: Loss = -11027.548750909176
Iteration 9600: Loss = -11027.548791020261
1
Iteration 9700: Loss = -11027.547957518056
Iteration 9800: Loss = -11027.546411643534
Iteration 9900: Loss = -11027.519919179755
pi: tensor([[0.6880, 0.3120],
        [0.2593, 0.7407]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4544, 0.5456], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2936, 0.1033],
         [0.8743, 0.2078]],

        [[0.3336, 0.0966],
         [0.7227, 0.9740]],

        [[0.0960, 0.0907],
         [0.6099, 0.1275]],

        [[0.2078, 0.0927],
         [0.3203, 0.1416]],

        [[0.7419, 0.1001],
         [0.3628, 0.1945]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9291510399040422
Average Adjusted Rand Index: 0.9288061878399743
prior 3 times ------------------------------------------------------------------------------
Iteration 0: Loss = -30382.003509272756
Iteration 10: Loss = -11284.23140491415
Iteration 20: Loss = -11284.23140491415
1
Iteration 30: Loss = -11284.23140491415
2
Iteration 40: Loss = -11284.23140491415
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 8.3990e-27],
        [1.0000e+00, 8.1810e-33]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 7.6542e-27], dtype=torch.float64)
beta: tensor([[[0.1683, 0.1827],
         [0.0618, 0.4514]],

        [[0.2642, 0.2773],
         [0.0536, 0.7714]],

        [[0.9560, 0.1505],
         [0.7419, 0.4341]],

        [[0.5747, 0.1964],
         [0.6412, 0.3662]],

        [[0.9848, 0.1747],
         [0.3389, 0.7610]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
ours 3 times ------------------------------------------------------------------------------
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30382.003509272756
Iteration 100: Loss = -11351.572606664095
Iteration 200: Loss = -11305.338452130189
Iteration 300: Loss = -11295.57714653744
Iteration 400: Loss = -11291.576722129908
Iteration 500: Loss = -11289.435902561832
Iteration 600: Loss = -11288.138799950877
Iteration 700: Loss = -11287.275054463078
Iteration 800: Loss = -11286.657328923413
Iteration 900: Loss = -11286.191112602291
Iteration 1000: Loss = -11285.82377233453
Iteration 1100: Loss = -11285.522162110377
Iteration 1200: Loss = -11285.261829813766
Iteration 1300: Loss = -11285.018956239965
Iteration 1400: Loss = -11284.758771547818
Iteration 1500: Loss = -11284.419476609248
Iteration 1600: Loss = -11284.008264820997
Iteration 1700: Loss = -11283.719138827168
Iteration 1800: Loss = -11283.53057625807
Iteration 1900: Loss = -11283.387094795653
Iteration 2000: Loss = -11283.271159271746
Iteration 2100: Loss = -11283.174163788752
Iteration 2200: Loss = -11283.090608745406
Iteration 2300: Loss = -11283.016788123196
Iteration 2400: Loss = -11282.950062895638
Iteration 2500: Loss = -11282.888463701665
Iteration 2600: Loss = -11282.830465336565
Iteration 2700: Loss = -11282.774839984424
Iteration 2800: Loss = -11282.720550725573
Iteration 2900: Loss = -11282.666666867217
Iteration 3000: Loss = -11282.612287860571
Iteration 3100: Loss = -11282.556462378774
Iteration 3200: Loss = -11282.49808366732
Iteration 3300: Loss = -11282.43572757145
Iteration 3400: Loss = -11282.36736759879
Iteration 3500: Loss = -11282.289879279257
Iteration 3600: Loss = -11282.217629897497
Iteration 3700: Loss = -11282.087104939274
Iteration 3800: Loss = -11281.950205409014
Iteration 3900: Loss = -11281.798207757245
Iteration 4000: Loss = -11281.653713496446
Iteration 4100: Loss = -11281.529104685378
Iteration 4200: Loss = -11281.710396638951
1
Iteration 4300: Loss = -11281.340031632912
Iteration 4400: Loss = -11281.269355759312
Iteration 4500: Loss = -11281.210180619626
Iteration 4600: Loss = -11281.159951107544
Iteration 4700: Loss = -11281.116627271924
Iteration 4800: Loss = -11281.079415575494
Iteration 4900: Loss = -11281.0452815092
Iteration 5000: Loss = -11281.015948437
Iteration 5100: Loss = -11280.990517861344
Iteration 5200: Loss = -11280.96877669368
Iteration 5300: Loss = -11280.950282253134
Iteration 5400: Loss = -11280.934439842224
Iteration 5500: Loss = -11280.92065344548
Iteration 5600: Loss = -11280.908506190612
Iteration 5700: Loss = -11280.897639885501
Iteration 5800: Loss = -11280.88782538718
Iteration 5900: Loss = -11280.879198295414
Iteration 6000: Loss = -11280.870731124132
Iteration 6100: Loss = -11280.863267763465
Iteration 6200: Loss = -11280.856402542666
Iteration 6300: Loss = -11280.850038298824
Iteration 6400: Loss = -11280.844393735517
Iteration 6500: Loss = -11280.838453911727
Iteration 6600: Loss = -11280.833106332817
Iteration 6700: Loss = -11280.827997657456
Iteration 6800: Loss = -11280.823131281582
Iteration 6900: Loss = -11280.818751944436
Iteration 7000: Loss = -11280.814310991987
Iteration 7100: Loss = -11280.810496777478
Iteration 7200: Loss = -11280.807126315965
Iteration 7300: Loss = -11280.804185442477
Iteration 7400: Loss = -11280.801640705155
Iteration 7500: Loss = -11280.799381486833
Iteration 7600: Loss = -11280.797402503993
Iteration 7700: Loss = -11280.79562810863
Iteration 7800: Loss = -11280.794016179261
Iteration 7900: Loss = -11280.793308157481
Iteration 8000: Loss = -11280.791158063965
Iteration 8100: Loss = -11280.789878147763
Iteration 8200: Loss = -11280.788679079446
Iteration 8300: Loss = -11280.787553279577
Iteration 8400: Loss = -11280.788227701323
1
Iteration 8500: Loss = -11280.785498403067
Iteration 8600: Loss = -11280.784565898508
Iteration 8700: Loss = -11280.783691645707
Iteration 8800: Loss = -11280.782873156624
Iteration 8900: Loss = -11280.7860326065
1
Iteration 9000: Loss = -11280.781391196306
Iteration 9100: Loss = -11280.78072457178
Iteration 9200: Loss = -11280.780102303395
Iteration 9300: Loss = -11280.779520931512
Iteration 9400: Loss = -11280.779064143153
Iteration 9500: Loss = -11280.778465908643
Iteration 9600: Loss = -11280.777986867766
Iteration 9700: Loss = -11280.77753524391
Iteration 9800: Loss = -11280.777108151313
Iteration 9900: Loss = -11280.776955068899
pi: tensor([[0.9413, 0.0587],
        [0.9966, 0.0034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9883, 0.0117], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1658, 0.2101],
         [0.0618, 0.3429]],

        [[0.2642, 0.2342],
         [0.0536, 0.7714]],

        [[0.9560, 0.1690],
         [0.7419, 0.4341]],

        [[0.5747, 0.2061],
         [0.6412, 0.3662]],

        [[0.9848, 0.2009],
         [0.3389, 0.7610]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.00476355833355593
Average Adjusted Rand Index: 0.0025140686831474227
prior 4 times ------------------------------------------------------------------------------
Iteration 0: Loss = -38944.19510364025
Iteration 10: Loss = -11284.230335781791
Iteration 20: Loss = -11284.09429830095
Iteration 30: Loss = -11281.590392567628
Iteration 40: Loss = -11281.452238763115
Iteration 50: Loss = -11281.459150096536
1
Iteration 60: Loss = -11281.45942203468
2
Iteration 70: Loss = -11281.431944417338
Iteration 80: Loss = -11281.410166130889
Iteration 90: Loss = -11281.408157547396
Iteration 100: Loss = -11281.407915615217
Iteration 110: Loss = -11281.407860260742
Iteration 120: Loss = -11281.407845452133
Iteration 130: Loss = -11281.40784147041
Iteration 140: Loss = -11281.407840414922
Iteration 150: Loss = -11281.407840138738
Iteration 160: Loss = -11281.407840067188
Iteration 170: Loss = -11281.40784004879
Iteration 180: Loss = -11281.407840044092
Iteration 190: Loss = -11281.407840042892
Iteration 200: Loss = -11281.407840042591
Iteration 210: Loss = -11281.407840042517
Iteration 220: Loss = -11281.407840042495
Iteration 230: Loss = -11281.407840042491
Iteration 240: Loss = -11281.407840042488
Iteration 250: Loss = -11281.407840042486
Iteration 260: Loss = -11281.407840042488
1
Iteration 270: Loss = -11281.407840042488
2
Iteration 280: Loss = -11281.407840042488
3
Stopping early at iteration 279 due to no improvement.
pi: tensor([[0.9719, 0.0281],
        [0.9694, 0.0306]], dtype=torch.float64)
alpha: tensor([0.9721, 0.0279], dtype=torch.float64)
beta: tensor([[[0.1672, 0.1925],
         [0.5436, 0.2752]],

        [[0.7591, 0.2505],
         [0.1461, 0.6889]],

        [[0.7073, 0.1604],
         [0.8969, 0.8222]],

        [[0.5087, 0.0833],
         [0.4040, 0.9719]],

        [[0.8653, 0.1873],
         [0.7836, 0.3641]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002569651861066151
Average Adjusted Rand Index: -0.0015541722481668405
ours 4 times ------------------------------------------------------------------------------
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38944.19510364025
Iteration 100: Loss = -11427.392497006023
Iteration 200: Loss = -11330.45668320617
Iteration 300: Loss = -11309.627771224785
Iteration 400: Loss = -11300.682610057376
Iteration 500: Loss = -11295.870428795648
Iteration 600: Loss = -11292.936920476452
Iteration 700: Loss = -11290.996173989432
Iteration 800: Loss = -11289.635679757599
Iteration 900: Loss = -11288.639655890498
Iteration 1000: Loss = -11287.885354579872
Iteration 1100: Loss = -11287.298406866676
Iteration 1200: Loss = -11286.831310732796
Iteration 1300: Loss = -11286.452373946348
Iteration 1400: Loss = -11286.13964991892
Iteration 1500: Loss = -11285.87727405298
Iteration 1600: Loss = -11285.652928672762
Iteration 1700: Loss = -11285.455287247689
Iteration 1800: Loss = -11285.272252586836
Iteration 1900: Loss = -11285.10505322784
Iteration 2000: Loss = -11284.94830875527
Iteration 2100: Loss = -11284.78953089904
Iteration 2200: Loss = -11284.61414103588
Iteration 2300: Loss = -11284.43131547893
Iteration 2400: Loss = -11284.236209659117
Iteration 2500: Loss = -11284.037199997074
Iteration 2600: Loss = -11283.841663033883
Iteration 2700: Loss = -11283.679009389227
Iteration 2800: Loss = -11283.510559089535
Iteration 2900: Loss = -11283.31151478612
Iteration 3000: Loss = -11283.158127319055
Iteration 3100: Loss = -11283.056260009476
Iteration 3200: Loss = -11282.961373175558
Iteration 3300: Loss = -11282.829868886576
Iteration 3400: Loss = -11282.66131990173
Iteration 3500: Loss = -11282.50950771751
Iteration 3600: Loss = -11282.216933338697
Iteration 3700: Loss = -11281.918823507374
Iteration 3800: Loss = -11281.711447318216
Iteration 3900: Loss = -11281.585424328296
Iteration 4000: Loss = -11281.494772550892
Iteration 4100: Loss = -11281.435333772632
Iteration 4200: Loss = -11281.382790336582
Iteration 4300: Loss = -11281.329601617785
Iteration 4400: Loss = -11281.287871703918
Iteration 4500: Loss = -11281.253915745214
Iteration 4600: Loss = -11281.225606446662
Iteration 4700: Loss = -11281.200397293918
Iteration 4800: Loss = -11281.177652825516
Iteration 4900: Loss = -11281.156194022109
Iteration 5000: Loss = -11281.135247652986
Iteration 5100: Loss = -11281.115713757032
Iteration 5200: Loss = -11281.098291016177
Iteration 5300: Loss = -11281.08280932708
Iteration 5400: Loss = -11281.06906892842
Iteration 5500: Loss = -11281.057300166103
Iteration 5600: Loss = -11281.047319058398
Iteration 5700: Loss = -11281.038696850452
Iteration 5800: Loss = -11281.031172457138
Iteration 5900: Loss = -11281.02451083695
Iteration 6000: Loss = -11281.018500376127
Iteration 6100: Loss = -11281.012995879002
Iteration 6200: Loss = -11281.007902404788
Iteration 6300: Loss = -11281.003167627849
Iteration 6400: Loss = -11280.99877100697
Iteration 6500: Loss = -11280.99470908379
Iteration 6600: Loss = -11280.99097802748
Iteration 6700: Loss = -11280.987559020965
Iteration 6800: Loss = -11280.984440303777
Iteration 6900: Loss = -11280.981602469657
Iteration 7000: Loss = -11280.979022882082
Iteration 7100: Loss = -11280.97667280498
Iteration 7200: Loss = -11280.974839254848
Iteration 7300: Loss = -11280.972544487631
Iteration 7400: Loss = -11280.970718421995
Iteration 7500: Loss = -11280.969026305058
Iteration 7600: Loss = -11280.9674527917
Iteration 7700: Loss = -11280.965984691451
Iteration 7800: Loss = -11280.964615942688
Iteration 7900: Loss = -11280.96332221812
Iteration 8000: Loss = -11280.962111136434
Iteration 8100: Loss = -11280.960971561028
Iteration 8200: Loss = -11280.959898348554
Iteration 8300: Loss = -11280.958939140944
Iteration 8400: Loss = -11280.957933759773
Iteration 8500: Loss = -11280.95703481825
Iteration 8600: Loss = -11280.956186976988
Iteration 8700: Loss = -11280.955387166741
Iteration 8800: Loss = -11280.954661470054
Iteration 8900: Loss = -11280.953920465907
Iteration 9000: Loss = -11280.953248361919
Iteration 9100: Loss = -11280.952613995349
Iteration 9200: Loss = -11280.952015145736
Iteration 9300: Loss = -11280.95336233999
1
Iteration 9400: Loss = -11280.95091599322
Iteration 9500: Loss = -11280.950411872907
Iteration 9600: Loss = -11280.94993584269
Iteration 9700: Loss = -11280.949486461986
Iteration 9800: Loss = -11280.949061879117
Iteration 9900: Loss = -11280.948660718543
pi: tensor([[9.7193e-01, 2.8066e-02],
        [9.9971e-01, 2.8923e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 3.6553e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1689, 0.4128],
         [0.5436, 0.3740]],

        [[0.7591, 0.2513],
         [0.1461, 0.6889]],

        [[0.7073, 0.1606],
         [0.8969, 0.8222]],

        [[0.5087, 0.0778],
         [0.4040, 0.9719]],

        [[0.8653, 0.2007],
         [0.7836, 0.3641]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002064007744755877
Average Adjusted Rand Index: -0.00041751076873370213
prior 5 times ------------------------------------------------------------------------------
Iteration 0: Loss = -24321.66113680115
Iteration 10: Loss = -11281.970182350127
Iteration 20: Loss = -11281.438237901397
Iteration 30: Loss = -11281.34004408938
Iteration 40: Loss = -11281.20252859365
Iteration 50: Loss = -11281.114303632117
Iteration 60: Loss = -11281.073747709725
Iteration 70: Loss = -11281.05764197728
Iteration 80: Loss = -11281.052617922729
Iteration 90: Loss = -11281.052313833656
Iteration 100: Loss = -11281.053951766975
1
Iteration 110: Loss = -11281.056311868613
2
Iteration 120: Loss = -11281.058843456767
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.9166, 0.0834],
        [0.8169, 0.1831]], dtype=torch.float64)
alpha: tensor([0.9080, 0.0920], dtype=torch.float64)
beta: tensor([[[0.1621, 0.1934],
         [0.2234, 0.2531]],

        [[0.5867, 0.2208],
         [0.1832, 0.3722]],

        [[0.7148, 0.1744],
         [0.1457, 0.3856]],

        [[0.6798, 0.1912],
         [0.8756, 0.7896]],

        [[0.6849, 0.1886],
         [0.7315, 0.6521]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.00476355833355593
Average Adjusted Rand Index: 0.0025140686831474227
ours 5 times ------------------------------------------------------------------------------
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24321.661136801147
Iteration 100: Loss = -11391.494785000119
Iteration 200: Loss = -11323.224903020066
Iteration 300: Loss = -11301.201113609615
Iteration 400: Loss = -11292.300454519713
Iteration 500: Loss = -11289.288734068663
Iteration 600: Loss = -11287.71196884663
Iteration 700: Loss = -11286.69848128804
Iteration 800: Loss = -11286.01076750469
Iteration 900: Loss = -11285.509296371922
Iteration 1000: Loss = -11285.135715572977
Iteration 1100: Loss = -11284.837204703012
Iteration 1200: Loss = -11284.590323415567
Iteration 1300: Loss = -11284.379257885563
Iteration 1400: Loss = -11284.187167653756
Iteration 1500: Loss = -11283.94803917501
Iteration 1600: Loss = -11283.64274094243
Iteration 1700: Loss = -11283.494789826176
Iteration 1800: Loss = -11283.36612317235
Iteration 1900: Loss = -11283.266310196934
Iteration 2000: Loss = -11283.182281835114
Iteration 2100: Loss = -11283.109283540463
Iteration 2200: Loss = -11283.045421939723
Iteration 2300: Loss = -11282.989218957513
Iteration 2400: Loss = -11282.93947385988
Iteration 2500: Loss = -11282.895210987115
Iteration 2600: Loss = -11282.855641351083
Iteration 2700: Loss = -11282.82012727319
Iteration 2800: Loss = -11282.788153548006
Iteration 2900: Loss = -11282.759304077215
Iteration 3000: Loss = -11282.73322884119
Iteration 3100: Loss = -11282.709601554365
Iteration 3200: Loss = -11282.688117721997
Iteration 3300: Loss = -11282.668517631042
Iteration 3400: Loss = -11282.650584866178
Iteration 3500: Loss = -11282.634134737767
Iteration 3600: Loss = -11282.61900580899
Iteration 3700: Loss = -11282.605054188309
Iteration 3800: Loss = -11282.592148571235
Iteration 3900: Loss = -11282.58016412151
Iteration 4000: Loss = -11282.56897209734
Iteration 4100: Loss = -11282.5584166795
Iteration 4200: Loss = -11282.548247052726
Iteration 4300: Loss = -11282.537840103842
Iteration 4400: Loss = -11282.524250383141
Iteration 4500: Loss = -11282.46951974729
Iteration 4600: Loss = -11282.334783677818
Iteration 4700: Loss = -11280.717436523282
Iteration 4800: Loss = -11279.995941106774
Iteration 4900: Loss = -11279.598614070832
Iteration 5000: Loss = -11279.428374490924
Iteration 5100: Loss = -11269.469490792997
Iteration 5200: Loss = -11265.784992215285
Iteration 5300: Loss = -11257.102729627088
Iteration 5400: Loss = -11250.622656745692
Iteration 5500: Loss = -11250.414945522045
Iteration 5600: Loss = -11250.324237681682
Iteration 5700: Loss = -11248.96168388708
Iteration 5800: Loss = -11236.80670839641
Iteration 5900: Loss = -11175.138469325182
Iteration 6000: Loss = -11118.298869496248
Iteration 6100: Loss = -11090.226944561542
Iteration 6200: Loss = -11089.580782068224
Iteration 6300: Loss = -11086.35329006969
Iteration 6400: Loss = -11079.257890821178
Iteration 6500: Loss = -11076.57555818373
Iteration 6600: Loss = -11071.608253862196
Iteration 6700: Loss = -11048.258125104128
Iteration 6800: Loss = -11048.190097942057
Iteration 6900: Loss = -11047.964001108796
Iteration 7000: Loss = -11041.767608078735
Iteration 7100: Loss = -11041.495342318904
Iteration 7200: Loss = -11041.148705140227
Iteration 7300: Loss = -11041.12673303404
Iteration 7400: Loss = -11039.577779440762
Iteration 7500: Loss = -11039.545959486
Iteration 7600: Loss = -11039.52996682377
Iteration 7700: Loss = -11039.52578322811
Iteration 7800: Loss = -11039.52317532416
Iteration 7900: Loss = -11039.520649681192
Iteration 8000: Loss = -11039.517960291916
Iteration 8100: Loss = -11039.511879087599
Iteration 8200: Loss = -11039.506562711163
Iteration 8300: Loss = -11039.506089468705
Iteration 8400: Loss = -11039.50252775164
Iteration 8500: Loss = -11039.49666269633
Iteration 8600: Loss = -11039.495182812056
Iteration 8700: Loss = -11039.493423634742
Iteration 8800: Loss = -11039.492380784131
Iteration 8900: Loss = -11039.492064093942
Iteration 9000: Loss = -11039.478432026777
Iteration 9100: Loss = -11039.47580728647
Iteration 9200: Loss = -11035.673538952073
Iteration 9300: Loss = -11035.671494842813
Iteration 9400: Loss = -11035.67090496854
Iteration 9500: Loss = -11035.66897704564
Iteration 9600: Loss = -11035.666073682673
Iteration 9700: Loss = -11035.66014093139
Iteration 9800: Loss = -11035.655180463229
Iteration 9900: Loss = -11035.65497474974
pi: tensor([[0.7440, 0.2560],
        [0.3090, 0.6910]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5223, 0.4777], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2102, 0.1043],
         [0.2234, 0.2880]],

        [[0.5867, 0.0963],
         [0.1832, 0.3722]],

        [[0.7148, 0.0907],
         [0.1457, 0.3856]],

        [[0.6798, 0.0926],
         [0.8756, 0.7896]],

        [[0.6849, 0.1000],
         [0.7315, 0.6521]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824276204858761
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9061133849594307
Average Adjusted Rand Index: 0.9056160535582176
prior 6 times ------------------------------------------------------------------------------
Iteration 0: Loss = -17360.44325402235
Iteration 10: Loss = -11282.655595225988
Iteration 20: Loss = -11282.33348450111
Iteration 30: Loss = -11282.168002426339
Iteration 40: Loss = -11282.107694280903
Iteration 50: Loss = -11282.082509444734
Iteration 60: Loss = -11282.066302848338
Iteration 70: Loss = -11282.051449548388
Iteration 80: Loss = -11282.036076516619
Iteration 90: Loss = -11282.019891953867
Iteration 100: Loss = -11282.002954146781
Iteration 110: Loss = -11281.985351263524
Iteration 120: Loss = -11281.967139903889
Iteration 130: Loss = -11281.9483464797
Iteration 140: Loss = -11281.928978185422
Iteration 150: Loss = -11281.909031795258
Iteration 160: Loss = -11281.888499211349
Iteration 170: Loss = -11281.86737075059
Iteration 180: Loss = -11281.84563714167
Iteration 190: Loss = -11281.823290868264
Iteration 200: Loss = -11281.800327224311
Iteration 210: Loss = -11281.776745281082
Iteration 220: Loss = -11281.752548871616
Iteration 230: Loss = -11281.7277476464
Iteration 240: Loss = -11281.702358223803
Iteration 250: Loss = -11281.676405438635
Iteration 260: Loss = -11281.649923674995
Iteration 270: Loss = -11281.622958252281
Iteration 280: Loss = -11281.595566813214
Iteration 290: Loss = -11281.567820639855
Iteration 300: Loss = -11281.539805797569
Iteration 310: Loss = -11281.51162397938
Iteration 320: Loss = -11281.483392896895
Iteration 330: Loss = -11281.455246043433
Iteration 340: Loss = -11281.42733164679
Iteration 350: Loss = -11281.399810640121
Iteration 360: Loss = -11281.372853519642
Iteration 370: Loss = -11281.346636032502
Iteration 380: Loss = -11281.32133375178
Iteration 390: Loss = -11281.297115742273
Iteration 400: Loss = -11281.274137685716
Iteration 410: Loss = -11281.252534989835
Iteration 420: Loss = -11281.232416515879
Iteration 430: Loss = -11281.213859586602
Iteration 440: Loss = -11281.196906854628
Iteration 450: Loss = -11281.181565416393
Iteration 460: Loss = -11281.167808277076
Iteration 470: Loss = -11281.155577961694
Iteration 480: Loss = -11281.144791793808
Iteration 490: Loss = -11281.135348184947
Iteration 500: Loss = -11281.127133225807
Iteration 510: Loss = -11281.12002694037
Iteration 520: Loss = -11281.113908722931
Iteration 530: Loss = -11281.10866167611
Iteration 540: Loss = -11281.104175759625
Iteration 550: Loss = -11281.100349811473
Iteration 560: Loss = -11281.097092601784
Iteration 570: Loss = -11281.0943231256
Iteration 580: Loss = -11281.091970345886
Iteration 590: Loss = -11281.089972576181
Iteration 600: Loss = -11281.088276657132
Iteration 610: Loss = -11281.086837042852
Iteration 620: Loss = -11281.085614877778
Iteration 630: Loss = -11281.084577115515
Iteration 640: Loss = -11281.083695709098
Iteration 650: Loss = -11281.08294688625
Iteration 660: Loss = -11281.08231051288
Iteration 670: Loss = -11281.081769541552
Iteration 680: Loss = -11281.08130953826
Iteration 690: Loss = -11281.080918279107
Iteration 700: Loss = -11281.080585408154
Iteration 710: Loss = -11281.080302148104
Iteration 720: Loss = -11281.080061056033
Iteration 730: Loss = -11281.079855817394
Iteration 740: Loss = -11281.07968107236
Iteration 750: Loss = -11281.079532269354
Iteration 760: Loss = -11281.079405541494
Iteration 770: Loss = -11281.079297602317
Iteration 780: Loss = -11281.079205657634
Iteration 790: Loss = -11281.079127331042
Iteration 800: Loss = -11281.079060600889
Iteration 810: Loss = -11281.07900374688
Iteration 820: Loss = -11281.078955304822
Iteration 830: Loss = -11281.078914028269
Iteration 840: Loss = -11281.07887885597
Iteration 850: Loss = -11281.078848884199
Iteration 860: Loss = -11281.078823343303
Iteration 870: Loss = -11281.078801577716
Iteration 880: Loss = -11281.078783029017
Iteration 890: Loss = -11281.07876722148
Iteration 900: Loss = -11281.078753749809
Iteration 910: Loss = -11281.078742268692
Iteration 920: Loss = -11281.078732483891
Iteration 930: Loss = -11281.07872414471
Iteration 940: Loss = -11281.07871703751
Iteration 950: Loss = -11281.078710980242
Iteration 960: Loss = -11281.07870581778
Iteration 970: Loss = -11281.078701417908
Iteration 980: Loss = -11281.078697667968
Iteration 990: Loss = -11281.07869447194
Iteration 1000: Loss = -11281.078691747996
Iteration 1010: Loss = -11281.078689426402
Iteration 1020: Loss = -11281.07868744772
Iteration 1030: Loss = -11281.078685761297
Iteration 1040: Loss = -11281.07868432396
Iteration 1050: Loss = -11281.078683098925
Iteration 1060: Loss = -11281.078682054824
Iteration 1070: Loss = -11281.078681164941
Iteration 1080: Loss = -11281.078680406492
Iteration 1090: Loss = -11281.078679760063
Iteration 1100: Loss = -11281.07867920911
Iteration 1110: Loss = -11281.078678739532
Iteration 1120: Loss = -11281.078678339309
Iteration 1130: Loss = -11281.078677998197
Iteration 1140: Loss = -11281.07867770747
Iteration 1150: Loss = -11281.078677459678
Iteration 1160: Loss = -11281.078677248484
Iteration 1170: Loss = -11281.078677068483
Iteration 1180: Loss = -11281.078676915069
Iteration 1190: Loss = -11281.078676784311
Iteration 1200: Loss = -11281.078676672865
Iteration 1210: Loss = -11281.078676577881
Iteration 1220: Loss = -11281.078676496925
Iteration 1230: Loss = -11281.07867642793
Iteration 1240: Loss = -11281.078676369121
Iteration 1250: Loss = -11281.078676318997
Iteration 1260: Loss = -11281.07867627628
Iteration 1270: Loss = -11281.078676239867
Iteration 1280: Loss = -11281.078676208834
Iteration 1290: Loss = -11281.078676182382
Iteration 1300: Loss = -11281.078676159841
Iteration 1310: Loss = -11281.078676140627
Iteration 1320: Loss = -11281.078676124254
Iteration 1330: Loss = -11281.078676110294
Iteration 1340: Loss = -11281.0786760984
Iteration 1350: Loss = -11281.078676088262
Iteration 1360: Loss = -11281.078676079616
Iteration 1370: Loss = -11281.078676072255
Iteration 1380: Loss = -11281.078676065978
Iteration 1390: Loss = -11281.078676060628
Iteration 1400: Loss = -11281.078676056068
Iteration 1410: Loss = -11281.07867605218
Iteration 1420: Loss = -11281.07867604887
Iteration 1430: Loss = -11281.078676046047
Iteration 1440: Loss = -11281.078676043639
Iteration 1450: Loss = -11281.078676041587
Iteration 1460: Loss = -11281.07867603984
Iteration 1470: Loss = -11281.07867603835
Iteration 1480: Loss = -11281.07867603708
Iteration 1490: Loss = -11281.078676035999
Iteration 1500: Loss = -11281.078676035075
Iteration 1510: Loss = -11281.07867603429
Iteration 1520: Loss = -11281.07867603362
Iteration 1530: Loss = -11281.07867603305
Iteration 1540: Loss = -11281.078676032563
Iteration 1550: Loss = -11281.078676032146
Iteration 1560: Loss = -11281.078676031791
Iteration 1570: Loss = -11281.078676031491
Iteration 1580: Loss = -11281.078676031235
Iteration 1590: Loss = -11281.078676031018
Iteration 1600: Loss = -11281.07867603083
Iteration 1610: Loss = -11281.078676030671
Iteration 1620: Loss = -11281.078676030535
Iteration 1630: Loss = -11281.078676030422
Iteration 1640: Loss = -11281.078676030324
Iteration 1650: Loss = -11281.078676030238
Iteration 1660: Loss = -11281.078676030165
Iteration 1670: Loss = -11281.078676030105
Iteration 1680: Loss = -11281.078676030054
Iteration 1690: Loss = -11281.07867603001
Iteration 1700: Loss = -11281.078676029974
Iteration 1710: Loss = -11281.078676029938
Iteration 1720: Loss = -11281.07867602991
Iteration 1730: Loss = -11281.078676029889
Iteration 1740: Loss = -11281.078676029869
Iteration 1750: Loss = -11281.07867602985
Iteration 1760: Loss = -11281.078676029834
Iteration 1770: Loss = -11281.078676029825
Iteration 1780: Loss = -11281.078676029814
Iteration 1790: Loss = -11281.078676029805
Iteration 1800: Loss = -11281.078676029798
Iteration 1810: Loss = -11281.07867602979
Iteration 1820: Loss = -11281.078676029783
Iteration 1830: Loss = -11281.07867602978
Iteration 1840: Loss = -11281.078676029776
Iteration 1850: Loss = -11281.078676029774
Iteration 1860: Loss = -11281.07867602977
Iteration 1870: Loss = -11281.078676029769
Iteration 1880: Loss = -11281.078676029765
Iteration 1890: Loss = -11281.078676029761
Iteration 1900: Loss = -11281.078676029761
1
Iteration 1910: Loss = -11281.078676029763
2
Iteration 1920: Loss = -11281.07867602976
Iteration 1930: Loss = -11281.07867602976
1
Iteration 1940: Loss = -11281.078676029756
Iteration 1950: Loss = -11281.07867602976
1
Iteration 1960: Loss = -11281.078676029758
2
Iteration 1970: Loss = -11281.078676029754
Iteration 1980: Loss = -11281.078676029756
1
Iteration 1990: Loss = -11281.078676029756
2
Iteration 2000: Loss = -11281.078676029756
3
Stopping early at iteration 1999 due to no improvement.
pi: tensor([[0.9084, 0.0916],
        [0.8003, 0.1997]], dtype=torch.float64)
alpha: tensor([0.8980, 0.1020], dtype=torch.float64)
beta: tensor([[[0.1617, 0.1921],
         [0.2037, 0.2475]],

        [[0.1840, 0.2176],
         [0.0772, 0.4419]],

        [[0.5838, 0.1743],
         [0.7893, 0.4723]],

        [[0.4901, 0.1896],
         [0.1488, 0.0971]],

        [[0.4746, 0.1876],
         [0.0892, 0.3015]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.00476355833355593
Average Adjusted Rand Index: 0.0025140686831474227
ours 6 times ------------------------------------------------------------------------------
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17360.443254022346
Iteration 100: Loss = -11347.041692783325
Iteration 200: Loss = -11298.471260339958
Iteration 300: Loss = -11288.35109563617
Iteration 400: Loss = -11285.571331576117
Iteration 500: Loss = -11284.216578324018
Iteration 600: Loss = -11283.426397213985
Iteration 700: Loss = -11282.904975678226
Iteration 800: Loss = -11282.515599411501
Iteration 900: Loss = -11282.186628199659
Iteration 1000: Loss = -11281.890528250167
Iteration 1100: Loss = -11281.6239159537
Iteration 1200: Loss = -11281.396172848683
Iteration 1300: Loss = -11281.224665843205
Iteration 1400: Loss = -11281.112973465419
Iteration 1500: Loss = -11281.044047662444
Iteration 1600: Loss = -11281.0014088484
Iteration 1700: Loss = -11280.971454967621
Iteration 1800: Loss = -11280.948574810793
Iteration 1900: Loss = -11280.930583630276
Iteration 2000: Loss = -11280.916471931334
Iteration 2100: Loss = -11280.905340800506
Iteration 2200: Loss = -11280.896304598506
Iteration 2300: Loss = -11280.88852341183
Iteration 2400: Loss = -11280.880393985171
Iteration 2500: Loss = -11280.8715133628
Iteration 2600: Loss = -11280.866006765193
Iteration 2700: Loss = -11280.861901836124
Iteration 2800: Loss = -11280.858376164011
Iteration 2900: Loss = -11280.855057410072
Iteration 3000: Loss = -11280.851691836715
Iteration 3100: Loss = -11280.848459985818
Iteration 3200: Loss = -11280.845557120416
Iteration 3300: Loss = -11280.842780132918
Iteration 3400: Loss = -11280.840039092694
Iteration 3500: Loss = -11280.837543219235
Iteration 3600: Loss = -11280.835438401562
Iteration 3700: Loss = -11280.833653964017
Iteration 3800: Loss = -11280.832085600836
Iteration 3900: Loss = -11280.830664573394
Iteration 4000: Loss = -11280.829350014728
Iteration 4100: Loss = -11280.828114961285
Iteration 4200: Loss = -11280.826935930732
Iteration 4300: Loss = -11280.825779663037
Iteration 4400: Loss = -11280.824558840273
Iteration 4500: Loss = -11280.822770186063
Iteration 4600: Loss = -11280.80602025235
Iteration 4700: Loss = -11280.79682139106
Iteration 4800: Loss = -11280.795488988022
Iteration 4900: Loss = -11280.794511641043
Iteration 5000: Loss = -11280.793652006718
Iteration 5100: Loss = -11280.792858311375
Iteration 5200: Loss = -11280.792113284186
Iteration 5300: Loss = -11280.791409223471
Iteration 5400: Loss = -11280.790741864543
Iteration 5500: Loss = -11280.790108502886
Iteration 5600: Loss = -11280.789507254005
Iteration 5700: Loss = -11280.78893708725
Iteration 5800: Loss = -11280.788395571473
Iteration 5900: Loss = -11280.787883010293
Iteration 6000: Loss = -11280.787398223112
Iteration 6100: Loss = -11280.78694057979
Iteration 6200: Loss = -11280.787366594328
1
Iteration 6300: Loss = -11280.786396206306
Iteration 6400: Loss = -11280.785726129794
Iteration 6500: Loss = -11280.785368603492
Iteration 6600: Loss = -11280.785035783902
Iteration 6700: Loss = -11280.784785916154
Iteration 6800: Loss = -11280.784439216428
Iteration 6900: Loss = -11280.784172904918
Iteration 7000: Loss = -11280.78392938366
Iteration 7100: Loss = -11280.78369608597
Iteration 7200: Loss = -11280.78348354408
Iteration 7300: Loss = -11280.783416216274
Iteration 7400: Loss = -11280.783101972482
Iteration 7500: Loss = -11280.782930548263
Iteration 7600: Loss = -11280.787847467076
1
Iteration 7700: Loss = -11280.782619519985
Iteration 7800: Loss = -11280.782477900028
Iteration 7900: Loss = -11280.782344241472
Iteration 8000: Loss = -11280.782270794993
Iteration 8100: Loss = -11280.782097568823
Iteration 8200: Loss = -11280.78198335707
Iteration 8300: Loss = -11280.782107473498
1
Iteration 8400: Loss = -11280.781770652524
Iteration 8500: Loss = -11280.781671087745
Iteration 8600: Loss = -11280.781575852523
Iteration 8700: Loss = -11280.79409606205
1
Iteration 8800: Loss = -11280.781403906785
Iteration 8900: Loss = -11280.781312262521
Iteration 9000: Loss = -11280.78123061605
Iteration 9100: Loss = -11280.782680414386
1
Iteration 9200: Loss = -11280.781074590004
Iteration 9300: Loss = -11280.780995535059
Iteration 9400: Loss = -11280.780913516612
Iteration 9500: Loss = -11280.782348077968
1
Iteration 9600: Loss = -11280.780627938993
Iteration 9700: Loss = -11280.779997475547
Iteration 9800: Loss = -11280.77884995834
Iteration 9900: Loss = -11280.853558837478
1
pi: tensor([[9.4215e-01, 5.7846e-02],
        [9.9959e-01, 4.1416e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9891, 0.0109], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1658, 0.2100],
         [0.2037, 0.3475]],

        [[0.1840, 0.2346],
         [0.0772, 0.4419]],

        [[0.5838, 0.1687],
         [0.7893, 0.4723]],

        [[0.4901, 0.2067],
         [0.1488, 0.0971]],

        [[0.4746, 0.2018],
         [0.0892, 0.3015]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.00476355833355593
Average Adjusted Rand Index: 0.0025140686831474227
prior 7 times ------------------------------------------------------------------------------
Iteration 0: Loss = -41449.11996196245
Iteration 10: Loss = -11284.23140491415
Iteration 20: Loss = -11284.23140491415
1
Iteration 30: Loss = -11284.23140491415
2
Iteration 40: Loss = -11284.231404914144
Iteration 50: Loss = -11284.231404914124
Iteration 60: Loss = -11284.231404911352
Iteration 70: Loss = -11284.231404478269
Iteration 80: Loss = -11284.231011413623
Iteration 90: Loss = -11283.72380262305
Iteration 100: Loss = -11281.326519590897
Iteration 110: Loss = -11281.293785935335
Iteration 120: Loss = -11281.303066852397
1
Iteration 130: Loss = -11281.306245624563
2
Iteration 140: Loss = -11281.307141491516
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[9.7159e-01, 2.8413e-02],
        [1.0000e+00, 1.0788e-19]], dtype=torch.float64)
alpha: tensor([0.9728, 0.0272], dtype=torch.float64)
beta: tensor([[[0.1685, 0.1158],
         [0.2132, 0.2132]],

        [[0.7460, 0.2521],
         [0.1557, 0.9260]],

        [[0.4283, 0.1538],
         [0.0582, 0.3219]],

        [[0.3987, 0.0860],
         [0.0810, 0.0739]],

        [[0.7849, 0.1761],
         [0.1649, 0.0526]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005986414753463625
Average Adjusted Rand Index: -0.0018793625138016704
ours 7 times ------------------------------------------------------------------------------
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41449.11996196245
Iteration 100: Loss = -11637.467565623554
Iteration 200: Loss = -11451.031428976758
Iteration 300: Loss = -11396.949010339404
Iteration 400: Loss = -11365.820623805332
Iteration 500: Loss = -11342.32488822419
Iteration 600: Loss = -11322.971599355176
Iteration 700: Loss = -11309.634414718892
Iteration 800: Loss = -11305.087006358737
Iteration 900: Loss = -11294.855545470518
Iteration 1000: Loss = -11291.763362894857
Iteration 1100: Loss = -11290.414852516811
Iteration 1200: Loss = -11289.441522306992
Iteration 1300: Loss = -11288.69277589593
Iteration 1400: Loss = -11288.095653811139
Iteration 1500: Loss = -11287.607940512375
Iteration 1600: Loss = -11287.202434747265
Iteration 1700: Loss = -11286.860462621768
Iteration 1800: Loss = -11286.568675536633
Iteration 1900: Loss = -11286.317225965106
Iteration 2000: Loss = -11286.098663771063
Iteration 2100: Loss = -11285.907231978987
Iteration 2200: Loss = -11285.738400206958
Iteration 2300: Loss = -11285.588546550349
Iteration 2400: Loss = -11285.454734945139
Iteration 2500: Loss = -11285.334552584713
Iteration 2600: Loss = -11285.225977603392
Iteration 2700: Loss = -11285.127257974376
Iteration 2800: Loss = -11285.036809345529
Iteration 2900: Loss = -11284.9531426679
Iteration 3000: Loss = -11284.874814473298
Iteration 3100: Loss = -11284.800435487452
Iteration 3200: Loss = -11284.728730153285
Iteration 3300: Loss = -11284.658459086719
Iteration 3400: Loss = -11284.588155074496
Iteration 3500: Loss = -11284.515185078699
Iteration 3600: Loss = -11284.43136563709
Iteration 3700: Loss = -11284.307703128849
Iteration 3800: Loss = -11284.115305051586
Iteration 3900: Loss = -11283.925297326692
Iteration 4000: Loss = -11283.769272726968
Iteration 4100: Loss = -11283.657365113915
Iteration 4200: Loss = -11283.574940526029
Iteration 4300: Loss = -11283.505180965758
Iteration 4400: Loss = -11283.424232079356
Iteration 4500: Loss = -11283.305573550982
Iteration 4600: Loss = -11283.240694615853
Iteration 4700: Loss = -11283.172952149363
Iteration 4800: Loss = -11283.092492200127
Iteration 4900: Loss = -11282.98894525226
Iteration 5000: Loss = -11282.93498352503
Iteration 5100: Loss = -11282.897952455838
Iteration 5200: Loss = -11282.867032786082
Iteration 5300: Loss = -11282.840419387228
Iteration 5400: Loss = -11282.81755565813
Iteration 5500: Loss = -11282.79787145892
Iteration 5600: Loss = -11282.780723783593
Iteration 5700: Loss = -11282.765105458104
Iteration 5800: Loss = -11282.74939817664
Iteration 5900: Loss = -11282.731864796038
Iteration 6000: Loss = -11282.68573038959
Iteration 6100: Loss = -11282.663405285006
Iteration 6200: Loss = -11282.652120507486
Iteration 6300: Loss = -11282.641619378548
Iteration 6400: Loss = -11282.630419724052
Iteration 6500: Loss = -11282.615521003241
Iteration 6600: Loss = -11282.60027981607
Iteration 6700: Loss = -11282.591616311622
Iteration 6800: Loss = -11282.585023593268
Iteration 6900: Loss = -11282.579179843044
Iteration 7000: Loss = -11282.573750200307
Iteration 7100: Loss = -11282.568482233863
Iteration 7200: Loss = -11282.562754369379
Iteration 7300: Loss = -11282.553827336757
Iteration 7400: Loss = -11282.508662579448
Iteration 7500: Loss = -11282.499465754785
Iteration 7600: Loss = -11282.493722202902
Iteration 7700: Loss = -11282.489267225108
Iteration 7800: Loss = -11282.488920899666
Iteration 7900: Loss = -11282.482216398023
Iteration 8000: Loss = -11282.47921617745
Iteration 8100: Loss = -11282.476449107216
Iteration 8200: Loss = -11282.473909429138
Iteration 8300: Loss = -11282.471461286601
Iteration 8400: Loss = -11282.469391880853
Iteration 8500: Loss = -11282.467059978146
Iteration 8600: Loss = -11282.465072398576
Iteration 8700: Loss = -11282.463136188628
Iteration 8800: Loss = -11282.46146243048
Iteration 8900: Loss = -11282.459617876848
Iteration 9000: Loss = -11282.474710560593
1
Iteration 9100: Loss = -11282.456424744714
Iteration 9200: Loss = -11282.454914804977
Iteration 9300: Loss = -11282.551798797705
1
Iteration 9400: Loss = -11282.451854896906
Iteration 9500: Loss = -11282.449740914304
Iteration 9600: Loss = -11282.408489337071
Iteration 9700: Loss = -11280.596791991407
Iteration 9800: Loss = -11280.11521302614
Iteration 9900: Loss = -11268.273613479205
pi: tensor([[9.9998e-01, 1.6509e-05],
        [6.3246e-01, 3.6754e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5728, 0.4272], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1680, 0.1075],
         [0.2132, 0.3077]],

        [[0.7460, 0.1857],
         [0.1557, 0.9260]],

        [[0.4283, 0.1938],
         [0.0582, 0.3219]],

        [[0.3987, 0.2391],
         [0.0810, 0.0739]],

        [[0.7849, 0.3686],
         [0.1649, 0.0526]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208065164923572
time is 1
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 71
Adjusted Rand Index: 0.17092306753706302
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.12117636797515446
Average Adjusted Rand Index: 0.22075764000449252
True loss:  11051.37456775153
new:  [-0.00018949043915461998, 0.00476355833355593, 0.9291510399040422, 0.00476355833355593, 0.002064007744755877, 0.9061133849594307, 0.00476355833355593, 0.12117636797515446] [0.6619858906732845, 0.0025140686831474227, 0.9288061878399743, 0.0025140686831474227, -0.00041751076873370213, 0.9056160535582176, 0.0025140686831474227, 0.22075764000449252] [11139.594394811798, 11280.883680171146, 11027.519800174096, 11280.776321369496, 11280.948443218316, 11035.65476448085, 11280.778862503847, 11260.339478453627]
prior:  [0.00476355833355593, 0.0, 0.00476355833355593, 0.0, 0.0002569651861066151, 0.00476355833355593, 0.00476355833355593, -0.0005986414753463625] [0.0025140686831474227, 0.0, 0.0025140686831474227, 0.0, -0.0015541722481668405, 0.0025140686831474227, 0.0025140686831474227, -0.0018793625138016704] [11281.060510423004, 11284.231404914148, 11281.057398150031, 11284.23140491415, 11281.407840042488, 11281.058843456767, 11281.078676029756, 11281.307141491516]
Traceback (most recent call last):
  File "/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/main.py", line 347, in <module>
    print("-----------------------------------------------------------------------------------------")
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/main.py", line 276, in main
    kmeans_global_ARI, kmeans_average_ARI,
AttributeError: 'float' object has no attribute 'item'
