nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [19:36<32:21:10, 1176.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [53:02<45:18:15, 1664.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [1:18:00<42:47:50, 1588.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [1:45:27<42:58:41, 1611.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [2:09:19<40:48:58, 1546.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [2:34:47<40:13:16, 1540.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [2:54:43<36:53:26, 1428.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [3:17:31<35:59:58, 1408.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [3:40:23<35:19:10, 1397.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [4:06:12<36:06:13, 1444.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [4:27:45<34:33:39, 1397.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [4:50:13<33:47:56, 1382.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [5:11:54<32:48:49, 1357.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [5:37:56<33:54:46, 1419.61s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [6:10:45<37:25:38, 1585.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [6:38:31<37:33:27, 1609.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [7:00:33<35:07:03, 1523.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [7:28:11<35:37:03, 1563.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [7:49:28<33:14:39, 1477.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [8:18:55<34:45:53, 1564.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [8:38:56<31:55:59, 1455.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [9:03:24<31:36:56, 1459.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-----------------------------------------------------------------------------------------
This iteration is 0
True Objective function: Loss = -11192.13682012116
Iteration 0: Loss = -11543.354441982734
Iteration 10: Loss = -11514.823666269489
Iteration 20: Loss = -11509.17431009428
Iteration 30: Loss = -11508.8527704518
Iteration 40: Loss = -11506.263042098504
Iteration 50: Loss = -11189.992991586978
Iteration 60: Loss = -11177.54232167457
Iteration 70: Loss = -11177.529734639684
Iteration 80: Loss = -11177.528040175734
Iteration 90: Loss = -11177.527485781484
Iteration 100: Loss = -11177.527319544852
Iteration 110: Loss = -11177.527243417133
Iteration 120: Loss = -11177.52724166445
Iteration 130: Loss = -11177.52723098615
Iteration 140: Loss = -11177.527233036839
1
Iteration 150: Loss = -11177.527222299936
Iteration 160: Loss = -11177.527229472538
1
Iteration 170: Loss = -11177.527224494132
2
Iteration 180: Loss = -11177.527222769239
3
Stopping early at iteration 180 due to no improvement.
pi: tensor([[0.7430, 0.2570],
        [0.1923, 0.8077]], dtype=torch.float64)
alpha: tensor([0.4479, 0.5521])
beta: tensor([[[0.1900, 0.0970],
         [0.7684, 0.2878]],

        [[0.1814, 0.1057],
         [0.6132, 0.8539]],

        [[0.4645, 0.1029],
         [0.7466, 0.0021]],

        [[0.9508, 0.0944],
         [0.2581, 0.2715]],

        [[0.9069, 0.0897],
         [0.2759, 0.6445]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8822858823962049
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9291510399040422
Average Adjusted Rand Index: 0.9289391779678662
Iteration 0: Loss = -11628.023233525208
Iteration 10: Loss = -11412.406031193914
Iteration 20: Loss = -11177.487844501977
Iteration 30: Loss = -11177.524122389474
1
Iteration 40: Loss = -11177.52659370554
2
Iteration 50: Loss = -11177.527015157377
3
Stopping early at iteration 50 due to no improvement.
pi: tensor([[0.8077, 0.1923],
        [0.2570, 0.7430]], dtype=torch.float64)
alpha: tensor([0.5521, 0.4479])
beta: tensor([[[0.2878, 0.0970],
         [0.6396, 0.1900]],

        [[0.1795, 0.1057],
         [0.5813, 0.6782]],

        [[0.8610, 0.1029],
         [0.9971, 0.3503]],

        [[0.8749, 0.0944],
         [0.3186, 0.8992]],

        [[0.0598, 0.0897],
         [0.1638, 0.3977]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8822858823962049
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9291510399040422
Average Adjusted Rand Index: 0.9289391779678662
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23723.816096822622
Iteration 100: Loss = -11174.164107288292
Iteration 200: Loss = -11172.542179944916
Iteration 300: Loss = -11172.434264347263
Iteration 400: Loss = -11172.392090809682
Iteration 500: Loss = -11172.37017118542
Iteration 600: Loss = -11172.356967769652
Iteration 700: Loss = -11172.347207647175
Iteration 800: Loss = -11172.340045466754
Iteration 900: Loss = -11172.349698136197
1
Iteration 1000: Loss = -11172.333015822525
Iteration 1100: Loss = -11172.33076546831
Iteration 1200: Loss = -11172.329447683547
Iteration 1300: Loss = -11172.329137747662
Iteration 1400: Loss = -11172.327736458325
Iteration 1500: Loss = -11172.328843120633
1
Iteration 1600: Loss = -11172.325034331148
Iteration 1700: Loss = -11172.324393306104
Iteration 1800: Loss = -11172.32388642801
Iteration 1900: Loss = -11172.323623610988
Iteration 2000: Loss = -11172.328538703068
1
Iteration 2100: Loss = -11172.323030554113
Iteration 2200: Loss = -11172.322836864649
Iteration 2300: Loss = -11172.323360476721
1
Iteration 2400: Loss = -11172.323729707674
2
Iteration 2500: Loss = -11172.323174345318
3
Iteration 2600: Loss = -11172.321681898075
Iteration 2700: Loss = -11172.321640249138
Iteration 2800: Loss = -11172.321314651537
Iteration 2900: Loss = -11172.321579738657
1
Iteration 3000: Loss = -11172.32105118734
Iteration 3100: Loss = -11172.320988563632
Iteration 3200: Loss = -11172.320912626688
Iteration 3300: Loss = -11172.320837095422
Iteration 3400: Loss = -11172.321067333773
1
Iteration 3500: Loss = -11172.32066842921
Iteration 3600: Loss = -11172.320637419874
Iteration 3700: Loss = -11172.320553972151
Iteration 3800: Loss = -11172.320503742747
Iteration 3900: Loss = -11172.322830853032
1
Iteration 4000: Loss = -11172.320421717219
Iteration 4100: Loss = -11172.321852001862
1
Iteration 4200: Loss = -11172.323439972564
2
Iteration 4300: Loss = -11172.324590867833
3
Iteration 4400: Loss = -11172.320273888992
Iteration 4500: Loss = -11172.32329755631
1
Iteration 4600: Loss = -11172.320189423855
Iteration 4700: Loss = -11172.321771567475
1
Iteration 4800: Loss = -11172.320165214935
Iteration 4900: Loss = -11172.322761336127
1
Iteration 5000: Loss = -11172.32010190586
Iteration 5100: Loss = -11172.325437579348
1
Iteration 5200: Loss = -11172.32009340526
Iteration 5300: Loss = -11172.322092057038
1
Iteration 5400: Loss = -11172.322238594516
2
Iteration 5500: Loss = -11172.320510645359
3
Iteration 5600: Loss = -11172.31999053141
Iteration 5700: Loss = -11172.32030406486
1
Iteration 5800: Loss = -11172.320009932722
2
Iteration 5900: Loss = -11172.320038961885
3
Iteration 6000: Loss = -11172.321594689776
4
Iteration 6100: Loss = -11172.319933556337
Iteration 6200: Loss = -11172.320097456173
1
Iteration 6300: Loss = -11172.320184633758
2
Iteration 6400: Loss = -11172.32143288754
3
Iteration 6500: Loss = -11172.31990009601
Iteration 6600: Loss = -11172.320041830819
1
Iteration 6700: Loss = -11172.320430624504
2
Iteration 6800: Loss = -11172.321367556493
3
Iteration 6900: Loss = -11172.319830678662
Iteration 7000: Loss = -11172.319841088429
1
Iteration 7100: Loss = -11172.32840759625
2
Iteration 7200: Loss = -11172.32193969032
3
Iteration 7300: Loss = -11172.323704789713
4
Iteration 7400: Loss = -11172.320555080609
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.8199, 0.1801],
        [0.2448, 0.7552]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5021, 0.4979], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2948, 0.0975],
         [0.5079, 0.1936]],

        [[0.5991, 0.1048],
         [0.7121, 0.6129]],

        [[0.6514, 0.1035],
         [0.6116, 0.5808]],

        [[0.6436, 0.0944],
         [0.5415, 0.6156]],

        [[0.5213, 0.0897],
         [0.6110, 0.6804]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9760953012611165
Average Adjusted Rand Index: 0.9761522019741415
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23782.133735707208
Iteration 100: Loss = -11522.441130735333
Iteration 200: Loss = -11491.010630678506
Iteration 300: Loss = -11389.884164436195
Iteration 400: Loss = -11370.36904677039
Iteration 500: Loss = -11269.447647382867
Iteration 600: Loss = -11209.145249017316
Iteration 700: Loss = -11190.470085791248
Iteration 800: Loss = -11190.294232333521
Iteration 900: Loss = -11186.75250269335
Iteration 1000: Loss = -11186.719574297293
Iteration 1100: Loss = -11180.483374186031
Iteration 1200: Loss = -11180.465515144277
Iteration 1300: Loss = -11172.462429504427
Iteration 1400: Loss = -11172.42254060672
Iteration 1500: Loss = -11172.415881141538
Iteration 1600: Loss = -11172.411216586466
Iteration 1700: Loss = -11172.407341446315
Iteration 1800: Loss = -11172.403777522657
Iteration 1900: Loss = -11172.4009990305
Iteration 2000: Loss = -11172.3989616954
Iteration 2100: Loss = -11172.397276575239
Iteration 2200: Loss = -11172.395845051977
Iteration 2300: Loss = -11172.394630034945
Iteration 2400: Loss = -11172.393539620452
Iteration 2500: Loss = -11172.392640481488
Iteration 2600: Loss = -11172.391841931609
Iteration 2700: Loss = -11172.391129280752
Iteration 2800: Loss = -11172.390482616314
Iteration 2900: Loss = -11172.389906241264
Iteration 3000: Loss = -11172.38940931198
Iteration 3100: Loss = -11172.388910458767
Iteration 3200: Loss = -11172.388492400867
Iteration 3300: Loss = -11172.388092873145
Iteration 3400: Loss = -11172.387604685106
Iteration 3500: Loss = -11172.387230746172
Iteration 3600: Loss = -11172.386824602738
Iteration 3700: Loss = -11172.386389147114
Iteration 3800: Loss = -11172.385357398718
Iteration 3900: Loss = -11172.379016926687
Iteration 4000: Loss = -11172.378575556026
Iteration 4100: Loss = -11172.378349165594
Iteration 4200: Loss = -11172.378232539555
Iteration 4300: Loss = -11172.377961945802
Iteration 4400: Loss = -11172.377885790345
Iteration 4500: Loss = -11172.377695665073
Iteration 4600: Loss = -11172.381073963405
1
Iteration 4700: Loss = -11172.378192244902
2
Iteration 4800: Loss = -11172.377246944552
Iteration 4900: Loss = -11172.377318224595
1
Iteration 5000: Loss = -11172.377038078042
Iteration 5100: Loss = -11172.37765655374
1
Iteration 5200: Loss = -11172.376223843694
Iteration 5300: Loss = -11172.381080953195
1
Iteration 5400: Loss = -11172.373963794247
Iteration 5500: Loss = -11172.391504847297
1
Iteration 5600: Loss = -11172.373527860547
Iteration 5700: Loss = -11172.373749472192
1
Iteration 5800: Loss = -11172.37333725098
Iteration 5900: Loss = -11172.374888077893
1
Iteration 6000: Loss = -11172.373209051502
Iteration 6100: Loss = -11172.373405447368
1
Iteration 6200: Loss = -11172.373094186309
Iteration 6300: Loss = -11172.373213487326
1
Iteration 6400: Loss = -11172.373040375418
Iteration 6500: Loss = -11172.373270622295
1
Iteration 6600: Loss = -11172.37295749437
Iteration 6700: Loss = -11172.37315430629
1
Iteration 6800: Loss = -11172.374164117404
2
Iteration 6900: Loss = -11172.372848505494
Iteration 7000: Loss = -11172.373159931023
1
Iteration 7100: Loss = -11172.372760121845
Iteration 7200: Loss = -11172.372736179848
Iteration 7300: Loss = -11172.375096299911
1
Iteration 7400: Loss = -11172.371824928221
Iteration 7500: Loss = -11172.371568661223
Iteration 7600: Loss = -11172.367890052055
Iteration 7700: Loss = -11172.33738780751
Iteration 7800: Loss = -11172.336818806803
Iteration 7900: Loss = -11172.339182920658
1
Iteration 8000: Loss = -11172.336722692422
Iteration 8100: Loss = -11172.337241893416
1
Iteration 8200: Loss = -11172.336987488454
2
Iteration 8300: Loss = -11172.336672394536
Iteration 8400: Loss = -11172.337263576877
1
Iteration 8500: Loss = -11172.336644839761
Iteration 8600: Loss = -11172.336782915634
1
Iteration 8700: Loss = -11172.336779448136
2
Iteration 8800: Loss = -11172.33776388309
3
Iteration 8900: Loss = -11172.33831434671
4
Iteration 9000: Loss = -11172.339383048657
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.8203, 0.1797],
        [0.2462, 0.7538]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5004, 0.4996], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2947, 0.0975],
         [0.7270, 0.1939]],

        [[0.6895, 0.1048],
         [0.5594, 0.6512]],

        [[0.6596, 0.1035],
         [0.5545, 0.6616]],

        [[0.6658, 0.0944],
         [0.7032, 0.6838]],

        [[0.6344, 0.0897],
         [0.5408, 0.6874]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9760953012611165
Average Adjusted Rand Index: 0.9761522019741415
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22329.658392268484
Iteration 100: Loss = -11531.674900979258
Iteration 200: Loss = -11531.152905805568
Iteration 300: Loss = -11531.000888177241
Iteration 400: Loss = -11530.791362474143
Iteration 500: Loss = -11528.346772478148
Iteration 600: Loss = -11509.18519101471
Iteration 700: Loss = -11421.46401883869
Iteration 800: Loss = -11369.18848549498
Iteration 900: Loss = -11246.213049019436
Iteration 1000: Loss = -11191.160502840023
Iteration 1100: Loss = -11172.708986770449
Iteration 1200: Loss = -11172.625643327769
Iteration 1300: Loss = -11172.598477112404
Iteration 1400: Loss = -11172.581366809593
Iteration 1500: Loss = -11172.565765610849
Iteration 1600: Loss = -11172.522429371773
Iteration 1700: Loss = -11172.465755387155
Iteration 1800: Loss = -11172.440433239015
Iteration 1900: Loss = -11172.435614369744
Iteration 2000: Loss = -11172.432186387676
Iteration 2100: Loss = -11172.429851120733
Iteration 2200: Loss = -11172.427940766642
Iteration 2300: Loss = -11172.426262171637
Iteration 2400: Loss = -11172.424826123222
Iteration 2500: Loss = -11172.424355686833
Iteration 2600: Loss = -11172.422528698657
Iteration 2700: Loss = -11172.42152351496
Iteration 2800: Loss = -11172.420701373763
Iteration 2900: Loss = -11172.419982029525
Iteration 3000: Loss = -11172.419251991854
Iteration 3100: Loss = -11172.418678782706
Iteration 3200: Loss = -11172.418157178405
Iteration 3300: Loss = -11172.41760703685
Iteration 3400: Loss = -11172.417155243162
Iteration 3500: Loss = -11172.417058496547
Iteration 3600: Loss = -11172.416169325696
Iteration 3700: Loss = -11172.415459281447
Iteration 3800: Loss = -11172.41411811088
Iteration 3900: Loss = -11172.413485273839
Iteration 4000: Loss = -11172.415882177866
1
Iteration 4100: Loss = -11172.417889362385
2
Iteration 4200: Loss = -11172.411953665574
Iteration 4300: Loss = -11172.41132121416
Iteration 4400: Loss = -11172.42325836405
1
Iteration 4500: Loss = -11172.41076175292
Iteration 4600: Loss = -11172.410578796309
Iteration 4700: Loss = -11172.410341108345
Iteration 4800: Loss = -11172.410133568585
Iteration 4900: Loss = -11172.411015261856
1
Iteration 5000: Loss = -11172.409667149379
Iteration 5100: Loss = -11172.41038140912
1
Iteration 5200: Loss = -11172.4091220703
Iteration 5300: Loss = -11172.408471710223
Iteration 5400: Loss = -11172.408310629491
Iteration 5500: Loss = -11172.407146652246
Iteration 5600: Loss = -11172.404409617096
Iteration 5700: Loss = -11172.408253723037
1
Iteration 5800: Loss = -11172.404216709218
Iteration 5900: Loss = -11172.406827671542
1
Iteration 6000: Loss = -11172.40398381852
Iteration 6100: Loss = -11172.404069486807
1
Iteration 6200: Loss = -11172.403704934306
Iteration 6300: Loss = -11172.403600325044
Iteration 6400: Loss = -11172.403366018567
Iteration 6500: Loss = -11172.403308665891
Iteration 6600: Loss = -11172.403124158514
Iteration 6700: Loss = -11172.403202526573
1
Iteration 6800: Loss = -11172.40316792631
2
Iteration 6900: Loss = -11172.40984050848
3
Iteration 7000: Loss = -11172.403001052082
Iteration 7100: Loss = -11172.403299037269
1
Iteration 7200: Loss = -11172.402898406606
Iteration 7300: Loss = -11172.403006676059
1
Iteration 7400: Loss = -11172.40283740357
Iteration 7500: Loss = -11172.409962829144
1
Iteration 7600: Loss = -11172.402783889449
Iteration 7700: Loss = -11172.402935191845
1
Iteration 7800: Loss = -11172.40260689977
Iteration 7900: Loss = -11172.400841517889
Iteration 8000: Loss = -11172.40076720562
Iteration 8100: Loss = -11172.400489122105
Iteration 8200: Loss = -11172.400470181787
Iteration 8300: Loss = -11172.40175985573
1
Iteration 8400: Loss = -11172.400450026535
Iteration 8500: Loss = -11172.400409035603
Iteration 8600: Loss = -11172.402007728047
1
Iteration 8700: Loss = -11172.40040057698
Iteration 8800: Loss = -11172.400392209192
Iteration 8900: Loss = -11172.400364566516
Iteration 9000: Loss = -11172.402418183425
1
Iteration 9100: Loss = -11172.381195040665
Iteration 9200: Loss = -11172.334227739815
Iteration 9300: Loss = -11172.438214952399
1
Iteration 9400: Loss = -11172.330103172251
Iteration 9500: Loss = -11172.33172736738
1
Iteration 9600: Loss = -11172.330022416394
Iteration 9700: Loss = -11172.330177239113
1
Iteration 9800: Loss = -11172.330022194865
Iteration 9900: Loss = -11172.3301412439
1
Iteration 10000: Loss = -11172.32964136065
Iteration 10100: Loss = -11172.331183416456
1
Iteration 10200: Loss = -11172.339806228229
2
Iteration 10300: Loss = -11172.33039474391
3
Iteration 10400: Loss = -11172.330143837797
4
Iteration 10500: Loss = -11172.330326186422
5
Stopping early at iteration 10500 due to no improvement.
pi: tensor([[0.8194, 0.1806],
        [0.2456, 0.7544]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5014, 0.4986], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2948, 0.0975],
         [0.7069, 0.1935]],

        [[0.7218, 0.1049],
         [0.6899, 0.6364]],

        [[0.5028, 0.1036],
         [0.6291, 0.6928]],

        [[0.5981, 0.0942],
         [0.6838, 0.6287]],

        [[0.6818, 0.0897],
         [0.5089, 0.6816]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9760953012611165
Average Adjusted Rand Index: 0.9761522019741415
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23572.73477899939
Iteration 100: Loss = -11447.931009406064
Iteration 200: Loss = -11187.705339084974
Iteration 300: Loss = -11177.680169915327
Iteration 400: Loss = -11177.447644521924
Iteration 500: Loss = -11177.323573960777
Iteration 600: Loss = -11176.504599268146
Iteration 700: Loss = -11176.469160701086
Iteration 800: Loss = -11176.446632357838
Iteration 900: Loss = -11176.432286847275
Iteration 1000: Loss = -11176.421881593726
Iteration 1100: Loss = -11176.413957361186
Iteration 1200: Loss = -11176.407818834961
Iteration 1300: Loss = -11176.402905682939
Iteration 1400: Loss = -11176.398840522852
Iteration 1500: Loss = -11176.395146645898
Iteration 1600: Loss = -11176.390420781752
Iteration 1700: Loss = -11176.386946693265
Iteration 1800: Loss = -11176.385032643024
Iteration 1900: Loss = -11176.383433669333
Iteration 2000: Loss = -11176.382049972352
Iteration 2100: Loss = -11176.38082173702
Iteration 2200: Loss = -11176.379804315506
Iteration 2300: Loss = -11176.378867021687
Iteration 2400: Loss = -11176.378017901683
Iteration 2500: Loss = -11176.377299099213
Iteration 2600: Loss = -11176.376634779106
Iteration 2700: Loss = -11176.376032185311
Iteration 2800: Loss = -11176.375481295518
Iteration 2900: Loss = -11176.37481114889
Iteration 3000: Loss = -11176.37391197473
Iteration 3100: Loss = -11176.35660034411
Iteration 3200: Loss = -11176.29673057485
Iteration 3300: Loss = -11176.295749871448
Iteration 3400: Loss = -11175.109852732126
Iteration 3500: Loss = -11175.065162592684
Iteration 3600: Loss = -11175.064622741193
Iteration 3700: Loss = -11175.064344593247
Iteration 3800: Loss = -11175.064187825401
Iteration 3900: Loss = -11175.06416491324
Iteration 4000: Loss = -11175.063944514719
Iteration 4100: Loss = -11175.06358088498
Iteration 4200: Loss = -11175.063357606312
Iteration 4300: Loss = -11175.0638457505
1
Iteration 4400: Loss = -11175.063064596538
Iteration 4500: Loss = -11175.064008392004
1
Iteration 4600: Loss = -11175.062927725072
Iteration 4700: Loss = -11175.064348885027
1
Iteration 4800: Loss = -11175.062621597104
Iteration 4900: Loss = -11175.062633039288
1
Iteration 5000: Loss = -11175.062500596681
Iteration 5100: Loss = -11175.062389653614
Iteration 5200: Loss = -11175.063117518797
1
Iteration 5300: Loss = -11175.061946225196
Iteration 5400: Loss = -11175.061747169153
Iteration 5500: Loss = -11175.063258022943
1
Iteration 5600: Loss = -11175.066123512444
2
Iteration 5700: Loss = -11175.06172142521
Iteration 5800: Loss = -11175.061530236473
Iteration 5900: Loss = -11175.06158694798
1
Iteration 6000: Loss = -11175.06239196113
2
Iteration 6100: Loss = -11175.061363987035
Iteration 6200: Loss = -11175.061090312618
Iteration 6300: Loss = -11172.412732091607
Iteration 6400: Loss = -11172.410236323753
Iteration 6500: Loss = -11172.41987264797
1
Iteration 6600: Loss = -11172.41018575705
Iteration 6700: Loss = -11172.41499464216
1
Iteration 6800: Loss = -11172.410089500338
Iteration 6900: Loss = -11172.410102173682
1
Iteration 7000: Loss = -11172.410095651978
2
Iteration 7100: Loss = -11172.410722278048
3
Iteration 7200: Loss = -11172.410082306746
Iteration 7300: Loss = -11172.409963485494
Iteration 7400: Loss = -11172.411066646857
1
Iteration 7500: Loss = -11172.408270415252
Iteration 7600: Loss = -11172.407971669656
Iteration 7700: Loss = -11172.40821762974
1
Iteration 7800: Loss = -11172.405910978816
Iteration 7900: Loss = -11172.40580234828
Iteration 8000: Loss = -11172.407272508673
1
Iteration 8100: Loss = -11172.406810648605
2
Iteration 8200: Loss = -11172.408032707206
3
Iteration 8300: Loss = -11172.406424580302
4
Iteration 8400: Loss = -11172.408379403854
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[0.8187, 0.1813],
        [0.2471, 0.7529]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5037, 0.4963], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2948, 0.0975],
         [0.6614, 0.1936]],

        [[0.5682, 0.1048],
         [0.6697, 0.5493]],

        [[0.6910, 0.1035],
         [0.6835, 0.6351]],

        [[0.5838, 0.0944],
         [0.7137, 0.6481]],

        [[0.5537, 0.0897],
         [0.5892, 0.6059]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9760953012611165
Average Adjusted Rand Index: 0.9761522019741415
11192.13682012116
[0.9760953012611165, 0.9760953012611165, 0.9760953012611165, 0.9760953012611165] [0.9761522019741415, 0.9761522019741415, 0.9761522019741415, 0.9761522019741415] [11172.320555080609, 11172.339383048657, 11172.330326186422, 11172.408379403854]
-----------------------------------------------------------------------------------------
This iteration is 1
True Objective function: Loss = -11127.029110912514
Iteration 0: Loss = -11561.946341999124
Iteration 10: Loss = -11324.843086949111
Iteration 20: Loss = -11107.71977691308
Iteration 30: Loss = -11106.79073226652
Iteration 40: Loss = -11106.738990975608
Iteration 50: Loss = -11106.738057416238
Iteration 60: Loss = -11106.737998601595
Iteration 70: Loss = -11106.737994735326
Iteration 80: Loss = -11106.737971862056
Iteration 90: Loss = -11106.737982767272
1
Iteration 100: Loss = -11106.73798188407
2
Iteration 110: Loss = -11106.737983080926
3
Stopping early at iteration 110 due to no improvement.
pi: tensor([[0.7465, 0.2535],
        [0.2416, 0.7584]], dtype=torch.float64)
alpha: tensor([0.5172, 0.4828])
beta: tensor([[[0.1964, 0.0950],
         [0.2564, 0.2971]],

        [[0.5161, 0.1050],
         [0.6084, 0.8188]],

        [[0.9032, 0.0926],
         [0.0690, 0.2033]],

        [[0.4322, 0.1019],
         [0.9832, 0.8326]],

        [[0.9371, 0.1009],
         [0.2899, 0.5372]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9368977352403174
Average Adjusted Rand Index: 0.9369686312299594
Iteration 0: Loss = -11417.97493274896
Iteration 10: Loss = -11415.507565673146
Iteration 20: Loss = -11405.954512844884
Iteration 30: Loss = -11403.814100038846
Iteration 40: Loss = -11360.672948256555
Iteration 50: Loss = -11108.852016615121
Iteration 60: Loss = -11106.857659985168
Iteration 70: Loss = -11106.739472501344
Iteration 80: Loss = -11106.737830120052
Iteration 90: Loss = -11106.737919591103
1
Iteration 100: Loss = -11106.737980897633
2
Iteration 110: Loss = -11106.737979637974
3
Stopping early at iteration 110 due to no improvement.
pi: tensor([[0.7584, 0.2416],
        [0.2535, 0.7465]], dtype=torch.float64)
alpha: tensor([0.4828, 0.5172])
beta: tensor([[[0.2971, 0.0950],
         [0.1918, 0.1964]],

        [[0.7309, 0.1050],
         [0.1094, 0.5647]],

        [[0.5986, 0.0926],
         [0.1928, 0.8483]],

        [[0.5933, 0.1019],
         [0.2195, 0.8490]],

        [[0.8449, 0.1009],
         [0.1407, 0.7005]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9368977352403174
Average Adjusted Rand Index: 0.9369686312299594
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22270.55739866431
Iteration 100: Loss = -11418.936657133961
Iteration 200: Loss = -11415.667297279715
Iteration 300: Loss = -11411.071385676174
Iteration 400: Loss = -11408.222794213545
Iteration 500: Loss = -11401.120166663597
Iteration 600: Loss = -11378.74059733625
Iteration 700: Loss = -11306.323669202558
Iteration 800: Loss = -11244.985837987268
Iteration 900: Loss = -11208.498332609208
Iteration 1000: Loss = -11197.734783019103
Iteration 1100: Loss = -11189.778751007536
Iteration 1200: Loss = -11184.9298345872
Iteration 1300: Loss = -11184.766506901433
Iteration 1400: Loss = -11184.183142919435
Iteration 1500: Loss = -11184.016116727187
Iteration 1600: Loss = -11183.953841282815
Iteration 1700: Loss = -11183.912726471475
Iteration 1800: Loss = -11183.796982454873
Iteration 1900: Loss = -11183.292805030214
Iteration 2000: Loss = -11180.756050805667
Iteration 2100: Loss = -11180.712675672312
Iteration 2200: Loss = -11180.6652840271
Iteration 2300: Loss = -11180.484631252724
Iteration 2400: Loss = -11179.60652288912
Iteration 2500: Loss = -11179.58698497352
Iteration 2600: Loss = -11179.57590375066
Iteration 2700: Loss = -11179.54703881302
Iteration 2800: Loss = -11179.534016120335
Iteration 2900: Loss = -11179.523102366344
Iteration 3000: Loss = -11179.50499155322
Iteration 3100: Loss = -11179.463256052566
Iteration 3200: Loss = -11179.440581660398
Iteration 3300: Loss = -11179.42085097866
Iteration 3400: Loss = -11179.391392948073
Iteration 3500: Loss = -11179.293216811364
Iteration 3600: Loss = -11129.198275377978
Iteration 3700: Loss = -11104.69427972045
Iteration 3800: Loss = -11104.618773271783
Iteration 3900: Loss = -11104.553627378451
Iteration 4000: Loss = -11102.8421529371
Iteration 4100: Loss = -11102.69309163087
Iteration 4200: Loss = -11102.64747380716
Iteration 4300: Loss = -11102.622117584902
Iteration 4400: Loss = -11102.620784140136
Iteration 4500: Loss = -11102.614721268978
Iteration 4600: Loss = -11102.60681768764
Iteration 4700: Loss = -11102.60743597877
1
Iteration 4800: Loss = -11102.597783038602
Iteration 4900: Loss = -11102.596710922364
Iteration 5000: Loss = -11102.595658753045
Iteration 5100: Loss = -11102.594995723228
Iteration 5200: Loss = -11102.594019289761
Iteration 5300: Loss = -11102.596505783598
1
Iteration 5400: Loss = -11102.592643209675
Iteration 5500: Loss = -11102.592086656226
Iteration 5600: Loss = -11102.591617257638
Iteration 5700: Loss = -11102.590967181939
Iteration 5800: Loss = -11102.590580648024
Iteration 5900: Loss = -11102.59126504268
1
Iteration 6000: Loss = -11102.589545011173
Iteration 6100: Loss = -11102.589825966226
1
Iteration 6200: Loss = -11102.588645686816
Iteration 6300: Loss = -11102.591129316805
1
Iteration 6400: Loss = -11102.584506691923
Iteration 6500: Loss = -11102.583344734223
Iteration 6600: Loss = -11102.582220894692
Iteration 6700: Loss = -11102.581930164508
Iteration 6800: Loss = -11102.581800400058
Iteration 6900: Loss = -11102.581183702903
Iteration 7000: Loss = -11102.581672093238
1
Iteration 7100: Loss = -11102.580353547279
Iteration 7200: Loss = -11102.580295831278
Iteration 7300: Loss = -11102.58285187392
1
Iteration 7400: Loss = -11102.579458150638
Iteration 7500: Loss = -11102.578839228943
Iteration 7600: Loss = -11102.577029147104
Iteration 7700: Loss = -11102.57574355555
Iteration 7800: Loss = -11102.575593602074
Iteration 7900: Loss = -11102.575039544308
Iteration 8000: Loss = -11102.569929274356
Iteration 8100: Loss = -11102.568699031552
Iteration 8200: Loss = -11102.569010382145
1
Iteration 8300: Loss = -11102.568540424552
Iteration 8400: Loss = -11102.568761608512
1
Iteration 8500: Loss = -11102.568550114163
2
Iteration 8600: Loss = -11102.568258214731
Iteration 8700: Loss = -11102.56810925951
Iteration 8800: Loss = -11102.566720965671
Iteration 8900: Loss = -11102.421621738986
Iteration 9000: Loss = -11102.457903765926
1
Iteration 9100: Loss = -11102.419059427422
Iteration 9200: Loss = -11102.432764209034
1
Iteration 9300: Loss = -11102.418923537292
Iteration 9400: Loss = -11102.448276138337
1
Iteration 9500: Loss = -11102.418785742826
Iteration 9600: Loss = -11102.430726259898
1
Iteration 9700: Loss = -11102.418662564463
Iteration 9800: Loss = -11102.418617441228
Iteration 9900: Loss = -11102.423839966199
1
Iteration 10000: Loss = -11102.418468231761
Iteration 10100: Loss = -11102.41844767498
Iteration 10200: Loss = -11102.41838012543
Iteration 10300: Loss = -11102.418176144212
Iteration 10400: Loss = -11102.437517394786
1
Iteration 10500: Loss = -11102.585002269067
2
Iteration 10600: Loss = -11102.417375173729
Iteration 10700: Loss = -11102.418131099304
1
Iteration 10800: Loss = -11102.436351706709
2
Iteration 10900: Loss = -11102.417327873634
Iteration 11000: Loss = -11102.417427915321
1
Iteration 11100: Loss = -11102.439559382603
2
Iteration 11200: Loss = -11102.428003065399
3
Iteration 11300: Loss = -11102.51508442886
4
Iteration 11400: Loss = -11102.417224554923
Iteration 11500: Loss = -11102.417359243422
1
Iteration 11600: Loss = -11102.675178250132
2
Iteration 11700: Loss = -11102.417150222362
Iteration 11800: Loss = -11102.42649083426
1
Iteration 11900: Loss = -11102.417188782
2
Iteration 12000: Loss = -11102.417182960675
3
Iteration 12100: Loss = -11102.455006393266
4
Iteration 12200: Loss = -11102.417134882115
Iteration 12300: Loss = -11102.418618325037
1
Iteration 12400: Loss = -11102.417102129088
Iteration 12500: Loss = -11102.418136099932
1
Iteration 12600: Loss = -11102.417070422352
Iteration 12700: Loss = -11102.420341350547
1
Iteration 12800: Loss = -11102.417093260046
2
Iteration 12900: Loss = -11102.454688224292
3
Iteration 13000: Loss = -11102.417086065125
4
Iteration 13100: Loss = -11102.417060491085
Iteration 13200: Loss = -11102.41709539829
1
Iteration 13300: Loss = -11102.417026709692
Iteration 13400: Loss = -11102.421418324171
1
Iteration 13500: Loss = -11102.417019258195
Iteration 13600: Loss = -11102.43667870242
1
Iteration 13700: Loss = -11102.416521961106
Iteration 13800: Loss = -11102.424060937508
1
Iteration 13900: Loss = -11102.6241888189
2
Iteration 14000: Loss = -11102.416519125654
Iteration 14100: Loss = -11102.417815243614
1
Iteration 14200: Loss = -11102.416862458058
2
Iteration 14300: Loss = -11102.416514647048
Iteration 14400: Loss = -11102.41651691338
1
Iteration 14500: Loss = -11102.416511467092
Iteration 14600: Loss = -11102.416474265272
Iteration 14700: Loss = -11102.416577655194
1
Iteration 14800: Loss = -11102.416458245587
Iteration 14900: Loss = -11102.416554820638
1
Iteration 15000: Loss = -11102.416435986926
Iteration 15100: Loss = -11102.422453980906
1
Iteration 15200: Loss = -11102.416428975519
Iteration 15300: Loss = -11102.53091451936
1
Iteration 15400: Loss = -11102.416394227603
Iteration 15500: Loss = -11102.416334483822
Iteration 15600: Loss = -11102.418565676622
1
Iteration 15700: Loss = -11102.41636042282
2
Iteration 15800: Loss = -11102.416320829212
Iteration 15900: Loss = -11102.41661628513
1
Iteration 16000: Loss = -11102.416300599678
Iteration 16100: Loss = -11102.416327863903
1
Iteration 16200: Loss = -11102.416346040345
2
Iteration 16300: Loss = -11102.416932312583
3
Iteration 16400: Loss = -11102.416874136514
4
Iteration 16500: Loss = -11102.446431364639
5
Stopping early at iteration 16500 due to no improvement.
pi: tensor([[0.7554, 0.2446],
        [0.2305, 0.7695]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5415, 0.4585], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.0951],
         [0.5714, 0.3031]],

        [[0.5721, 0.1046],
         [0.5961, 0.5942]],

        [[0.6276, 0.0925],
         [0.5161, 0.5840]],

        [[0.5995, 0.1012],
         [0.5157, 0.6929]],

        [[0.7120, 0.1011],
         [0.6999, 0.6291]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9368977708572352
Average Adjusted Rand Index: 0.9366453775376467
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22613.050556458555
Iteration 100: Loss = -11416.534258059806
Iteration 200: Loss = -11414.113187387997
Iteration 300: Loss = -11413.106496952343
Iteration 400: Loss = -11412.274188973082
Iteration 500: Loss = -11410.743567558005
Iteration 600: Loss = -11352.371443522135
Iteration 700: Loss = -11218.030250909116
Iteration 800: Loss = -11188.591842099744
Iteration 900: Loss = -11187.646873292777
Iteration 1000: Loss = -11187.251030206558
Iteration 1100: Loss = -11184.651414672628
Iteration 1200: Loss = -11183.345687016767
Iteration 1300: Loss = -11183.265476859347
Iteration 1400: Loss = -11181.405491731344
Iteration 1500: Loss = -11181.360924283656
Iteration 1600: Loss = -11181.350753768633
Iteration 1700: Loss = -11181.338560540999
Iteration 1800: Loss = -11181.250727858422
Iteration 1900: Loss = -11180.209111709348
Iteration 2000: Loss = -11180.142254545715
Iteration 2100: Loss = -11180.136515664071
Iteration 2200: Loss = -11180.132775001794
Iteration 2300: Loss = -11180.133357382478
1
Iteration 2400: Loss = -11180.127316437009
Iteration 2500: Loss = -11180.125152568027
Iteration 2600: Loss = -11180.12329489093
Iteration 2700: Loss = -11180.121074200555
Iteration 2800: Loss = -11180.10711758372
Iteration 2900: Loss = -11180.00654452935
Iteration 3000: Loss = -11180.00485689847
Iteration 3100: Loss = -11180.007460659986
1
Iteration 3200: Loss = -11180.001477357202
Iteration 3300: Loss = -11180.000760981313
Iteration 3400: Loss = -11180.002105448915
1
Iteration 3500: Loss = -11179.999552005813
Iteration 3600: Loss = -11179.998977649746
Iteration 3700: Loss = -11179.998414096324
Iteration 3800: Loss = -11179.997498908298
Iteration 3900: Loss = -11179.454509884747
Iteration 4000: Loss = -11179.448796321425
Iteration 4100: Loss = -11179.45828526607
1
Iteration 4200: Loss = -11179.446965528388
Iteration 4300: Loss = -11179.444698376545
Iteration 4400: Loss = -11179.443562273575
Iteration 4500: Loss = -11179.442696509754
Iteration 4600: Loss = -11179.442145188852
Iteration 4700: Loss = -11179.441383720705
Iteration 4800: Loss = -11179.439901637874
Iteration 4900: Loss = -11179.436408546462
Iteration 5000: Loss = -11179.434903380592
Iteration 5100: Loss = -11179.428986275867
Iteration 5200: Loss = -11179.40516906372
Iteration 5300: Loss = -11179.393729444273
Iteration 5400: Loss = -11179.370108368887
Iteration 5500: Loss = -11131.237184871441
Iteration 5600: Loss = -11131.139246680355
Iteration 5700: Loss = -11131.122289117795
Iteration 5800: Loss = -11131.107181008461
Iteration 5900: Loss = -11126.25708263305
Iteration 6000: Loss = -11126.252518039004
Iteration 6100: Loss = -11111.801194423597
Iteration 6200: Loss = -11111.79926547521
Iteration 6300: Loss = -11111.789672232026
Iteration 6400: Loss = -11111.782823701664
Iteration 6500: Loss = -11111.781995932308
Iteration 6600: Loss = -11111.77664773747
Iteration 6700: Loss = -11111.776705346436
1
Iteration 6800: Loss = -11109.976788608628
Iteration 6900: Loss = -11109.977327572728
1
Iteration 7000: Loss = -11102.769489391123
Iteration 7100: Loss = -11102.755558957797
Iteration 7200: Loss = -11102.75488973164
Iteration 7300: Loss = -11102.751674411804
Iteration 7400: Loss = -11102.751147630144
Iteration 7500: Loss = -11102.75009419527
Iteration 7600: Loss = -11102.668667716225
Iteration 7700: Loss = -11102.606841850995
Iteration 7800: Loss = -11102.606778076482
Iteration 7900: Loss = -11102.695463596912
1
Iteration 8000: Loss = -11102.606675814463
Iteration 8100: Loss = -11102.60641932326
Iteration 8200: Loss = -11102.611630534397
1
Iteration 8300: Loss = -11102.629383101461
2
Iteration 8400: Loss = -11102.605469596901
Iteration 8500: Loss = -11102.603250173182
Iteration 8600: Loss = -11102.605495129827
1
Iteration 8700: Loss = -11102.603708214827
2
Iteration 8800: Loss = -11102.60315160099
Iteration 8900: Loss = -11102.606929047557
1
Iteration 9000: Loss = -11102.603108361507
Iteration 9100: Loss = -11102.612485810901
1
Iteration 9200: Loss = -11102.60300454645
Iteration 9300: Loss = -11102.629894052281
1
Iteration 9400: Loss = -11102.602775606827
Iteration 9500: Loss = -11102.606537120653
1
Iteration 9600: Loss = -11102.585892535084
Iteration 9700: Loss = -11102.585751412445
Iteration 9800: Loss = -11102.585830727836
1
Iteration 9900: Loss = -11102.585725605948
Iteration 10000: Loss = -11102.603927718563
1
Iteration 10100: Loss = -11102.585572676471
Iteration 10200: Loss = -11102.605705392054
1
Iteration 10300: Loss = -11102.585429223604
Iteration 10400: Loss = -11102.58556373961
1
Iteration 10500: Loss = -11102.623224612764
2
Iteration 10600: Loss = -11102.585051667118
Iteration 10700: Loss = -11102.584300977387
Iteration 10800: Loss = -11102.60648215856
1
Iteration 10900: Loss = -11102.578347800138
Iteration 11000: Loss = -11102.44820146243
Iteration 11100: Loss = -11102.430541504364
Iteration 11200: Loss = -11102.433262984026
1
Iteration 11300: Loss = -11102.430517570048
Iteration 11400: Loss = -11102.503062294385
1
Iteration 11500: Loss = -11102.430471713278
Iteration 11600: Loss = -11102.430225086418
Iteration 11700: Loss = -11102.430256142774
1
Iteration 11800: Loss = -11102.430037716245
Iteration 11900: Loss = -11102.42979018947
Iteration 12000: Loss = -11102.423152338462
Iteration 12100: Loss = -11102.428151255688
1
Iteration 12200: Loss = -11102.423107290231
Iteration 12300: Loss = -11102.423035008482
Iteration 12400: Loss = -11102.42363471768
1
Iteration 12500: Loss = -11102.422428963353
Iteration 12600: Loss = -11102.42241922901
Iteration 12700: Loss = -11102.42284376696
1
Iteration 12800: Loss = -11102.422501713661
2
Iteration 12900: Loss = -11102.423698961395
3
Iteration 13000: Loss = -11102.50407781486
4
Iteration 13100: Loss = -11102.422259368512
Iteration 13200: Loss = -11102.423342716314
1
Iteration 13300: Loss = -11102.422312521625
2
Iteration 13400: Loss = -11102.422240082942
Iteration 13500: Loss = -11102.422931310324
1
Iteration 13600: Loss = -11102.422250201564
2
Iteration 13700: Loss = -11102.42370631066
3
Iteration 13800: Loss = -11102.425046980286
4
Iteration 13900: Loss = -11102.411228061841
Iteration 14000: Loss = -11102.411429910473
1
Iteration 14100: Loss = -11102.411195685452
Iteration 14200: Loss = -11102.41139056021
1
Iteration 14300: Loss = -11102.440138615631
2
Iteration 14400: Loss = -11102.410970412286
Iteration 14500: Loss = -11102.419396592668
1
Iteration 14600: Loss = -11102.409232197158
Iteration 14700: Loss = -11102.409238012644
1
Iteration 14800: Loss = -11102.410076590424
2
Iteration 14900: Loss = -11102.409164544308
Iteration 15000: Loss = -11102.417489768977
1
Iteration 15100: Loss = -11102.408776792732
Iteration 15200: Loss = -11102.408791504009
1
Iteration 15300: Loss = -11102.408796795156
2
Iteration 15400: Loss = -11102.408843335743
3
Iteration 15500: Loss = -11102.408755075763
Iteration 15600: Loss = -11102.408976447905
1
Iteration 15700: Loss = -11102.408747501173
Iteration 15800: Loss = -11102.41313355868
1
Iteration 15900: Loss = -11102.408753516396
2
Iteration 16000: Loss = -11102.593425806059
3
Iteration 16100: Loss = -11102.408747103958
Iteration 16200: Loss = -11102.408744873983
Iteration 16300: Loss = -11102.408642444883
Iteration 16400: Loss = -11102.40629753904
Iteration 16500: Loss = -11102.40638064943
1
Iteration 16600: Loss = -11102.406333472201
2
Iteration 16700: Loss = -11102.406288398195
Iteration 16800: Loss = -11102.406521687633
1
Iteration 16900: Loss = -11102.408409884609
2
Iteration 17000: Loss = -11102.4078188815
3
Iteration 17100: Loss = -11102.406252558416
Iteration 17200: Loss = -11102.4069219768
1
Iteration 17300: Loss = -11102.414206128413
2
Iteration 17400: Loss = -11102.425655176414
3
Iteration 17500: Loss = -11102.400157932734
Iteration 17600: Loss = -11102.399991239168
Iteration 17700: Loss = -11102.451076243244
1
Iteration 17800: Loss = -11102.39991019863
Iteration 17900: Loss = -11102.432581304936
1
Iteration 18000: Loss = -11102.399905226861
Iteration 18100: Loss = -11102.404029990606
1
Iteration 18200: Loss = -11102.417390896815
2
Iteration 18300: Loss = -11102.544248344708
3
Iteration 18400: Loss = -11102.3999529292
4
Iteration 18500: Loss = -11102.400121379818
5
Stopping early at iteration 18500 due to no improvement.
pi: tensor([[0.7544, 0.2456],
        [0.2306, 0.7694]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5404, 0.4596], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2013, 0.0948],
         [0.6174, 0.3028]],

        [[0.6024, 0.1043],
         [0.6773, 0.5119]],

        [[0.5761, 0.0922],
         [0.6942, 0.5009]],

        [[0.7176, 0.1011],
         [0.5245, 0.6745]],

        [[0.5325, 0.1009],
         [0.6536, 0.6425]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9368977708572352
Average Adjusted Rand Index: 0.9366453775376467
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23297.26324486764
Iteration 100: Loss = -11418.491147689936
Iteration 200: Loss = -11415.315097628045
Iteration 300: Loss = -11413.420716727918
Iteration 400: Loss = -11405.395037355165
Iteration 500: Loss = -11371.544487999015
Iteration 600: Loss = -11261.152983377133
Iteration 700: Loss = -11208.03569580794
Iteration 800: Loss = -11192.694953042743
Iteration 900: Loss = -11184.481487998395
Iteration 1000: Loss = -11184.170239514053
Iteration 1100: Loss = -11183.899916680673
Iteration 1200: Loss = -11183.578642208859
Iteration 1300: Loss = -11183.013695209362
Iteration 1400: Loss = -11182.941724682496
Iteration 1500: Loss = -11182.831320784046
Iteration 1600: Loss = -11182.77481463462
Iteration 1700: Loss = -11182.725009949943
Iteration 1800: Loss = -11182.69760808814
Iteration 1900: Loss = -11182.67464727714
Iteration 2000: Loss = -11182.653029415396
Iteration 2100: Loss = -11182.62257442596
Iteration 2200: Loss = -11182.585417300415
Iteration 2300: Loss = -11181.290958457015
Iteration 2400: Loss = -11181.198660346607
Iteration 2500: Loss = -11179.536117895215
Iteration 2600: Loss = -11179.52630952473
Iteration 2700: Loss = -11179.518577780644
Iteration 2800: Loss = -11179.510856650246
Iteration 2900: Loss = -11179.492820323667
Iteration 3000: Loss = -11179.477094789341
Iteration 3100: Loss = -11179.472076694237
Iteration 3200: Loss = -11179.468399783462
Iteration 3300: Loss = -11179.46416127098
Iteration 3400: Loss = -11179.460533208374
Iteration 3500: Loss = -11179.45625066692
Iteration 3600: Loss = -11179.382138881012
Iteration 3700: Loss = -11179.334579558936
Iteration 3800: Loss = -11179.336992089231
1
Iteration 3900: Loss = -11179.333327481947
Iteration 4000: Loss = -11179.328540583447
Iteration 4100: Loss = -11179.324808985217
Iteration 4200: Loss = -11179.309538811769
Iteration 4300: Loss = -11179.309767515222
1
Iteration 4400: Loss = -11179.306603993211
Iteration 4500: Loss = -11179.307714218652
1
Iteration 4600: Loss = -11179.303978217304
Iteration 4700: Loss = -11179.30295467664
Iteration 4800: Loss = -11179.303710850774
1
Iteration 4900: Loss = -11179.30179871105
Iteration 5000: Loss = -11179.300429401164
Iteration 5100: Loss = -11179.299999050776
Iteration 5200: Loss = -11179.299025338107
Iteration 5300: Loss = -11179.298408316856
Iteration 5400: Loss = -11179.2996725586
1
Iteration 5500: Loss = -11179.297211943465
Iteration 5600: Loss = -11179.296719862918
Iteration 5700: Loss = -11179.296246139274
Iteration 5800: Loss = -11179.2957278275
Iteration 5900: Loss = -11179.2953365469
Iteration 6000: Loss = -11179.305556740379
1
Iteration 6100: Loss = -11179.29450005223
Iteration 6200: Loss = -11179.294971351022
1
Iteration 6300: Loss = -11179.293819416802
Iteration 6400: Loss = -11179.295554491862
1
Iteration 6500: Loss = -11179.29321566268
Iteration 6600: Loss = -11179.295265348414
1
Iteration 6700: Loss = -11179.292690556093
Iteration 6800: Loss = -11179.292417124441
Iteration 6900: Loss = -11179.29808089599
1
Iteration 7000: Loss = -11179.291973748961
Iteration 7100: Loss = -11179.291726191364
Iteration 7200: Loss = -11179.293522561591
1
Iteration 7300: Loss = -11179.29131494718
Iteration 7400: Loss = -11179.291093667043
Iteration 7500: Loss = -11179.290935208634
Iteration 7600: Loss = -11179.290753586572
Iteration 7700: Loss = -11179.291474641499
1
Iteration 7800: Loss = -11179.290628239434
Iteration 7900: Loss = -11179.290296396854
Iteration 8000: Loss = -11179.290189367157
Iteration 8100: Loss = -11179.289971996484
Iteration 8200: Loss = -11179.290127834847
1
Iteration 8300: Loss = -11179.289845906718
Iteration 8400: Loss = -11179.289651082101
Iteration 8500: Loss = -11179.2912792102
1
Iteration 8600: Loss = -11179.289911781729
2
Iteration 8700: Loss = -11179.29604748829
3
Iteration 8800: Loss = -11179.290337147808
4
Iteration 8900: Loss = -11179.289135065039
Iteration 9000: Loss = -11179.356009374806
1
Iteration 9100: Loss = -11179.288885734326
Iteration 9200: Loss = -11179.321722477893
1
Iteration 9300: Loss = -11179.288664830108
Iteration 9400: Loss = -11179.30534516502
1
Iteration 9500: Loss = -11179.288356590878
Iteration 9600: Loss = -11179.288000979102
Iteration 9700: Loss = -11179.359672880182
1
Iteration 9800: Loss = -11179.287380881233
Iteration 9900: Loss = -11179.28697590122
Iteration 10000: Loss = -11179.53486553465
1
Iteration 10100: Loss = -11179.283144269792
Iteration 10200: Loss = -11179.268689335722
Iteration 10300: Loss = -11179.241309852001
Iteration 10400: Loss = -11131.123831391082
Iteration 10500: Loss = -11104.385756254336
Iteration 10600: Loss = -11104.391138860921
1
Iteration 10700: Loss = -11102.625359853202
Iteration 10800: Loss = -11102.635020204012
1
Iteration 10900: Loss = -11102.628661837332
2
Iteration 11000: Loss = -11102.620942752183
Iteration 11100: Loss = -11102.622320412118
1
Iteration 11200: Loss = -11102.623409970665
2
Iteration 11300: Loss = -11102.599352214216
Iteration 11400: Loss = -11102.489593183065
Iteration 11500: Loss = -11102.457424179262
Iteration 11600: Loss = -11102.457777449827
1
Iteration 11700: Loss = -11102.457159931733
Iteration 11800: Loss = -11102.455768538654
Iteration 11900: Loss = -11102.436154615518
Iteration 12000: Loss = -11102.433929435661
Iteration 12100: Loss = -11102.433707557571
Iteration 12200: Loss = -11102.437409341639
1
Iteration 12300: Loss = -11102.43352024023
Iteration 12400: Loss = -11102.443901534047
1
Iteration 12500: Loss = -11102.433442943573
Iteration 12600: Loss = -11102.433460851385
1
Iteration 12700: Loss = -11102.43347092881
2
Iteration 12800: Loss = -11102.439845436302
3
Iteration 12900: Loss = -11102.490151858901
4
Iteration 13000: Loss = -11102.494526120041
5
Stopping early at iteration 13000 due to no improvement.
pi: tensor([[0.7704, 0.2296],
        [0.2466, 0.7534]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4614, 0.5386], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3022, 0.0940],
         [0.5873, 0.2018]],

        [[0.6135, 0.1040],
         [0.5367, 0.5966]],

        [[0.7275, 0.0919],
         [0.6850, 0.6952]],

        [[0.6927, 0.1008],
         [0.6759, 0.5255]],

        [[0.6013, 0.1006],
         [0.7082, 0.5505]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9368977708572352
Average Adjusted Rand Index: 0.9366453775376467
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20718.52943847519
Iteration 100: Loss = -11417.959735076298
Iteration 200: Loss = -11412.9701167691
Iteration 300: Loss = -11410.686514356603
Iteration 400: Loss = -11406.943244401931
Iteration 500: Loss = -11378.13740083447
Iteration 600: Loss = -11281.776568032452
Iteration 700: Loss = -11236.732664402458
Iteration 800: Loss = -11210.31321665596
Iteration 900: Loss = -11196.116584605108
Iteration 1000: Loss = -11190.850503675672
Iteration 1100: Loss = -11190.667312044205
Iteration 1200: Loss = -11190.574664570615
Iteration 1300: Loss = -11190.502645593133
Iteration 1400: Loss = -11190.296268827573
Iteration 1500: Loss = -11189.014050303002
Iteration 1600: Loss = -11188.97943517911
Iteration 1700: Loss = -11188.939382784609
Iteration 1800: Loss = -11188.539159043596
Iteration 1900: Loss = -11188.503774429553
Iteration 2000: Loss = -11188.44210055451
Iteration 2100: Loss = -11188.272258676952
Iteration 2200: Loss = -11188.20091840971
Iteration 2300: Loss = -11185.253748770367
Iteration 2400: Loss = -11184.839489984606
Iteration 2500: Loss = -11184.822972282043
Iteration 2600: Loss = -11184.782999612386
Iteration 2700: Loss = -11184.7763722789
Iteration 2800: Loss = -11184.769848665392
Iteration 2900: Loss = -11184.764570287314
Iteration 3000: Loss = -11184.753347014985
Iteration 3100: Loss = -11184.737243201796
Iteration 3200: Loss = -11184.731282962071
Iteration 3300: Loss = -11184.720868000724
Iteration 3400: Loss = -11184.717555647318
Iteration 3500: Loss = -11184.71588939115
Iteration 3600: Loss = -11184.703352270226
Iteration 3700: Loss = -11184.628180729991
Iteration 3800: Loss = -11184.624813762406
Iteration 3900: Loss = -11184.597931066997
Iteration 4000: Loss = -11184.585181367012
Iteration 4100: Loss = -11184.56121258129
Iteration 4200: Loss = -11184.49988240072
Iteration 4300: Loss = -11133.142158639672
Iteration 4400: Loss = -11133.023722997144
Iteration 4500: Loss = -11120.417344658106
Iteration 4600: Loss = -11117.338292824383
Iteration 4700: Loss = -11117.331815304731
Iteration 4800: Loss = -11117.318228572876
Iteration 4900: Loss = -11111.306149419532
Iteration 5000: Loss = -11111.256750350434
Iteration 5100: Loss = -11111.226204306722
Iteration 5200: Loss = -11111.225217008956
Iteration 5300: Loss = -11111.221754513064
Iteration 5400: Loss = -11111.21181504354
Iteration 5500: Loss = -11111.207275873954
Iteration 5600: Loss = -11111.206768805469
Iteration 5700: Loss = -11111.206294964133
Iteration 5800: Loss = -11111.205622512927
Iteration 5900: Loss = -11111.204665296316
Iteration 6000: Loss = -11109.472754963248
Iteration 6100: Loss = -11109.472393267255
Iteration 6200: Loss = -11109.47117889385
Iteration 6300: Loss = -11109.469800442552
Iteration 6400: Loss = -11109.468146865456
Iteration 6500: Loss = -11109.469347437636
1
Iteration 6600: Loss = -11109.462990072368
Iteration 6700: Loss = -11109.323624885814
Iteration 6800: Loss = -11109.347269407981
1
Iteration 6900: Loss = -11109.323403543585
Iteration 7000: Loss = -11109.323554725557
1
Iteration 7100: Loss = -11109.32624903652
2
Iteration 7200: Loss = -11109.323062605281
Iteration 7300: Loss = -11109.334824446692
1
Iteration 7400: Loss = -11109.322922532405
Iteration 7500: Loss = -11109.322866462991
Iteration 7600: Loss = -11109.322816246775
Iteration 7700: Loss = -11109.322700651663
Iteration 7800: Loss = -11109.374779451371
1
Iteration 7900: Loss = -11109.322578548925
Iteration 8000: Loss = -11109.322473930015
Iteration 8100: Loss = -11109.33805303327
1
Iteration 8200: Loss = -11109.322291658636
Iteration 8300: Loss = -11109.322142613682
Iteration 8400: Loss = -11109.32214333847
1
Iteration 8500: Loss = -11109.322009932059
Iteration 8600: Loss = -11109.321972386084
Iteration 8700: Loss = -11109.32191400547
Iteration 8800: Loss = -11109.322010322694
1
Iteration 8900: Loss = -11109.32181086832
Iteration 9000: Loss = -11109.32169015765
Iteration 9100: Loss = -11109.321648237008
Iteration 9200: Loss = -11109.321482124631
Iteration 9300: Loss = -11109.322366111148
1
Iteration 9400: Loss = -11109.321345004406
Iteration 9500: Loss = -11109.32126812634
Iteration 9600: Loss = -11109.322006878316
1
Iteration 9700: Loss = -11109.31400114722
Iteration 9800: Loss = -11109.667247195926
1
Iteration 9900: Loss = -11109.313613422091
Iteration 10000: Loss = -11109.320242970747
1
Iteration 10100: Loss = -11109.313583097677
Iteration 10200: Loss = -11109.313949226189
1
Iteration 10300: Loss = -11109.414343681065
2
Iteration 10400: Loss = -11109.313468488095
Iteration 10500: Loss = -11109.318280234833
1
Iteration 10600: Loss = -11109.320244374318
2
Iteration 10700: Loss = -11109.438748503824
3
Iteration 10800: Loss = -11109.31316754364
Iteration 10900: Loss = -11109.31265937641
Iteration 11000: Loss = -11109.312826677056
1
Iteration 11100: Loss = -11109.352040690055
2
Iteration 11200: Loss = -11109.312196084014
Iteration 11300: Loss = -11109.310347050852
Iteration 11400: Loss = -11109.405055665586
1
Iteration 11500: Loss = -11109.309490436359
Iteration 11600: Loss = -11109.320240489129
1
Iteration 11700: Loss = -11109.30892064629
Iteration 11800: Loss = -11102.421969541498
Iteration 11900: Loss = -11102.428214754851
1
Iteration 12000: Loss = -11102.418421309163
Iteration 12100: Loss = -11102.418940681244
1
Iteration 12200: Loss = -11102.418169845194
Iteration 12300: Loss = -11102.419759566325
1
Iteration 12400: Loss = -11102.416354688672
Iteration 12500: Loss = -11102.416528868383
1
Iteration 12600: Loss = -11102.416361486687
2
Iteration 12700: Loss = -11102.508168981989
3
Iteration 12800: Loss = -11102.41637263156
4
Iteration 12900: Loss = -11102.416366555444
5
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[0.7544, 0.2456],
        [0.2303, 0.7697]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5403, 0.4597], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2013, 0.0948],
         [0.6957, 0.3027]],

        [[0.6748, 0.1043],
         [0.6836, 0.5381]],

        [[0.5890, 0.0922],
         [0.6981, 0.5589]],

        [[0.5061, 0.1011],
         [0.6033, 0.6698]],

        [[0.6683, 0.1008],
         [0.6832, 0.7221]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9368977708572352
Average Adjusted Rand Index: 0.9366453775376467
11127.029110912514
[0.9368977708572352, 0.9368977708572352, 0.9368977708572352, 0.9368977708572352] [0.9366453775376467, 0.9366453775376467, 0.9366453775376467, 0.9366453775376467] [11102.446431364639, 11102.400121379818, 11102.494526120041, 11102.416366555444]
-----------------------------------------------------------------------------------------
This iteration is 2
True Objective function: Loss = -11476.865171555504
Iteration 0: Loss = -11769.74821345456
Iteration 10: Loss = -11769.457814903646
Iteration 20: Loss = -11758.22655752688
Iteration 30: Loss = -11737.943330256137
Iteration 40: Loss = -11460.379689711006
Iteration 50: Loss = -11455.992753182445
Iteration 60: Loss = -11456.005163734555
1
Iteration 70: Loss = -11456.005254585554
2
Iteration 80: Loss = -11456.005252762085
3
Stopping early at iteration 80 due to no improvement.
pi: tensor([[0.7510, 0.2490],
        [0.2188, 0.7812]], dtype=torch.float64)
alpha: tensor([0.4496, 0.5504])
beta: tensor([[[0.1906, 0.1089],
         [0.6432, 0.2939]],

        [[0.8592, 0.1090],
         [0.0598, 0.0344]],

        [[0.1640, 0.0941],
         [0.2865, 0.2302]],

        [[0.7921, 0.1222],
         [0.6368, 0.4042]],

        [[0.0304, 0.1029],
         [0.7037, 0.1471]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8822858823962049
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524765057602629
Average Adjusted Rand Index: 0.9524566436163727
Iteration 0: Loss = -12004.484359734812
Iteration 10: Loss = -11456.155399839612
Iteration 20: Loss = -11456.002462203242
Iteration 30: Loss = -11456.005231177038
1
Iteration 40: Loss = -11456.005256301169
2
Iteration 50: Loss = -11456.00525133475
3
Stopping early at iteration 50 due to no improvement.
pi: tensor([[0.7812, 0.2188],
        [0.2490, 0.7510]], dtype=torch.float64)
alpha: tensor([0.5504, 0.4496])
beta: tensor([[[0.2939, 0.1089],
         [0.2249, 0.1906]],

        [[0.5674, 0.1090],
         [0.2108, 0.3288]],

        [[0.7983, 0.0941],
         [0.1218, 0.4449]],

        [[0.5578, 0.1222],
         [0.4477, 0.2993]],

        [[0.9835, 0.1029],
         [0.1938, 0.5949]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8822858823962049
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524765057602629
Average Adjusted Rand Index: 0.9524566436163727
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22486.075762897362
Iteration 100: Loss = -11769.783649071993
Iteration 200: Loss = -11768.360459712147
Iteration 300: Loss = -11763.369357212
Iteration 400: Loss = -11753.902594902172
Iteration 500: Loss = -11651.374695448121
Iteration 600: Loss = -11539.220131009828
Iteration 700: Loss = -11509.537649725311
Iteration 800: Loss = -11485.69554598632
Iteration 900: Loss = -11476.042873938371
Iteration 1000: Loss = -11470.433286754143
Iteration 1100: Loss = -11465.208837282164
Iteration 1200: Loss = -11453.643723439347
Iteration 1300: Loss = -11450.328552044934
Iteration 1400: Loss = -11450.105927434262
Iteration 1500: Loss = -11449.948079659833
Iteration 1600: Loss = -11449.894563130365
Iteration 1700: Loss = -11449.86051943541
Iteration 1800: Loss = -11449.83323448054
Iteration 1900: Loss = -11449.809396275054
Iteration 2000: Loss = -11449.786834453274
Iteration 2100: Loss = -11449.768131177172
Iteration 2200: Loss = -11449.755429059462
Iteration 2300: Loss = -11449.745182223103
Iteration 2400: Loss = -11449.735939396383
Iteration 2500: Loss = -11449.725971728138
Iteration 2600: Loss = -11449.70776842988
Iteration 2700: Loss = -11449.578926143622
Iteration 2800: Loss = -11449.56388328436
Iteration 2900: Loss = -11449.558681439381
Iteration 3000: Loss = -11449.554278010084
Iteration 3100: Loss = -11449.55000157037
Iteration 3200: Loss = -11449.544806257385
Iteration 3300: Loss = -11449.539502751131
Iteration 3400: Loss = -11449.485478357383
Iteration 3500: Loss = -11449.4693539113
Iteration 3600: Loss = -11449.47057501287
1
Iteration 3700: Loss = -11449.4645810693
Iteration 3800: Loss = -11449.46298997889
Iteration 3900: Loss = -11449.461401022307
Iteration 4000: Loss = -11449.46007962106
Iteration 4100: Loss = -11449.458833075558
Iteration 4200: Loss = -11449.45791880774
Iteration 4300: Loss = -11449.461836089678
1
Iteration 4400: Loss = -11449.455853418362
Iteration 4500: Loss = -11449.455014811358
Iteration 4600: Loss = -11449.454548129317
Iteration 4700: Loss = -11449.455094894765
1
Iteration 4800: Loss = -11449.452871970754
Iteration 4900: Loss = -11449.452938329308
1
Iteration 5000: Loss = -11449.45161034391
Iteration 5100: Loss = -11449.455862507484
1
Iteration 5200: Loss = -11449.45054857341
Iteration 5300: Loss = -11449.450402969245
Iteration 5400: Loss = -11449.451215134459
1
Iteration 5500: Loss = -11449.449305910795
Iteration 5600: Loss = -11449.449821988224
1
Iteration 5700: Loss = -11449.45197358109
2
Iteration 5800: Loss = -11449.450185622854
3
Iteration 5900: Loss = -11449.448249648496
Iteration 6000: Loss = -11449.447888206972
Iteration 6100: Loss = -11449.451002200629
1
Iteration 6200: Loss = -11449.44757400974
Iteration 6300: Loss = -11449.454292957676
1
Iteration 6400: Loss = -11449.447424844157
Iteration 6500: Loss = -11449.446564482058
Iteration 6600: Loss = -11449.446342726776
Iteration 6700: Loss = -11449.446185962228
Iteration 6800: Loss = -11449.449199402276
1
Iteration 6900: Loss = -11449.445828302194
Iteration 7000: Loss = -11449.445733536524
Iteration 7100: Loss = -11449.445574679099
Iteration 7200: Loss = -11449.445463875834
Iteration 7300: Loss = -11449.447874731817
1
Iteration 7400: Loss = -11449.447197338597
2
Iteration 7500: Loss = -11449.457061907653
3
Iteration 7600: Loss = -11449.446532805092
4
Iteration 7700: Loss = -11449.458020649461
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7716, 0.2284],
        [0.2007, 0.7993]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4475, 0.5525], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1933, 0.1112],
         [0.5080, 0.3013]],

        [[0.5827, 0.1088],
         [0.6504, 0.5432]],

        [[0.5965, 0.0945],
         [0.5902, 0.5537]],

        [[0.6948, 0.1240],
         [0.5557, 0.5969]],

        [[0.5035, 0.1030],
         [0.6377, 0.6897]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448420005390695
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207385189720222
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9214379584608567
Average Adjusted Rand Index: 0.9216009523870667
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21095.652926196835
Iteration 100: Loss = -11769.08921783706
Iteration 200: Loss = -11767.571332303205
Iteration 300: Loss = -11765.824612349381
Iteration 400: Loss = -11757.03965493717
Iteration 500: Loss = -11703.060159257942
Iteration 600: Loss = -11515.058110417114
Iteration 700: Loss = -11462.049635333888
Iteration 800: Loss = -11459.58195457596
Iteration 900: Loss = -11454.109512581106
Iteration 1000: Loss = -11453.657545810518
Iteration 1100: Loss = -11453.437644167798
Iteration 1200: Loss = -11453.362832927081
Iteration 1300: Loss = -11453.273956891528
Iteration 1400: Loss = -11453.15052978211
Iteration 1500: Loss = -11453.11355979298
Iteration 1600: Loss = -11452.881102057609
Iteration 1700: Loss = -11451.251490845705
Iteration 1800: Loss = -11451.17504126058
Iteration 1900: Loss = -11449.742982342139
Iteration 2000: Loss = -11449.714355099735
Iteration 2100: Loss = -11449.69798066744
Iteration 2200: Loss = -11449.68350288713
Iteration 2300: Loss = -11449.560620910772
Iteration 2400: Loss = -11449.543943412482
Iteration 2500: Loss = -11449.539340845933
Iteration 2600: Loss = -11449.535531781676
Iteration 2700: Loss = -11449.532766717082
Iteration 2800: Loss = -11449.529423601378
Iteration 2900: Loss = -11449.526963482696
Iteration 3000: Loss = -11449.524721728067
Iteration 3100: Loss = -11449.522713522556
Iteration 3200: Loss = -11449.520859274773
Iteration 3300: Loss = -11449.519192158483
Iteration 3400: Loss = -11449.517807485778
Iteration 3500: Loss = -11449.51620285097
Iteration 3600: Loss = -11449.514853639897
Iteration 3700: Loss = -11449.513574778674
Iteration 3800: Loss = -11449.512680970789
Iteration 3900: Loss = -11449.511519085681
Iteration 4000: Loss = -11449.515807129717
1
Iteration 4100: Loss = -11449.508582214963
Iteration 4200: Loss = -11449.507053288313
Iteration 4300: Loss = -11449.507075638263
1
Iteration 4400: Loss = -11449.503186888169
Iteration 4500: Loss = -11449.50110439049
Iteration 4600: Loss = -11449.495059823657
Iteration 4700: Loss = -11449.477512047803
Iteration 4800: Loss = -11449.475546777703
Iteration 4900: Loss = -11449.475014736505
Iteration 5000: Loss = -11449.474662426688
Iteration 5100: Loss = -11449.474151430932
Iteration 5200: Loss = -11449.473752441132
Iteration 5300: Loss = -11449.47350134306
Iteration 5400: Loss = -11449.475779081411
1
Iteration 5500: Loss = -11449.472443928176
Iteration 5600: Loss = -11449.47207110666
Iteration 5700: Loss = -11449.471775425356
Iteration 5800: Loss = -11449.47188082662
1
Iteration 5900: Loss = -11449.471849252759
2
Iteration 6000: Loss = -11449.471102092208
Iteration 6100: Loss = -11449.4771313879
1
Iteration 6200: Loss = -11449.473150013977
2
Iteration 6300: Loss = -11449.470729462102
Iteration 6400: Loss = -11449.4694492509
Iteration 6500: Loss = -11449.467785880859
Iteration 6600: Loss = -11449.478674222019
1
Iteration 6700: Loss = -11449.461923372124
Iteration 6800: Loss = -11449.456923305446
Iteration 6900: Loss = -11449.457588489731
1
Iteration 7000: Loss = -11449.45622474735
Iteration 7100: Loss = -11449.455475630664
Iteration 7200: Loss = -11449.454353066621
Iteration 7300: Loss = -11449.477261985947
1
Iteration 7400: Loss = -11449.453917017432
Iteration 7500: Loss = -11449.453821941941
Iteration 7600: Loss = -11449.475269634384
1
Iteration 7700: Loss = -11449.453637916708
Iteration 7800: Loss = -11449.453630862188
Iteration 7900: Loss = -11449.479295920264
1
Iteration 8000: Loss = -11449.453478430894
Iteration 8100: Loss = -11449.45344055775
Iteration 8200: Loss = -11449.618558787044
1
Iteration 8300: Loss = -11449.453299771487
Iteration 8400: Loss = -11449.453409244754
1
Iteration 8500: Loss = -11449.453240774477
Iteration 8600: Loss = -11449.45308643606
Iteration 8700: Loss = -11449.454764682832
1
Iteration 8800: Loss = -11449.452888745695
Iteration 8900: Loss = -11449.462804746901
1
Iteration 9000: Loss = -11449.452386363335
Iteration 9100: Loss = -11449.452289118539
Iteration 9200: Loss = -11449.452340768432
1
Iteration 9300: Loss = -11449.45215841942
Iteration 9400: Loss = -11449.453100469897
1
Iteration 9500: Loss = -11449.452031767392
Iteration 9600: Loss = -11449.452315651857
1
Iteration 9700: Loss = -11449.451985536936
Iteration 9800: Loss = -11449.459258339126
1
Iteration 9900: Loss = -11449.451869879202
Iteration 10000: Loss = -11449.452306499716
1
Iteration 10100: Loss = -11449.459475349959
2
Iteration 10200: Loss = -11449.451575505345
Iteration 10300: Loss = -11449.469144974279
1
Iteration 10400: Loss = -11449.452022289379
2
Iteration 10500: Loss = -11449.451569852496
Iteration 10600: Loss = -11449.460702388715
1
Iteration 10700: Loss = -11449.451546414657
Iteration 10800: Loss = -11449.451457777152
Iteration 10900: Loss = -11449.672236452914
1
Iteration 11000: Loss = -11449.451394538015
Iteration 11100: Loss = -11449.45155967732
1
Iteration 11200: Loss = -11449.45140257546
2
Iteration 11300: Loss = -11449.451356871014
Iteration 11400: Loss = -11449.454166318357
1
Iteration 11500: Loss = -11449.451348793684
Iteration 11600: Loss = -11449.46387336301
1
Iteration 11700: Loss = -11449.45130916325
Iteration 11800: Loss = -11449.48060565785
1
Iteration 11900: Loss = -11449.451316769604
2
Iteration 12000: Loss = -11449.451284420822
Iteration 12100: Loss = -11449.451561818783
1
Iteration 12200: Loss = -11449.451274396488
Iteration 12300: Loss = -11449.451620013224
1
Iteration 12400: Loss = -11449.452448832273
2
Iteration 12500: Loss = -11449.466677205966
3
Iteration 12600: Loss = -11449.451074606737
Iteration 12700: Loss = -11449.462847096893
1
Iteration 12800: Loss = -11449.504031724919
2
Iteration 12900: Loss = -11449.452452540127
3
Iteration 13000: Loss = -11449.450967486559
Iteration 13100: Loss = -11449.4644590273
1
Iteration 13200: Loss = -11449.450920410074
Iteration 13300: Loss = -11449.451980654832
1
Iteration 13400: Loss = -11449.450880067254
Iteration 13500: Loss = -11449.443479471549
Iteration 13600: Loss = -11449.442757088116
Iteration 13700: Loss = -11449.445655316551
1
Iteration 13800: Loss = -11449.467778539834
2
Iteration 13900: Loss = -11449.442014686392
Iteration 14000: Loss = -11449.49697870601
1
Iteration 14100: Loss = -11449.444573958594
2
Iteration 14200: Loss = -11449.5473583619
3
Iteration 14300: Loss = -11449.442015504019
4
Iteration 14400: Loss = -11449.45174543618
5
Stopping early at iteration 14400 due to no improvement.
pi: tensor([[0.7968, 0.2032],
        [0.2258, 0.7742]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5495, 0.4505], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3027, 0.1114],
         [0.6569, 0.1932]],

        [[0.6406, 0.1086],
         [0.6206, 0.5342]],

        [[0.5946, 0.0944],
         [0.5368, 0.5782]],

        [[0.6780, 0.1242],
         [0.5931, 0.6932]],

        [[0.5511, 0.1031],
         [0.5828, 0.5121]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448420005390695
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9214379584608567
Average Adjusted Rand Index: 0.9216009523870667
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24079.80980976468
Iteration 100: Loss = -11769.284708521167
Iteration 200: Loss = -11764.819850141579
Iteration 300: Loss = -11757.219825567574
Iteration 400: Loss = -11727.492935345017
Iteration 500: Loss = -11575.301748636528
Iteration 600: Loss = -11531.756370724328
Iteration 700: Loss = -11519.457386988975
Iteration 800: Loss = -11504.42852904609
Iteration 900: Loss = -11502.814870198346
Iteration 1000: Loss = -11502.385560408748
Iteration 1100: Loss = -11491.033749139806
Iteration 1200: Loss = -11484.957177860653
Iteration 1300: Loss = -11478.65067168051
Iteration 1400: Loss = -11478.102558951028
Iteration 1500: Loss = -11469.98058371977
Iteration 1600: Loss = -11469.931732253177
Iteration 1700: Loss = -11469.817812460584
Iteration 1800: Loss = -11469.781191858183
Iteration 1900: Loss = -11469.740301774687
Iteration 2000: Loss = -11464.773165993
Iteration 2100: Loss = -11457.682905542119
Iteration 2200: Loss = -11457.31759147928
Iteration 2300: Loss = -11457.296121719677
Iteration 2400: Loss = -11457.284443641209
Iteration 2500: Loss = -11457.276190771016
Iteration 2600: Loss = -11457.269478336077
Iteration 2700: Loss = -11457.263581476489
Iteration 2800: Loss = -11457.256881630368
Iteration 2900: Loss = -11457.233068735917
Iteration 3000: Loss = -11449.766511193544
Iteration 3100: Loss = -11449.755691455852
Iteration 3200: Loss = -11449.75095299217
Iteration 3300: Loss = -11449.747415733966
Iteration 3400: Loss = -11449.745009931425
Iteration 3500: Loss = -11449.74198476269
Iteration 3600: Loss = -11449.739861361853
Iteration 3700: Loss = -11449.737531665665
Iteration 3800: Loss = -11449.73561477405
Iteration 3900: Loss = -11449.734408419554
Iteration 4000: Loss = -11449.733176322885
Iteration 4100: Loss = -11449.73204198436
Iteration 4200: Loss = -11449.731047359792
Iteration 4300: Loss = -11449.729965464932
Iteration 4400: Loss = -11449.729173804539
Iteration 4500: Loss = -11449.728475054128
Iteration 4600: Loss = -11449.723601271982
Iteration 4700: Loss = -11449.68470117645
Iteration 4800: Loss = -11449.683619641606
Iteration 4900: Loss = -11449.682736526747
Iteration 5000: Loss = -11449.67703359245
Iteration 5100: Loss = -11449.62670507751
Iteration 5200: Loss = -11449.625000143187
Iteration 5300: Loss = -11449.624621351844
Iteration 5400: Loss = -11449.62888078005
1
Iteration 5500: Loss = -11449.62607588554
2
Iteration 5600: Loss = -11449.631611318797
3
Iteration 5700: Loss = -11449.62217611
Iteration 5800: Loss = -11449.619672837995
Iteration 5900: Loss = -11449.449334870178
Iteration 6000: Loss = -11449.448339090244
Iteration 6100: Loss = -11449.450468036728
1
Iteration 6200: Loss = -11449.44772099001
Iteration 6300: Loss = -11449.448749379857
1
Iteration 6400: Loss = -11449.447398817594
Iteration 6500: Loss = -11449.44838507478
1
Iteration 6600: Loss = -11449.447055643412
Iteration 6700: Loss = -11449.446849612272
Iteration 6800: Loss = -11449.447149743255
1
Iteration 6900: Loss = -11449.44698021356
2
Iteration 7000: Loss = -11449.44658222204
Iteration 7100: Loss = -11449.447343053345
1
Iteration 7200: Loss = -11449.447668960947
2
Iteration 7300: Loss = -11449.445858609652
Iteration 7400: Loss = -11449.448423220649
1
Iteration 7500: Loss = -11449.45616043717
2
Iteration 7600: Loss = -11449.482153356235
3
Iteration 7700: Loss = -11449.445435588763
Iteration 7800: Loss = -11449.445766079572
1
Iteration 7900: Loss = -11449.445212227329
Iteration 8000: Loss = -11449.445085120215
Iteration 8100: Loss = -11449.445019672738
Iteration 8200: Loss = -11449.445018798175
Iteration 8300: Loss = -11449.444874954394
Iteration 8400: Loss = -11449.446603325847
1
Iteration 8500: Loss = -11449.44475822766
Iteration 8600: Loss = -11449.444714739257
Iteration 8700: Loss = -11449.461820734943
1
Iteration 8800: Loss = -11449.44459654255
Iteration 8900: Loss = -11449.444551567492
Iteration 9000: Loss = -11449.444677946776
1
Iteration 9100: Loss = -11449.444495334497
Iteration 9200: Loss = -11449.444482375531
Iteration 9300: Loss = -11449.444900892455
1
Iteration 9400: Loss = -11449.444435803927
Iteration 9500: Loss = -11449.447990621793
1
Iteration 9600: Loss = -11449.45926852556
2
Iteration 9700: Loss = -11449.44436764119
Iteration 9800: Loss = -11449.444347443028
Iteration 9900: Loss = -11449.571305159981
1
Iteration 10000: Loss = -11449.44423179722
Iteration 10100: Loss = -11449.444939642088
1
Iteration 10200: Loss = -11449.445373476072
2
Iteration 10300: Loss = -11449.44464215222
3
Iteration 10400: Loss = -11449.446587234786
4
Iteration 10500: Loss = -11449.444009168898
Iteration 10600: Loss = -11449.44499215689
1
Iteration 10700: Loss = -11449.444022791944
2
Iteration 10800: Loss = -11449.450866896042
3
Iteration 10900: Loss = -11449.4438562148
Iteration 11000: Loss = -11449.44407453035
1
Iteration 11100: Loss = -11449.446002220508
2
Iteration 11200: Loss = -11449.591696506755
3
Iteration 11300: Loss = -11449.443968592514
4
Iteration 11400: Loss = -11449.443874111628
5
Stopping early at iteration 11400 due to no improvement.
pi: tensor([[0.7735, 0.2265],
        [0.2021, 0.7979]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4502, 0.5498], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1933, 0.1112],
         [0.5216, 0.3024]],

        [[0.5420, 0.1088],
         [0.5503, 0.6317]],

        [[0.7063, 0.0945],
         [0.5858, 0.5633]],

        [[0.6682, 0.1240],
         [0.6298, 0.5028]],

        [[0.6849, 0.1030],
         [0.5279, 0.5961]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448420005390695
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207385189720222
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9214379584608567
Average Adjusted Rand Index: 0.9216009523870667
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20908.611866134244
Iteration 100: Loss = -11768.053034855266
Iteration 200: Loss = -11763.594469597707
Iteration 300: Loss = -11761.691586177816
Iteration 400: Loss = -11761.076778086142
Iteration 500: Loss = -11760.825043680405
Iteration 600: Loss = -11760.744953072253
Iteration 700: Loss = -11760.706632673277
Iteration 800: Loss = -11760.68349554349
Iteration 900: Loss = -11760.664751002374
Iteration 1000: Loss = -11760.639967973366
Iteration 1100: Loss = -11760.600415361658
Iteration 1200: Loss = -11760.280540697946
Iteration 1300: Loss = -11753.40319339149
Iteration 1400: Loss = -11729.524419089223
Iteration 1500: Loss = -11620.81734357051
Iteration 1600: Loss = -11560.282179696913
Iteration 1700: Loss = -11547.627792870624
Iteration 1800: Loss = -11534.714006797201
Iteration 1900: Loss = -11527.121880720175
Iteration 2000: Loss = -11524.776991931461
Iteration 2100: Loss = -11523.40767965935
Iteration 2200: Loss = -11523.340889093275
Iteration 2300: Loss = -11523.232381812148
Iteration 2400: Loss = -11515.522891376322
Iteration 2500: Loss = -11515.499129923326
Iteration 2600: Loss = -11515.447295080783
Iteration 2700: Loss = -11515.4236880587
Iteration 2800: Loss = -11515.408293872126
Iteration 2900: Loss = -11514.95344543546
Iteration 3000: Loss = -11514.947986689183
Iteration 3100: Loss = -11514.944927355413
Iteration 3200: Loss = -11514.942403181705
Iteration 3300: Loss = -11514.916942983537
Iteration 3400: Loss = -11514.914675090566
Iteration 3500: Loss = -11514.910691340376
Iteration 3600: Loss = -11514.781774937974
Iteration 3700: Loss = -11514.765489028867
Iteration 3800: Loss = -11514.752630201021
Iteration 3900: Loss = -11514.751564209271
Iteration 4000: Loss = -11514.750891751479
Iteration 4100: Loss = -11514.750033139278
Iteration 4200: Loss = -11514.749000255075
Iteration 4300: Loss = -11514.748434758663
Iteration 4400: Loss = -11514.747776329914
Iteration 4500: Loss = -11514.74701322717
Iteration 4600: Loss = -11514.746390192218
Iteration 4700: Loss = -11514.7456759279
Iteration 4800: Loss = -11514.748881770529
1
Iteration 4900: Loss = -11514.747469033624
2
Iteration 5000: Loss = -11514.742663285459
Iteration 5100: Loss = -11514.741917165617
Iteration 5200: Loss = -11514.74381919899
1
Iteration 5300: Loss = -11514.74327253398
2
Iteration 5400: Loss = -11514.742541880043
3
Iteration 5500: Loss = -11514.740385547526
Iteration 5600: Loss = -11514.742842925849
1
Iteration 5700: Loss = -11514.739486586494
Iteration 5800: Loss = -11514.741410425211
1
Iteration 5900: Loss = -11514.738770466469
Iteration 6000: Loss = -11514.73738547134
Iteration 6100: Loss = -11514.736406667245
Iteration 6200: Loss = -11514.736199523433
Iteration 6300: Loss = -11514.736077292338
Iteration 6400: Loss = -11514.735925715037
Iteration 6500: Loss = -11514.7357219446
Iteration 6600: Loss = -11514.551779637695
Iteration 6700: Loss = -11514.528403765806
Iteration 6800: Loss = -11514.520843338933
Iteration 6900: Loss = -11514.51903931652
Iteration 7000: Loss = -11514.518893781496
Iteration 7100: Loss = -11514.534014125855
1
Iteration 7200: Loss = -11514.518669185432
Iteration 7300: Loss = -11514.518195248978
Iteration 7400: Loss = -11510.560942451008
Iteration 7500: Loss = -11510.550816706635
Iteration 7600: Loss = -11510.550321328317
Iteration 7700: Loss = -11510.549975601893
Iteration 7800: Loss = -11510.549826750652
Iteration 7900: Loss = -11510.550404322777
1
Iteration 8000: Loss = -11510.549608367855
Iteration 8100: Loss = -11510.552330283292
1
Iteration 8200: Loss = -11510.548473705017
Iteration 8300: Loss = -11510.546972408685
Iteration 8400: Loss = -11510.546911263793
Iteration 8500: Loss = -11510.546390866117
Iteration 8600: Loss = -11510.541243074496
Iteration 8700: Loss = -11510.541160316567
Iteration 8800: Loss = -11510.541172272577
1
Iteration 8900: Loss = -11510.54227962266
2
Iteration 9000: Loss = -11510.541079340464
Iteration 9100: Loss = -11510.541064986592
Iteration 9200: Loss = -11510.54283351798
1
Iteration 9300: Loss = -11510.541041495197
Iteration 9400: Loss = -11510.541005157007
Iteration 9500: Loss = -11510.541017776288
1
Iteration 9600: Loss = -11510.540948651884
Iteration 9700: Loss = -11510.540869766086
Iteration 9800: Loss = -11510.539431116593
Iteration 9900: Loss = -11510.53896217607
Iteration 10000: Loss = -11510.733635723489
1
Iteration 10100: Loss = -11510.538878455805
Iteration 10200: Loss = -11510.53851400524
Iteration 10300: Loss = -11510.538257989816
Iteration 10400: Loss = -11510.537892609715
Iteration 10500: Loss = -11510.538650561686
1
Iteration 10600: Loss = -11510.537759134539
Iteration 10700: Loss = -11510.537501666233
Iteration 10800: Loss = -11510.812283009021
1
Iteration 10900: Loss = -11510.536210112728
Iteration 11000: Loss = -11510.566501213587
1
Iteration 11100: Loss = -11510.536228949213
2
Iteration 11200: Loss = -11510.536175783122
Iteration 11300: Loss = -11510.536644248854
1
Iteration 11400: Loss = -11510.536278319309
2
Iteration 11500: Loss = -11510.53670582641
3
Iteration 11600: Loss = -11510.5362981546
4
Iteration 11700: Loss = -11510.536701601417
5
Stopping early at iteration 11700 due to no improvement.
pi: tensor([[0.2854, 0.7146],
        [0.8484, 0.1516]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3892, 0.6108], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2385, 0.1056],
         [0.5232, 0.2754]],

        [[0.5091, 0.1077],
         [0.5165, 0.6872]],

        [[0.7267, 0.0932],
         [0.5962, 0.6483]],

        [[0.6806, 0.1136],
         [0.6713, 0.5786]],

        [[0.5821, 0.1019],
         [0.6846, 0.5090]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7369243452069851
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 9
Adjusted Rand Index: 0.6691937320900481
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8073834974290766
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.048279046059688926
Average Adjusted Rand Index: 0.8346996566725118
11476.865171555504
[0.9214379584608567, 0.9214379584608567, 0.9214379584608567, 0.048279046059688926] [0.9216009523870667, 0.9216009523870667, 0.9216009523870667, 0.8346996566725118] [11449.458020649461, 11449.45174543618, 11449.443874111628, 11510.536701601417]
-----------------------------------------------------------------------------------------
This iteration is 3
True Objective function: Loss = -11455.920568808191
Iteration 0: Loss = -11838.805014150872
Iteration 10: Loss = -11523.761757178037
Iteration 20: Loss = -11444.52545833015
Iteration 30: Loss = -11444.168884687382
Iteration 40: Loss = -11444.1220364913
Iteration 50: Loss = -11444.116688310683
Iteration 60: Loss = -11444.116091781289
Iteration 70: Loss = -11444.116051653098
Iteration 80: Loss = -11444.116054711989
1
Iteration 90: Loss = -11444.116054480084
2
Iteration 100: Loss = -11444.11605029171
Iteration 110: Loss = -11444.116050297385
1
Iteration 120: Loss = -11444.116050297385
2
Iteration 130: Loss = -11444.116050297385
3
Stopping early at iteration 130 due to no improvement.
pi: tensor([[0.6982, 0.3018],
        [0.2699, 0.7301]], dtype=torch.float64)
alpha: tensor([0.4565, 0.5435])
beta: tensor([[[0.2052, 0.1106],
         [0.7175, 0.2829]],

        [[0.7274, 0.1059],
         [0.0116, 0.1021]],

        [[0.6805, 0.1003],
         [0.0987, 0.1168]],

        [[0.0696, 0.1082],
         [0.0349, 0.3757]],

        [[0.0147, 0.1032],
         [0.7828, 0.9255]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9524797973942409
Average Adjusted Rand Index: 0.9531313131313132
Iteration 0: Loss = -11696.031699862708
Iteration 10: Loss = -11695.847177283764
Iteration 20: Loss = -11694.376056726594
Iteration 30: Loss = -11692.212473131702
Iteration 40: Loss = -11689.489649951174
Iteration 50: Loss = -11689.159503468458
Iteration 60: Loss = -11689.175698075966
1
Iteration 70: Loss = -11689.180491792898
2
Iteration 80: Loss = -11689.181651984016
3
Stopping early at iteration 80 due to no improvement.
pi: tensor([[1.0000e+00, 1.4020e-06],
        [7.7666e-01, 2.2334e-01]], dtype=torch.float64)
alpha: tensor([0.9877, 0.0123])
beta: tensor([[[1.7610e-01, 3.0824e-01],
         [6.4524e-01, 5.1648e-04]],

        [[5.2191e-01, 3.1233e-01],
         [1.5666e-01, 2.0356e-01]],

        [[4.9402e-01, 2.2087e-01],
         [1.9978e-01, 8.1100e-01]],

        [[3.8740e-01, 1.4008e-01],
         [5.9640e-01, 1.7645e-01]],

        [[6.4530e-01, 2.4004e-01],
         [8.6616e-01, 3.7781e-01]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.011562904917628186
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0020078049292671526
Average Adjusted Rand Index: -0.002312580983525637
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20642.385813918114
Iteration 100: Loss = -11692.18366230436
Iteration 200: Loss = -11685.770481705564
Iteration 300: Loss = -11683.331542420707
Iteration 400: Loss = -11639.862289739047
Iteration 500: Loss = -11639.475428475545
Iteration 600: Loss = -11639.391056994884
Iteration 700: Loss = -11639.339892153817
Iteration 800: Loss = -11639.305877413588
Iteration 900: Loss = -11639.282713936598
Iteration 1000: Loss = -11639.266660371117
Iteration 1100: Loss = -11639.25542846432
Iteration 1200: Loss = -11639.24739391083
Iteration 1300: Loss = -11639.241605132302
Iteration 1400: Loss = -11639.237304676362
Iteration 1500: Loss = -11639.234091678733
Iteration 1600: Loss = -11639.231587300388
Iteration 1700: Loss = -11639.229582947068
Iteration 1800: Loss = -11639.227872723273
Iteration 1900: Loss = -11639.226323692277
Iteration 2000: Loss = -11639.224607230888
Iteration 2100: Loss = -11639.222503924166
Iteration 2200: Loss = -11639.219631941001
Iteration 2300: Loss = -11639.215272773363
Iteration 2400: Loss = -11639.208797485815
Iteration 2500: Loss = -11639.19988580845
Iteration 2600: Loss = -11639.18919594027
Iteration 2700: Loss = -11639.178104751598
Iteration 2800: Loss = -11639.168238008799
Iteration 2900: Loss = -11639.160290690503
Iteration 3000: Loss = -11639.153964965562
Iteration 3100: Loss = -11639.149036123064
Iteration 3200: Loss = -11639.145100685997
Iteration 3300: Loss = -11639.141915685213
Iteration 3400: Loss = -11639.139280571466
Iteration 3500: Loss = -11639.137111275993
Iteration 3600: Loss = -11639.135237314158
Iteration 3700: Loss = -11639.133620601277
Iteration 3800: Loss = -11639.132296487576
Iteration 3900: Loss = -11639.131016161386
Iteration 4000: Loss = -11639.129976154278
Iteration 4100: Loss = -11639.129020387252
Iteration 4200: Loss = -11639.12815725931
Iteration 4300: Loss = -11639.12742327484
Iteration 4400: Loss = -11639.126730951115
Iteration 4500: Loss = -11639.126101223063
Iteration 4600: Loss = -11639.125576321492
Iteration 4700: Loss = -11639.125039251161
Iteration 4800: Loss = -11639.12454652018
Iteration 4900: Loss = -11639.12411131206
Iteration 5000: Loss = -11639.123719189396
Iteration 5100: Loss = -11639.123376420557
Iteration 5200: Loss = -11639.123079229596
Iteration 5300: Loss = -11639.131398117885
1
Iteration 5400: Loss = -11639.12244693855
Iteration 5500: Loss = -11639.122191420469
Iteration 5600: Loss = -11639.12191608388
Iteration 5700: Loss = -11639.121672162539
Iteration 5800: Loss = -11639.121468438963
Iteration 5900: Loss = -11639.121854914942
1
Iteration 6000: Loss = -11639.121050252605
Iteration 6100: Loss = -11639.120874026376
Iteration 6200: Loss = -11639.121518036143
1
Iteration 6300: Loss = -11639.120530719061
Iteration 6400: Loss = -11639.120371854018
Iteration 6500: Loss = -11639.121162820904
1
Iteration 6600: Loss = -11639.120042147564
Iteration 6700: Loss = -11639.119958948888
Iteration 6800: Loss = -11639.11986171402
Iteration 6900: Loss = -11639.119719186736
Iteration 7000: Loss = -11639.11963302995
Iteration 7100: Loss = -11639.12053239451
1
Iteration 7200: Loss = -11639.119389622741
Iteration 7300: Loss = -11639.119341563042
Iteration 7400: Loss = -11639.119649273503
1
Iteration 7500: Loss = -11639.119141390114
Iteration 7600: Loss = -11639.12077973342
1
Iteration 7700: Loss = -11639.119001587544
Iteration 7800: Loss = -11639.157395185824
1
Iteration 7900: Loss = -11639.11882852248
Iteration 8000: Loss = -11639.1187845965
Iteration 8100: Loss = -11639.179890061689
1
Iteration 8200: Loss = -11639.11867217383
Iteration 8300: Loss = -11639.118602611356
Iteration 8400: Loss = -11639.544144273083
1
Iteration 8500: Loss = -11639.118526415203
Iteration 8600: Loss = -11639.11844192993
Iteration 8700: Loss = -11639.121201246666
1
Iteration 8800: Loss = -11639.11843150127
Iteration 8900: Loss = -11639.118350858042
Iteration 9000: Loss = -11639.118301942357
Iteration 9100: Loss = -11639.148248752806
1
Iteration 9200: Loss = -11639.118228314233
Iteration 9300: Loss = -11639.118182572607
Iteration 9400: Loss = -11639.11816568768
Iteration 9500: Loss = -11639.11849988766
1
Iteration 9600: Loss = -11639.118103678566
Iteration 9700: Loss = -11639.118087226181
Iteration 9800: Loss = -11639.231757797757
1
Iteration 9900: Loss = -11639.118006354945
Iteration 10000: Loss = -11639.117960602645
Iteration 10100: Loss = -11639.312346971466
1
Iteration 10200: Loss = -11639.117944708027
Iteration 10300: Loss = -11639.117907571117
Iteration 10400: Loss = -11639.125475868044
1
Iteration 10500: Loss = -11639.117877073357
Iteration 10600: Loss = -11639.117899362116
1
Iteration 10700: Loss = -11639.122934799087
2
Iteration 10800: Loss = -11639.117809819703
Iteration 10900: Loss = -11639.14164285663
1
Iteration 11000: Loss = -11639.117842838057
2
Iteration 11100: Loss = -11639.117860207507
3
Iteration 11200: Loss = -11639.117845934792
4
Iteration 11300: Loss = -11639.190529837515
5
Stopping early at iteration 11300 due to no improvement.
pi: tensor([[9.9993e-01, 6.6814e-05],
        [9.8256e-01, 1.7444e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4312, 0.5688], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1775, 0.1123],
         [0.7032, 0.3083]],

        [[0.5604, 0.2946],
         [0.5906, 0.6768]],

        [[0.7287, 0.2222],
         [0.6199, 0.6351]],

        [[0.5516, 0.2410],
         [0.6955, 0.6525]],

        [[0.6826, 0.2245],
         [0.6517, 0.6848]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.020197243032895485
Average Adjusted Rand Index: 0.2
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23890.829098893417
Iteration 100: Loss = -11694.46895995179
Iteration 200: Loss = -11684.877353837575
Iteration 300: Loss = -11683.4435847823
Iteration 400: Loss = -11682.212283806623
Iteration 500: Loss = -11663.460378055674
Iteration 600: Loss = -11640.060852007515
Iteration 700: Loss = -11639.542324171241
Iteration 800: Loss = -11639.315132194473
Iteration 900: Loss = -11639.13100639265
Iteration 1000: Loss = -11638.555434574559
Iteration 1100: Loss = -11637.16719156494
Iteration 1200: Loss = -11630.87062647948
Iteration 1300: Loss = -11501.585972672296
Iteration 1400: Loss = -11475.172165356837
Iteration 1500: Loss = -11474.762420361449
Iteration 1600: Loss = -11470.698293401023
Iteration 1700: Loss = -11470.539705108231
Iteration 1800: Loss = -11470.411528464872
Iteration 1900: Loss = -11469.914757435583
Iteration 2000: Loss = -11469.871804566626
Iteration 2100: Loss = -11467.894700615181
Iteration 2200: Loss = -11467.877831963402
Iteration 2300: Loss = -11467.86935929732
Iteration 2400: Loss = -11467.862496570395
Iteration 2500: Loss = -11467.854130863616
Iteration 2600: Loss = -11467.84624747186
Iteration 2700: Loss = -11467.857454664629
1
Iteration 2800: Loss = -11467.832948861467
Iteration 2900: Loss = -11467.82228558719
Iteration 3000: Loss = -11467.734070772309
Iteration 3100: Loss = -11467.721653104947
Iteration 3200: Loss = -11467.507294275689
Iteration 3300: Loss = -11467.453630566266
Iteration 3400: Loss = -11467.451222725938
Iteration 3500: Loss = -11467.444259715041
Iteration 3600: Loss = -11467.420629003183
Iteration 3700: Loss = -11467.41821820357
Iteration 3800: Loss = -11467.412678356737
Iteration 3900: Loss = -11467.410661972832
Iteration 4000: Loss = -11467.411599471452
1
Iteration 4100: Loss = -11467.409160804838
Iteration 4200: Loss = -11467.40781067999
Iteration 4300: Loss = -11467.391583720284
Iteration 4400: Loss = -11466.071140542519
Iteration 4500: Loss = -11466.053250227505
Iteration 4600: Loss = -11466.052530511284
Iteration 4700: Loss = -11466.051967475107
Iteration 4800: Loss = -11466.051512098622
Iteration 4900: Loss = -11466.051211205639
Iteration 5000: Loss = -11466.050583822229
Iteration 5100: Loss = -11466.049655630703
Iteration 5200: Loss = -11466.04806946208
Iteration 5300: Loss = -11466.050790854479
1
Iteration 5400: Loss = -11466.047649826001
Iteration 5500: Loss = -11466.046935997272
Iteration 5600: Loss = -11466.04621424021
Iteration 5700: Loss = -11466.045947362762
Iteration 5800: Loss = -11466.045870669273
Iteration 5900: Loss = -11466.050605174907
1
Iteration 6000: Loss = -11466.045412503816
Iteration 6100: Loss = -11466.045041484902
Iteration 6200: Loss = -11466.044821536196
Iteration 6300: Loss = -11466.046637073341
1
Iteration 6400: Loss = -11466.042363900862
Iteration 6500: Loss = -11466.037340016266
Iteration 6600: Loss = -11466.036956695194
Iteration 6700: Loss = -11466.036692165468
Iteration 6800: Loss = -11466.036390382373
Iteration 6900: Loss = -11466.035116945863
Iteration 7000: Loss = -11465.07975186789
Iteration 7100: Loss = -11465.074029788295
Iteration 7200: Loss = -11465.073882581555
Iteration 7300: Loss = -11465.072011047228
Iteration 7400: Loss = -11464.994079921791
Iteration 7500: Loss = -11464.992807938335
Iteration 7600: Loss = -11464.992755500412
Iteration 7700: Loss = -11465.02312894081
1
Iteration 7800: Loss = -11464.986054325007
Iteration 7900: Loss = -11464.984490800456
Iteration 8000: Loss = -11464.983034179155
Iteration 8100: Loss = -11464.985269158107
1
Iteration 8200: Loss = -11464.982811231508
Iteration 8300: Loss = -11464.98241308788
Iteration 8400: Loss = -11464.98177622085
Iteration 8500: Loss = -11464.968688033012
Iteration 8600: Loss = -11464.968460412461
Iteration 8700: Loss = -11465.132341863002
1
Iteration 8800: Loss = -11464.968267965804
Iteration 8900: Loss = -11464.968153156231
Iteration 9000: Loss = -11464.972809771236
1
Iteration 9100: Loss = -11464.968091900237
Iteration 9200: Loss = -11464.968036519083
Iteration 9300: Loss = -11464.97709718872
1
Iteration 9400: Loss = -11464.967992892334
Iteration 9500: Loss = -11464.967932593641
Iteration 9600: Loss = -11465.00153748122
1
Iteration 9700: Loss = -11464.965951689133
Iteration 9800: Loss = -11464.973870379348
1
Iteration 9900: Loss = -11464.966944136995
2
Iteration 10000: Loss = -11464.961806539952
Iteration 10100: Loss = -11464.97248565119
1
Iteration 10200: Loss = -11464.96550318114
2
Iteration 10300: Loss = -11464.961155067615
Iteration 10400: Loss = -11464.969364574332
1
Iteration 10500: Loss = -11464.835293500717
Iteration 10600: Loss = -11464.83321167516
Iteration 10700: Loss = -11464.833493171009
1
Iteration 10800: Loss = -11464.832051455738
Iteration 10900: Loss = -11464.829200063843
Iteration 11000: Loss = -11464.82939507305
1
Iteration 11100: Loss = -11464.913266184543
2
Iteration 11200: Loss = -11464.828881827236
Iteration 11300: Loss = -11464.820128791458
Iteration 11400: Loss = -11464.820779332316
1
Iteration 11500: Loss = -11464.819853509347
Iteration 11600: Loss = -11464.822050837638
1
Iteration 11700: Loss = -11464.817343459374
Iteration 11800: Loss = -11464.875771726036
1
Iteration 11900: Loss = -11464.817141513748
Iteration 12000: Loss = -11464.856556631203
1
Iteration 12100: Loss = -11464.817126817396
Iteration 12200: Loss = -11464.817756087203
1
Iteration 12300: Loss = -11464.81690668393
Iteration 12400: Loss = -11464.817075557647
1
Iteration 12500: Loss = -11464.858935374246
2
Iteration 12600: Loss = -11464.816344681794
Iteration 12700: Loss = -11464.81755245325
1
Iteration 12800: Loss = -11464.816468070676
2
Iteration 12900: Loss = -11464.816177347157
Iteration 13000: Loss = -11464.845615161279
1
Iteration 13100: Loss = -11464.816168178839
Iteration 13200: Loss = -11464.815882484738
Iteration 13300: Loss = -11464.9034036759
1
Iteration 13400: Loss = -11464.81269514344
Iteration 13500: Loss = -11464.817053409686
1
Iteration 13600: Loss = -11464.954701967376
2
Iteration 13700: Loss = -11464.809013123675
Iteration 13800: Loss = -11464.810755216118
1
Iteration 13900: Loss = -11464.810542547768
2
Iteration 14000: Loss = -11464.830028240296
3
Iteration 14100: Loss = -11464.807579228693
Iteration 14200: Loss = -11464.805912860342
Iteration 14300: Loss = -11464.992726193563
1
Iteration 14400: Loss = -11464.805913976104
2
Iteration 14500: Loss = -11464.807083896743
3
Iteration 14600: Loss = -11465.036415653862
4
Iteration 14700: Loss = -11464.805872949588
Iteration 14800: Loss = -11464.808631663136
1
Iteration 14900: Loss = -11464.81286119222
2
Iteration 15000: Loss = -11464.805912093447
3
Iteration 15100: Loss = -11464.814965947859
4
Iteration 15200: Loss = -11464.810899194952
5
Stopping early at iteration 15200 due to no improvement.
pi: tensor([[0.3483, 0.6517],
        [0.8171, 0.1829]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4125, 0.5875], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2320, 0.1094],
         [0.6198, 0.2795]],

        [[0.6064, 0.1039],
         [0.6720, 0.5098]],

        [[0.5623, 0.0997],
         [0.6942, 0.6790]],

        [[0.7277, 0.0999],
         [0.6076, 0.6770]],

        [[0.5525, 0.1031],
         [0.7075, 0.5338]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 15
Adjusted Rand Index: 0.4852686308492201
time is 2
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 10
Adjusted Rand Index: 0.6363636363636364
time is 4
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.08110063852918034
Average Adjusted Rand Index: 0.8004868470114974
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22399.39408920078
Iteration 100: Loss = -11686.169041539522
Iteration 200: Loss = -11684.82582310153
Iteration 300: Loss = -11674.40118792202
Iteration 400: Loss = -11639.412345496505
Iteration 500: Loss = -11639.233772495416
Iteration 600: Loss = -11639.115588739394
Iteration 700: Loss = -11639.03532361583
Iteration 800: Loss = -11638.983772916334
Iteration 900: Loss = -11638.950913413362
Iteration 1000: Loss = -11638.927785453123
Iteration 1100: Loss = -11638.897283705217
Iteration 1200: Loss = -11637.739622220704
Iteration 1300: Loss = -11632.276604328428
Iteration 1400: Loss = -11477.204877377477
Iteration 1500: Loss = -11475.194070592606
Iteration 1600: Loss = -11475.099948307276
Iteration 1700: Loss = -11475.068974235015
Iteration 1800: Loss = -11475.033972226303
Iteration 1900: Loss = -11475.027712090105
Iteration 2000: Loss = -11473.389597581947
Iteration 2100: Loss = -11473.320884844472
Iteration 2200: Loss = -11473.307074037646
Iteration 2300: Loss = -11473.29978013101
Iteration 2400: Loss = -11473.29129227416
Iteration 2500: Loss = -11470.912244780482
Iteration 2600: Loss = -11470.25484699058
Iteration 2700: Loss = -11470.249730242309
Iteration 2800: Loss = -11470.11514650302
Iteration 2900: Loss = -11470.112382788457
Iteration 3000: Loss = -11470.115882132308
1
Iteration 3100: Loss = -11470.109837150487
Iteration 3200: Loss = -11470.109288448079
Iteration 3300: Loss = -11470.108026507662
Iteration 3400: Loss = -11470.106982191177
Iteration 3500: Loss = -11470.106354690603
Iteration 3600: Loss = -11469.938010935766
Iteration 3700: Loss = -11469.915237698544
Iteration 3800: Loss = -11469.914340064635
Iteration 3900: Loss = -11469.913326386288
Iteration 4000: Loss = -11469.913139530016
Iteration 4100: Loss = -11469.912948323408
Iteration 4200: Loss = -11469.912761712432
Iteration 4300: Loss = -11469.912579074311
Iteration 4400: Loss = -11469.912598735906
1
Iteration 4500: Loss = -11469.916744232953
2
Iteration 4600: Loss = -11469.866799883966
Iteration 4700: Loss = -11469.86585626682
Iteration 4800: Loss = -11469.856551953542
Iteration 4900: Loss = -11469.854429140605
Iteration 5000: Loss = -11469.8512371722
Iteration 5100: Loss = -11469.84379280889
Iteration 5200: Loss = -11469.844409536398
1
Iteration 5300: Loss = -11469.841046504627
Iteration 5400: Loss = -11469.838691343251
Iteration 5500: Loss = -11469.838373995744
Iteration 5600: Loss = -11469.83812378303
Iteration 5700: Loss = -11469.837944235936
Iteration 5800: Loss = -11469.835457656167
Iteration 5900: Loss = -11469.831936962422
Iteration 6000: Loss = -11469.831384411073
Iteration 6100: Loss = -11469.831528314378
1
Iteration 6200: Loss = -11469.831240035177
Iteration 6300: Loss = -11469.847345920056
1
Iteration 6400: Loss = -11469.830976880063
Iteration 6500: Loss = -11469.831112523434
1
Iteration 6600: Loss = -11469.830911814328
Iteration 6700: Loss = -11469.830921990348
1
Iteration 6800: Loss = -11469.830861224302
Iteration 6900: Loss = -11469.831708139807
1
Iteration 7000: Loss = -11469.830933380621
2
Iteration 7100: Loss = -11469.830900406068
3
Iteration 7200: Loss = -11469.830550697481
Iteration 7300: Loss = -11469.830448377756
Iteration 7400: Loss = -11469.830464901901
1
Iteration 7500: Loss = -11469.867585991553
2
Iteration 7600: Loss = -11469.830271748318
Iteration 7700: Loss = -11469.836367391868
1
Iteration 7800: Loss = -11469.829990005011
Iteration 7900: Loss = -11469.833079355221
1
Iteration 8000: Loss = -11469.828279799194
Iteration 8100: Loss = -11469.828180336239
Iteration 8200: Loss = -11469.828059277734
Iteration 8300: Loss = -11469.82798408493
Iteration 8400: Loss = -11469.827725927376
Iteration 8500: Loss = -11469.827971106275
1
Iteration 8600: Loss = -11469.827631433058
Iteration 8700: Loss = -11469.82762541623
Iteration 8800: Loss = -11469.827897842073
1
Iteration 8900: Loss = -11469.827568889077
Iteration 9000: Loss = -11469.817160965422
Iteration 9100: Loss = -11469.548405983254
Iteration 9200: Loss = -11469.53174005791
Iteration 9300: Loss = -11469.509895380637
Iteration 9400: Loss = -11469.511044847506
1
Iteration 9500: Loss = -11469.509883823335
Iteration 9600: Loss = -11469.510944770653
1
Iteration 9700: Loss = -11469.509857096758
Iteration 9800: Loss = -11469.889210502617
1
Iteration 9900: Loss = -11469.50983803792
Iteration 10000: Loss = -11469.516686698493
1
Iteration 10100: Loss = -11469.509813040046
Iteration 10200: Loss = -11469.545276797664
1
Iteration 10300: Loss = -11469.508634638649
Iteration 10400: Loss = -11469.50912537818
1
Iteration 10500: Loss = -11469.508578639061
Iteration 10600: Loss = -11469.508511914855
Iteration 10700: Loss = -11469.509889063898
1
Iteration 10800: Loss = -11469.508436342725
Iteration 10900: Loss = -11469.535178216809
1
Iteration 11000: Loss = -11469.499314597233
Iteration 11100: Loss = -11469.510071977493
1
Iteration 11200: Loss = -11469.496503083872
Iteration 11300: Loss = -11469.498221078553
1
Iteration 11400: Loss = -11469.492015090302
Iteration 11500: Loss = -11469.49668762123
1
Iteration 11600: Loss = -11469.49182335308
Iteration 11700: Loss = -11469.510473038492
1
Iteration 11800: Loss = -11469.510153227955
2
Iteration 11900: Loss = -11468.54197456715
Iteration 12000: Loss = -11467.906686226874
Iteration 12100: Loss = -11467.904041237001
Iteration 12200: Loss = -11465.1091281319
Iteration 12300: Loss = -11465.098978463991
Iteration 12400: Loss = -11465.086656825173
Iteration 12500: Loss = -11465.081277638124
Iteration 12600: Loss = -11465.082378933224
1
Iteration 12700: Loss = -11465.008333142487
Iteration 12800: Loss = -11464.966010083725
Iteration 12900: Loss = -11465.031520309643
1
Iteration 13000: Loss = -11464.836675923141
Iteration 13100: Loss = -11464.846165405592
1
Iteration 13200: Loss = -11464.84230262244
2
Iteration 13300: Loss = -11464.834750079464
Iteration 13400: Loss = -11464.837276926675
1
Iteration 13500: Loss = -11464.833314963187
Iteration 13600: Loss = -11464.831344899036
Iteration 13700: Loss = -11464.831289257943
Iteration 13800: Loss = -11464.83459272756
1
Iteration 13900: Loss = -11465.130961013421
2
Iteration 14000: Loss = -11464.830703278009
Iteration 14100: Loss = -11464.830764421527
1
Iteration 14200: Loss = -11464.833497341375
2
Iteration 14300: Loss = -11464.890928735078
3
Iteration 14400: Loss = -11464.83007370317
Iteration 14500: Loss = -11464.83466857322
1
Iteration 14600: Loss = -11464.854251582465
2
Iteration 14700: Loss = -11464.843236802788
3
Iteration 14800: Loss = -11464.829464709903
Iteration 14900: Loss = -11464.82994407344
1
Iteration 15000: Loss = -11464.830492973688
2
Iteration 15100: Loss = -11464.990387970003
3
Iteration 15200: Loss = -11464.82888764464
Iteration 15300: Loss = -11464.828580124971
Iteration 15400: Loss = -11464.842368868289
1
Iteration 15500: Loss = -11464.828759113441
2
Iteration 15600: Loss = -11464.82851657527
Iteration 15700: Loss = -11464.828490793529
Iteration 15800: Loss = -11464.913496075522
1
Iteration 15900: Loss = -11464.828405392987
Iteration 16000: Loss = -11464.8290928765
1
Iteration 16100: Loss = -11464.826444981878
Iteration 16200: Loss = -11464.829249188502
1
Iteration 16300: Loss = -11464.838283703355
2
Iteration 16400: Loss = -11464.826598187985
3
Iteration 16500: Loss = -11464.82653676455
4
Iteration 16600: Loss = -11464.828218172774
5
Stopping early at iteration 16600 due to no improvement.
pi: tensor([[0.3475, 0.6525],
        [0.8169, 0.1831]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4119, 0.5881], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2322, 0.1096],
         [0.6716, 0.2792]],

        [[0.7130, 0.1040],
         [0.5359, 0.5876]],

        [[0.5336, 0.0996],
         [0.5660, 0.7013]],

        [[0.5860, 0.1001],
         [0.6723, 0.5121]],

        [[0.6361, 0.1028],
         [0.6010, 0.5735]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 15
Adjusted Rand Index: 0.4852686308492201
time is 2
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 10
Adjusted Rand Index: 0.6363636363636364
time is 4
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.08110063852918034
Average Adjusted Rand Index: 0.8004868470114974
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23400.421510858625
Iteration 100: Loss = -11645.968636649573
Iteration 200: Loss = -11639.83825660192
Iteration 300: Loss = -11639.668129398422
Iteration 400: Loss = -11639.530867910324
Iteration 500: Loss = -11639.342260830383
Iteration 600: Loss = -11639.120287127651
Iteration 700: Loss = -11638.878142714251
Iteration 800: Loss = -11636.597646639733
Iteration 900: Loss = -11636.283884364628
Iteration 1000: Loss = -11636.207994692888
Iteration 1100: Loss = -11636.185510242058
Iteration 1200: Loss = -11636.175368060514
Iteration 1300: Loss = -11636.165682741212
Iteration 1400: Loss = -11636.159906578307
Iteration 1500: Loss = -11636.15628373482
Iteration 1600: Loss = -11636.153512785417
Iteration 1700: Loss = -11636.151242743703
Iteration 1800: Loss = -11636.149398087884
Iteration 1900: Loss = -11636.147724395234
Iteration 2000: Loss = -11636.146117915981
Iteration 2100: Loss = -11636.144687842962
Iteration 2200: Loss = -11636.143537210499
Iteration 2300: Loss = -11636.14265707923
Iteration 2400: Loss = -11636.141862409448
Iteration 2500: Loss = -11636.141237050248
Iteration 2600: Loss = -11636.140647527027
Iteration 2700: Loss = -11636.140063629797
Iteration 2800: Loss = -11636.139575901076
Iteration 2900: Loss = -11636.13917484761
Iteration 3000: Loss = -11636.1387635757
Iteration 3100: Loss = -11636.13844506173
Iteration 3200: Loss = -11636.138132341217
Iteration 3300: Loss = -11636.137854734869
Iteration 3400: Loss = -11636.137607580162
Iteration 3500: Loss = -11636.137378108633
Iteration 3600: Loss = -11636.137100167985
Iteration 3700: Loss = -11636.136982227727
Iteration 3800: Loss = -11636.136727643776
Iteration 3900: Loss = -11636.136744761383
1
Iteration 4000: Loss = -11636.14310660422
2
Iteration 4100: Loss = -11636.136236633694
Iteration 4200: Loss = -11636.136184849527
Iteration 4300: Loss = -11636.135958140725
Iteration 4400: Loss = -11636.136046923628
1
Iteration 4500: Loss = -11636.135690069366
Iteration 4600: Loss = -11636.136062693355
1
Iteration 4700: Loss = -11636.135515663993
Iteration 4800: Loss = -11636.13543490682
Iteration 4900: Loss = -11636.13534157877
Iteration 5000: Loss = -11636.135269927277
Iteration 5100: Loss = -11636.136569991173
1
Iteration 5200: Loss = -11636.135069740969
Iteration 5300: Loss = -11636.135352116158
1
Iteration 5400: Loss = -11636.134962455335
Iteration 5500: Loss = -11636.134995484274
1
Iteration 5600: Loss = -11636.134845483633
Iteration 5700: Loss = -11636.135705148665
1
Iteration 5800: Loss = -11636.13738239622
2
Iteration 5900: Loss = -11636.134768964617
Iteration 6000: Loss = -11636.134817377791
1
Iteration 6100: Loss = -11636.134700366705
Iteration 6200: Loss = -11636.134867879073
1
Iteration 6300: Loss = -11636.135730659724
2
Iteration 6400: Loss = -11636.134707829964
3
Iteration 6500: Loss = -11636.134408620395
Iteration 6600: Loss = -11636.135932195022
1
Iteration 6700: Loss = -11636.134446880724
2
Iteration 6800: Loss = -11636.137416952632
3
Iteration 6900: Loss = -11636.134499268364
4
Iteration 7000: Loss = -11636.136865640345
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[1.4385e-05, 9.9999e-01],
        [7.9870e-02, 9.2013e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5681, 0.4319], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3087, 0.1121],
         [0.6913, 0.1810]],

        [[0.6548, 0.1225],
         [0.5453, 0.7285]],

        [[0.5792, 0.0986],
         [0.7288, 0.5491]],

        [[0.6969, 0.2092],
         [0.5094, 0.6973]],

        [[0.6914, 0.2083],
         [0.5655, 0.6578]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.02869987632888049
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.010377864674601068
Average Adjusted Rand Index: 0.20573997526577611
11455.920568808191
[0.020197243032895485, 0.08110063852918034, 0.08110063852918034, 0.010377864674601068] [0.2, 0.8004868470114974, 0.8004868470114974, 0.20573997526577611] [11639.190529837515, 11464.810899194952, 11464.828218172774, 11636.136865640345]
-----------------------------------------------------------------------------------------
This iteration is 4
True Objective function: Loss = -11356.587950948497
Iteration 0: Loss = -11604.85794565901
Iteration 10: Loss = -11604.857724210178
Iteration 20: Loss = -11604.841272735544
Iteration 30: Loss = -11602.068872326427
Iteration 40: Loss = -11583.940875127826
Iteration 50: Loss = -11581.889040983886
Iteration 60: Loss = -11580.93864719255
Iteration 70: Loss = -11580.732268908818
Iteration 80: Loss = -11580.722422354336
Iteration 90: Loss = -11580.721919753305
Iteration 100: Loss = -11580.72189405213
Iteration 110: Loss = -11580.721893922842
Iteration 120: Loss = -11580.721897799136
1
Iteration 130: Loss = -11580.721912266807
2
Iteration 140: Loss = -11580.721899080912
3
Stopping early at iteration 140 due to no improvement.
pi: tensor([[0.9176, 0.0824],
        [0.9870, 0.0130]], dtype=torch.float64)
alpha: tensor([0.9179, 0.0821])
beta: tensor([[[0.1743, 0.2427],
         [0.3557, 0.3763]],

        [[0.7889, 0.2244],
         [0.5812, 0.9770]],

        [[0.4339, 0.0870],
         [0.4614, 0.0372]],

        [[0.0842, 0.2270],
         [0.5600, 0.3704]],

        [[0.9692, 0.0783],
         [0.8199, 0.1839]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0028959952356207414
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0037746410354473374
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.020872495483242322
Global Adjusted Rand Index: 0.00022137629857025515
Average Adjusted Rand Index: 0.0018115008396550395
pi: tensor([[0.0010, 0.9990],
        [0.1836, 0.8164]], dtype=torch.float64)
alpha: tensor([0.1242, 0.8758])
beta: tensor([[[0.1275,    nan],
         [0.8643, 0.1809]],

        [[0.6427, 0.2200],
         [0.0150, 0.6134]],

        [[0.6710, 0.1786],
         [0.0043, 0.9776]],

        [[0.3400,    nan],
         [0.7949, 0.3618]],

        [[0.0820, 0.2900],
         [0.5861, 0.6063]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.048319907872549195
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.005369370103748339
Average Adjusted Rand Index: 0.00966398157450984
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22214.533554211
Iteration 100: Loss = -11601.874351627512
Iteration 200: Loss = -11594.603517857338
Iteration 300: Loss = -11586.795570121078
Iteration 400: Loss = -11575.496648021424
Iteration 500: Loss = -11551.72957425219
Iteration 600: Loss = -11545.858032628475
Iteration 700: Loss = -11542.60873583406
Iteration 800: Loss = -11542.051079892386
Iteration 900: Loss = -11529.790768077151
Iteration 1000: Loss = -11527.995066355676
Iteration 1100: Loss = -11517.369359649423
Iteration 1200: Loss = -11515.645549864992
Iteration 1300: Loss = -11366.009426729426
Iteration 1400: Loss = -11332.6208676499
Iteration 1500: Loss = -11323.51386295172
Iteration 1600: Loss = -11315.37140706414
Iteration 1700: Loss = -11315.08779713438
Iteration 1800: Loss = -11315.06950741243
Iteration 1900: Loss = -11315.063701244851
Iteration 2000: Loss = -11315.055387111188
Iteration 2100: Loss = -11315.043209025205
Iteration 2200: Loss = -11315.039141332183
Iteration 2300: Loss = -11315.035893359567
Iteration 2400: Loss = -11314.992166458595
Iteration 2500: Loss = -11314.900822119516
Iteration 2600: Loss = -11314.895654881291
Iteration 2700: Loss = -11314.894960074938
Iteration 2800: Loss = -11314.894320738376
Iteration 2900: Loss = -11314.893825853056
Iteration 3000: Loss = -11314.89323915467
Iteration 3100: Loss = -11314.893445258078
1
Iteration 3200: Loss = -11314.891896519413
Iteration 3300: Loss = -11314.079261247993
Iteration 3400: Loss = -11314.077874487206
Iteration 3500: Loss = -11314.071601272759
Iteration 3600: Loss = -11314.07058304384
Iteration 3700: Loss = -11314.077531358646
1
Iteration 3800: Loss = -11314.069971981615
Iteration 3900: Loss = -11314.070012227596
1
Iteration 4000: Loss = -11314.069321915924
Iteration 4100: Loss = -11314.068701442045
Iteration 4200: Loss = -11314.067314548622
Iteration 4300: Loss = -11314.061582887918
Iteration 4400: Loss = -11314.060647017352
Iteration 4500: Loss = -11314.057638261133
Iteration 4600: Loss = -11314.056434366466
Iteration 4700: Loss = -11314.056299894133
Iteration 4800: Loss = -11314.056007592255
Iteration 4900: Loss = -11314.032019851113
Iteration 5000: Loss = -11314.03194438466
Iteration 5100: Loss = -11314.03159075003
Iteration 5200: Loss = -11314.031382354618
Iteration 5300: Loss = -11314.031268129575
Iteration 5400: Loss = -11314.049675825298
1
Iteration 5500: Loss = -11314.030867521065
Iteration 5600: Loss = -11314.031692099172
1
Iteration 5700: Loss = -11314.030691640746
Iteration 5800: Loss = -11314.03065310735
Iteration 5900: Loss = -11314.030540396208
Iteration 6000: Loss = -11314.030434961749
Iteration 6100: Loss = -11314.031556094229
1
Iteration 6200: Loss = -11314.030074975333
Iteration 6300: Loss = -11314.030102398709
1
Iteration 6400: Loss = -11314.030126758176
2
Iteration 6500: Loss = -11314.029339507657
Iteration 6600: Loss = -11314.029036192871
Iteration 6700: Loss = -11314.028857429677
Iteration 6800: Loss = -11314.034957302647
1
Iteration 6900: Loss = -11314.028785566446
Iteration 7000: Loss = -11314.029168004923
1
Iteration 7100: Loss = -11314.02867137838
Iteration 7200: Loss = -11314.028770500938
1
Iteration 7300: Loss = -11314.030528557307
2
Iteration 7400: Loss = -11314.02854036951
Iteration 7500: Loss = -11314.028486833999
Iteration 7600: Loss = -11314.02864927618
1
Iteration 7700: Loss = -11314.030911032427
2
Iteration 7800: Loss = -11314.028349863962
Iteration 7900: Loss = -11314.02824072176
Iteration 8000: Loss = -11314.028518368266
1
Iteration 8100: Loss = -11314.032284580013
2
Iteration 8200: Loss = -11314.079685138842
3
Iteration 8300: Loss = -11314.02804321354
Iteration 8400: Loss = -11314.028206357341
1
Iteration 8500: Loss = -11314.037544159919
2
Iteration 8600: Loss = -11314.02794020045
Iteration 8700: Loss = -11314.105237538333
1
Iteration 8800: Loss = -11314.027873377723
Iteration 8900: Loss = -11314.028573163487
1
Iteration 9000: Loss = -11314.029604002948
2
Iteration 9100: Loss = -11314.027925660495
3
Iteration 9200: Loss = -11314.03034585036
4
Iteration 9300: Loss = -11314.040965628345
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.8024, 0.1976],
        [0.2117, 0.7883]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4792, 0.5208], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1936, 0.1072],
         [0.6906, 0.3079]],

        [[0.5862, 0.1088],
         [0.5277, 0.6459]],

        [[0.5336, 0.0997],
         [0.7296, 0.5983]],

        [[0.5279, 0.1165],
         [0.6313, 0.6285]],

        [[0.5457, 0.1073],
         [0.5019, 0.5541]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369810324672876
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
Global Adjusted Rand Index: 0.8758473162609216
Average Adjusted Rand Index: 0.8773332854685482
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24913.968253239847
Iteration 100: Loss = -11603.394866916531
Iteration 200: Loss = -11595.573273098287
Iteration 300: Loss = -11527.091311525524
Iteration 400: Loss = -11415.231007487553
Iteration 500: Loss = -11338.81495938568
Iteration 600: Loss = -11328.614416651471
Iteration 700: Loss = -11324.251841306363
Iteration 800: Loss = -11320.314804573385
Iteration 900: Loss = -11320.199080746994
Iteration 1000: Loss = -11318.158528932734
Iteration 1100: Loss = -11316.918452161086
Iteration 1200: Loss = -11316.86539783865
Iteration 1300: Loss = -11316.777861962593
Iteration 1400: Loss = -11316.762354902356
Iteration 1500: Loss = -11316.746441990621
Iteration 1600: Loss = -11316.661292972567
Iteration 1700: Loss = -11316.624892882191
Iteration 1800: Loss = -11316.617726892166
Iteration 1900: Loss = -11316.61208825722
Iteration 2000: Loss = -11316.606276514187
Iteration 2100: Loss = -11316.573069590506
Iteration 2200: Loss = -11314.66210794423
Iteration 2300: Loss = -11314.53140781703
Iteration 2400: Loss = -11314.528894133238
Iteration 2500: Loss = -11314.526561717938
Iteration 2600: Loss = -11314.524234795314
Iteration 2700: Loss = -11314.520384876716
Iteration 2800: Loss = -11314.475709677672
Iteration 2900: Loss = -11314.467049920506
Iteration 3000: Loss = -11314.465237987428
Iteration 3100: Loss = -11314.463838149813
Iteration 3200: Loss = -11314.461702002518
Iteration 3300: Loss = -11314.268992684307
Iteration 3400: Loss = -11314.26557656268
Iteration 3500: Loss = -11314.264147694164
Iteration 3600: Loss = -11314.2165168799
Iteration 3700: Loss = -11314.186854044698
Iteration 3800: Loss = -11314.187850171304
1
Iteration 3900: Loss = -11314.18516183902
Iteration 4000: Loss = -11314.190766346712
1
Iteration 4100: Loss = -11314.180559066474
Iteration 4200: Loss = -11314.180007941972
Iteration 4300: Loss = -11314.1793413731
Iteration 4400: Loss = -11314.16231058667
Iteration 4500: Loss = -11314.149738612876
Iteration 4600: Loss = -11314.14696026151
Iteration 4700: Loss = -11314.14652284231
Iteration 4800: Loss = -11314.146509021606
Iteration 4900: Loss = -11314.145990794777
Iteration 5000: Loss = -11314.145769852132
Iteration 5100: Loss = -11314.145586847173
Iteration 5200: Loss = -11314.145143260983
Iteration 5300: Loss = -11314.147722035275
1
Iteration 5400: Loss = -11314.137751644621
Iteration 5500: Loss = -11314.143162281896
1
Iteration 5600: Loss = -11314.135577195128
Iteration 5700: Loss = -11314.136040937961
1
Iteration 5800: Loss = -11314.134139337719
Iteration 5900: Loss = -11314.132349296558
Iteration 6000: Loss = -11314.130904370277
Iteration 6100: Loss = -11314.136742447623
1
Iteration 6200: Loss = -11314.130430934958
Iteration 6300: Loss = -11314.140965352117
1
Iteration 6400: Loss = -11314.128804344633
Iteration 6500: Loss = -11314.1290711468
1
Iteration 6600: Loss = -11314.129313490781
2
Iteration 6700: Loss = -11314.129694893754
3
Iteration 6800: Loss = -11314.12689963135
Iteration 6900: Loss = -11314.126840412217
Iteration 7000: Loss = -11314.12697579165
1
Iteration 7100: Loss = -11314.12656766346
Iteration 7200: Loss = -11314.12737046208
1
Iteration 7300: Loss = -11314.126281845261
Iteration 7400: Loss = -11314.125882472
Iteration 7500: Loss = -11314.124267553972
Iteration 7600: Loss = -11314.122815480368
Iteration 7700: Loss = -11314.122697101146
Iteration 7800: Loss = -11314.12251859845
Iteration 7900: Loss = -11314.12266414961
1
Iteration 8000: Loss = -11314.12297426165
2
Iteration 8100: Loss = -11314.149291828227
3
Iteration 8200: Loss = -11314.117369392823
Iteration 8300: Loss = -11314.117805060872
1
Iteration 8400: Loss = -11314.118459104338
2
Iteration 8500: Loss = -11314.117254412116
Iteration 8600: Loss = -11314.117718629142
1
Iteration 8700: Loss = -11314.117203172933
Iteration 8800: Loss = -11314.117319422689
1
Iteration 8900: Loss = -11314.117122953603
Iteration 9000: Loss = -11314.126321754726
1
Iteration 9100: Loss = -11314.11589235206
Iteration 9200: Loss = -11314.115878662544
Iteration 9300: Loss = -11314.115911991088
1
Iteration 9400: Loss = -11314.115824758901
Iteration 9500: Loss = -11314.121257305844
1
Iteration 9600: Loss = -11314.115789053825
Iteration 9700: Loss = -11314.200205644414
1
Iteration 9800: Loss = -11314.115752934553
Iteration 9900: Loss = -11314.115294446574
Iteration 10000: Loss = -11314.115310335133
1
Iteration 10100: Loss = -11314.218470229282
2
Iteration 10200: Loss = -11314.117318947146
3
Iteration 10300: Loss = -11314.114935170413
Iteration 10400: Loss = -11314.114631681563
Iteration 10500: Loss = -11314.165095364404
1
Iteration 10600: Loss = -11314.111789050357
Iteration 10700: Loss = -11314.111744500584
Iteration 10800: Loss = -11314.153809480957
1
Iteration 10900: Loss = -11314.11177654108
2
Iteration 11000: Loss = -11314.111708909752
Iteration 11100: Loss = -11314.111948039588
1
Iteration 11200: Loss = -11314.111651056439
Iteration 11300: Loss = -11314.132770453682
1
Iteration 11400: Loss = -11314.087499751397
Iteration 11500: Loss = -11314.087431083211
Iteration 11600: Loss = -11314.087439040895
1
Iteration 11700: Loss = -11314.086818089294
Iteration 11800: Loss = -11314.083919705155
Iteration 11900: Loss = -11314.080886830245
Iteration 12000: Loss = -11314.084865780982
1
Iteration 12100: Loss = -11314.07627215901
Iteration 12200: Loss = -11314.082357496194
1
Iteration 12300: Loss = -11314.0927958468
2
Iteration 12400: Loss = -11314.172066066083
3
Iteration 12500: Loss = -11314.081348453108
4
Iteration 12600: Loss = -11314.12201422203
5
Stopping early at iteration 12600 due to no improvement.
pi: tensor([[0.8022, 0.1978],
        [0.2076, 0.7924]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4737, 0.5263], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1950, 0.1066],
         [0.5261, 0.3063]],

        [[0.6533, 0.1089],
         [0.6316, 0.5175]],

        [[0.6901, 0.0992],
         [0.5098, 0.6656]],

        [[0.6913, 0.1158],
         [0.5388, 0.6081]],

        [[0.6804, 0.1067],
         [0.5808, 0.6653]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369810324672876
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
Global Adjusted Rand Index: 0.8758473162609216
Average Adjusted Rand Index: 0.8773332854685482
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20482.6232861145
Iteration 100: Loss = -11604.975112086218
Iteration 200: Loss = -11600.769124269193
Iteration 300: Loss = -11598.241424792672
Iteration 400: Loss = -11595.937723533509
Iteration 500: Loss = -11580.039492136148
Iteration 600: Loss = -11400.625477574455
Iteration 700: Loss = -11319.015317132675
Iteration 800: Loss = -11316.797848271413
Iteration 900: Loss = -11316.14388157083
Iteration 1000: Loss = -11315.863035506074
Iteration 1100: Loss = -11315.745139941147
Iteration 1200: Loss = -11315.670628463933
Iteration 1300: Loss = -11315.476529609105
Iteration 1400: Loss = -11314.196758979839
Iteration 1500: Loss = -11314.15094921453
Iteration 1600: Loss = -11314.117122651496
Iteration 1700: Loss = -11314.100231589318
Iteration 1800: Loss = -11314.087038321635
Iteration 1900: Loss = -11314.076132099764
Iteration 2000: Loss = -11314.065662727155
Iteration 2100: Loss = -11314.0533346408
Iteration 2200: Loss = -11314.067518757922
1
Iteration 2300: Loss = -11314.04002971663
Iteration 2400: Loss = -11314.034596646678
Iteration 2500: Loss = -11314.029281835563
Iteration 2600: Loss = -11314.023300101302
Iteration 2700: Loss = -11314.014774455065
Iteration 2800: Loss = -11314.00648749688
Iteration 2900: Loss = -11314.002865428503
Iteration 3000: Loss = -11313.998852699027
Iteration 3100: Loss = -11313.979502631872
Iteration 3200: Loss = -11313.98571358717
1
Iteration 3300: Loss = -11313.971542971789
Iteration 3400: Loss = -11313.970068162515
Iteration 3500: Loss = -11313.9687913549
Iteration 3600: Loss = -11313.967781354975
Iteration 3700: Loss = -11313.966310263637
Iteration 3800: Loss = -11313.96488500204
Iteration 3900: Loss = -11313.96339506854
Iteration 4000: Loss = -11313.965268300928
1
Iteration 4100: Loss = -11313.962097968737
Iteration 4200: Loss = -11313.95543898781
Iteration 4300: Loss = -11313.954642931976
Iteration 4400: Loss = -11313.95443153219
Iteration 4500: Loss = -11313.953586513031
Iteration 4600: Loss = -11313.95313258344
Iteration 4700: Loss = -11313.952668273958
Iteration 4800: Loss = -11313.952280182042
Iteration 4900: Loss = -11313.951858145247
Iteration 5000: Loss = -11313.965018935187
1
Iteration 5100: Loss = -11313.951028048892
Iteration 5200: Loss = -11313.95060977967
Iteration 5300: Loss = -11313.950274474038
Iteration 5400: Loss = -11313.949765512925
Iteration 5500: Loss = -11313.952970634904
1
Iteration 5600: Loss = -11313.949019045687
Iteration 5700: Loss = -11313.94844630714
Iteration 5800: Loss = -11313.948317170172
Iteration 5900: Loss = -11313.946343965064
Iteration 6000: Loss = -11313.945892331854
Iteration 6100: Loss = -11313.945534090273
Iteration 6200: Loss = -11313.945209006952
Iteration 6300: Loss = -11313.945005947335
Iteration 6400: Loss = -11313.944734600323
Iteration 6500: Loss = -11313.944318580365
Iteration 6600: Loss = -11313.94263228987
Iteration 6700: Loss = -11313.950437458529
1
Iteration 6800: Loss = -11313.942527740523
Iteration 6900: Loss = -11313.940584176216
Iteration 7000: Loss = -11313.939835937443
Iteration 7100: Loss = -11313.939655512659
Iteration 7200: Loss = -11313.943288431763
1
Iteration 7300: Loss = -11313.943814748636
2
Iteration 7400: Loss = -11313.941683442743
3
Iteration 7500: Loss = -11313.93773680359
Iteration 7600: Loss = -11313.937654358158
Iteration 7700: Loss = -11313.944631428656
1
Iteration 7800: Loss = -11314.011060401768
2
Iteration 7900: Loss = -11313.937283675183
Iteration 8000: Loss = -11313.93730018298
1
Iteration 8100: Loss = -11313.937300453186
2
Iteration 8200: Loss = -11313.94046785378
3
Iteration 8300: Loss = -11313.936730217749
Iteration 8400: Loss = -11313.937663739101
1
Iteration 8500: Loss = -11313.938640669356
2
Iteration 8600: Loss = -11313.93619011707
Iteration 8700: Loss = -11313.936287674105
1
Iteration 8800: Loss = -11313.935969442045
Iteration 8900: Loss = -11313.936118606003
1
Iteration 9000: Loss = -11313.935851604156
Iteration 9100: Loss = -11313.935637596118
Iteration 9200: Loss = -11313.935344247146
Iteration 9300: Loss = -11313.935439760156
1
Iteration 9400: Loss = -11313.935264100484
Iteration 9500: Loss = -11313.93534107598
1
Iteration 9600: Loss = -11313.950984418436
2
Iteration 9700: Loss = -11313.942955020048
3
Iteration 9800: Loss = -11313.94030638492
4
Iteration 9900: Loss = -11313.936926019209
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7903, 0.2097],
        [0.1970, 0.8030]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5229, 0.4771], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3075, 0.1070],
         [0.6410, 0.1941]],

        [[0.6185, 0.1092],
         [0.7137, 0.5066]],

        [[0.5975, 0.0997],
         [0.5890, 0.6933]],

        [[0.5843, 0.1163],
         [0.6920, 0.7288]],

        [[0.6699, 0.1072],
         [0.6343, 0.5333]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369810324672876
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080702804390127
Global Adjusted Rand Index: 0.8758473162609216
Average Adjusted Rand Index: 0.8773332854685482
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21509.293417894427
Iteration 100: Loss = -11694.956975392402
Iteration 200: Loss = -11610.925310652094
Iteration 300: Loss = -11604.586924210302
Iteration 400: Loss = -11602.317888942423
Iteration 500: Loss = -11600.113919475943
Iteration 600: Loss = -11598.772728100543
Iteration 700: Loss = -11598.316555331221
Iteration 800: Loss = -11598.069630109381
Iteration 900: Loss = -11597.913579333099
Iteration 1000: Loss = -11597.788597619658
Iteration 1100: Loss = -11597.452298527976
Iteration 1200: Loss = -11596.304569119617
Iteration 1300: Loss = -11595.363937499003
Iteration 1400: Loss = -11594.918479370453
Iteration 1500: Loss = -11594.869007339197
Iteration 1600: Loss = -11594.843998791268
Iteration 1700: Loss = -11594.826178296214
Iteration 1800: Loss = -11594.807177684655
Iteration 1900: Loss = -11594.698843558686
Iteration 2000: Loss = -11594.583990946274
Iteration 2100: Loss = -11593.875016380847
Iteration 2200: Loss = -11593.626042550211
Iteration 2300: Loss = -11593.314069797765
Iteration 2400: Loss = -11593.228793481658
Iteration 2500: Loss = -11592.626921014577
Iteration 2600: Loss = -11590.637157491323
Iteration 2700: Loss = -11590.142104585095
Iteration 2800: Loss = -11580.696223037905
Iteration 2900: Loss = -11537.514953258078
Iteration 3000: Loss = -11504.6150374508
Iteration 3100: Loss = -11499.336478353416
Iteration 3200: Loss = -11498.737590536211
Iteration 3300: Loss = -11498.536503131001
Iteration 3400: Loss = -11498.364695225815
Iteration 3500: Loss = -11498.348443315816
Iteration 3600: Loss = -11498.074569318265
Iteration 3700: Loss = -11498.056849936322
Iteration 3800: Loss = -11498.023444772783
Iteration 3900: Loss = -11498.025241595626
1
Iteration 4000: Loss = -11498.014738104777
Iteration 4100: Loss = -11497.966257942051
Iteration 4200: Loss = -11493.688161706938
Iteration 4300: Loss = -11483.790597934201
Iteration 4400: Loss = -11476.285464159362
Iteration 4500: Loss = -11470.260758115723
Iteration 4600: Loss = -11458.65884790693
Iteration 4700: Loss = -11453.803102006166
Iteration 4800: Loss = -11432.319616625438
Iteration 4900: Loss = -11426.354434203686
Iteration 5000: Loss = -11421.720760760785
Iteration 5100: Loss = -11408.004819315296
Iteration 5200: Loss = -11402.960419526236
Iteration 5300: Loss = -11394.837792984332
Iteration 5400: Loss = -11392.775059339434
Iteration 5500: Loss = -11388.067907112369
Iteration 5600: Loss = -11375.822584465204
Iteration 5700: Loss = -11365.819482793198
Iteration 5800: Loss = -11365.739852476541
Iteration 5900: Loss = -11359.101956682996
Iteration 6000: Loss = -11354.005994480362
Iteration 6100: Loss = -11347.884571011804
Iteration 6200: Loss = -11345.656947412088
Iteration 6300: Loss = -11345.65279339413
Iteration 6400: Loss = -11345.652130954622
Iteration 6500: Loss = -11345.651473896629
Iteration 6600: Loss = -11345.64402353927
Iteration 6700: Loss = -11335.59178701227
Iteration 6800: Loss = -11335.592471008938
1
Iteration 6900: Loss = -11335.584166825856
Iteration 7000: Loss = -11321.743435596181
Iteration 7100: Loss = -11321.733774112854
Iteration 7200: Loss = -11321.731225546997
Iteration 7300: Loss = -11321.73087056797
Iteration 7400: Loss = -11321.722303208307
Iteration 7500: Loss = -11321.570251502359
Iteration 7600: Loss = -11321.560884329052
Iteration 7700: Loss = -11321.562397764703
1
Iteration 7800: Loss = -11321.559499069526
Iteration 7900: Loss = -11321.558816712815
Iteration 8000: Loss = -11321.558546957123
Iteration 8100: Loss = -11321.558030701077
Iteration 8200: Loss = -11321.556202023938
Iteration 8300: Loss = -11314.01445709728
Iteration 8400: Loss = -11314.014258626132
Iteration 8500: Loss = -11314.015658579228
1
Iteration 8600: Loss = -11314.027431857163
2
Iteration 8700: Loss = -11314.050936459815
3
Iteration 8800: Loss = -11314.013779330106
Iteration 8900: Loss = -11314.013341338656
Iteration 9000: Loss = -11314.015613818832
1
Iteration 9100: Loss = -11314.01493605863
2
Iteration 9200: Loss = -11314.011606536664
Iteration 9300: Loss = -11314.011568285865
Iteration 9400: Loss = -11314.012736234556
1
Iteration 9500: Loss = -11314.01076037226
Iteration 9600: Loss = -11314.010701365372
Iteration 9700: Loss = -11314.01001911681
Iteration 9800: Loss = -11314.00427777342
Iteration 9900: Loss = -11313.955913913147
Iteration 10000: Loss = -11313.956591681983
1
Iteration 10100: Loss = -11313.959198671586
2
Iteration 10200: Loss = -11313.995703621016
3
Iteration 10300: Loss = -11313.98819876626
4
Iteration 10400: Loss = -11313.954783807087
Iteration 10500: Loss = -11313.96091875878
1
Iteration 10600: Loss = -11313.954728773597
Iteration 10700: Loss = -11313.96039737482
1
Iteration 10800: Loss = -11313.954413646868
Iteration 10900: Loss = -11313.954295830878
Iteration 11000: Loss = -11313.954305500753
1
Iteration 11100: Loss = -11313.954210991722
Iteration 11200: Loss = -11313.959348225475
1
Iteration 11300: Loss = -11313.953815303805
Iteration 11400: Loss = -11313.953719334144
Iteration 11500: Loss = -11313.954979475198
1
Iteration 11600: Loss = -11313.954273712749
2
Iteration 11700: Loss = -11313.954009597182
3
Iteration 11800: Loss = -11313.953927235836
4
Iteration 11900: Loss = -11313.972057800122
5
Stopping early at iteration 11900 due to no improvement.
pi: tensor([[0.8020, 0.1980],
        [0.2085, 0.7915]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4761, 0.5239], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1946, 0.1074],
         [0.5696, 0.3068]],

        [[0.5556, 0.1087],
         [0.6940, 0.5606]],

        [[0.6927, 0.0994],
         [0.6538, 0.5900]],

        [[0.5051, 0.1160],
         [0.5931, 0.5670]],

        [[0.5332, 0.1069],
         [0.5153, 0.5516]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369810324672876
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
Global Adjusted Rand Index: 0.8758473162609216
Average Adjusted Rand Index: 0.8773332854685482
11356.587950948497
[0.8758473162609216, 0.8758473162609216, 0.8758473162609216, 0.8758473162609216] [0.8773332854685482, 0.8773332854685482, 0.8773332854685482, 0.8773332854685482] [11314.040965628345, 11314.12201422203, 11313.936926019209, 11313.972057800122]
-----------------------------------------------------------------------------------------
This iteration is 5
True Objective function: Loss = -11231.999608799195
Iteration 0: Loss = -11659.009724110463
Iteration 10: Loss = -11502.489658314264
Iteration 20: Loss = -11379.674837456145
Iteration 30: Loss = -11208.684274856247
Iteration 40: Loss = -11208.675000992853
Iteration 50: Loss = -11208.673168966572
Iteration 60: Loss = -11208.672793778507
Iteration 70: Loss = -11208.672699512455
Iteration 80: Loss = -11208.672690236957
Iteration 90: Loss = -11208.672680719188
Iteration 100: Loss = -11208.67268524122
1
Iteration 110: Loss = -11208.672685795524
2
Iteration 120: Loss = -11208.67268583463
3
Stopping early at iteration 120 due to no improvement.
pi: tensor([[0.7103, 0.2897],
        [0.2771, 0.7229]], dtype=torch.float64)
alpha: tensor([0.4808, 0.5192])
beta: tensor([[[0.2990, 0.1055],
         [0.4142, 0.2032]],

        [[0.5454, 0.1045],
         [0.8987, 0.5623]],

        [[0.4187, 0.0966],
         [0.7522, 0.7401]],

        [[0.7716, 0.0972],
         [0.4577, 0.0139]],

        [[0.7736, 0.0967],
         [0.4488, 0.2215]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291543349113979
Average Adjusted Rand Index: 0.9296170652106713
Iteration 0: Loss = -11620.109509601782
Iteration 10: Loss = -11219.721376516985
Iteration 20: Loss = -11208.661547262323
Iteration 30: Loss = -11208.670054792858
1
Iteration 40: Loss = -11208.672119386276
2
Iteration 50: Loss = -11208.672562505704
3
Stopping early at iteration 50 due to no improvement.
pi: tensor([[0.7229, 0.2771],
        [0.2897, 0.7103]], dtype=torch.float64)
alpha: tensor([0.5192, 0.4808])
beta: tensor([[[0.2032, 0.1055],
         [0.4047, 0.2990]],

        [[0.2442, 0.1045],
         [0.1074, 0.1539]],

        [[0.4782, 0.0966],
         [0.9325, 0.9217]],

        [[0.5964, 0.0972],
         [0.7708, 0.2743]],

        [[0.9163, 0.0967],
         [0.1015, 0.1829]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448582102964589
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291543349113979
Average Adjusted Rand Index: 0.9296170652106713
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20987.361144139835
Iteration 100: Loss = -11509.816168205032
Iteration 200: Loss = -11509.429877489672
Iteration 300: Loss = -11509.333391399821
Iteration 400: Loss = -11509.259146105254
Iteration 500: Loss = -11509.169074941934
Iteration 600: Loss = -11509.031147069934
Iteration 700: Loss = -11508.757337825755
Iteration 800: Loss = -11507.056182998524
Iteration 900: Loss = -11500.947198404665
Iteration 1000: Loss = -11500.24324853898
Iteration 1100: Loss = -11499.69872586804
Iteration 1200: Loss = -11499.18624553828
Iteration 1300: Loss = -11499.057343368679
Iteration 1400: Loss = -11498.55142128529
Iteration 1500: Loss = -11497.112760287342
Iteration 1600: Loss = -11496.77107403898
Iteration 1700: Loss = -11496.704291941565
Iteration 1800: Loss = -11496.661730127791
Iteration 1900: Loss = -11496.627872797
Iteration 2000: Loss = -11496.603680622671
Iteration 2100: Loss = -11496.582509497572
Iteration 2200: Loss = -11496.564000232713
Iteration 2300: Loss = -11496.550168954704
Iteration 2400: Loss = -11496.538215887667
Iteration 2500: Loss = -11496.529912356613
Iteration 2600: Loss = -11496.522862079872
Iteration 2700: Loss = -11496.515898916898
Iteration 2800: Loss = -11496.509251974863
Iteration 2900: Loss = -11496.505505538527
Iteration 3000: Loss = -11496.502618604907
Iteration 3100: Loss = -11496.500154892892
Iteration 3200: Loss = -11496.498084104025
Iteration 3300: Loss = -11496.496250668344
Iteration 3400: Loss = -11496.494680808657
Iteration 3500: Loss = -11496.493303532547
Iteration 3600: Loss = -11496.492105963594
Iteration 3700: Loss = -11496.491060796345
Iteration 3800: Loss = -11496.490098582479
Iteration 3900: Loss = -11496.489231840129
Iteration 4000: Loss = -11496.488455425038
Iteration 4100: Loss = -11496.487780655134
Iteration 4200: Loss = -11496.487137643975
Iteration 4300: Loss = -11496.486551379268
Iteration 4400: Loss = -11496.485979232588
Iteration 4500: Loss = -11496.485506772917
Iteration 4600: Loss = -11496.484976948583
Iteration 4700: Loss = -11496.484329748942
Iteration 4800: Loss = -11496.483453963456
Iteration 4900: Loss = -11496.48140516471
Iteration 5000: Loss = -11496.479151035772
Iteration 5100: Loss = -11496.478695550431
Iteration 5200: Loss = -11496.479220267494
1
Iteration 5300: Loss = -11496.47791828641
Iteration 5400: Loss = -11496.47755383085
Iteration 5500: Loss = -11496.477318389248
Iteration 5600: Loss = -11496.477047961554
Iteration 5700: Loss = -11496.476974326071
Iteration 5800: Loss = -11496.476681027538
Iteration 5900: Loss = -11496.47649130105
Iteration 6000: Loss = -11496.476327906757
Iteration 6100: Loss = -11496.47621026678
Iteration 6200: Loss = -11496.476184639088
Iteration 6300: Loss = -11496.475920536988
Iteration 6400: Loss = -11496.475776652442
Iteration 6500: Loss = -11496.475612915528
Iteration 6600: Loss = -11496.475431740057
Iteration 6700: Loss = -11496.475067632084
Iteration 6800: Loss = -11496.474546363494
Iteration 6900: Loss = -11496.474397906954
Iteration 7000: Loss = -11496.474272617077
Iteration 7100: Loss = -11496.474207372077
Iteration 7200: Loss = -11496.474154883672
Iteration 7300: Loss = -11496.474169016637
1
Iteration 7400: Loss = -11496.47398922034
Iteration 7500: Loss = -11496.47399790354
1
Iteration 7600: Loss = -11496.473897169986
Iteration 7700: Loss = -11496.475147068395
1
Iteration 7800: Loss = -11496.473913846512
2
Iteration 7900: Loss = -11496.473787722412
Iteration 8000: Loss = -11496.474205492403
1
Iteration 8100: Loss = -11496.482321968184
2
Iteration 8200: Loss = -11496.47361618033
Iteration 8300: Loss = -11496.476914650862
1
Iteration 8400: Loss = -11496.473560728278
Iteration 8500: Loss = -11496.473506362543
Iteration 8600: Loss = -11496.474960442825
1
Iteration 8700: Loss = -11496.473443900102
Iteration 8800: Loss = -11496.473441867474
Iteration 8900: Loss = -11496.47347723208
1
Iteration 9000: Loss = -11496.473308888271
Iteration 9100: Loss = -11496.473322984602
1
Iteration 9200: Loss = -11496.473291158343
Iteration 9300: Loss = -11496.473194827606
Iteration 9400: Loss = -11496.47312256252
Iteration 9500: Loss = -11496.47656697075
1
Iteration 9600: Loss = -11496.472966871757
Iteration 9700: Loss = -11496.472879373348
Iteration 9800: Loss = -11496.495953998849
1
Iteration 9900: Loss = -11496.47280386409
Iteration 10000: Loss = -11496.491541809728
1
Iteration 10100: Loss = -11496.47379924765
2
Iteration 10200: Loss = -11496.560464370645
3
Iteration 10300: Loss = -11496.47229697812
Iteration 10400: Loss = -11496.500516399168
1
Iteration 10500: Loss = -11496.471918267625
Iteration 10600: Loss = -11496.472306464764
1
Iteration 10700: Loss = -11496.472280003498
2
Iteration 10800: Loss = -11496.471839012347
Iteration 10900: Loss = -11496.484742013728
1
Iteration 11000: Loss = -11496.471712636421
Iteration 11100: Loss = -11496.766398511989
1
Iteration 11200: Loss = -11496.473909963308
2
Iteration 11300: Loss = -11496.492131093453
3
Iteration 11400: Loss = -11496.471716505977
4
Iteration 11500: Loss = -11496.49393947022
5
Stopping early at iteration 11500 due to no improvement.
pi: tensor([[9.9999e-01, 9.4575e-06],
        [1.0953e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0930, 0.9070], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1362, 0.2072],
         [0.5249, 0.1756]],

        [[0.7259, 0.2154],
         [0.7072, 0.6880]],

        [[0.6930, 0.2353],
         [0.6406, 0.5633]],

        [[0.5749, 0.1339],
         [0.5705, 0.6330]],

        [[0.6466, 0.1138],
         [0.5353, 0.7079]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.018265910862823947
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.021590537319478416
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0028641099491338354
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.005131431169739117
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.016398586040285303
Global Adjusted Rand Index: 0.0007941997225989234
Average Adjusted Rand Index: 0.009651898620742942
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20695.122242322323
Iteration 100: Loss = -11509.13701054213
Iteration 200: Loss = -11507.729553412972
Iteration 300: Loss = -11504.368528589093
Iteration 400: Loss = -11503.036059000497
Iteration 500: Loss = -11501.562224731208
Iteration 600: Loss = -11500.988883516438
Iteration 700: Loss = -11500.480835177072
Iteration 800: Loss = -11499.413165285448
Iteration 900: Loss = -11478.130514159842
Iteration 1000: Loss = -11264.664804358445
Iteration 1100: Loss = -11224.222127425815
Iteration 1200: Loss = -11212.990541232934
Iteration 1300: Loss = -11210.319982384655
Iteration 1400: Loss = -11209.51040477495
Iteration 1500: Loss = -11209.136258574472
Iteration 1600: Loss = -11205.805199036107
Iteration 1700: Loss = -11205.752117486933
Iteration 1800: Loss = -11205.709925172392
Iteration 1900: Loss = -11205.675691123059
Iteration 2000: Loss = -11205.639711666383
Iteration 2100: Loss = -11205.623279299098
Iteration 2200: Loss = -11205.611887095478
Iteration 2300: Loss = -11205.60207953809
Iteration 2400: Loss = -11205.593524549487
Iteration 2500: Loss = -11205.585935909112
Iteration 2600: Loss = -11205.591283425889
1
Iteration 2700: Loss = -11205.55616731097
Iteration 2800: Loss = -11205.53819226123
Iteration 2900: Loss = -11205.53124990372
Iteration 3000: Loss = -11205.518297112041
Iteration 3100: Loss = -11205.502260161953
Iteration 3200: Loss = -11205.498450391768
Iteration 3300: Loss = -11205.499249627997
1
Iteration 3400: Loss = -11205.453698430658
Iteration 3500: Loss = -11205.417191660717
Iteration 3600: Loss = -11205.268542879923
Iteration 3700: Loss = -11205.266118999707
Iteration 3800: Loss = -11205.264524172382
Iteration 3900: Loss = -11205.263022876363
Iteration 4000: Loss = -11205.261760769754
Iteration 4100: Loss = -11205.268846682575
1
Iteration 4200: Loss = -11205.259639792526
Iteration 4300: Loss = -11205.258737872187
Iteration 4400: Loss = -11205.25790393612
Iteration 4500: Loss = -11205.26177060372
1
Iteration 4600: Loss = -11205.256527263184
Iteration 4700: Loss = -11205.255803845324
Iteration 4800: Loss = -11205.255772563867
Iteration 4900: Loss = -11205.254973349107
Iteration 5000: Loss = -11205.253933277343
Iteration 5100: Loss = -11205.25346165744
Iteration 5200: Loss = -11205.263073002692
1
Iteration 5300: Loss = -11205.253221801997
Iteration 5400: Loss = -11205.252676962049
Iteration 5500: Loss = -11205.258578183011
1
Iteration 5600: Loss = -11205.251403289041
Iteration 5700: Loss = -11205.251637090898
1
Iteration 5800: Loss = -11205.25063454039
Iteration 5900: Loss = -11205.251118188537
1
Iteration 6000: Loss = -11205.250154988442
Iteration 6100: Loss = -11205.249431729178
Iteration 6200: Loss = -11205.249770969553
1
Iteration 6300: Loss = -11205.247371636775
Iteration 6400: Loss = -11205.24285870916
Iteration 6500: Loss = -11205.248118522708
1
Iteration 6600: Loss = -11205.24019948982
Iteration 6700: Loss = -11205.241088297149
1
Iteration 6800: Loss = -11205.239737823387
Iteration 6900: Loss = -11205.239088729444
Iteration 7000: Loss = -11205.238768439232
Iteration 7100: Loss = -11205.2449328482
1
Iteration 7200: Loss = -11205.23824743892
Iteration 7300: Loss = -11205.236922763943
Iteration 7400: Loss = -11205.241629131002
1
Iteration 7500: Loss = -11205.236706327916
Iteration 7600: Loss = -11205.359694905628
1
Iteration 7700: Loss = -11205.236525790586
Iteration 7800: Loss = -11205.23647548987
Iteration 7900: Loss = -11205.237787776554
1
Iteration 8000: Loss = -11205.2368110412
2
Iteration 8100: Loss = -11205.237964805745
3
Iteration 8200: Loss = -11205.239089666375
4
Iteration 8300: Loss = -11205.236122659633
Iteration 8400: Loss = -11205.37308828137
1
Iteration 8500: Loss = -11205.23605105483
Iteration 8600: Loss = -11205.236053037452
1
Iteration 8700: Loss = -11205.235957669507
Iteration 8800: Loss = -11205.245019998198
1
Iteration 8900: Loss = -11205.238725840112
2
Iteration 9000: Loss = -11205.236082645235
3
Iteration 9100: Loss = -11205.235825487436
Iteration 9200: Loss = -11205.231054131042
Iteration 9300: Loss = -11205.277659330592
1
Iteration 9400: Loss = -11205.254375497903
2
Iteration 9500: Loss = -11205.260267766738
3
Iteration 9600: Loss = -11205.325791669957
4
Iteration 9700: Loss = -11205.317817375582
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.7178, 0.2822],
        [0.2652, 0.7348]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4675, 0.5325], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3078, 0.1059],
         [0.6439, 0.2059]],

        [[0.7108, 0.1047],
         [0.6241, 0.7040]],

        [[0.6014, 0.0969],
         [0.6051, 0.7042]],

        [[0.6172, 0.0977],
         [0.5319, 0.5550]],

        [[0.5037, 0.0967],
         [0.6911, 0.5134]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824276204858761
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368977906606135
Average Adjusted Rand Index: 0.9371309472485547
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22729.030219394146
Iteration 100: Loss = -11507.882222798533
Iteration 200: Loss = -11505.569883878417
Iteration 300: Loss = -11504.648418675579
Iteration 400: Loss = -11502.041336803257
Iteration 500: Loss = -11501.220841046648
Iteration 600: Loss = -11500.632262528594
Iteration 700: Loss = -11499.962204136773
Iteration 800: Loss = -11499.41098840851
Iteration 900: Loss = -11498.994785376262
Iteration 1000: Loss = -11498.309552683315
Iteration 1100: Loss = -11452.36596631453
Iteration 1200: Loss = -11341.887418695456
Iteration 1300: Loss = -11333.1875611141
Iteration 1400: Loss = -11332.476660130538
Iteration 1500: Loss = -11331.72838477458
Iteration 1600: Loss = -11331.650806108684
Iteration 1700: Loss = -11329.736852154338
Iteration 1800: Loss = -11329.37133087614
Iteration 1900: Loss = -11329.316792532125
Iteration 2000: Loss = -11329.294388175129
Iteration 2100: Loss = -11329.280034643576
Iteration 2200: Loss = -11329.264922685446
Iteration 2300: Loss = -11329.248614075139
Iteration 2400: Loss = -11325.249494796344
Iteration 2500: Loss = -11325.232299515805
Iteration 2600: Loss = -11325.224518409119
Iteration 2700: Loss = -11324.637904659387
Iteration 2800: Loss = -11322.257402916537
Iteration 2900: Loss = -11322.247180950708
Iteration 3000: Loss = -11322.235031649901
Iteration 3100: Loss = -11321.98453885885
Iteration 3200: Loss = -11320.566104621397
Iteration 3300: Loss = -11320.294585265672
Iteration 3400: Loss = -11320.287392517705
Iteration 3500: Loss = -11320.27019437874
Iteration 3600: Loss = -11320.266859444962
Iteration 3700: Loss = -11320.262478353494
Iteration 3800: Loss = -11320.257678468597
Iteration 3900: Loss = -11320.251408318332
Iteration 4000: Loss = -11320.249739121402
Iteration 4100: Loss = -11320.24846853975
Iteration 4200: Loss = -11320.246985690294
Iteration 4300: Loss = -11320.243748126923
Iteration 4400: Loss = -11320.240074423084
Iteration 4500: Loss = -11320.23835245377
Iteration 4600: Loss = -11320.237237643534
Iteration 4700: Loss = -11320.235025127491
Iteration 4800: Loss = -11320.232270031742
Iteration 4900: Loss = -11320.228753367439
Iteration 5000: Loss = -11320.222931277047
Iteration 5100: Loss = -11320.22000911267
Iteration 5200: Loss = -11320.211161402123
Iteration 5300: Loss = -11319.075968737268
Iteration 5400: Loss = -11319.06885885958
Iteration 5500: Loss = -11319.061885392082
Iteration 5600: Loss = -11319.047008281237
Iteration 5700: Loss = -11319.025605605693
Iteration 5800: Loss = -11318.978731930942
Iteration 5900: Loss = -11318.965174026353
Iteration 6000: Loss = -11318.95866387597
Iteration 6100: Loss = -11318.930767052907
Iteration 6200: Loss = -11318.928883949357
Iteration 6300: Loss = -11318.923681085302
Iteration 6400: Loss = -11318.920150778702
Iteration 6500: Loss = -11318.923394888967
1
Iteration 6600: Loss = -11318.91669455979
Iteration 6700: Loss = -11318.89170039697
Iteration 6800: Loss = -11318.941134767962
1
Iteration 6900: Loss = -11318.824974128975
Iteration 7000: Loss = -11318.82392491605
Iteration 7100: Loss = -11318.827409241292
1
Iteration 7200: Loss = -11318.82202951531
Iteration 7300: Loss = -11318.70975964214
Iteration 7400: Loss = -11318.68486482929
Iteration 7500: Loss = -11318.654051947435
Iteration 7600: Loss = -11318.65233909957
Iteration 7700: Loss = -11318.644729875119
Iteration 7800: Loss = -11318.640730719868
Iteration 7900: Loss = -11318.125231323678
Iteration 8000: Loss = -11317.950343952274
Iteration 8100: Loss = -11317.950171405088
Iteration 8200: Loss = -11317.949564067809
Iteration 8300: Loss = -11317.788778235554
Iteration 8400: Loss = -11317.804147849703
1
Iteration 8500: Loss = -11317.72526905061
Iteration 8600: Loss = -11317.720553736697
Iteration 8700: Loss = -11317.696693249583
Iteration 8800: Loss = -11317.693155245595
Iteration 8900: Loss = -11317.692969376973
Iteration 9000: Loss = -11317.692867749682
Iteration 9100: Loss = -11317.690834251189
Iteration 9200: Loss = -11317.67551569019
Iteration 9300: Loss = -11317.675399205371
Iteration 9400: Loss = -11317.679170681891
1
Iteration 9500: Loss = -11317.675193815703
Iteration 9600: Loss = -11317.675003280603
Iteration 9700: Loss = -11317.6705210152
Iteration 9800: Loss = -11317.669887945574
Iteration 9900: Loss = -11317.669805616324
Iteration 10000: Loss = -11317.670692569493
1
Iteration 10100: Loss = -11317.669800222757
Iteration 10200: Loss = -11317.669293120369
Iteration 10300: Loss = -11317.650448916567
Iteration 10400: Loss = -11317.798912893624
1
Iteration 10500: Loss = -11317.649560407408
Iteration 10600: Loss = -11317.670487749741
1
Iteration 10700: Loss = -11317.649417986588
Iteration 10800: Loss = -11317.649959210237
1
Iteration 10900: Loss = -11317.64922055224
Iteration 11000: Loss = -11317.647472870667
Iteration 11100: Loss = -11317.496596796167
Iteration 11200: Loss = -11317.659546082214
1
Iteration 11300: Loss = -11317.475468606437
Iteration 11400: Loss = -11317.479156960022
1
Iteration 11500: Loss = -11317.475441435596
Iteration 11600: Loss = -11317.475603316763
1
Iteration 11700: Loss = -11317.475439259095
Iteration 11800: Loss = -11317.477936824964
1
Iteration 11900: Loss = -11317.475399973222
Iteration 12000: Loss = -11317.475549029457
1
Iteration 12100: Loss = -11317.475442485571
2
Iteration 12200: Loss = -11317.475389857056
Iteration 12300: Loss = -11317.492366297573
1
Iteration 12400: Loss = -11317.474979261127
Iteration 12500: Loss = -11317.47587307217
1
Iteration 12600: Loss = -11317.475040426534
2
Iteration 12700: Loss = -11317.484403700826
3
Iteration 12800: Loss = -11317.474971239983
Iteration 12900: Loss = -11317.474317132903
Iteration 13000: Loss = -11317.476034785523
1
Iteration 13100: Loss = -11317.47386339547
Iteration 13200: Loss = -11317.487972191671
1
Iteration 13300: Loss = -11317.473802125087
Iteration 13400: Loss = -11317.468860604917
Iteration 13500: Loss = -11317.525658079034
1
Iteration 13600: Loss = -11317.475524672538
2
Iteration 13700: Loss = -11317.464926957648
Iteration 13800: Loss = -11317.465093562683
1
Iteration 13900: Loss = -11317.476958599878
2
Iteration 14000: Loss = -11317.463833522326
Iteration 14100: Loss = -11317.47737917403
1
Iteration 14200: Loss = -11317.463632201296
Iteration 14300: Loss = -11317.47004531018
1
Iteration 14400: Loss = -11317.463617962514
Iteration 14500: Loss = -11317.490722183127
1
Iteration 14600: Loss = -11317.463599555926
Iteration 14700: Loss = -11317.50646981699
1
Iteration 14800: Loss = -11317.463528125412
Iteration 14900: Loss = -11317.463499765077
Iteration 15000: Loss = -11317.463503911971
1
Iteration 15100: Loss = -11317.463396312916
Iteration 15200: Loss = -11317.474455021713
1
Iteration 15300: Loss = -11317.463398070513
2
Iteration 15400: Loss = -11317.48665125386
3
Iteration 15500: Loss = -11317.4633524699
Iteration 15600: Loss = -11317.465657953087
1
Iteration 15700: Loss = -11317.466584973528
2
Iteration 15800: Loss = -11317.465748419276
3
Iteration 15900: Loss = -11317.494356554045
4
Iteration 16000: Loss = -11317.494013186928
5
Stopping early at iteration 16000 due to no improvement.
pi: tensor([[0.5191, 0.4809],
        [0.5946, 0.4054]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9605, 0.0395], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1940, 0.1443],
         [0.6359, 0.3078]],

        [[0.6821, 0.1055],
         [0.7096, 0.6608]],

        [[0.6866, 0.0965],
         [0.5963, 0.5366]],

        [[0.6496, 0.1045],
         [0.6323, 0.5208]],

        [[0.5926, 0.0970],
         [0.6464, 0.6231]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 19
Adjusted Rand Index: 0.37855677651733366
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.23668230472972152
Average Adjusted Rand Index: 0.6677109164670297
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21345.60938598143
Iteration 100: Loss = -11508.811032580119
Iteration 200: Loss = -11504.754606780521
Iteration 300: Loss = -11503.817504525214
Iteration 400: Loss = -11502.851781671014
Iteration 500: Loss = -11501.354439764404
Iteration 600: Loss = -11500.521310994021
Iteration 700: Loss = -11498.686180553404
Iteration 800: Loss = -11421.25978186578
Iteration 900: Loss = -11257.501211684666
Iteration 1000: Loss = -11215.11226050362
Iteration 1100: Loss = -11213.710771943843
Iteration 1200: Loss = -11212.323851063287
Iteration 1300: Loss = -11209.95932780514
Iteration 1400: Loss = -11209.793906254346
Iteration 1500: Loss = -11209.722280713688
Iteration 1600: Loss = -11209.660491625806
Iteration 1700: Loss = -11208.953586893655
Iteration 1800: Loss = -11206.193067174874
Iteration 1900: Loss = -11205.636484165141
Iteration 2000: Loss = -11205.611192770586
Iteration 2100: Loss = -11205.598475263594
Iteration 2200: Loss = -11205.586794413904
Iteration 2300: Loss = -11205.557280446032
Iteration 2400: Loss = -11205.548863322449
Iteration 2500: Loss = -11205.542035006203
Iteration 2600: Loss = -11205.53126159665
Iteration 2700: Loss = -11205.491118142414
Iteration 2800: Loss = -11205.481415055
Iteration 2900: Loss = -11205.481582977489
1
Iteration 3000: Loss = -11205.476564623232
Iteration 3100: Loss = -11205.474821700253
Iteration 3200: Loss = -11205.476539276982
1
Iteration 3300: Loss = -11205.470909104635
Iteration 3400: Loss = -11205.469465763972
Iteration 3500: Loss = -11205.471027487438
1
Iteration 3600: Loss = -11205.46674962587
Iteration 3700: Loss = -11205.465652514175
Iteration 3800: Loss = -11205.464569332125
Iteration 3900: Loss = -11205.466243606415
1
Iteration 4000: Loss = -11205.462630070977
Iteration 4100: Loss = -11205.462860604726
1
Iteration 4200: Loss = -11205.460877339658
Iteration 4300: Loss = -11205.461178335177
1
Iteration 4400: Loss = -11205.459903544643
Iteration 4500: Loss = -11205.460206094336
1
Iteration 4600: Loss = -11205.458814834015
Iteration 4700: Loss = -11205.465142188386
1
Iteration 4800: Loss = -11205.456320247427
Iteration 4900: Loss = -11205.455157859866
Iteration 5000: Loss = -11205.295658608287
Iteration 5100: Loss = -11205.291937164367
Iteration 5200: Loss = -11205.290914739207
Iteration 5300: Loss = -11205.290398156103
Iteration 5400: Loss = -11205.289929089535
Iteration 5500: Loss = -11205.289300311808
Iteration 5600: Loss = -11205.287514163705
Iteration 5700: Loss = -11205.2809966321
Iteration 5800: Loss = -11205.28045646591
Iteration 5900: Loss = -11205.277853285263
Iteration 6000: Loss = -11205.237122653652
Iteration 6100: Loss = -11205.236024872653
Iteration 6200: Loss = -11205.235934350718
Iteration 6300: Loss = -11205.236196788639
1
Iteration 6400: Loss = -11205.235372982617
Iteration 6500: Loss = -11205.235198076085
Iteration 6600: Loss = -11205.235952287167
1
Iteration 6700: Loss = -11205.237211766274
2
Iteration 6800: Loss = -11205.23514582664
Iteration 6900: Loss = -11205.234825104804
Iteration 7000: Loss = -11205.236478538425
1
Iteration 7100: Loss = -11205.234614626965
Iteration 7200: Loss = -11205.258562400673
1
Iteration 7300: Loss = -11205.24517175302
2
Iteration 7400: Loss = -11205.23427394696
Iteration 7500: Loss = -11205.234490125013
1
Iteration 7600: Loss = -11205.23413244939
Iteration 7700: Loss = -11205.234032377446
Iteration 7800: Loss = -11205.233959848976
Iteration 7900: Loss = -11205.233886954207
Iteration 8000: Loss = -11205.233479682849
Iteration 8100: Loss = -11205.231022980124
Iteration 8200: Loss = -11205.230525762086
Iteration 8300: Loss = -11205.245314439679
1
Iteration 8400: Loss = -11205.230402032135
Iteration 8500: Loss = -11205.23035193071
Iteration 8600: Loss = -11205.230779867232
1
Iteration 8700: Loss = -11205.230272062778
Iteration 8800: Loss = -11205.259608794526
1
Iteration 8900: Loss = -11205.230203525414
Iteration 9000: Loss = -11205.230223135408
1
Iteration 9100: Loss = -11205.230452663318
2
Iteration 9200: Loss = -11205.231213479585
3
Iteration 9300: Loss = -11205.27121196394
4
Iteration 9400: Loss = -11205.231501806196
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.7210, 0.2790],
        [0.2666, 0.7334]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4705, 0.5295], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3065, 0.1054],
         [0.6482, 0.2069]],

        [[0.5727, 0.1041],
         [0.5013, 0.5893]],

        [[0.6973, 0.0966],
         [0.6517, 0.5953]],

        [[0.6859, 0.0971],
         [0.6634, 0.6394]],

        [[0.6754, 0.0966],
         [0.7158, 0.5470]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824276204858761
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368977906606135
Average Adjusted Rand Index: 0.9371309472485547
11231.999608799195
[0.0007941997225989234, 0.9368977906606135, 0.23668230472972152, 0.9368977906606135] [0.009651898620742942, 0.9371309472485547, 0.6677109164670297, 0.9371309472485547] [11496.49393947022, 11205.317817375582, 11317.494013186928, 11205.231501806196]
-----------------------------------------------------------------------------------------
This iteration is 6
True Objective function: Loss = -11173.137324635527
Iteration 0: Loss = -11650.81859534101
Iteration 10: Loss = -11447.08007120452
Iteration 20: Loss = -11205.240352943825
Iteration 30: Loss = -11160.15632986715
Iteration 40: Loss = -11160.110733579839
Iteration 50: Loss = -11160.109514249001
Iteration 60: Loss = -11160.109489536828
Iteration 70: Loss = -11160.109473575427
Iteration 80: Loss = -11160.109475151272
1
Iteration 90: Loss = -11160.109473372746
Iteration 100: Loss = -11160.109473372748
1
Iteration 110: Loss = -11160.109473372748
2
Iteration 120: Loss = -11160.109473372748
3
Stopping early at iteration 120 due to no improvement.
pi: tensor([[0.7326, 0.2674],
        [0.2612, 0.7388]], dtype=torch.float64)
alpha: tensor([0.4921, 0.5079])
beta: tensor([[[0.1977, 0.1052],
         [0.5835, 0.2901]],

        [[0.9239, 0.0972],
         [0.0784, 0.1281]],

        [[0.8487, 0.0984],
         [0.1909, 0.9343]],

        [[0.6624, 0.1002],
         [0.4249, 0.6184]],

        [[0.2474, 0.0975],
         [0.6696, 0.8527]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446731544891418
Average Adjusted Rand Index: 0.9449671959812246
Iteration 0: Loss = -11497.266400526898
Iteration 10: Loss = -11448.461572390477
Iteration 20: Loss = -11443.205811144197
Iteration 30: Loss = -11370.839942766916
Iteration 40: Loss = -11160.212315917212
Iteration 50: Loss = -11160.110417341642
Iteration 60: Loss = -11160.109504919887
Iteration 70: Loss = -11160.109489207398
Iteration 80: Loss = -11160.109470331821
Iteration 90: Loss = -11160.109476550182
1
Iteration 100: Loss = -11160.109476733098
2
Iteration 110: Loss = -11160.10947640555
3
Stopping early at iteration 110 due to no improvement.
pi: tensor([[0.7326, 0.2674],
        [0.2612, 0.7388]], dtype=torch.float64)
alpha: tensor([0.4921, 0.5079])
beta: tensor([[[0.1977, 0.1052],
         [0.0287, 0.2901]],

        [[0.7503, 0.0972],
         [0.6401, 0.7411]],

        [[0.2153, 0.0984],
         [0.1248, 0.4854]],

        [[0.6752, 0.1002],
         [0.2419, 0.2222]],

        [[0.9425, 0.0975],
         [0.9295, 0.8702]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446731544891418
Average Adjusted Rand Index: 0.9449671959812246
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20290.157318790585
Iteration 100: Loss = -11453.878662426563
Iteration 200: Loss = -11451.739227153645
Iteration 300: Loss = -11446.811010620057
Iteration 400: Loss = -11445.09087394664
Iteration 500: Loss = -11407.194080299154
Iteration 600: Loss = -11406.522504270948
Iteration 700: Loss = -11406.33702668811
Iteration 800: Loss = -11406.300395901279
Iteration 900: Loss = -11406.28545320295
Iteration 1000: Loss = -11406.27690907896
Iteration 1100: Loss = -11406.271381527891
Iteration 1200: Loss = -11406.267518010849
Iteration 1300: Loss = -11406.264773617986
Iteration 1400: Loss = -11406.262649746193
Iteration 1500: Loss = -11406.26102216191
Iteration 1600: Loss = -11406.259755670291
Iteration 1700: Loss = -11406.258722725337
Iteration 1800: Loss = -11406.257870667772
Iteration 1900: Loss = -11406.25718485594
Iteration 2000: Loss = -11406.25657346281
Iteration 2100: Loss = -11406.256050078257
Iteration 2200: Loss = -11406.255622760798
Iteration 2300: Loss = -11406.255265603628
Iteration 2400: Loss = -11406.254957793524
Iteration 2500: Loss = -11406.254674410711
Iteration 2600: Loss = -11406.25444944972
Iteration 2700: Loss = -11406.254425351246
Iteration 2800: Loss = -11406.254024937132
Iteration 2900: Loss = -11406.253870725388
Iteration 3000: Loss = -11406.25599708059
1
Iteration 3100: Loss = -11406.253576118303
Iteration 3200: Loss = -11406.253462152523
Iteration 3300: Loss = -11406.25988192119
1
Iteration 3400: Loss = -11406.253296786914
Iteration 3500: Loss = -11406.253186671582
Iteration 3600: Loss = -11406.253101476932
Iteration 3700: Loss = -11406.253022118499
Iteration 3800: Loss = -11406.252969245481
Iteration 3900: Loss = -11406.252943549996
Iteration 4000: Loss = -11406.252861553035
Iteration 4100: Loss = -11406.25290939853
1
Iteration 4200: Loss = -11406.252806212884
Iteration 4300: Loss = -11406.252745355281
Iteration 4400: Loss = -11406.252712959571
Iteration 4500: Loss = -11406.252731538916
1
Iteration 4600: Loss = -11406.252703982036
Iteration 4700: Loss = -11406.253176924947
1
Iteration 4800: Loss = -11406.253570710922
2
Iteration 4900: Loss = -11406.25278002492
3
Iteration 5000: Loss = -11406.255746118215
4
Iteration 5100: Loss = -11406.253059137804
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.9578, 0.0422],
        [0.9570, 0.0430]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4909, 0.5091], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1717, 0.1060],
         [0.5580, 0.3014]],

        [[0.5970, 0.0979],
         [0.6879, 0.6985]],

        [[0.7040, 0.2616],
         [0.5753, 0.6099]],

        [[0.5283, 0.2163],
         [0.7057, 0.6559]],

        [[0.6561, 0.2552],
         [0.7108, 0.5526]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.039212948139009274
Average Adjusted Rand Index: 0.1903644849105811
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19122.95506526031
Iteration 100: Loss = -11454.697383277602
Iteration 200: Loss = -11453.651632597855
Iteration 300: Loss = -11451.100482248487
Iteration 400: Loss = -11449.853036940935
Iteration 500: Loss = -11447.526784993468
Iteration 600: Loss = -11407.15821473151
Iteration 700: Loss = -11406.425505299509
Iteration 800: Loss = -11406.313669282774
Iteration 900: Loss = -11406.28513847154
Iteration 1000: Loss = -11406.274900606557
Iteration 1100: Loss = -11406.269390178639
Iteration 1200: Loss = -11406.265919692481
Iteration 1300: Loss = -11406.263495738578
Iteration 1400: Loss = -11406.26168533185
Iteration 1500: Loss = -11406.260286634631
Iteration 1600: Loss = -11406.259210234406
Iteration 1700: Loss = -11406.258261048843
Iteration 1800: Loss = -11406.257519233604
Iteration 1900: Loss = -11406.256870282868
Iteration 2000: Loss = -11406.256310373712
Iteration 2100: Loss = -11406.255860323232
Iteration 2200: Loss = -11406.255515181356
Iteration 2300: Loss = -11406.255152295991
Iteration 2400: Loss = -11406.254890602217
Iteration 2500: Loss = -11406.254636626103
Iteration 2600: Loss = -11406.254433456423
Iteration 2700: Loss = -11406.25422976487
Iteration 2800: Loss = -11406.254051491811
Iteration 2900: Loss = -11406.253868975991
Iteration 3000: Loss = -11406.253751495095
Iteration 3100: Loss = -11406.253623757808
Iteration 3200: Loss = -11406.253539827328
Iteration 3300: Loss = -11406.253412881077
Iteration 3400: Loss = -11406.25330580495
Iteration 3500: Loss = -11406.253238392432
Iteration 3600: Loss = -11406.253184158937
Iteration 3700: Loss = -11406.253093949786
Iteration 3800: Loss = -11406.25305693106
Iteration 3900: Loss = -11406.253036568338
Iteration 4000: Loss = -11406.252931857945
Iteration 4100: Loss = -11406.252895905342
Iteration 4200: Loss = -11406.252842572616
Iteration 4300: Loss = -11406.25635100028
1
Iteration 4400: Loss = -11406.25322070853
2
Iteration 4500: Loss = -11406.259582255898
3
Iteration 4600: Loss = -11406.25271218496
Iteration 4700: Loss = -11406.254088161624
1
Iteration 4800: Loss = -11406.25322686534
2
Iteration 4900: Loss = -11406.252799468908
3
Iteration 5000: Loss = -11406.25263230215
Iteration 5100: Loss = -11406.252901694508
1
Iteration 5200: Loss = -11406.252564265636
Iteration 5300: Loss = -11406.252580616501
1
Iteration 5400: Loss = -11406.260600207866
2
Iteration 5500: Loss = -11406.252519717258
Iteration 5600: Loss = -11406.256041641094
1
Iteration 5700: Loss = -11406.252852590449
2
Iteration 5800: Loss = -11406.252479125691
Iteration 5900: Loss = -11406.252489189266
1
Iteration 6000: Loss = -11406.257041105313
2
Iteration 6100: Loss = -11406.252443081336
Iteration 6200: Loss = -11406.252429187463
Iteration 6300: Loss = -11406.252834196894
1
Iteration 6400: Loss = -11406.252457590257
2
Iteration 6500: Loss = -11406.252411194537
Iteration 6600: Loss = -11406.252427605094
1
Iteration 6700: Loss = -11406.252409501423
Iteration 6800: Loss = -11406.252665186279
1
Iteration 6900: Loss = -11406.254496192212
2
Iteration 7000: Loss = -11406.252388556326
Iteration 7100: Loss = -11406.277160663132
1
Iteration 7200: Loss = -11406.252337802192
Iteration 7300: Loss = -11406.2589079334
1
Iteration 7400: Loss = -11406.252378432808
2
Iteration 7500: Loss = -11406.25237826918
3
Iteration 7600: Loss = -11406.258990521384
4
Iteration 7700: Loss = -11406.25239184524
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.9582, 0.0418],
        [0.9570, 0.0430]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4910, 0.5090], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1717, 0.1060],
         [0.5210, 0.3014]],

        [[0.6335, 0.0979],
         [0.7271, 0.5489]],

        [[0.6412, 0.2616],
         [0.5950, 0.5294]],

        [[0.6928, 0.2163],
         [0.5232, 0.7284]],

        [[0.5250, 0.2553],
         [0.5490, 0.5873]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.039212948139009274
Average Adjusted Rand Index: 0.1903644849105811
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22639.41689191287
Iteration 100: Loss = -11456.044077086943
Iteration 200: Loss = -11454.662348905258
Iteration 300: Loss = -11451.16023117014
Iteration 400: Loss = -11448.47401421558
Iteration 500: Loss = -11446.137353092352
Iteration 600: Loss = -11436.624123193833
Iteration 700: Loss = -11370.952568905384
Iteration 800: Loss = -11229.294683410622
Iteration 900: Loss = -11184.18107102512
Iteration 1000: Loss = -11175.869158473512
Iteration 1100: Loss = -11163.687676798212
Iteration 1200: Loss = -11157.136059874401
Iteration 1300: Loss = -11156.935082763433
Iteration 1400: Loss = -11156.813987527401
Iteration 1500: Loss = -11156.71888720038
Iteration 1600: Loss = -11156.635389195677
Iteration 1700: Loss = -11156.503974998142
Iteration 1800: Loss = -11156.461037591846
Iteration 1900: Loss = -11156.435334265994
Iteration 2000: Loss = -11156.414272427079
Iteration 2100: Loss = -11156.396490013
Iteration 2200: Loss = -11156.380618371475
Iteration 2300: Loss = -11156.363600315197
Iteration 2400: Loss = -11156.337016432106
Iteration 2500: Loss = -11156.302114847505
Iteration 2600: Loss = -11156.28724695799
Iteration 2700: Loss = -11155.88321283586
Iteration 2800: Loss = -11155.866295336184
Iteration 2900: Loss = -11155.86498098247
Iteration 3000: Loss = -11155.855758490748
Iteration 3100: Loss = -11155.851302409048
Iteration 3200: Loss = -11155.847786039074
Iteration 3300: Loss = -11155.853222729045
1
Iteration 3400: Loss = -11155.840717754583
Iteration 3500: Loss = -11155.837485774067
Iteration 3600: Loss = -11155.835038263522
Iteration 3700: Loss = -11155.83201485664
Iteration 3800: Loss = -11155.829829630153
Iteration 3900: Loss = -11155.82819818914
Iteration 4000: Loss = -11155.825063556606
Iteration 4100: Loss = -11155.817566500426
Iteration 4200: Loss = -11155.786879889802
Iteration 4300: Loss = -11155.784614259857
Iteration 4400: Loss = -11155.783190646094
Iteration 4500: Loss = -11155.792484305804
1
Iteration 4600: Loss = -11155.78026494414
Iteration 4700: Loss = -11155.779973402541
Iteration 4800: Loss = -11155.777872805444
Iteration 4900: Loss = -11155.779763269102
1
Iteration 5000: Loss = -11155.789400272359
2
Iteration 5100: Loss = -11155.776677873635
Iteration 5200: Loss = -11155.774364659444
Iteration 5300: Loss = -11155.773435039528
Iteration 5400: Loss = -11155.773670089004
1
Iteration 5500: Loss = -11155.773375479792
Iteration 5600: Loss = -11155.770472139557
Iteration 5700: Loss = -11155.769825503363
Iteration 5800: Loss = -11155.769194343797
Iteration 5900: Loss = -11155.76891409187
Iteration 6000: Loss = -11155.769816979786
1
Iteration 6100: Loss = -11155.767261405379
Iteration 6200: Loss = -11155.766613978241
Iteration 6300: Loss = -11155.768645340864
1
Iteration 6400: Loss = -11155.765305141384
Iteration 6500: Loss = -11155.768024767
1
Iteration 6600: Loss = -11155.764355794554
Iteration 6700: Loss = -11155.771567031174
1
Iteration 6800: Loss = -11155.763824685984
Iteration 6900: Loss = -11155.776443514815
1
Iteration 7000: Loss = -11155.76345858294
Iteration 7100: Loss = -11155.76324047245
Iteration 7200: Loss = -11155.763662934889
1
Iteration 7300: Loss = -11155.762896900962
Iteration 7400: Loss = -11155.767987139738
1
Iteration 7500: Loss = -11155.863310155333
2
Iteration 7600: Loss = -11155.763834439764
3
Iteration 7700: Loss = -11155.766159847237
4
Iteration 7800: Loss = -11155.76199626076
Iteration 7900: Loss = -11155.750771836365
Iteration 8000: Loss = -11155.750966994374
1
Iteration 8100: Loss = -11155.766124090183
2
Iteration 8200: Loss = -11155.74514481641
Iteration 8300: Loss = -11155.745499904631
1
Iteration 8400: Loss = -11155.74449121416
Iteration 8500: Loss = -11155.74680084111
1
Iteration 8600: Loss = -11155.755299433531
2
Iteration 8700: Loss = -11155.74409243721
Iteration 8800: Loss = -11155.764670132383
1
Iteration 8900: Loss = -11155.743908499684
Iteration 9000: Loss = -11155.746759994865
1
Iteration 9100: Loss = -11155.74366823008
Iteration 9200: Loss = -11155.743575907123
Iteration 9300: Loss = -11155.757810463483
1
Iteration 9400: Loss = -11155.74414561443
2
Iteration 9500: Loss = -11155.743463289826
Iteration 9600: Loss = -11155.74360723354
1
Iteration 9700: Loss = -11155.746683941372
2
Iteration 9800: Loss = -11155.743267126983
Iteration 9900: Loss = -11155.745019403514
1
Iteration 10000: Loss = -11155.743199950937
Iteration 10100: Loss = -11155.743541492518
1
Iteration 10200: Loss = -11155.86068145665
2
Iteration 10300: Loss = -11155.743074841748
Iteration 10400: Loss = -11155.744500722793
1
Iteration 10500: Loss = -11155.743016506807
Iteration 10600: Loss = -11155.742977109361
Iteration 10700: Loss = -11155.749989700975
1
Iteration 10800: Loss = -11155.742930335426
Iteration 10900: Loss = -11155.743779595145
1
Iteration 11000: Loss = -11155.747210166524
2
Iteration 11100: Loss = -11155.758313326063
3
Iteration 11200: Loss = -11155.742978172259
4
Iteration 11300: Loss = -11155.742848289343
Iteration 11400: Loss = -11155.855308166012
1
Iteration 11500: Loss = -11155.742767492213
Iteration 11600: Loss = -11155.74531483186
1
Iteration 11700: Loss = -11155.754514398704
2
Iteration 11800: Loss = -11155.877706415524
3
Iteration 11900: Loss = -11155.74267609979
Iteration 12000: Loss = -11155.74322897672
1
Iteration 12100: Loss = -11155.742543812965
Iteration 12200: Loss = -11155.742041391366
Iteration 12300: Loss = -11155.755202235687
1
Iteration 12400: Loss = -11155.741901657091
Iteration 12500: Loss = -11155.747526183966
1
Iteration 12600: Loss = -11155.74647085592
2
Iteration 12700: Loss = -11155.751707166944
3
Iteration 12800: Loss = -11155.743948145733
4
Iteration 12900: Loss = -11155.744254176463
5
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[0.7611, 0.2389],
        [0.2480, 0.7520]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5090, 0.4910], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2949, 0.1052],
         [0.6180, 0.2021]],

        [[0.6521, 0.0973],
         [0.6883, 0.7272]],

        [[0.6484, 0.0985],
         [0.5063, 0.5983]],

        [[0.6314, 0.1002],
         [0.5535, 0.6678]],

        [[0.6694, 0.0973],
         [0.6076, 0.6326]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.844846433231073
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291540625000051
Average Adjusted Rand Index: 0.929613468448888
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19699.040989431556
Iteration 100: Loss = -11454.962230418101
Iteration 200: Loss = -11453.832095590298
Iteration 300: Loss = -11448.364848374942
Iteration 400: Loss = -11439.58639489027
Iteration 500: Loss = -11259.149621678782
Iteration 600: Loss = -11218.611567682707
Iteration 700: Loss = -11191.285884832008
Iteration 800: Loss = -11179.775649018644
Iteration 900: Loss = -11177.541528618056
Iteration 1000: Loss = -11162.977104910253
Iteration 1100: Loss = -11162.639149545672
Iteration 1200: Loss = -11156.079753506177
Iteration 1300: Loss = -11156.034063797226
Iteration 1400: Loss = -11156.007692203377
Iteration 1500: Loss = -11155.989293143286
Iteration 1600: Loss = -11155.973479372857
Iteration 1700: Loss = -11155.95575020628
Iteration 1800: Loss = -11155.911439231077
Iteration 1900: Loss = -11155.875060269125
Iteration 2000: Loss = -11155.868607509894
Iteration 2100: Loss = -11155.862358948178
Iteration 2200: Loss = -11155.856740894977
Iteration 2300: Loss = -11155.851911104812
Iteration 2400: Loss = -11155.845101786545
Iteration 2500: Loss = -11155.824912452776
Iteration 2600: Loss = -11155.787110753285
Iteration 2700: Loss = -11155.78350742412
Iteration 2800: Loss = -11155.780864166054
Iteration 2900: Loss = -11155.77868873512
Iteration 3000: Loss = -11155.776739160014
Iteration 3100: Loss = -11155.775412101695
Iteration 3200: Loss = -11155.773728227126
Iteration 3300: Loss = -11155.772673998243
Iteration 3400: Loss = -11155.774776171233
1
Iteration 3500: Loss = -11155.770878150328
Iteration 3600: Loss = -11155.770105529104
Iteration 3700: Loss = -11155.76947908116
Iteration 3800: Loss = -11155.764265680587
Iteration 3900: Loss = -11155.752096812876
Iteration 4000: Loss = -11155.751051403626
Iteration 4100: Loss = -11155.7492085297
Iteration 4200: Loss = -11155.748122447392
Iteration 4300: Loss = -11155.76865070609
1
Iteration 4400: Loss = -11155.746522017695
Iteration 4500: Loss = -11155.746510255296
Iteration 4600: Loss = -11155.747684552156
1
Iteration 4700: Loss = -11155.745849122035
Iteration 4800: Loss = -11155.745426407999
Iteration 4900: Loss = -11155.745193040704
Iteration 5000: Loss = -11155.74679629648
1
Iteration 5100: Loss = -11155.74481994944
Iteration 5200: Loss = -11155.744837544036
1
Iteration 5300: Loss = -11155.74454531716
Iteration 5400: Loss = -11155.744419127526
Iteration 5500: Loss = -11155.744233108795
Iteration 5600: Loss = -11155.744184600024
Iteration 5700: Loss = -11155.757746267138
1
Iteration 5800: Loss = -11155.744106898075
Iteration 5900: Loss = -11155.743850977757
Iteration 6000: Loss = -11155.767067815395
1
Iteration 6100: Loss = -11155.74384457975
Iteration 6200: Loss = -11155.751540851485
1
Iteration 6300: Loss = -11155.743460887383
Iteration 6400: Loss = -11155.74417122837
1
Iteration 6500: Loss = -11155.743354585842
Iteration 6600: Loss = -11155.748952867583
1
Iteration 6700: Loss = -11155.75447768295
2
Iteration 6800: Loss = -11155.743167877848
Iteration 6900: Loss = -11155.747331139826
1
Iteration 7000: Loss = -11155.758686008914
2
Iteration 7100: Loss = -11155.743418028058
3
Iteration 7200: Loss = -11155.742986924959
Iteration 7300: Loss = -11155.743235988739
1
Iteration 7400: Loss = -11155.751792096913
2
Iteration 7500: Loss = -11155.744050682411
3
Iteration 7600: Loss = -11155.744011968152
4
Iteration 7700: Loss = -11155.7428525054
Iteration 7800: Loss = -11155.742946098831
1
Iteration 7900: Loss = -11155.775865046264
2
Iteration 8000: Loss = -11155.743346594132
3
Iteration 8100: Loss = -11155.742798153286
Iteration 8200: Loss = -11155.743469247815
1
Iteration 8300: Loss = -11155.74263182927
Iteration 8400: Loss = -11155.74258219969
Iteration 8500: Loss = -11155.742450893025
Iteration 8600: Loss = -11155.871418868739
1
Iteration 8700: Loss = -11155.7419985343
Iteration 8800: Loss = -11155.746289906938
1
Iteration 8900: Loss = -11155.741917782327
Iteration 9000: Loss = -11155.74199028089
1
Iteration 9100: Loss = -11155.742169428771
2
Iteration 9200: Loss = -11155.742036400508
3
Iteration 9300: Loss = -11155.748257679692
4
Iteration 9400: Loss = -11155.768315612822
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.7594, 0.2406],
        [0.2470, 0.7530]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5062, 0.4938], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2962, 0.1050],
         [0.5080, 0.2019]],

        [[0.6792, 0.0972],
         [0.5585, 0.6943]],

        [[0.7279, 0.0986],
         [0.6643, 0.5894]],

        [[0.6469, 0.1005],
         [0.5196, 0.5284]],

        [[0.7134, 0.0980],
         [0.6417, 0.5390]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.844846433231073
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291540625000051
Average Adjusted Rand Index: 0.929613468448888
11173.137324635527
[0.039212948139009274, 0.039212948139009274, 0.9291540625000051, 0.9291540625000051] [0.1903644849105811, 0.1903644849105811, 0.929613468448888, 0.929613468448888] [11406.253059137804, 11406.25239184524, 11155.744254176463, 11155.768315612822]
-----------------------------------------------------------------------------------------
This iteration is 7
True Objective function: Loss = -11260.94225842151
Iteration 0: Loss = -11502.014116334594
Iteration 10: Loss = -11496.803373312536
Iteration 20: Loss = -11493.459959706934
Iteration 30: Loss = -11492.04622772388
Iteration 40: Loss = -11491.269647130663
Iteration 50: Loss = -11490.958786327184
Iteration 60: Loss = -11490.988205168222
1
Iteration 70: Loss = -11491.081724177982
2
Iteration 80: Loss = -11491.16872223851
3
Stopping early at iteration 80 due to no improvement.
pi: tensor([[0.0781, 0.9219],
        [0.1033, 0.8967]], dtype=torch.float64)
alpha: tensor([0.0947, 0.9053])
beta: tensor([[[0.2591, 0.0789],
         [0.1209, 0.1697]],

        [[0.7390, 0.2290],
         [0.4323, 0.9832]],

        [[0.5237, 0.1981],
         [0.6229, 0.3620]],

        [[0.5432, 0.2104],
         [0.7799, 0.2100]],

        [[0.1647, 0.1985],
         [0.2349, 0.8403]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.015850928847493215
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.002821183194503557
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00033249265490082464
Average Adjusted Rand Index: 0.0026059491305979315
Iteration 0: Loss = -11671.176186078945
Iteration 10: Loss = -11499.038175636617
Iteration 20: Loss = -11484.615011630956
Iteration 30: Loss = -11246.731233215829
Iteration 40: Loss = -11246.57449853818
Iteration 50: Loss = -11246.574571347446
1
Iteration 60: Loss = -11246.57456976463
2
Iteration 70: Loss = -11246.574566998144
3
Stopping early at iteration 70 due to no improvement.
pi: tensor([[0.7472, 0.2528],
        [0.2273, 0.7727]], dtype=torch.float64)
alpha: tensor([0.4928, 0.5072])
beta: tensor([[[0.2896, 0.1018],
         [0.3201, 0.1988]],

        [[0.5728, 0.1042],
         [0.4581, 0.3952]],

        [[0.2757, 0.0988],
         [0.6728, 0.6854]],

        [[0.3965, 0.1123],
         [0.6771, 0.7921]],

        [[0.8207, 0.1056],
         [0.5302, 0.7818]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9603205123301661
Average Adjusted Rand Index: 0.9601586834545799
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23051.172514773138
Iteration 100: Loss = -11476.590177651731
Iteration 200: Loss = -11251.943325836828
Iteration 300: Loss = -11246.48972618178
Iteration 400: Loss = -11246.298134573412
Iteration 500: Loss = -11246.24010641106
Iteration 600: Loss = -11246.203132257247
Iteration 700: Loss = -11246.124257090438
Iteration 800: Loss = -11246.110341037194
Iteration 900: Loss = -11246.097299493687
Iteration 1000: Loss = -11242.040297059546
Iteration 1100: Loss = -11242.025656315582
Iteration 1200: Loss = -11242.018617606223
Iteration 1300: Loss = -11242.014538067238
Iteration 1400: Loss = -11242.011726623827
Iteration 1500: Loss = -11242.008537663329
Iteration 1600: Loss = -11242.006606696037
Iteration 1700: Loss = -11241.992985737275
Iteration 1800: Loss = -11241.991417933205
Iteration 1900: Loss = -11241.990537974367
Iteration 2000: Loss = -11241.989783502742
Iteration 2100: Loss = -11241.989143717838
Iteration 2200: Loss = -11241.988583137396
Iteration 2300: Loss = -11241.988000510695
Iteration 2400: Loss = -11241.987546877077
Iteration 2500: Loss = -11241.98717597698
Iteration 2600: Loss = -11241.986867804691
Iteration 2700: Loss = -11241.986524810265
Iteration 2800: Loss = -11241.986012455018
Iteration 2900: Loss = -11241.985576179406
Iteration 3000: Loss = -11241.984919181137
Iteration 3100: Loss = -11241.983181804808
Iteration 3200: Loss = -11241.974798058633
Iteration 3300: Loss = -11241.97438111534
Iteration 3400: Loss = -11241.993483662003
1
Iteration 3500: Loss = -11241.974114383156
Iteration 3600: Loss = -11241.973942060431
Iteration 3700: Loss = -11241.974675907491
1
Iteration 3800: Loss = -11241.973729153906
Iteration 3900: Loss = -11241.973788552961
1
Iteration 4000: Loss = -11241.97357980037
Iteration 4100: Loss = -11241.973424171265
Iteration 4200: Loss = -11241.979358303332
1
Iteration 4300: Loss = -11241.973228308287
Iteration 4400: Loss = -11241.97403147193
1
Iteration 4500: Loss = -11241.973122492263
Iteration 4600: Loss = -11241.97427170914
1
Iteration 4700: Loss = -11241.973151714878
2
Iteration 4800: Loss = -11241.97326865073
3
Iteration 4900: Loss = -11241.974722278872
4
Iteration 5000: Loss = -11241.970893529446
Iteration 5100: Loss = -11241.970901272316
1
Iteration 5200: Loss = -11241.970804961213
Iteration 5300: Loss = -11241.97079164819
Iteration 5400: Loss = -11241.970771984452
Iteration 5500: Loss = -11241.971730211013
1
Iteration 5600: Loss = -11241.970673730435
Iteration 5700: Loss = -11241.977247874822
1
Iteration 5800: Loss = -11241.970645689884
Iteration 5900: Loss = -11241.970631111362
Iteration 6000: Loss = -11241.970642694589
1
Iteration 6100: Loss = -11241.970579090816
Iteration 6200: Loss = -11242.00736068501
1
Iteration 6300: Loss = -11241.970571877971
Iteration 6400: Loss = -11241.970549167265
Iteration 6500: Loss = -11241.970580596815
1
Iteration 6600: Loss = -11241.970530965744
Iteration 6700: Loss = -11241.970647263073
1
Iteration 6800: Loss = -11241.970431307263
Iteration 6900: Loss = -11241.970495271415
1
Iteration 7000: Loss = -11241.973368258095
2
Iteration 7100: Loss = -11241.970501103826
3
Iteration 7200: Loss = -11241.97117973774
4
Iteration 7300: Loss = -11241.972097112006
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.7924, 0.2076],
        [0.2383, 0.7617]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4859, 0.5141], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2025, 0.1009],
         [0.6887, 0.2963]],

        [[0.5289, 0.1040],
         [0.5854, 0.5984]],

        [[0.6640, 0.0989],
         [0.6944, 0.5870]],

        [[0.6968, 0.1131],
         [0.5849, 0.5244]],

        [[0.7232, 0.1055],
         [0.7274, 0.6593]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9681922747607277
Average Adjusted Rand Index: 0.96815934172729
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20720.824015709597
Iteration 100: Loss = -11502.224633399117
Iteration 200: Loss = -11501.724611759182
Iteration 300: Loss = -11501.548145560639
Iteration 400: Loss = -11501.213079640273
Iteration 500: Loss = -11499.826646355494
Iteration 600: Loss = -11495.62408151671
Iteration 700: Loss = -11471.59898083095
Iteration 800: Loss = -11329.865903932841
Iteration 900: Loss = -11321.786877735978
Iteration 1000: Loss = -11305.275353542023
Iteration 1100: Loss = -11302.152050558829
Iteration 1200: Loss = -11299.915401698807
Iteration 1300: Loss = -11299.802067292092
Iteration 1400: Loss = -11299.718623771325
Iteration 1500: Loss = -11299.64282815202
Iteration 1600: Loss = -11299.218752050736
Iteration 1700: Loss = -11299.148238190315
Iteration 1800: Loss = -11298.76302385545
Iteration 1900: Loss = -11297.916434711557
Iteration 2000: Loss = -11297.90451529393
Iteration 2100: Loss = -11297.894872071434
Iteration 2200: Loss = -11297.886078481113
Iteration 2300: Loss = -11297.876138743899
Iteration 2400: Loss = -11297.856301396961
Iteration 2500: Loss = -11297.815025485159
Iteration 2600: Loss = -11297.807568830394
Iteration 2700: Loss = -11297.803141992083
Iteration 2800: Loss = -11297.79908610832
Iteration 2900: Loss = -11297.79475718698
Iteration 3000: Loss = -11297.789052911286
Iteration 3100: Loss = -11297.785015803927
Iteration 3200: Loss = -11297.779068787293
Iteration 3300: Loss = -11297.752620313033
Iteration 3400: Loss = -11297.750570217358
Iteration 3500: Loss = -11297.751209589323
1
Iteration 3600: Loss = -11297.747063195042
Iteration 3700: Loss = -11297.745957014067
Iteration 3800: Loss = -11297.744862720743
Iteration 3900: Loss = -11297.74289351782
Iteration 4000: Loss = -11297.740803952907
Iteration 4100: Loss = -11297.740405494931
Iteration 4200: Loss = -11297.741296638642
1
Iteration 4300: Loss = -11297.740101531275
Iteration 4400: Loss = -11297.740826343459
1
Iteration 4500: Loss = -11297.74136129552
2
Iteration 4600: Loss = -11297.738646206022
Iteration 4700: Loss = -11297.741024975834
1
Iteration 4800: Loss = -11297.736704200246
Iteration 4900: Loss = -11297.736396614247
Iteration 5000: Loss = -11297.737758016367
1
Iteration 5100: Loss = -11297.738773548554
2
Iteration 5200: Loss = -11297.73560195886
Iteration 5300: Loss = -11297.742129229688
1
Iteration 5400: Loss = -11297.735089153028
Iteration 5500: Loss = -11297.734750211004
Iteration 5600: Loss = -11297.734554238737
Iteration 5700: Loss = -11297.735186173208
1
Iteration 5800: Loss = -11297.734261562167
Iteration 5900: Loss = -11297.733983488848
Iteration 6000: Loss = -11297.734787634407
1
Iteration 6100: Loss = -11297.733691963313
Iteration 6200: Loss = -11297.7340707178
1
Iteration 6300: Loss = -11297.733416409314
Iteration 6400: Loss = -11297.735348931968
1
Iteration 6500: Loss = -11297.733137891175
Iteration 6600: Loss = -11297.73308671263
Iteration 6700: Loss = -11297.733162490815
1
Iteration 6800: Loss = -11297.732851714749
Iteration 6900: Loss = -11297.732748533392
Iteration 7000: Loss = -11297.73311920424
1
Iteration 7100: Loss = -11297.732529741928
Iteration 7200: Loss = -11297.732421992821
Iteration 7300: Loss = -11297.732357078134
Iteration 7400: Loss = -11297.732195657289
Iteration 7500: Loss = -11297.732122766165
Iteration 7600: Loss = -11297.732040147583
Iteration 7700: Loss = -11297.731884565173
Iteration 7800: Loss = -11297.731588876544
Iteration 7900: Loss = -11297.733465325871
1
Iteration 8000: Loss = -11297.732965124673
2
Iteration 8100: Loss = -11297.739510566887
3
Iteration 8200: Loss = -11297.732137798166
4
Iteration 8300: Loss = -11297.730548857979
Iteration 8400: Loss = -11297.730365792557
Iteration 8500: Loss = -11297.73072348134
1
Iteration 8600: Loss = -11297.730575546773
2
Iteration 8700: Loss = -11297.73000125799
Iteration 8800: Loss = -11297.729875695799
Iteration 8900: Loss = -11297.729767376966
Iteration 9000: Loss = -11297.729856279857
1
Iteration 9100: Loss = -11297.729414176263
Iteration 9200: Loss = -11297.729355562054
Iteration 9300: Loss = -11297.729382443877
1
Iteration 9400: Loss = -11297.730723149545
2
Iteration 9500: Loss = -11297.739153590166
3
Iteration 9600: Loss = -11297.729489119778
4
Iteration 9700: Loss = -11297.729278066414
Iteration 9800: Loss = -11297.740978751326
1
Iteration 9900: Loss = -11297.734864832824
2
Iteration 10000: Loss = -11297.730923018467
3
Iteration 10100: Loss = -11297.858543154922
4
Iteration 10200: Loss = -11297.729135061112
Iteration 10300: Loss = -11297.730219438947
1
Iteration 10400: Loss = -11297.731725368662
2
Iteration 10500: Loss = -11297.729092488597
Iteration 10600: Loss = -11297.72929804393
1
Iteration 10700: Loss = -11297.731417668807
2
Iteration 10800: Loss = -11297.730797992144
3
Iteration 10900: Loss = -11297.729618541465
4
Iteration 11000: Loss = -11297.729075367264
Iteration 11100: Loss = -11297.729391518773
1
Iteration 11200: Loss = -11297.72903569826
Iteration 11300: Loss = -11297.729454789242
1
Iteration 11400: Loss = -11297.72970617796
2
Iteration 11500: Loss = -11297.739552311152
3
Iteration 11600: Loss = -11297.728957420379
Iteration 11700: Loss = -11297.72907331889
1
Iteration 11800: Loss = -11297.89693218044
2
Iteration 11900: Loss = -11297.72886000597
Iteration 12000: Loss = -11297.729295218176
1
Iteration 12100: Loss = -11297.747000087147
2
Iteration 12200: Loss = -11297.728857869542
Iteration 12300: Loss = -11297.72917230933
1
Iteration 12400: Loss = -11297.729801521104
2
Iteration 12500: Loss = -11297.729468378247
3
Iteration 12600: Loss = -11297.735214931772
4
Iteration 12700: Loss = -11297.752136498646
5
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[0.6764, 0.3236],
        [0.2733, 0.7267]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9178, 0.0822], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1941, 0.0820],
         [0.6991, 0.3039]],

        [[0.6716, 0.1055],
         [0.5320, 0.6491]],

        [[0.6818, 0.0989],
         [0.6481, 0.5191]],

        [[0.7238, 0.1130],
         [0.5594, 0.5891]],

        [[0.7240, 0.1057],
         [0.5793, 0.5844]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.015850928847493215
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5707077533057847
Average Adjusted Rand Index: 0.7714904443775213
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23144.586575324145
Iteration 100: Loss = -11503.292102883504
Iteration 200: Loss = -11502.181723596728
Iteration 300: Loss = -11501.76820193832
Iteration 400: Loss = -11501.486156024133
Iteration 500: Loss = -11501.049057990664
Iteration 600: Loss = -11498.644482344705
Iteration 700: Loss = -11494.645457751496
Iteration 800: Loss = -11493.060692639972
Iteration 900: Loss = -11492.261226555824
Iteration 1000: Loss = -11491.857305078574
Iteration 1100: Loss = -11491.53944368839
Iteration 1200: Loss = -11491.233322236545
Iteration 1300: Loss = -11490.935186511537
Iteration 1400: Loss = -11490.699147570356
Iteration 1500: Loss = -11490.546483708915
Iteration 1600: Loss = -11490.444852859442
Iteration 1700: Loss = -11490.375678030287
Iteration 1800: Loss = -11490.32437600658
Iteration 1900: Loss = -11490.272078615559
Iteration 2000: Loss = -11490.17875427071
Iteration 2100: Loss = -11488.790451417959
Iteration 2200: Loss = -11300.905448206791
Iteration 2300: Loss = -11298.74001548975
Iteration 2400: Loss = -11298.528568357886
Iteration 2500: Loss = -11298.392783517487
Iteration 2600: Loss = -11298.34134723968
Iteration 2700: Loss = -11298.312133902282
Iteration 2800: Loss = -11298.291245541024
Iteration 2900: Loss = -11298.263850622767
Iteration 3000: Loss = -11298.160337789694
Iteration 3100: Loss = -11298.136274703884
Iteration 3200: Loss = -11298.081700837523
Iteration 3300: Loss = -11298.050760182381
Iteration 3400: Loss = -11298.017484676631
Iteration 3500: Loss = -11298.00982730207
Iteration 3600: Loss = -11298.003896506829
Iteration 3700: Loss = -11298.002968994979
Iteration 3800: Loss = -11297.986919938247
Iteration 3900: Loss = -11297.980463091171
Iteration 4000: Loss = -11297.976713422997
Iteration 4100: Loss = -11297.974554429176
Iteration 4200: Loss = -11297.970927582817
Iteration 4300: Loss = -11297.966611712965
Iteration 4400: Loss = -11297.94938410291
Iteration 4500: Loss = -11297.923815266051
Iteration 4600: Loss = -11297.915653254562
Iteration 4700: Loss = -11297.914296570001
Iteration 4800: Loss = -11297.912088019708
Iteration 4900: Loss = -11297.910260687475
Iteration 5000: Loss = -11297.908166890096
Iteration 5100: Loss = -11297.904842412916
Iteration 5200: Loss = -11297.899679754424
Iteration 5300: Loss = -11297.889764764319
Iteration 5400: Loss = -11297.879706271739
Iteration 5500: Loss = -11297.876852522071
Iteration 5600: Loss = -11297.875385761248
Iteration 5700: Loss = -11297.876330665387
1
Iteration 5800: Loss = -11297.871709921586
Iteration 5900: Loss = -11297.87071887486
Iteration 6000: Loss = -11297.869823223647
Iteration 6100: Loss = -11297.869159736818
Iteration 6200: Loss = -11297.868054478844
Iteration 6300: Loss = -11297.866710926974
Iteration 6400: Loss = -11297.86521023569
Iteration 6500: Loss = -11297.867940202526
1
Iteration 6600: Loss = -11297.858736217913
Iteration 6700: Loss = -11297.857988197817
Iteration 6800: Loss = -11297.8570779547
Iteration 6900: Loss = -11297.854675147768
Iteration 7000: Loss = -11297.855589390743
1
Iteration 7100: Loss = -11297.864461310419
2
Iteration 7200: Loss = -11297.853517364383
Iteration 7300: Loss = -11297.853323331008
Iteration 7400: Loss = -11297.853149725106
Iteration 7500: Loss = -11297.852997661712
Iteration 7600: Loss = -11297.85322125599
1
Iteration 7700: Loss = -11297.85272802882
Iteration 7800: Loss = -11297.852611879911
Iteration 7900: Loss = -11297.85245457575
Iteration 8000: Loss = -11297.852336566317
Iteration 8100: Loss = -11297.852908523173
1
Iteration 8200: Loss = -11297.851993582914
Iteration 8300: Loss = -11297.852852967197
1
Iteration 8400: Loss = -11297.851482325954
Iteration 8500: Loss = -11297.85125731477
Iteration 8600: Loss = -11297.851516639828
1
Iteration 8700: Loss = -11297.898302050397
2
Iteration 8800: Loss = -11297.8506315891
Iteration 8900: Loss = -11297.850670768827
1
Iteration 9000: Loss = -11297.854052170533
2
Iteration 9100: Loss = -11297.892443003377
3
Iteration 9200: Loss = -11297.796175146459
Iteration 9300: Loss = -11297.796354249935
1
Iteration 9400: Loss = -11297.795882482014
Iteration 9500: Loss = -11297.798568896575
1
Iteration 9600: Loss = -11297.793315262104
Iteration 9700: Loss = -11297.793054154923
Iteration 9800: Loss = -11297.792938481822
Iteration 9900: Loss = -11297.792709256164
Iteration 10000: Loss = -11297.792536818968
Iteration 10100: Loss = -11297.792488323455
Iteration 10200: Loss = -11297.792144260682
Iteration 10300: Loss = -11297.792011370575
Iteration 10400: Loss = -11297.801413337675
1
Iteration 10500: Loss = -11297.791796930094
Iteration 10600: Loss = -11297.811596750214
1
Iteration 10700: Loss = -11297.754534782922
Iteration 10800: Loss = -11297.733338137505
Iteration 10900: Loss = -11297.731111516727
Iteration 11000: Loss = -11297.730264886926
Iteration 11100: Loss = -11297.730592885291
1
Iteration 11200: Loss = -11297.735223058722
2
Iteration 11300: Loss = -11297.733583213207
3
Iteration 11400: Loss = -11297.730062516277
Iteration 11500: Loss = -11297.730544272508
1
Iteration 11600: Loss = -11297.739084496392
2
Iteration 11700: Loss = -11297.755577065296
3
Iteration 11800: Loss = -11297.72987886478
Iteration 11900: Loss = -11297.730211094235
1
Iteration 12000: Loss = -11297.81428351456
2
Iteration 12100: Loss = -11297.72937000983
Iteration 12200: Loss = -11297.729456549792
1
Iteration 12300: Loss = -11297.739090299487
2
Iteration 12400: Loss = -11297.731121217666
3
Iteration 12500: Loss = -11297.72928974697
Iteration 12600: Loss = -11297.72967104716
1
Iteration 12700: Loss = -11297.75489485538
2
Iteration 12800: Loss = -11297.729259179076
Iteration 12900: Loss = -11297.734308576748
1
Iteration 13000: Loss = -11297.903350729981
2
Iteration 13100: Loss = -11297.729248277268
Iteration 13200: Loss = -11297.732291004908
1
Iteration 13300: Loss = -11297.729142149106
Iteration 13400: Loss = -11297.729144353867
1
Iteration 13500: Loss = -11297.732840756966
2
Iteration 13600: Loss = -11297.73158839681
3
Iteration 13700: Loss = -11297.729371405198
4
Iteration 13800: Loss = -11297.72929431025
5
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[0.6745, 0.3255],
        [0.2729, 0.7271]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9171, 0.0829], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1949, 0.0824],
         [0.6677, 0.3032]],

        [[0.5414, 0.1052],
         [0.5940, 0.7298]],

        [[0.7085, 0.0987],
         [0.5585, 0.6199]],

        [[0.5269, 0.1126],
         [0.6931, 0.6086]],

        [[0.6785, 0.1055],
         [0.5482, 0.5532]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.015850928847493215
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5707077533057847
Average Adjusted Rand Index: 0.7714904443775213
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24011.25623583918
Iteration 100: Loss = -11493.652955474263
Iteration 200: Loss = -11289.642230497082
Iteration 300: Loss = -11243.178674019868
Iteration 400: Loss = -11242.369115605034
Iteration 500: Loss = -11242.207942336923
Iteration 600: Loss = -11242.1400732011
Iteration 700: Loss = -11242.100654633025
Iteration 800: Loss = -11242.074846744967
Iteration 900: Loss = -11242.055678098332
Iteration 1000: Loss = -11242.039679801112
Iteration 1100: Loss = -11242.027389244078
Iteration 1200: Loss = -11242.019678339335
Iteration 1300: Loss = -11242.013890275057
Iteration 1400: Loss = -11242.009301128362
Iteration 1500: Loss = -11242.005479277956
Iteration 1600: Loss = -11242.002109944255
Iteration 1700: Loss = -11241.998606216572
Iteration 1800: Loss = -11241.992032305725
Iteration 1900: Loss = -11241.987133138287
Iteration 2000: Loss = -11241.985544060786
Iteration 2100: Loss = -11241.984158044212
Iteration 2200: Loss = -11241.982965417425
Iteration 2300: Loss = -11241.981889296949
Iteration 2400: Loss = -11241.980877484217
Iteration 2500: Loss = -11241.9799502193
Iteration 2600: Loss = -11241.979054943777
Iteration 2700: Loss = -11241.97825242988
Iteration 2800: Loss = -11241.977504841385
Iteration 2900: Loss = -11241.976825113086
Iteration 3000: Loss = -11241.976317616669
Iteration 3100: Loss = -11241.97589879457
Iteration 3200: Loss = -11241.975479346936
Iteration 3300: Loss = -11241.975198353177
Iteration 3400: Loss = -11241.974904734265
Iteration 3500: Loss = -11241.97458228885
Iteration 3600: Loss = -11241.974336161236
Iteration 3700: Loss = -11241.974092755752
Iteration 3800: Loss = -11241.973991118624
Iteration 3900: Loss = -11241.97373439726
Iteration 4000: Loss = -11241.975914271341
1
Iteration 4100: Loss = -11241.973349324986
Iteration 4200: Loss = -11241.973156088116
Iteration 4300: Loss = -11241.973322987746
1
Iteration 4400: Loss = -11241.972915016002
Iteration 4500: Loss = -11241.972884236167
Iteration 4600: Loss = -11241.972644819596
Iteration 4700: Loss = -11241.972785987375
1
Iteration 4800: Loss = -11241.972450849396
Iteration 4900: Loss = -11241.973856756471
1
Iteration 5000: Loss = -11241.973722427974
2
Iteration 5100: Loss = -11241.975819651703
3
Iteration 5200: Loss = -11241.9719369227
Iteration 5300: Loss = -11241.981829876157
1
Iteration 5400: Loss = -11241.971775089361
Iteration 5500: Loss = -11241.971812506506
1
Iteration 5600: Loss = -11241.971712817123
Iteration 5700: Loss = -11241.971464971635
Iteration 5800: Loss = -11241.971426446771
Iteration 5900: Loss = -11241.972259974484
1
Iteration 6000: Loss = -11241.971335617722
Iteration 6100: Loss = -11241.971444198718
1
Iteration 6200: Loss = -11241.971198611425
Iteration 6300: Loss = -11241.9711412662
Iteration 6400: Loss = -11241.97094365925
Iteration 6500: Loss = -11241.971732453523
1
Iteration 6600: Loss = -11241.970745324585
Iteration 6700: Loss = -11241.979720183805
1
Iteration 6800: Loss = -11241.970653842955
Iteration 6900: Loss = -11241.973032069014
1
Iteration 7000: Loss = -11241.975736226881
2
Iteration 7100: Loss = -11241.97305709093
3
Iteration 7200: Loss = -11241.97044721827
Iteration 7300: Loss = -11241.983221237171
1
Iteration 7400: Loss = -11241.973045640896
2
Iteration 7500: Loss = -11241.97050267708
3
Iteration 7600: Loss = -11241.970331651903
Iteration 7700: Loss = -11241.97029391211
Iteration 7800: Loss = -11241.970386611229
1
Iteration 7900: Loss = -11241.971207708413
2
Iteration 8000: Loss = -11241.970390445913
3
Iteration 8100: Loss = -11241.970384349577
4
Iteration 8200: Loss = -11241.971250254148
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.7590, 0.2410],
        [0.2083, 0.7917]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5155, 0.4845], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2963, 0.1009],
         [0.7092, 0.2025]],

        [[0.5513, 0.1040],
         [0.7238, 0.6403]],

        [[0.5944, 0.0989],
         [0.5781, 0.5331]],

        [[0.7077, 0.1131],
         [0.6920, 0.5320]],

        [[0.6116, 0.1055],
         [0.6885, 0.6217]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9681922747607277
Average Adjusted Rand Index: 0.96815934172729
11260.94225842151
[0.9681922747607277, 0.5707077533057847, 0.5707077533057847, 0.9681922747607277] [0.96815934172729, 0.7714904443775213, 0.7714904443775213, 0.96815934172729] [11241.972097112006, 11297.752136498646, 11297.72929431025, 11241.971250254148]
-----------------------------------------------------------------------------------------
This iteration is 8
True Objective function: Loss = -11289.962499424195
Iteration 0: Loss = -11480.215460695337
Iteration 10: Loss = -11476.5659533368
Iteration 20: Loss = -11475.548423646722
Iteration 30: Loss = -11475.337038450743
Iteration 40: Loss = -11475.134602846118
Iteration 50: Loss = -11475.129622257587
Iteration 60: Loss = -11475.12894121885
Iteration 70: Loss = -11475.128770124844
Iteration 80: Loss = -11475.128692883032
Iteration 90: Loss = -11475.128664785143
Iteration 100: Loss = -11475.128677558141
1
Iteration 110: Loss = -11475.128667929675
2
Iteration 120: Loss = -11475.128663222351
Iteration 130: Loss = -11475.128666462666
1
Iteration 140: Loss = -11475.128659730563
Iteration 150: Loss = -11475.128682977802
1
Iteration 160: Loss = -11475.12865824883
Iteration 170: Loss = -11475.128663190486
1
Iteration 180: Loss = -11475.128669147136
2
Iteration 190: Loss = -11475.128666058205
3
Stopping early at iteration 190 due to no improvement.
pi: tensor([[0.0916, 0.9084],
        [0.0331, 0.9669]], dtype=torch.float64)
alpha: tensor([0.0353, 0.9647])
beta: tensor([[[0.1900, 0.2097],
         [0.9274, 0.1731]],

        [[0.7977, 0.0959],
         [0.9063, 0.5927]],

        [[0.5977, 0.1080],
         [0.3663, 0.5802]],

        [[0.8290, 0.2048],
         [0.5440, 0.8692]],

        [[0.3340, 0.2661],
         [0.9829, 0.9779]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 4.2092736862769894e-05
Average Adjusted Rand Index: -0.00015692302765368048
Iteration 0: Loss = -11754.82401038779
Iteration 10: Loss = -11476.83396865309
Iteration 20: Loss = -11467.183236243904
Iteration 30: Loss = -11323.917310490573
Iteration 40: Loss = -11260.503524551734
Iteration 50: Loss = -11260.431043421873
Iteration 60: Loss = -11260.430715240076
Iteration 70: Loss = -11260.430745994037
1
Iteration 80: Loss = -11260.43074726347
2
Iteration 90: Loss = -11260.43074273982
3
Stopping early at iteration 90 due to no improvement.
pi: tensor([[0.7124, 0.2876],
        [0.2373, 0.7627]], dtype=torch.float64)
alpha: tensor([0.4590, 0.5410])
beta: tensor([[[0.2950, 0.1058],
         [0.5940, 0.1971]],

        [[0.2976, 0.1024],
         [0.8878, 0.4912]],

        [[0.0053, 0.1078],
         [0.8965, 0.5783]],

        [[0.9786, 0.1124],
         [0.3798, 0.6426]],

        [[0.5675, 0.1067],
         [0.3346, 0.3945]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8909163826588622
Average Adjusted Rand Index: 0.8923564323247938
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20328.865471950816
Iteration 100: Loss = -11479.025643364186
Iteration 200: Loss = -11477.754347133818
Iteration 300: Loss = -11476.729081507008
Iteration 400: Loss = -11475.481599589919
Iteration 500: Loss = -11474.537210032084
Iteration 600: Loss = -11474.040021773506
Iteration 700: Loss = -11473.829277000486
Iteration 800: Loss = -11473.754817934476
Iteration 900: Loss = -11473.72162331207
Iteration 1000: Loss = -11473.701796494135
Iteration 1100: Loss = -11473.689315636422
Iteration 1200: Loss = -11473.681631675205
Iteration 1300: Loss = -11473.676889507717
Iteration 1400: Loss = -11473.673709470031
Iteration 1500: Loss = -11473.671368975985
Iteration 1600: Loss = -11473.669463233098
Iteration 1700: Loss = -11473.667685993885
Iteration 1800: Loss = -11473.665960126493
Iteration 1900: Loss = -11473.663971732767
Iteration 2000: Loss = -11473.661647522915
Iteration 2100: Loss = -11473.658625170012
Iteration 2200: Loss = -11473.654458412984
Iteration 2300: Loss = -11473.648468676
Iteration 2400: Loss = -11473.639566740934
Iteration 2500: Loss = -11473.626147378007
Iteration 2600: Loss = -11473.605827804438
Iteration 2700: Loss = -11473.57548512299
Iteration 2800: Loss = -11473.531128392357
Iteration 2900: Loss = -11473.442821626122
Iteration 3000: Loss = -11402.186334757145
Iteration 3100: Loss = -11399.750249959396
Iteration 3200: Loss = -11398.78836477871
Iteration 3300: Loss = -11398.650368223369
Iteration 3400: Loss = -11396.812773523854
Iteration 3500: Loss = -11384.055338063421
Iteration 3600: Loss = -11382.572884351417
Iteration 3700: Loss = -11374.954303083437
Iteration 3800: Loss = -11355.48478433885
Iteration 3900: Loss = -11354.861367880258
Iteration 4000: Loss = -11354.641737395827
Iteration 4100: Loss = -11354.531698607574
Iteration 4200: Loss = -11352.394373044754
Iteration 4300: Loss = -11352.286162890463
Iteration 4400: Loss = -11352.238522197451
Iteration 4500: Loss = -11346.728951760935
Iteration 4600: Loss = -11346.57390398501
Iteration 4700: Loss = -11346.510927545287
Iteration 4800: Loss = -11337.595542088955
Iteration 4900: Loss = -11336.944269616042
Iteration 5000: Loss = -11336.952421519884
1
Iteration 5100: Loss = -11336.937408556663
Iteration 5200: Loss = -11336.936649071828
Iteration 5300: Loss = -11336.936006480624
Iteration 5400: Loss = -11336.935796214108
Iteration 5500: Loss = -11336.934848941572
Iteration 5600: Loss = -11336.93428103478
Iteration 5700: Loss = -11336.933607613211
Iteration 5800: Loss = -11336.931678031144
Iteration 5900: Loss = -11336.930610002144
Iteration 6000: Loss = -11335.652039894316
Iteration 6100: Loss = -11328.660989626553
Iteration 6200: Loss = -11328.623222049204
Iteration 6300: Loss = -11328.621498713263
Iteration 6400: Loss = -11328.613691295632
Iteration 6500: Loss = -11328.536723555302
Iteration 6600: Loss = -11328.53493088889
Iteration 6700: Loss = -11328.53457383236
Iteration 6800: Loss = -11328.534083888664
Iteration 6900: Loss = -11328.533554449057
Iteration 7000: Loss = -11328.533994272017
1
Iteration 7100: Loss = -11328.533283622664
Iteration 7200: Loss = -11328.535735176849
1
Iteration 7300: Loss = -11328.53282154266
Iteration 7400: Loss = -11328.538128649046
1
Iteration 7500: Loss = -11328.531349091663
Iteration 7600: Loss = -11328.539325550702
1
Iteration 7700: Loss = -11328.516985318993
Iteration 7800: Loss = -11328.51865003512
1
Iteration 7900: Loss = -11328.516803814648
Iteration 8000: Loss = -11328.51730492545
1
Iteration 8100: Loss = -11328.516346018494
Iteration 8200: Loss = -11328.529527484934
1
Iteration 8300: Loss = -11328.516329854747
Iteration 8400: Loss = -11328.523117774217
1
Iteration 8500: Loss = -11328.51623945105
Iteration 8600: Loss = -11328.516249657581
1
Iteration 8700: Loss = -11328.517860768923
2
Iteration 8800: Loss = -11328.516201791286
Iteration 8900: Loss = -11328.516144625466
Iteration 9000: Loss = -11328.516283514205
1
Iteration 9100: Loss = -11328.516128249532
Iteration 9200: Loss = -11328.519752679247
1
Iteration 9300: Loss = -11328.53331994733
2
Iteration 9400: Loss = -11328.514816404324
Iteration 9500: Loss = -11328.525811915812
1
Iteration 9600: Loss = -11328.514792334867
Iteration 9700: Loss = -11328.515761660528
1
Iteration 9800: Loss = -11328.515352174913
2
Iteration 9900: Loss = -11328.514994545843
3
Iteration 10000: Loss = -11328.514847085993
4
Iteration 10100: Loss = -11328.51484391052
5
Stopping early at iteration 10100 due to no improvement.
pi: tensor([[0.6516, 0.3484],
        [0.3369, 0.6631]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5749, 0.4251], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2453, 0.1024],
         [0.6332, 0.2463]],

        [[0.7088, 0.0984],
         [0.6713, 0.5673]],

        [[0.7137, 0.1068],
         [0.5771, 0.6247]],

        [[0.6136, 0.1113],
         [0.5694, 0.6787]],

        [[0.5229, 0.1042],
         [0.6192, 0.6723]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 9
Adjusted Rand Index: 0.6691414645181857
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 10
Adjusted Rand Index: 0.6363408394869687
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 11
Adjusted Rand Index: 0.6044847344598628
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 91
Adjusted Rand Index: 0.6691233181935041
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
Global Adjusted Rand Index: 0.014412991830896064
Average Adjusted Rand Index: 0.6923013841895993
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20449.52193776484
Iteration 100: Loss = -11479.74920365345
Iteration 200: Loss = -11478.612986080183
Iteration 300: Loss = -11478.168486825549
Iteration 400: Loss = -11477.891036223207
Iteration 500: Loss = -11477.708536562986
Iteration 600: Loss = -11477.570568583365
Iteration 700: Loss = -11477.456908171023
Iteration 800: Loss = -11477.363160766434
Iteration 900: Loss = -11477.25251893619
Iteration 1000: Loss = -11476.952711346206
Iteration 1100: Loss = -11476.59793306914
Iteration 1200: Loss = -11476.384212040031
Iteration 1300: Loss = -11476.261371692173
Iteration 1400: Loss = -11476.221173858617
Iteration 1500: Loss = -11476.204739913028
Iteration 1600: Loss = -11476.195567658431
Iteration 1700: Loss = -11476.189831971707
Iteration 1800: Loss = -11476.186226270958
Iteration 1900: Loss = -11476.183736202718
Iteration 2000: Loss = -11476.182143741287
Iteration 2100: Loss = -11476.181014017035
Iteration 2200: Loss = -11476.180233989697
Iteration 2300: Loss = -11476.17967457575
Iteration 2400: Loss = -11476.179209777707
Iteration 2500: Loss = -11476.178936521754
Iteration 2600: Loss = -11476.178705275532
Iteration 2700: Loss = -11476.178511437029
Iteration 2800: Loss = -11476.17837586558
Iteration 2900: Loss = -11476.178287315146
Iteration 3000: Loss = -11476.178205122926
Iteration 3100: Loss = -11476.17817995374
Iteration 3200: Loss = -11476.178093914614
Iteration 3300: Loss = -11476.17806171894
Iteration 3400: Loss = -11476.178064831918
1
Iteration 3500: Loss = -11476.178026852362
Iteration 3600: Loss = -11476.17799417536
Iteration 3700: Loss = -11476.178010584557
1
Iteration 3800: Loss = -11476.177945582604
Iteration 3900: Loss = -11476.177948658504
1
Iteration 4000: Loss = -11476.177964170827
2
Iteration 4100: Loss = -11476.177943823644
Iteration 4200: Loss = -11476.177905021419
Iteration 4300: Loss = -11476.177915245138
1
Iteration 4400: Loss = -11476.17791813188
2
Iteration 4500: Loss = -11476.177966403618
3
Iteration 4600: Loss = -11476.177941198954
4
Iteration 4700: Loss = -11476.177862362158
Iteration 4800: Loss = -11476.17786954847
1
Iteration 4900: Loss = -11476.177906640896
2
Iteration 5000: Loss = -11476.177881644215
3
Iteration 5100: Loss = -11476.177875794787
4
Iteration 5200: Loss = -11476.177870632477
5
Stopping early at iteration 5200 due to no improvement.
pi: tensor([[0.9655, 0.0345],
        [0.9029, 0.0971]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8653, 0.1347], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1710, 0.2040],
         [0.6793, 0.2930]],

        [[0.6066, 0.2091],
         [0.5389, 0.7022]],

        [[0.6251, 0.1044],
         [0.5134, 0.6744]],

        [[0.6040, 0.2152],
         [0.5988, 0.5271]],

        [[0.6674, 0.2551],
         [0.5552, 0.6881]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0012410984355054515
Average Adjusted Rand Index: -0.0006142237715232486
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23086.600273800934
Iteration 100: Loss = -11480.757565490083
Iteration 200: Loss = -11479.121586047393
Iteration 300: Loss = -11478.528374072026
Iteration 400: Loss = -11478.196787921122
Iteration 500: Loss = -11477.924163160444
Iteration 600: Loss = -11477.636173991643
Iteration 700: Loss = -11477.273228612006
Iteration 800: Loss = -11476.821281643302
Iteration 900: Loss = -11476.304129363387
Iteration 1000: Loss = -11475.837355794432
Iteration 1100: Loss = -11475.54266288699
Iteration 1200: Loss = -11475.328029784514
Iteration 1300: Loss = -11475.116680939364
Iteration 1400: Loss = -11474.8706077115
Iteration 1500: Loss = -11474.602919723882
Iteration 1600: Loss = -11474.349317367092
Iteration 1700: Loss = -11474.158175495819
Iteration 1800: Loss = -11474.033780502737
Iteration 1900: Loss = -11473.950299375807
Iteration 2000: Loss = -11473.891495661474
Iteration 2100: Loss = -11473.845894762786
Iteration 2200: Loss = -11473.80545451144
Iteration 2300: Loss = -11473.771403143617
Iteration 2400: Loss = -11473.74668226871
Iteration 2500: Loss = -11473.729505240442
Iteration 2600: Loss = -11473.718162229454
Iteration 2700: Loss = -11473.710013521537
Iteration 2800: Loss = -11473.703358771114
Iteration 2900: Loss = -11473.697507826375
Iteration 3000: Loss = -11473.692308128617
Iteration 3100: Loss = -11473.688142411605
Iteration 3200: Loss = -11473.684989299254
Iteration 3300: Loss = -11473.682240979644
Iteration 3400: Loss = -11473.679531855456
Iteration 3500: Loss = -11473.676359242038
Iteration 3600: Loss = -11473.672865380884
Iteration 3700: Loss = -11473.669861567558
Iteration 3800: Loss = -11473.667194967185
Iteration 3900: Loss = -11473.664105908423
Iteration 4000: Loss = -11473.65978739541
Iteration 4100: Loss = -11473.652764793722
Iteration 4200: Loss = -11473.639003937378
Iteration 4300: Loss = -11473.599780265122
Iteration 4400: Loss = -11473.254462417937
Iteration 4500: Loss = -11472.630353412174
Iteration 4600: Loss = -11472.608272284504
Iteration 4700: Loss = -11472.597742814572
Iteration 4800: Loss = -11472.589863858997
Iteration 4900: Loss = -11472.580267890504
Iteration 5000: Loss = -11472.531053491564
Iteration 5100: Loss = -11471.757418115549
Iteration 5200: Loss = -11471.418436119868
Iteration 5300: Loss = -11468.973357204255
Iteration 5400: Loss = -11464.254741612904
Iteration 5500: Loss = -11264.797108545865
Iteration 5600: Loss = -11256.197137513323
Iteration 5700: Loss = -11255.96462546654
Iteration 5800: Loss = -11255.811434651567
Iteration 5900: Loss = -11255.73129619705
Iteration 6000: Loss = -11255.685002113587
Iteration 6100: Loss = -11255.670610869256
Iteration 6200: Loss = -11255.664813655803
Iteration 6300: Loss = -11255.65637417596
Iteration 6400: Loss = -11255.629330204378
Iteration 6500: Loss = -11255.623903365995
Iteration 6600: Loss = -11255.62112132357
Iteration 6700: Loss = -11255.59516297983
Iteration 6800: Loss = -11255.58396119118
Iteration 6900: Loss = -11255.580709278285
Iteration 7000: Loss = -11255.580301813925
Iteration 7100: Loss = -11255.578137667922
Iteration 7200: Loss = -11255.57596256932
Iteration 7300: Loss = -11255.573643539607
Iteration 7400: Loss = -11255.571991567931
Iteration 7500: Loss = -11255.570436874701
Iteration 7600: Loss = -11255.573021973998
1
Iteration 7700: Loss = -11255.560757663901
Iteration 7800: Loss = -11255.528053862105
Iteration 7900: Loss = -11255.486010006594
Iteration 8000: Loss = -11255.484241030785
Iteration 8100: Loss = -11255.482092672642
Iteration 8200: Loss = -11255.480242179952
Iteration 8300: Loss = -11255.498994525271
1
Iteration 8400: Loss = -11255.483019413477
2
Iteration 8500: Loss = -11255.485252099
3
Iteration 8600: Loss = -11255.397955875203
Iteration 8700: Loss = -11255.489021313459
1
Iteration 8800: Loss = -11255.39456034898
Iteration 8900: Loss = -11255.39557909575
1
Iteration 9000: Loss = -11255.39427969131
Iteration 9100: Loss = -11255.404928903605
1
Iteration 9200: Loss = -11255.42739011574
2
Iteration 9300: Loss = -11255.39554184492
3
Iteration 9400: Loss = -11255.39367818539
Iteration 9500: Loss = -11255.591381370468
1
Iteration 9600: Loss = -11255.39323303072
Iteration 9700: Loss = -11255.395030018624
1
Iteration 9800: Loss = -11255.392418066116
Iteration 9900: Loss = -11255.388465228241
Iteration 10000: Loss = -11255.374081276406
Iteration 10100: Loss = -11255.373415638358
Iteration 10200: Loss = -11255.384470215635
1
Iteration 10300: Loss = -11255.37322386612
Iteration 10400: Loss = -11255.372992098353
Iteration 10500: Loss = -11255.555617255019
1
Iteration 10600: Loss = -11255.372141165517
Iteration 10700: Loss = -11255.371493302991
Iteration 10800: Loss = -11255.402437663988
1
Iteration 10900: Loss = -11255.36993306691
Iteration 11000: Loss = -11255.369459243148
Iteration 11100: Loss = -11255.37832500478
1
Iteration 11200: Loss = -11255.366937744446
Iteration 11300: Loss = -11255.366582009758
Iteration 11400: Loss = -11255.459126828022
1
Iteration 11500: Loss = -11255.359997678199
Iteration 11600: Loss = -11255.357231183083
Iteration 11700: Loss = -11255.35735577353
1
Iteration 11800: Loss = -11255.35863585835
2
Iteration 11900: Loss = -11255.355913707623
Iteration 12000: Loss = -11255.355328467202
Iteration 12100: Loss = -11255.361335136111
1
Iteration 12200: Loss = -11255.365445504398
2
Iteration 12300: Loss = -11255.34551059935
Iteration 12400: Loss = -11255.346009380359
1
Iteration 12500: Loss = -11255.3484107059
2
Iteration 12600: Loss = -11255.345661173029
3
Iteration 12700: Loss = -11255.345481155618
Iteration 12800: Loss = -11255.346238315458
1
Iteration 12900: Loss = -11255.345365422963
Iteration 13000: Loss = -11255.34628899831
1
Iteration 13100: Loss = -11255.345288415136
Iteration 13200: Loss = -11255.346669855699
1
Iteration 13300: Loss = -11255.345248349382
Iteration 13400: Loss = -11255.350922027214
1
Iteration 13500: Loss = -11255.345230319052
Iteration 13600: Loss = -11255.39424606861
1
Iteration 13700: Loss = -11255.345209171985
Iteration 13800: Loss = -11255.3452002953
Iteration 13900: Loss = -11255.344445940698
Iteration 14000: Loss = -11255.34416269678
Iteration 14100: Loss = -11255.357803293735
1
Iteration 14200: Loss = -11254.97288514447
Iteration 14300: Loss = -11254.969761718197
Iteration 14400: Loss = -11254.969332774099
Iteration 14500: Loss = -11254.968990779984
Iteration 14600: Loss = -11254.969082035137
1
Iteration 14700: Loss = -11254.968768493502
Iteration 14800: Loss = -11255.013817949075
1
Iteration 14900: Loss = -11254.9687215848
Iteration 15000: Loss = -11254.968708792172
Iteration 15100: Loss = -11254.98648426718
1
Iteration 15200: Loss = -11254.968695129259
Iteration 15300: Loss = -11254.96867123618
Iteration 15400: Loss = -11254.969594322607
1
Iteration 15500: Loss = -11254.968656833038
Iteration 15600: Loss = -11254.985592941674
1
Iteration 15700: Loss = -11254.968652310261
Iteration 15800: Loss = -11254.96865668548
1
Iteration 15900: Loss = -11254.9708871831
2
Iteration 16000: Loss = -11254.968620923572
Iteration 16100: Loss = -11254.968592842622
Iteration 16200: Loss = -11254.970266718827
1
Iteration 16300: Loss = -11254.968586296158
Iteration 16400: Loss = -11254.968598513016
1
Iteration 16500: Loss = -11254.969024450877
2
Iteration 16600: Loss = -11254.968533890968
Iteration 16700: Loss = -11254.968512136631
Iteration 16800: Loss = -11254.968902185705
1
Iteration 16900: Loss = -11254.96850040326
Iteration 17000: Loss = -11254.968497193904
Iteration 17100: Loss = -11254.969235630291
1
Iteration 17200: Loss = -11254.968426089592
Iteration 17300: Loss = -11254.968433456885
1
Iteration 17400: Loss = -11254.96209861515
Iteration 17500: Loss = -11254.961660780964
Iteration 17600: Loss = -11254.97211877361
1
Iteration 17700: Loss = -11254.967932425048
2
Iteration 17800: Loss = -11254.961742604528
3
Iteration 17900: Loss = -11254.961717554193
4
Iteration 18000: Loss = -11255.01947842034
5
Stopping early at iteration 18000 due to no improvement.
pi: tensor([[0.7343, 0.2657],
        [0.2175, 0.7825]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4858, 0.5142], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3028, 0.1066],
         [0.6599, 0.2004]],

        [[0.6121, 0.1036],
         [0.7249, 0.5580]],

        [[0.6910, 0.1072],
         [0.6891, 0.6126]],

        [[0.5601, 0.1118],
         [0.6008, 0.5280]],

        [[0.5953, 0.1071],
         [0.6095, 0.6440]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721426378272603
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 8
Adjusted Rand Index: 0.7026262626262626
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.8683591954730375
Average Adjusted Rand Index: 0.8711131231394201
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22058.201504737142
Iteration 100: Loss = -11480.742966301392
Iteration 200: Loss = -11480.19330502149
Iteration 300: Loss = -11480.02791543321
Iteration 400: Loss = -11479.909000077543
Iteration 500: Loss = -11479.718538016325
Iteration 600: Loss = -11479.343452998763
Iteration 700: Loss = -11479.070158982922
Iteration 800: Loss = -11477.896878433561
Iteration 900: Loss = -11476.80946640086
Iteration 1000: Loss = -11476.54274993583
Iteration 1100: Loss = -11476.31116027177
Iteration 1200: Loss = -11476.183054906945
Iteration 1300: Loss = -11476.110861442396
Iteration 1400: Loss = -11476.04776274254
Iteration 1500: Loss = -11475.932593641795
Iteration 1600: Loss = -11475.257190333617
Iteration 1700: Loss = -11473.632422129265
Iteration 1800: Loss = -11473.494675117763
Iteration 1900: Loss = -11472.876908131853
Iteration 2000: Loss = -11472.358218079591
Iteration 2100: Loss = -11472.260919437826
Iteration 2200: Loss = -11472.18838208013
Iteration 2300: Loss = -11472.167177666492
Iteration 2400: Loss = -11472.145803828113
Iteration 2500: Loss = -11472.10831004766
Iteration 2600: Loss = -11472.099179173481
Iteration 2700: Loss = -11472.088797475722
Iteration 2800: Loss = -11472.050314515554
Iteration 2900: Loss = -11472.047386744776
Iteration 3000: Loss = -11472.04517896088
Iteration 3100: Loss = -11472.04383260188
Iteration 3200: Loss = -11472.042805329767
Iteration 3300: Loss = -11472.04184458598
Iteration 3400: Loss = -11472.041012616173
Iteration 3500: Loss = -11472.040330111873
Iteration 3600: Loss = -11472.039646586196
Iteration 3700: Loss = -11472.039046510869
Iteration 3800: Loss = -11472.038429537826
Iteration 3900: Loss = -11472.037834566847
Iteration 4000: Loss = -11472.03731700742
Iteration 4100: Loss = -11472.036959601592
Iteration 4200: Loss = -11472.036594247933
Iteration 4300: Loss = -11472.036162940776
Iteration 4400: Loss = -11472.035477319521
Iteration 4500: Loss = -11472.035067307383
Iteration 4600: Loss = -11472.034797261438
Iteration 4700: Loss = -11472.03455041098
Iteration 4800: Loss = -11472.034352240033
Iteration 4900: Loss = -11472.034190207303
Iteration 5000: Loss = -11472.040475151472
1
Iteration 5100: Loss = -11472.03391599807
Iteration 5200: Loss = -11472.033798804065
Iteration 5300: Loss = -11472.033672262432
Iteration 5400: Loss = -11472.033563768142
Iteration 5500: Loss = -11472.033536167537
Iteration 5600: Loss = -11472.033446152123
Iteration 5700: Loss = -11472.034276912387
1
Iteration 5800: Loss = -11472.033477439854
2
Iteration 5900: Loss = -11472.03341614118
Iteration 6000: Loss = -11472.03330277136
Iteration 6100: Loss = -11472.033258587215
Iteration 6200: Loss = -11472.033421740138
1
Iteration 6300: Loss = -11472.033190222388
Iteration 6400: Loss = -11472.033101582096
Iteration 6500: Loss = -11472.032506517202
Iteration 6600: Loss = -11472.017332940975
Iteration 6700: Loss = -11472.01741555423
1
Iteration 6800: Loss = -11472.017285490627
Iteration 6900: Loss = -11472.017418807103
1
Iteration 7000: Loss = -11472.020899748602
2
Iteration 7100: Loss = -11472.018174657996
3
Iteration 7200: Loss = -11472.018049216265
4
Iteration 7300: Loss = -11472.017240584217
Iteration 7400: Loss = -11472.018675419193
1
Iteration 7500: Loss = -11472.01717364901
Iteration 7600: Loss = -11472.024121365297
1
Iteration 7700: Loss = -11472.01716095487
Iteration 7800: Loss = -11472.017787508456
1
Iteration 7900: Loss = -11472.028211824496
2
Iteration 8000: Loss = -11472.17264331859
3
Iteration 8100: Loss = -11472.017580655203
4
Iteration 8200: Loss = -11472.017459808922
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.9704, 0.0296],
        [0.6173, 0.3827]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8973, 0.1027], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1803, 0.0983],
         [0.6551, 0.2281]],

        [[0.5874, 0.1100],
         [0.6678, 0.5907]],

        [[0.6438, 0.1010],
         [0.7308, 0.5303]],

        [[0.5995, 0.1907],
         [0.5932, 0.5531]],

        [[0.6584, 0.2497],
         [0.6251, 0.5744]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.010862123580924353
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: -0.004267232452421997
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.004000499577795329
Average Adjusted Rand Index: 0.0028052970054957523
11289.962499424195
[0.014412991830896064, 0.0012410984355054515, 0.8683591954730375, -0.004000499577795329] [0.6923013841895993, -0.0006142237715232486, 0.8711131231394201, 0.0028052970054957523] [11328.51484391052, 11476.177870632477, 11255.01947842034, 11472.017459808922]
-----------------------------------------------------------------------------------------
This iteration is 9
True Objective function: Loss = -11160.59352298985
Iteration 0: Loss = -11433.574955185743
Iteration 10: Loss = -11433.574951882332
Iteration 20: Loss = -11433.292054797239
Iteration 30: Loss = -11432.482925500239
Iteration 40: Loss = -11431.202315436576
Iteration 50: Loss = -11428.315226348104
Iteration 60: Loss = -11427.43997872093
Iteration 70: Loss = -11427.081331062758
Iteration 80: Loss = -11427.423864367816
1
Iteration 90: Loss = -11427.269264525576
2
Iteration 100: Loss = -11401.329050990125
Iteration 110: Loss = -11143.386531138709
Iteration 120: Loss = -11143.502107542132
1
Iteration 130: Loss = -11143.497292218917
2
Iteration 140: Loss = -11143.496138514527
3
Stopping early at iteration 140 due to no improvement.
pi: tensor([[0.7292, 0.2708],
        [0.2391, 0.7609]], dtype=torch.float64)
alpha: tensor([0.4931, 0.5069])
beta: tensor([[[0.1992, 0.1036],
         [0.3621, 0.2872]],

        [[0.1478, 0.1052],
         [0.0063, 0.5516]],

        [[0.0419, 0.0933],
         [0.3781, 0.2443]],

        [[0.6961, 0.1005],
         [0.4917, 0.7293]],

        [[0.3202, 0.0950],
         [0.5061, 0.9870]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080762963757459
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291540530311222
Average Adjusted Rand Index: 0.9297742888849884
pi: tensor([[0.5839, 0.4161],
        [0.4579, 0.5421]], dtype=torch.float64)
alpha: tensor([0.6153, 0.3847])
beta: tensor([[[0.1536, 0.2050],
         [0.5974, 0.1861]],

        [[0.5498, 0.1988],
         [0.8387, 0.2049]],

        [[0.8384, 0.2404],
         [0.3150, 0.6915]],

        [[0.0950, 0.1750],
         [0.2689, 0.2108]],

        [[0.1185,    nan],
         [0.5717, 0.9182]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.050506767648938106
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.04319638002167424
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 74
Adjusted Rand Index: 0.22257752796991986
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.03667221185277921
Average Adjusted Rand Index: 0.0628174727566755
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23705.61391601383
Iteration 100: Loss = -11434.511963762445
Iteration 200: Loss = -11432.735428662192
Iteration 300: Loss = -11431.317261009364
Iteration 400: Loss = -11428.46702590355
Iteration 500: Loss = -11425.925947590904
Iteration 600: Loss = -11409.344499405515
Iteration 700: Loss = -11282.87464746517
Iteration 800: Loss = -11236.881485951444
Iteration 900: Loss = -11221.98936095999
Iteration 1000: Loss = -11211.642771296907
Iteration 1100: Loss = -11211.20905477088
Iteration 1200: Loss = -11201.85445970515
Iteration 1300: Loss = -11201.671058399937
Iteration 1400: Loss = -11199.70762054159
Iteration 1500: Loss = -11195.829757421829
Iteration 1600: Loss = -11195.801996009934
Iteration 1700: Loss = -11195.783611632234
Iteration 1800: Loss = -11195.768402261087
Iteration 1900: Loss = -11195.75352634286
Iteration 2000: Loss = -11195.696258439088
Iteration 2100: Loss = -11195.634130938359
Iteration 2200: Loss = -11195.626137553756
Iteration 2300: Loss = -11195.61718139818
Iteration 2400: Loss = -11195.598592946006
Iteration 2500: Loss = -11195.589519969788
Iteration 2600: Loss = -11195.584873073272
Iteration 2700: Loss = -11195.581933894136
Iteration 2800: Loss = -11195.57965625138
Iteration 2900: Loss = -11195.577479934615
Iteration 3000: Loss = -11195.576332524895
Iteration 3100: Loss = -11195.5739718584
Iteration 3200: Loss = -11195.571760390652
Iteration 3300: Loss = -11195.570098615239
Iteration 3400: Loss = -11195.568694189653
Iteration 3500: Loss = -11195.571603076085
1
Iteration 3600: Loss = -11195.566320968906
Iteration 3700: Loss = -11195.565456394423
Iteration 3800: Loss = -11195.565723081761
1
Iteration 3900: Loss = -11195.563854726335
Iteration 4000: Loss = -11195.563389348268
Iteration 4100: Loss = -11195.562714625894
Iteration 4200: Loss = -11195.562036449799
Iteration 4300: Loss = -11195.561157058939
Iteration 4400: Loss = -11195.561091985808
Iteration 4500: Loss = -11195.565425306018
1
Iteration 4600: Loss = -11195.386703292688
Iteration 4700: Loss = -11195.380368196036
Iteration 4800: Loss = -11195.370843355824
Iteration 4900: Loss = -11195.368640657494
Iteration 5000: Loss = -11195.366692313535
Iteration 5100: Loss = -11195.36598071558
Iteration 5200: Loss = -11195.365194607484
Iteration 5300: Loss = -11195.329799465842
Iteration 5400: Loss = -11195.32766376408
Iteration 5500: Loss = -11195.327253980444
Iteration 5600: Loss = -11195.327176212573
Iteration 5700: Loss = -11195.330290797794
1
Iteration 5800: Loss = -11195.327260227108
2
Iteration 5900: Loss = -11195.326358265522
Iteration 6000: Loss = -11195.325826681434
Iteration 6100: Loss = -11195.32528312688
Iteration 6200: Loss = -11195.324727480536
Iteration 6300: Loss = -11195.308177256546
Iteration 6400: Loss = -11195.30711761252
Iteration 6500: Loss = -11195.306940796747
Iteration 6600: Loss = -11195.330474382275
1
Iteration 6700: Loss = -11195.306675550542
Iteration 6800: Loss = -11195.306550644656
Iteration 6900: Loss = -11195.306480357654
Iteration 7000: Loss = -11195.30632271397
Iteration 7100: Loss = -11195.306270938456
Iteration 7200: Loss = -11195.306388635958
1
Iteration 7300: Loss = -11191.213112691312
Iteration 7400: Loss = -11191.204468513182
Iteration 7500: Loss = -11191.204590906167
1
Iteration 7600: Loss = -11191.203590635834
Iteration 7700: Loss = -11190.866969915282
Iteration 7800: Loss = -11190.8414023934
Iteration 7900: Loss = -11190.8409579173
Iteration 8000: Loss = -11190.839530228865
Iteration 8100: Loss = -11190.839358774856
Iteration 8200: Loss = -11190.84384309074
1
Iteration 8300: Loss = -11190.839224789894
Iteration 8400: Loss = -11190.839257161753
1
Iteration 8500: Loss = -11190.839141388982
Iteration 8600: Loss = -11190.839044922224
Iteration 8700: Loss = -11190.840575391303
1
Iteration 8800: Loss = -11190.838863337693
Iteration 8900: Loss = -11190.83873703359
Iteration 9000: Loss = -11190.83617715111
Iteration 9100: Loss = -11190.834868983831
Iteration 9200: Loss = -11190.842581105666
1
Iteration 9300: Loss = -11190.834767543465
Iteration 9400: Loss = -11190.91108665929
1
Iteration 9500: Loss = -11190.834526060467
Iteration 9600: Loss = -11190.83539375794
1
Iteration 9700: Loss = -11190.834229975233
Iteration 9800: Loss = -11190.829229065737
Iteration 9900: Loss = -11190.829763339185
1
Iteration 10000: Loss = -11190.829118712754
Iteration 10100: Loss = -11190.829270485685
1
Iteration 10200: Loss = -11190.828834658132
Iteration 10300: Loss = -11190.831747836623
1
Iteration 10400: Loss = -11190.829787938705
2
Iteration 10500: Loss = -11190.873281081953
3
Iteration 10600: Loss = -11190.828632308261
Iteration 10700: Loss = -11190.837481961968
1
Iteration 10800: Loss = -11190.829186913417
2
Iteration 10900: Loss = -11190.828722680335
3
Iteration 11000: Loss = -11190.827133580167
Iteration 11100: Loss = -11190.827081959795
Iteration 11200: Loss = -11190.829215801416
1
Iteration 11300: Loss = -11190.8268810214
Iteration 11400: Loss = -11190.945983875075
1
Iteration 11500: Loss = -11190.823619363924
Iteration 11600: Loss = -11190.867660002454
1
Iteration 11700: Loss = -11190.825216075313
2
Iteration 11800: Loss = -11190.824404146464
3
Iteration 11900: Loss = -11190.824689677338
4
Iteration 12000: Loss = -11190.828258506015
5
Stopping early at iteration 12000 due to no improvement.
pi: tensor([[0.7889, 0.2111],
        [0.3352, 0.6648]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0111, 0.9889], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2993, 0.3242],
         [0.6460, 0.1839]],

        [[0.5609, 0.1068],
         [0.6010, 0.5226]],

        [[0.5996, 0.0942],
         [0.5117, 0.5047]],

        [[0.6863, 0.1016],
         [0.6157, 0.7026]],

        [[0.5968, 0.0952],
         [0.7247, 0.6306]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.6329055560106824
Average Adjusted Rand Index: 0.7531767857902987
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21544.107109277837
Iteration 100: Loss = -11474.798703429933
Iteration 200: Loss = -11432.14458481708
Iteration 300: Loss = -11428.506708898385
Iteration 400: Loss = -11428.184299888457
Iteration 500: Loss = -11428.041469457023
Iteration 600: Loss = -11427.95696931437
Iteration 700: Loss = -11427.900832663103
Iteration 800: Loss = -11427.861597034998
Iteration 900: Loss = -11427.831875914033
Iteration 1000: Loss = -11427.809596243367
Iteration 1100: Loss = -11427.792180565948
Iteration 1200: Loss = -11427.778155516486
Iteration 1300: Loss = -11427.766677937987
Iteration 1400: Loss = -11427.756990227499
Iteration 1500: Loss = -11427.748752373465
Iteration 1600: Loss = -11427.760823938419
1
Iteration 1700: Loss = -11427.735585877603
Iteration 1800: Loss = -11427.730459159997
Iteration 1900: Loss = -11427.726342768532
Iteration 2000: Loss = -11427.722602747179
Iteration 2100: Loss = -11427.719521674308
Iteration 2200: Loss = -11427.71680952208
Iteration 2300: Loss = -11427.714393796892
Iteration 2400: Loss = -11427.712201519624
Iteration 2500: Loss = -11427.710301033307
Iteration 2600: Loss = -11427.70856749384
Iteration 2700: Loss = -11427.707049488336
Iteration 2800: Loss = -11427.70564635366
Iteration 2900: Loss = -11427.70437083659
Iteration 3000: Loss = -11427.703284851204
Iteration 3100: Loss = -11427.702122683579
Iteration 3200: Loss = -11427.701191254753
Iteration 3300: Loss = -11427.700367651832
Iteration 3400: Loss = -11427.699506596391
Iteration 3500: Loss = -11427.698790179924
Iteration 3600: Loss = -11427.698136195173
Iteration 3700: Loss = -11427.697443296222
Iteration 3800: Loss = -11427.697519081252
1
Iteration 3900: Loss = -11427.696361891669
Iteration 4000: Loss = -11427.69586150092
Iteration 4100: Loss = -11427.695390166227
Iteration 4200: Loss = -11427.69495313666
Iteration 4300: Loss = -11427.694842615814
Iteration 4400: Loss = -11427.69420732113
Iteration 4500: Loss = -11427.710086194087
1
Iteration 4600: Loss = -11427.69354910932
Iteration 4700: Loss = -11427.693231247044
Iteration 4800: Loss = -11427.693005852356
Iteration 4900: Loss = -11427.692723290382
Iteration 5000: Loss = -11427.700820617742
1
Iteration 5100: Loss = -11427.692266675556
Iteration 5200: Loss = -11427.692025842278
Iteration 5300: Loss = -11427.691909029587
Iteration 5400: Loss = -11427.691653298985
Iteration 5500: Loss = -11427.700536232418
1
Iteration 5600: Loss = -11427.691316166995
Iteration 5700: Loss = -11427.691185857353
Iteration 5800: Loss = -11427.691059022027
Iteration 5900: Loss = -11427.690858061593
Iteration 6000: Loss = -11427.69206199823
1
Iteration 6100: Loss = -11427.69060806343
Iteration 6200: Loss = -11427.690754213805
1
Iteration 6300: Loss = -11427.690434658038
Iteration 6400: Loss = -11427.69028560817
Iteration 6500: Loss = -11427.690583333588
1
Iteration 6600: Loss = -11427.690154548902
Iteration 6700: Loss = -11427.690714880442
1
Iteration 6800: Loss = -11427.689961643871
Iteration 6900: Loss = -11427.689898960807
Iteration 7000: Loss = -11427.689807095265
Iteration 7100: Loss = -11427.689803826428
Iteration 7200: Loss = -11427.689683627133
Iteration 7300: Loss = -11427.689813586983
1
Iteration 7400: Loss = -11427.689583044059
Iteration 7500: Loss = -11427.689533407047
Iteration 7600: Loss = -11427.690169080819
1
Iteration 7700: Loss = -11427.689438030446
Iteration 7800: Loss = -11427.751697016982
1
Iteration 7900: Loss = -11427.689357700381
Iteration 8000: Loss = -11427.689270460714
Iteration 8100: Loss = -11427.692901792914
1
Iteration 8200: Loss = -11427.689241814849
Iteration 8300: Loss = -11427.689187916349
Iteration 8400: Loss = -11427.689757113349
1
Iteration 8500: Loss = -11427.689125523197
Iteration 8600: Loss = -11427.689101184438
Iteration 8700: Loss = -11427.689312182503
1
Iteration 8800: Loss = -11427.688998655423
Iteration 8900: Loss = -11427.690290938152
1
Iteration 9000: Loss = -11427.689004355681
2
Iteration 9100: Loss = -11427.68896990809
Iteration 9200: Loss = -11427.876449652294
1
Iteration 9300: Loss = -11427.688918686721
Iteration 9400: Loss = -11427.688906739264
Iteration 9500: Loss = -11427.691749943646
1
Iteration 9600: Loss = -11427.688896067333
Iteration 9700: Loss = -11427.689065350052
1
Iteration 9800: Loss = -11427.688825503988
Iteration 9900: Loss = -11427.688849634615
1
Iteration 10000: Loss = -11427.688778189922
Iteration 10100: Loss = -11427.698189598292
1
Iteration 10200: Loss = -11427.699343826864
2
Iteration 10300: Loss = -11427.688832945008
3
Iteration 10400: Loss = -11427.68881782053
4
Iteration 10500: Loss = -11427.688764576229
Iteration 10600: Loss = -11427.689772100506
1
Iteration 10700: Loss = -11427.688724337771
Iteration 10800: Loss = -11427.68880276846
1
Iteration 10900: Loss = -11427.69561126872
2
Iteration 11000: Loss = -11427.688712647505
Iteration 11100: Loss = -11427.688900007748
1
Iteration 11200: Loss = -11427.704327893818
2
Iteration 11300: Loss = -11427.688978442946
3
Iteration 11400: Loss = -11427.6887144254
4
Iteration 11500: Loss = -11427.712338958487
5
Stopping early at iteration 11500 due to no improvement.
pi: tensor([[1.0000e+00, 2.4972e-06],
        [5.6931e-01, 4.3069e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0107, 0.9893], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1691, 0.3255],
         [0.5383, 0.1669]],

        [[0.5448, 0.1748],
         [0.6400, 0.7028]],

        [[0.6809, 0.1870],
         [0.5882, 0.5460]],

        [[0.6730, 0.2454],
         [0.7127, 0.6552]],

        [[0.6849, 0.2213],
         [0.6360, 0.6231]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.008763529051498655
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.00021025550424088273
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: -0.0009674006779129739
Average Adjusted Rand Index: 0.00379094283233095
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20538.24621486857
Iteration 100: Loss = -11433.502875905053
Iteration 200: Loss = -11432.156634691013
Iteration 300: Loss = -11431.19066917141
Iteration 400: Loss = -11430.134421724835
Iteration 500: Loss = -11427.633811484018
Iteration 600: Loss = -11420.134928658978
Iteration 700: Loss = -11276.992057385723
Iteration 800: Loss = -11231.526935493834
Iteration 900: Loss = -11205.68469783798
Iteration 1000: Loss = -11200.944371193777
Iteration 1100: Loss = -11196.06055126885
Iteration 1200: Loss = -11193.85011793433
Iteration 1300: Loss = -11191.60169579423
Iteration 1400: Loss = -11191.54147972204
Iteration 1500: Loss = -11191.495631931237
Iteration 1600: Loss = -11191.443606459452
Iteration 1700: Loss = -11191.387804458007
Iteration 1800: Loss = -11191.363016524785
Iteration 1900: Loss = -11191.347766294999
Iteration 2000: Loss = -11191.33572001669
Iteration 2100: Loss = -11191.326007311267
Iteration 2200: Loss = -11191.317261113658
Iteration 2300: Loss = -11191.307802003676
Iteration 2400: Loss = -11191.299901703032
Iteration 2500: Loss = -11191.294931572931
Iteration 2600: Loss = -11191.290268974803
Iteration 2700: Loss = -11191.28526335033
Iteration 2800: Loss = -11191.275996635375
Iteration 2900: Loss = -11191.24736175883
Iteration 3000: Loss = -11191.243346580091
Iteration 3100: Loss = -11191.24021089394
Iteration 3200: Loss = -11191.240166525584
Iteration 3300: Loss = -11191.236006049046
Iteration 3400: Loss = -11191.234195376921
Iteration 3500: Loss = -11191.23291643923
Iteration 3600: Loss = -11191.23135974975
Iteration 3700: Loss = -11191.234770669056
1
Iteration 3800: Loss = -11191.229044886491
Iteration 3900: Loss = -11191.227693584768
Iteration 4000: Loss = -11191.226062345811
Iteration 4100: Loss = -11191.22485051159
Iteration 4200: Loss = -11191.214053410147
Iteration 4300: Loss = -11191.212461747673
Iteration 4400: Loss = -11191.222397165817
1
Iteration 4500: Loss = -11191.209691229553
Iteration 4600: Loss = -11191.17551850464
Iteration 4700: Loss = -11191.145925889303
Iteration 4800: Loss = -11191.131554165418
Iteration 4900: Loss = -11191.124217763932
Iteration 5000: Loss = -11191.123448956363
Iteration 5100: Loss = -11191.122345320791
Iteration 5200: Loss = -11191.131689196845
1
Iteration 5300: Loss = -11191.121119671825
Iteration 5400: Loss = -11191.121223640066
1
Iteration 5500: Loss = -11191.119707956332
Iteration 5600: Loss = -11191.11972259684
1
Iteration 5700: Loss = -11191.12104752921
2
Iteration 5800: Loss = -11191.118706364185
Iteration 5900: Loss = -11191.118565562114
Iteration 6000: Loss = -11191.118271520183
Iteration 6100: Loss = -11191.120362369424
1
Iteration 6200: Loss = -11191.117834848597
Iteration 6300: Loss = -11191.117663280183
Iteration 6400: Loss = -11191.117495332652
Iteration 6500: Loss = -11191.117233572086
Iteration 6600: Loss = -11191.117221001208
Iteration 6700: Loss = -11191.117025827665
Iteration 6800: Loss = -11191.114034180711
Iteration 6900: Loss = -11191.11270957312
Iteration 7000: Loss = -11191.201585994193
1
Iteration 7100: Loss = -11190.750938838111
Iteration 7200: Loss = -11190.75058058796
Iteration 7300: Loss = -11190.750342627622
Iteration 7400: Loss = -11190.74972267941
Iteration 7500: Loss = -11190.74670759205
Iteration 7600: Loss = -11190.746194587822
Iteration 7700: Loss = -11190.746014572325
Iteration 7800: Loss = -11190.745948347902
Iteration 7900: Loss = -11190.745877801284
Iteration 8000: Loss = -11190.745940131214
1
Iteration 8100: Loss = -11190.745740434117
Iteration 8200: Loss = -11190.746927803295
1
Iteration 8300: Loss = -11190.745650099372
Iteration 8400: Loss = -11190.897820329803
1
Iteration 8500: Loss = -11190.745488780149
Iteration 8600: Loss = -11190.745407398328
Iteration 8700: Loss = -11190.745380304023
Iteration 8800: Loss = -11190.745235827442
Iteration 8900: Loss = -11190.759723489693
1
Iteration 9000: Loss = -11190.745037552742
Iteration 9100: Loss = -11190.755465118438
1
Iteration 9200: Loss = -11190.744914891866
Iteration 9300: Loss = -11190.74692379025
1
Iteration 9400: Loss = -11190.751543671295
2
Iteration 9500: Loss = -11190.744404918167
Iteration 9600: Loss = -11190.743087098761
Iteration 9700: Loss = -11190.78856017112
1
Iteration 9800: Loss = -11190.73954646067
Iteration 9900: Loss = -11190.741054335409
1
Iteration 10000: Loss = -11190.743158668629
2
Iteration 10100: Loss = -11190.788993600352
3
Iteration 10200: Loss = -11190.739374405415
Iteration 10300: Loss = -11190.739214642375
Iteration 10400: Loss = -11190.769837432841
1
Iteration 10500: Loss = -11190.747696379301
2
Iteration 10600: Loss = -11190.7392027626
Iteration 10700: Loss = -11190.77774850747
1
Iteration 10800: Loss = -11190.737788568988
Iteration 10900: Loss = -11190.739925916056
1
Iteration 11000: Loss = -11190.737725039186
Iteration 11100: Loss = -11190.73883281379
1
Iteration 11200: Loss = -11190.737716365738
Iteration 11300: Loss = -11190.774449352162
1
Iteration 11400: Loss = -11190.737594013124
Iteration 11500: Loss = -11190.77512684184
1
Iteration 11600: Loss = -11190.748766624512
2
Iteration 11700: Loss = -11190.737388278907
Iteration 11800: Loss = -11190.737606060917
1
Iteration 11900: Loss = -11190.753585855426
2
Iteration 12000: Loss = -11190.737386787268
Iteration 12100: Loss = -11190.746895019382
1
Iteration 12200: Loss = -11190.737365662035
Iteration 12300: Loss = -11190.985845982428
1
Iteration 12400: Loss = -11190.737461461902
2
Iteration 12500: Loss = -11190.741128693831
3
Iteration 12600: Loss = -11190.749661291728
4
Iteration 12700: Loss = -11190.738374805342
5
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[0.7891, 0.2109],
        [0.3357, 0.6643]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0103, 0.9897], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2992, 0.3241],
         [0.5236, 0.1841]],

        [[0.5510, 0.1068],
         [0.6586, 0.6951]],

        [[0.5507, 0.0942],
         [0.6774, 0.5354]],

        [[0.6543, 0.1007],
         [0.7018, 0.5973]],

        [[0.6889, 0.0951],
         [0.6206, 0.7161]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.6329055560106824
Average Adjusted Rand Index: 0.7531767857902987
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23074.546372428667
Iteration 100: Loss = -11435.819634034606
Iteration 200: Loss = -11433.620670065018
Iteration 300: Loss = -11432.812601442689
Iteration 400: Loss = -11431.401672300897
Iteration 500: Loss = -11429.028183527555
Iteration 600: Loss = -11426.736634672712
Iteration 700: Loss = -11421.970599136952
Iteration 800: Loss = -11333.062481618972
Iteration 900: Loss = -11264.923197075084
Iteration 1000: Loss = -11213.753699772366
Iteration 1100: Loss = -11209.538396787733
Iteration 1200: Loss = -11200.347773850335
Iteration 1300: Loss = -11196.319369074397
Iteration 1400: Loss = -11196.089776022427
Iteration 1500: Loss = -11195.966809504816
Iteration 1600: Loss = -11195.880586657588
Iteration 1700: Loss = -11195.813848341448
Iteration 1800: Loss = -11195.752338007265
Iteration 1900: Loss = -11195.685152799771
Iteration 2000: Loss = -11195.648373899885
Iteration 2100: Loss = -11191.59684131713
Iteration 2200: Loss = -11191.528214282944
Iteration 2300: Loss = -11191.479436854133
Iteration 2400: Loss = -11191.459968398178
Iteration 2500: Loss = -11191.442595195525
Iteration 2600: Loss = -11191.422652009696
Iteration 2700: Loss = -11191.39551816615
Iteration 2800: Loss = -11191.377542785185
Iteration 2900: Loss = -11191.364791684757
Iteration 3000: Loss = -11191.340865008291
Iteration 3100: Loss = -11191.307855913099
Iteration 3200: Loss = -11191.244154141294
Iteration 3300: Loss = -11191.241164493356
Iteration 3400: Loss = -11191.234042836539
Iteration 3500: Loss = -11191.229981673203
Iteration 3600: Loss = -11191.225929172968
Iteration 3700: Loss = -11191.222574368423
Iteration 3800: Loss = -11191.219679366026
Iteration 3900: Loss = -11191.219104220794
Iteration 4000: Loss = -11191.213355089012
Iteration 4100: Loss = -11191.213032009033
Iteration 4200: Loss = -11191.207757556527
Iteration 4300: Loss = -11191.205849912438
Iteration 4400: Loss = -11191.2028910094
Iteration 4500: Loss = -11191.20106949138
Iteration 4600: Loss = -11191.199807389388
Iteration 4700: Loss = -11191.200244785776
1
Iteration 4800: Loss = -11191.197396392194
Iteration 4900: Loss = -11191.203765066315
1
Iteration 5000: Loss = -11191.195661141173
Iteration 5100: Loss = -11191.194623516285
Iteration 5200: Loss = -11191.194037290936
Iteration 5300: Loss = -11191.193134885933
Iteration 5400: Loss = -11191.194418846275
1
Iteration 5500: Loss = -11191.191818849855
Iteration 5600: Loss = -11191.19128727984
Iteration 5700: Loss = -11191.190819678313
Iteration 5800: Loss = -11191.192229597615
1
Iteration 5900: Loss = -11191.191036002916
2
Iteration 6000: Loss = -11191.189289477927
Iteration 6100: Loss = -11191.189174475776
Iteration 6200: Loss = -11191.190740803744
1
Iteration 6300: Loss = -11191.188178581426
Iteration 6400: Loss = -11191.187852160123
Iteration 6500: Loss = -11191.18864338929
1
Iteration 6600: Loss = -11191.187860104726
2
Iteration 6700: Loss = -11191.186878403238
Iteration 6800: Loss = -11191.188275950488
1
Iteration 6900: Loss = -11191.186400924424
Iteration 7000: Loss = -11191.186299756606
Iteration 7100: Loss = -11191.185874923356
Iteration 7200: Loss = -11191.185660419946
Iteration 7300: Loss = -11191.18544843447
Iteration 7400: Loss = -11191.185205851056
Iteration 7500: Loss = -11191.184908033274
Iteration 7600: Loss = -11191.184942565575
1
Iteration 7700: Loss = -11190.822179740271
Iteration 7800: Loss = -11190.90772653131
1
Iteration 7900: Loss = -11190.817024989954
Iteration 8000: Loss = -11190.818064816327
1
Iteration 8100: Loss = -11190.816762665741
Iteration 8200: Loss = -11190.845177736324
1
Iteration 8300: Loss = -11190.816406316662
Iteration 8400: Loss = -11190.816137660806
Iteration 8500: Loss = -11190.816752839142
1
Iteration 8600: Loss = -11190.815959347416
Iteration 8700: Loss = -11190.81583575124
Iteration 8800: Loss = -11190.815831736876
Iteration 8900: Loss = -11190.815665434937
Iteration 9000: Loss = -11190.972680207788
1
Iteration 9100: Loss = -11190.815511845552
Iteration 9200: Loss = -11190.81538433905
Iteration 9300: Loss = -11190.815408154493
1
Iteration 9400: Loss = -11191.089990173221
2
Iteration 9500: Loss = -11190.82296672719
3
Iteration 9600: Loss = -11190.812881666605
Iteration 9700: Loss = -11190.81249512003
Iteration 9800: Loss = -11190.81231503353
Iteration 9900: Loss = -11190.81286879022
1
Iteration 10000: Loss = -11190.811308997128
Iteration 10100: Loss = -11190.811475636649
1
Iteration 10200: Loss = -11190.811213191599
Iteration 10300: Loss = -11190.814586588183
1
Iteration 10400: Loss = -11190.821149975454
2
Iteration 10500: Loss = -11190.81170817061
3
Iteration 10600: Loss = -11190.811210505091
Iteration 10700: Loss = -11190.81287318878
1
Iteration 10800: Loss = -11190.949322604969
2
Iteration 10900: Loss = -11190.811376097496
3
Iteration 11000: Loss = -11190.812583458877
4
Iteration 11100: Loss = -11190.853170849947
5
Stopping early at iteration 11100 due to no improvement.
pi: tensor([[0.7900, 0.2100],
        [0.3378, 0.6622]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0110, 0.9890], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2981, 0.3242],
         [0.5995, 0.1849]],

        [[0.6723, 0.1063],
         [0.6420, 0.5572]],

        [[0.5806, 0.0937],
         [0.7055, 0.5243]],

        [[0.6605, 0.1009],
         [0.6481, 0.5339]],

        [[0.5890, 0.0949],
         [0.5507, 0.6720]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.6329055560106824
Average Adjusted Rand Index: 0.7531767857902987
11160.59352298985
[0.6329055560106824, -0.0009674006779129739, 0.6329055560106824, 0.6329055560106824] [0.7531767857902987, 0.00379094283233095, 0.7531767857902987, 0.7531767857902987] [11190.828258506015, 11427.712338958487, 11190.738374805342, 11190.853170849947]
-----------------------------------------------------------------------------------------
This iteration is 10
True Objective function: Loss = -11120.753720287514
Iteration 0: Loss = -11489.502576589191
Iteration 10: Loss = -11489.502576589191
1
Iteration 20: Loss = -11489.502576589248
2
Iteration 30: Loss = -11489.50258939857
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[1.0000e+00, 3.0860e-09],
        [1.0000e+00, 6.6450e-20]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 2.7950e-09])
beta: tensor([[[0.1736, 0.2772],
         [0.2781, 0.0426]],

        [[0.0134, 0.2830],
         [0.0694, 0.9785]],

        [[0.8747, 0.2315],
         [0.1034, 0.4177]],

        [[0.6291, 0.0556],
         [0.3405, 0.6409]],

        [[0.4949, 0.2399],
         [0.1983, 0.0361]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -11506.582808249601
Iteration 10: Loss = -11476.935349038078
Iteration 20: Loss = -11402.48461548899
Iteration 30: Loss = -11103.088831765535
Iteration 40: Loss = -11103.10939433749
1
Iteration 50: Loss = -11103.109487311918
2
Iteration 60: Loss = -11103.109506415478
3
Stopping early at iteration 60 due to no improvement.
pi: tensor([[0.7788, 0.2212],
        [0.2708, 0.7292]], dtype=torch.float64)
alpha: tensor([0.5577, 0.4423])
beta: tensor([[[0.2855, 0.0911],
         [0.4481, 0.1994]],

        [[0.8818, 0.0949],
         [0.2068, 0.0387]],

        [[0.6507, 0.0954],
         [0.7763, 0.2079]],

        [[0.2592, 0.0837],
         [0.7984, 0.7743]],

        [[0.0908, 0.0987],
         [0.8007, 0.7224]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.9214356566189069
Average Adjusted Rand Index: 0.9214519298664049
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24176.042705641583
Iteration 100: Loss = -11488.518272103844
Iteration 200: Loss = -11484.664109492584
Iteration 300: Loss = -11404.947704004695
Iteration 400: Loss = -11132.631575537003
Iteration 500: Loss = -11113.35269599294
Iteration 600: Loss = -11111.027372422717
Iteration 700: Loss = -11100.524162336871
Iteration 800: Loss = -11100.203789280962
Iteration 900: Loss = -11100.12990513369
Iteration 1000: Loss = -11100.08344962679
Iteration 1100: Loss = -11100.050037983507
Iteration 1200: Loss = -11100.025219374844
Iteration 1300: Loss = -11100.004396873677
Iteration 1400: Loss = -11099.984977139098
Iteration 1500: Loss = -11099.968021537228
Iteration 1600: Loss = -11099.957489374143
Iteration 1700: Loss = -11099.94917539205
Iteration 1800: Loss = -11099.941294950966
Iteration 1900: Loss = -11099.935291970176
Iteration 2000: Loss = -11099.930562063793
Iteration 2100: Loss = -11099.926552988238
Iteration 2200: Loss = -11099.923055470354
Iteration 2300: Loss = -11099.920043547518
Iteration 2400: Loss = -11099.917416724329
Iteration 2500: Loss = -11099.915118168274
Iteration 2600: Loss = -11099.913113306597
Iteration 2700: Loss = -11099.91063766646
Iteration 2800: Loss = -11099.902059007276
Iteration 2900: Loss = -11099.902497913985
1
Iteration 3000: Loss = -11099.89728676
Iteration 3100: Loss = -11099.89615843797
Iteration 3200: Loss = -11099.899315906721
1
Iteration 3300: Loss = -11099.89760705229
2
Iteration 3400: Loss = -11099.893358704398
Iteration 3500: Loss = -11099.89262709173
Iteration 3600: Loss = -11099.89374017866
1
Iteration 3700: Loss = -11099.891155163059
Iteration 3800: Loss = -11099.892285509633
1
Iteration 3900: Loss = -11099.889996361953
Iteration 4000: Loss = -11099.889482180786
Iteration 4100: Loss = -11099.89580706203
1
Iteration 4200: Loss = -11099.890561348082
2
Iteration 4300: Loss = -11099.887656759234
Iteration 4400: Loss = -11099.88790711206
1
Iteration 4500: Loss = -11099.886199896748
Iteration 4600: Loss = -11099.886532871253
1
Iteration 4700: Loss = -11099.88548464295
Iteration 4800: Loss = -11099.885126878413
Iteration 4900: Loss = -11099.884703464026
Iteration 5000: Loss = -11099.88431575258
Iteration 5100: Loss = -11099.88384159943
Iteration 5200: Loss = -11099.883065966524
Iteration 5300: Loss = -11099.889447447118
1
Iteration 5400: Loss = -11099.878766197748
Iteration 5500: Loss = -11099.873535372737
Iteration 5600: Loss = -11099.869541257736
Iteration 5700: Loss = -11099.86913079559
Iteration 5800: Loss = -11099.87148763492
1
Iteration 5900: Loss = -11099.875059591892
2
Iteration 6000: Loss = -11099.86883775979
Iteration 6100: Loss = -11099.868551801173
Iteration 6200: Loss = -11099.868268123191
Iteration 6300: Loss = -11099.868136628018
Iteration 6400: Loss = -11099.868735714243
1
Iteration 6500: Loss = -11099.868869154812
2
Iteration 6600: Loss = -11099.868134807866
Iteration 6700: Loss = -11099.867745321657
Iteration 6800: Loss = -11099.86774425263
Iteration 6900: Loss = -11099.867880058378
1
Iteration 7000: Loss = -11099.868236899923
2
Iteration 7100: Loss = -11099.868689594821
3
Iteration 7200: Loss = -11099.867983274915
4
Iteration 7300: Loss = -11099.867604352297
Iteration 7400: Loss = -11099.86728510381
Iteration 7500: Loss = -11099.867504204489
1
Iteration 7600: Loss = -11099.86708547687
Iteration 7700: Loss = -11099.910930242026
1
Iteration 7800: Loss = -11099.86696344973
Iteration 7900: Loss = -11099.866978537546
1
Iteration 8000: Loss = -11099.866776081479
Iteration 8100: Loss = -11099.86928588585
1
Iteration 8200: Loss = -11099.865961873085
Iteration 8300: Loss = -11099.882174839735
1
Iteration 8400: Loss = -11099.865861335122
Iteration 8500: Loss = -11099.86584912277
Iteration 8600: Loss = -11099.867626522167
1
Iteration 8700: Loss = -11099.865717743798
Iteration 8800: Loss = -11099.86555184459
Iteration 8900: Loss = -11099.865683021708
1
Iteration 9000: Loss = -11099.865297114995
Iteration 9100: Loss = -11099.865276114015
Iteration 9200: Loss = -11099.865328828988
1
Iteration 9300: Loss = -11099.865232026252
Iteration 9400: Loss = -11099.865220161777
Iteration 9500: Loss = -11099.865231529851
1
Iteration 9600: Loss = -11099.865157323839
Iteration 9700: Loss = -11099.866136877641
1
Iteration 9800: Loss = -11099.874983171265
2
Iteration 9900: Loss = -11099.865287769233
3
Iteration 10000: Loss = -11099.865197841667
4
Iteration 10100: Loss = -11099.872280258853
5
Stopping early at iteration 10100 due to no improvement.
pi: tensor([[0.7844, 0.2156],
        [0.2616, 0.7384]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5713, 0.4287], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2910, 0.0910],
         [0.5813, 0.2037]],

        [[0.5855, 0.0953],
         [0.7120, 0.5822]],

        [[0.6654, 0.0952],
         [0.6496, 0.6238]],

        [[0.7280, 0.0833],
         [0.6967, 0.6080]],

        [[0.6976, 0.0983],
         [0.5178, 0.5058]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.9524772731062711
Average Adjusted Rand Index: 0.9526457071249215
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21846.795822782067
Iteration 100: Loss = -11488.836721958822
Iteration 200: Loss = -11486.134603901968
Iteration 300: Loss = -11479.08556423906
Iteration 400: Loss = -11419.07121969707
Iteration 500: Loss = -11203.724904088345
Iteration 600: Loss = -11123.773467126086
Iteration 700: Loss = -11110.074271239566
Iteration 800: Loss = -11103.266140881795
Iteration 900: Loss = -11103.092552834289
Iteration 1000: Loss = -11102.985629302133
Iteration 1100: Loss = -11102.90712902084
Iteration 1200: Loss = -11102.795020139298
Iteration 1300: Loss = -11100.325544568002
Iteration 1400: Loss = -11100.18454538124
Iteration 1500: Loss = -11100.158487797675
Iteration 1600: Loss = -11100.136720950148
Iteration 1700: Loss = -11100.121739973927
Iteration 1800: Loss = -11100.090259985507
Iteration 1900: Loss = -11100.041697584635
Iteration 2000: Loss = -11100.032255094387
Iteration 2100: Loss = -11100.040168381056
1
Iteration 2200: Loss = -11100.018049837096
Iteration 2300: Loss = -11100.011008642212
Iteration 2400: Loss = -11100.002330460367
Iteration 2500: Loss = -11099.999256984944
Iteration 2600: Loss = -11099.988917703902
Iteration 2700: Loss = -11099.982936849354
Iteration 2800: Loss = -11099.979759772572
Iteration 2900: Loss = -11099.97576528196
Iteration 3000: Loss = -11099.95495761794
Iteration 3100: Loss = -11099.95021788375
Iteration 3200: Loss = -11099.947669856645
Iteration 3300: Loss = -11099.945436574868
Iteration 3400: Loss = -11099.942339797002
Iteration 3500: Loss = -11099.921840891935
Iteration 3600: Loss = -11099.92306446406
1
Iteration 3700: Loss = -11099.91891958546
Iteration 3800: Loss = -11099.917887539774
Iteration 3900: Loss = -11099.916913808947
Iteration 4000: Loss = -11099.916192497802
Iteration 4100: Loss = -11099.9150775928
Iteration 4200: Loss = -11099.914409214878
Iteration 4300: Loss = -11099.920580950069
1
Iteration 4400: Loss = -11099.9240374362
2
Iteration 4500: Loss = -11099.912395074049
Iteration 4600: Loss = -11099.924521452267
1
Iteration 4700: Loss = -11099.911507990118
Iteration 4800: Loss = -11099.932918470375
1
Iteration 4900: Loss = -11099.910422337534
Iteration 5000: Loss = -11099.910465543568
1
Iteration 5100: Loss = -11099.910251355941
Iteration 5200: Loss = -11099.909491987057
Iteration 5300: Loss = -11099.907791207334
Iteration 5400: Loss = -11099.904906198148
Iteration 5500: Loss = -11099.904669368212
Iteration 5600: Loss = -11099.9030371177
Iteration 5700: Loss = -11099.895103369667
Iteration 5800: Loss = -11099.891734814446
Iteration 5900: Loss = -11099.89163835514
Iteration 6000: Loss = -11099.891822244606
1
Iteration 6100: Loss = -11099.8909933358
Iteration 6200: Loss = -11099.890791488768
Iteration 6300: Loss = -11099.89072040983
Iteration 6400: Loss = -11099.890102694806
Iteration 6500: Loss = -11099.889521479474
Iteration 6600: Loss = -11099.888672613615
Iteration 6700: Loss = -11099.884130270026
Iteration 6800: Loss = -11099.879268810495
Iteration 6900: Loss = -11099.882120404585
1
Iteration 7000: Loss = -11099.874783883024
Iteration 7100: Loss = -11099.877396253085
1
Iteration 7200: Loss = -11099.8821889702
2
Iteration 7300: Loss = -11099.8744017982
Iteration 7400: Loss = -11099.880277461876
1
Iteration 7500: Loss = -11099.874252891517
Iteration 7600: Loss = -11099.87591986458
1
Iteration 7700: Loss = -11099.897231293655
2
Iteration 7800: Loss = -11099.874005589047
Iteration 7900: Loss = -11099.873933992474
Iteration 8000: Loss = -11099.874342690331
1
Iteration 8100: Loss = -11099.873771678136
Iteration 8200: Loss = -11099.876031523174
1
Iteration 8300: Loss = -11099.874546379058
2
Iteration 8400: Loss = -11099.873554100677
Iteration 8500: Loss = -11099.87375500571
1
Iteration 8600: Loss = -11099.876837038506
2
Iteration 8700: Loss = -11099.876964608444
3
Iteration 8800: Loss = -11099.87329012301
Iteration 8900: Loss = -11099.873258652744
Iteration 9000: Loss = -11099.998357314049
1
Iteration 9100: Loss = -11099.873207088833
Iteration 9200: Loss = -11099.88245761786
1
Iteration 9300: Loss = -11099.873136509026
Iteration 9400: Loss = -11099.875209933538
1
Iteration 9500: Loss = -11099.860048556173
Iteration 9600: Loss = -11099.857568304957
Iteration 9700: Loss = -11099.857922906234
1
Iteration 9800: Loss = -11099.881740832987
2
Iteration 9900: Loss = -11099.857423336281
Iteration 10000: Loss = -11099.858177839578
1
Iteration 10100: Loss = -11099.858100299965
2
Iteration 10200: Loss = -11099.910515700552
3
Iteration 10300: Loss = -11099.85971621014
4
Iteration 10400: Loss = -11099.860341012203
5
Stopping early at iteration 10400 due to no improvement.
pi: tensor([[0.7384, 0.2616],
        [0.2158, 0.7842]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4287, 0.5713], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2039, 0.0909],
         [0.5855, 0.2911]],

        [[0.5788, 0.0953],
         [0.5312, 0.6791]],

        [[0.6866, 0.0951],
         [0.6775, 0.6848]],

        [[0.6833, 0.0834],
         [0.5375, 0.6042]],

        [[0.6817, 0.0984],
         [0.6903, 0.6831]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.9524772731062711
Average Adjusted Rand Index: 0.9526457071249215
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22829.530701081057
Iteration 100: Loss = -11489.161646356637
Iteration 200: Loss = -11486.484548208631
Iteration 300: Loss = -11480.366612905398
Iteration 400: Loss = -11425.366424839456
Iteration 500: Loss = -11204.62229409025
Iteration 600: Loss = -11109.5799207887
Iteration 700: Loss = -11100.86753251592
Iteration 800: Loss = -11100.547616707387
Iteration 900: Loss = -11100.460262652232
Iteration 1000: Loss = -11100.40032402168
Iteration 1100: Loss = -11100.362566118127
Iteration 1200: Loss = -11100.335883205307
Iteration 1300: Loss = -11100.315116951191
Iteration 1400: Loss = -11100.285774133154
Iteration 1500: Loss = -11100.10430760183
Iteration 1600: Loss = -11100.096286351545
Iteration 1700: Loss = -11100.08525539108
Iteration 1800: Loss = -11100.078395466875
Iteration 1900: Loss = -11100.075564782732
Iteration 2000: Loss = -11100.06785146702
Iteration 2100: Loss = -11100.06184938096
Iteration 2200: Loss = -11100.056762684448
Iteration 2300: Loss = -11100.052130952556
Iteration 2400: Loss = -11100.047168386134
Iteration 2500: Loss = -11100.044733626104
Iteration 2600: Loss = -11100.041420844494
Iteration 2700: Loss = -11100.039251078877
Iteration 2800: Loss = -11100.03791631613
Iteration 2900: Loss = -11100.035479032309
Iteration 3000: Loss = -11100.029583600155
Iteration 3100: Loss = -11100.029553060416
Iteration 3200: Loss = -11100.027228866518
Iteration 3300: Loss = -11100.026933336698
Iteration 3400: Loss = -11100.031015996025
1
Iteration 3500: Loss = -11100.025007577211
Iteration 3600: Loss = -11100.024309625189
Iteration 3700: Loss = -11100.02397379812
Iteration 3800: Loss = -11100.018775049592
Iteration 3900: Loss = -11100.01722413437
Iteration 4000: Loss = -11100.016503581632
Iteration 4100: Loss = -11100.014228648975
Iteration 4200: Loss = -11100.012850728008
Iteration 4300: Loss = -11100.014735790377
1
Iteration 4400: Loss = -11100.010822154503
Iteration 4500: Loss = -11100.010495364555
Iteration 4600: Loss = -11100.01055117403
1
Iteration 4700: Loss = -11100.010695645797
2
Iteration 4800: Loss = -11100.010016584989
Iteration 4900: Loss = -11100.011356861756
1
Iteration 5000: Loss = -11100.009426910108
Iteration 5100: Loss = -11100.009540755129
1
Iteration 5200: Loss = -11100.009090559795
Iteration 5300: Loss = -11100.00891347942
Iteration 5400: Loss = -11100.00889748497
Iteration 5500: Loss = -11100.02074493844
1
Iteration 5600: Loss = -11100.008414960785
Iteration 5700: Loss = -11100.012805732546
1
Iteration 5800: Loss = -11100.008891847228
2
Iteration 5900: Loss = -11100.005659173252
Iteration 6000: Loss = -11100.005422512275
Iteration 6100: Loss = -11100.0052868823
Iteration 6200: Loss = -11100.005546262553
1
Iteration 6300: Loss = -11100.005565260717
2
Iteration 6400: Loss = -11100.01698574198
3
Iteration 6500: Loss = -11100.005174606365
Iteration 6600: Loss = -11100.004895088921
Iteration 6700: Loss = -11100.00480643066
Iteration 6800: Loss = -11100.005411747363
1
Iteration 6900: Loss = -11100.005602377616
2
Iteration 7000: Loss = -11100.005042197266
3
Iteration 7100: Loss = -11100.029692086096
4
Iteration 7200: Loss = -11100.028445360704
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.7849, 0.2151],
        [0.2620, 0.7380]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5730, 0.4270], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2903, 0.0909],
         [0.6325, 0.2038]],

        [[0.6276, 0.0954],
         [0.6057, 0.5226]],

        [[0.5318, 0.0952],
         [0.7271, 0.5908]],

        [[0.5580, 0.0835],
         [0.6353, 0.6324]],

        [[0.5265, 0.0986],
         [0.5341, 0.6476]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.9524772731062711
Average Adjusted Rand Index: 0.9526457071249215
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21409.361349266732
Iteration 100: Loss = -11490.003798183627
Iteration 200: Loss = -11487.950049434075
Iteration 300: Loss = -11482.951235196093
Iteration 400: Loss = -11464.57995457872
Iteration 500: Loss = -11393.683497947892
Iteration 600: Loss = -11175.27421704669
Iteration 700: Loss = -11125.328522817705
Iteration 800: Loss = -11104.872499590312
Iteration 900: Loss = -11104.359782033773
Iteration 1000: Loss = -11103.213235572193
Iteration 1100: Loss = -11103.09457214605
Iteration 1200: Loss = -11102.87808689445
Iteration 1300: Loss = -11102.789062541626
Iteration 1400: Loss = -11102.709128234554
Iteration 1500: Loss = -11102.673048054761
Iteration 1600: Loss = -11102.636270773417
Iteration 1700: Loss = -11102.607824076866
Iteration 1800: Loss = -11102.555620770541
Iteration 1900: Loss = -11100.283431804675
Iteration 2000: Loss = -11100.096466382876
Iteration 2100: Loss = -11100.079231222366
Iteration 2200: Loss = -11100.0729052984
Iteration 2300: Loss = -11100.06000621178
Iteration 2400: Loss = -11100.053251683055
Iteration 2500: Loss = -11100.047434336277
Iteration 2600: Loss = -11100.042375423283
Iteration 2700: Loss = -11100.03783738374
Iteration 2800: Loss = -11100.033877771326
Iteration 2900: Loss = -11100.030793375501
Iteration 3000: Loss = -11100.027255730382
Iteration 3100: Loss = -11100.024389589144
Iteration 3200: Loss = -11100.028997399391
1
Iteration 3300: Loss = -11100.019452636554
Iteration 3400: Loss = -11100.017267299274
Iteration 3500: Loss = -11100.033257074467
1
Iteration 3600: Loss = -11100.013179569754
Iteration 3700: Loss = -11100.010971079084
Iteration 3800: Loss = -11100.008993649322
Iteration 3900: Loss = -11099.98695135763
Iteration 4000: Loss = -11099.985063111042
Iteration 4100: Loss = -11099.963552511083
Iteration 4200: Loss = -11099.955437195607
Iteration 4300: Loss = -11099.952309132383
Iteration 4400: Loss = -11099.947999034854
Iteration 4500: Loss = -11099.941263761863
Iteration 4600: Loss = -11099.943962724172
1
Iteration 4700: Loss = -11099.939898688981
Iteration 4800: Loss = -11099.937791873686
Iteration 4900: Loss = -11099.936796252738
Iteration 5000: Loss = -11099.93764893432
1
Iteration 5100: Loss = -11099.933934328901
Iteration 5200: Loss = -11099.922003796677
Iteration 5300: Loss = -11099.911986896559
Iteration 5400: Loss = -11099.910613075936
Iteration 5500: Loss = -11099.910035336201
Iteration 5600: Loss = -11099.915393481462
1
Iteration 5700: Loss = -11099.9090799933
Iteration 5800: Loss = -11099.912629825403
1
Iteration 5900: Loss = -11099.908234498515
Iteration 6000: Loss = -11099.906664768916
Iteration 6100: Loss = -11099.90589852737
Iteration 6200: Loss = -11099.905552039343
Iteration 6300: Loss = -11099.906464201176
1
Iteration 6400: Loss = -11099.905084809998
Iteration 6500: Loss = -11099.905231448809
1
Iteration 6600: Loss = -11099.92062926317
2
Iteration 6700: Loss = -11099.912025347117
3
Iteration 6800: Loss = -11099.903612094831
Iteration 6900: Loss = -11099.902864750098
Iteration 7000: Loss = -11099.901257783664
Iteration 7100: Loss = -11099.900881585076
Iteration 7200: Loss = -11099.900517872657
Iteration 7300: Loss = -11099.902024118735
1
Iteration 7400: Loss = -11099.899821941146
Iteration 7500: Loss = -11099.905926551346
1
Iteration 7600: Loss = -11099.895855068069
Iteration 7700: Loss = -11099.89753600732
1
Iteration 7800: Loss = -11099.895229660078
Iteration 7900: Loss = -11099.894746338814
Iteration 8000: Loss = -11099.894174472538
Iteration 8100: Loss = -11099.894927539162
1
Iteration 8200: Loss = -11099.893947186278
Iteration 8300: Loss = -11099.89386356199
Iteration 8400: Loss = -11099.899339923346
1
Iteration 8500: Loss = -11099.894170584263
2
Iteration 8600: Loss = -11099.893662971403
Iteration 8700: Loss = -11099.893703986343
1
Iteration 8800: Loss = -11099.893444187299
Iteration 8900: Loss = -11099.8977051393
1
Iteration 9000: Loss = -11099.896838402077
2
Iteration 9100: Loss = -11099.893211970857
Iteration 9200: Loss = -11099.90432323671
1
Iteration 9300: Loss = -11099.893013251138
Iteration 9400: Loss = -11099.894809879757
1
Iteration 9500: Loss = -11099.89806425111
2
Iteration 9600: Loss = -11099.892786646655
Iteration 9700: Loss = -11099.893237959826
1
Iteration 9800: Loss = -11099.892478354373
Iteration 9900: Loss = -11099.891661231877
Iteration 10000: Loss = -11099.896282865822
1
Iteration 10100: Loss = -11099.891558720687
Iteration 10200: Loss = -11099.892226553635
1
Iteration 10300: Loss = -11099.910442559065
2
Iteration 10400: Loss = -11099.875928116451
Iteration 10500: Loss = -11099.875691884814
Iteration 10600: Loss = -11099.893809238842
1
Iteration 10700: Loss = -11099.87552031943
Iteration 10800: Loss = -11099.877979556153
1
Iteration 10900: Loss = -11099.91679617385
2
Iteration 11000: Loss = -11099.87819036351
3
Iteration 11100: Loss = -11100.036724137597
4
Iteration 11200: Loss = -11099.87539120352
Iteration 11300: Loss = -11099.889092232017
1
Iteration 11400: Loss = -11099.875558261268
2
Iteration 11500: Loss = -11099.877295534141
3
Iteration 11600: Loss = -11099.87577040848
4
Iteration 11700: Loss = -11099.875408344516
5
Stopping early at iteration 11700 due to no improvement.
pi: tensor([[0.7383, 0.2617],
        [0.2164, 0.7836]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4294, 0.5706], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2038, 0.0910],
         [0.6384, 0.2914]],

        [[0.6576, 0.0955],
         [0.6203, 0.5095]],

        [[0.5898, 0.0953],
         [0.5935, 0.5821]],

        [[0.6503, 0.0835],
         [0.6476, 0.6718]],

        [[0.7199, 0.0985],
         [0.5198, 0.6222]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.9524772731062711
Average Adjusted Rand Index: 0.9526457071249215
11120.753720287514
[0.9524772731062711, 0.9524772731062711, 0.9524772731062711, 0.9524772731062711] [0.9526457071249215, 0.9526457071249215, 0.9526457071249215, 0.9526457071249215] [11099.872280258853, 11099.860341012203, 11100.028445360704, 11099.875408344516]
-----------------------------------------------------------------------------------------
This iteration is 11
True Objective function: Loss = -11012.610165600014
Iteration 0: Loss = -11276.300040135635
Iteration 10: Loss = -11276.3000778169
1
Iteration 20: Loss = -11276.294763788988
Iteration 30: Loss = -11272.833291133713
Iteration 40: Loss = -11268.622953406626
Iteration 50: Loss = -11265.575261327927
Iteration 60: Loss = -11262.646306501105
Iteration 70: Loss = -11193.451968801019
Iteration 80: Loss = -10996.656771515889
Iteration 90: Loss = -10996.609088993408
Iteration 100: Loss = -10996.610754443655
1
Iteration 110: Loss = -10996.611257190994
2
Iteration 120: Loss = -10996.611375083423
3
Stopping early at iteration 120 due to no improvement.
pi: tensor([[0.7014, 0.2986],
        [0.2291, 0.7709]], dtype=torch.float64)
alpha: tensor([0.4598, 0.5402])
beta: tensor([[[0.2972, 0.0947],
         [0.1173, 0.1919]],

        [[0.9345, 0.1035],
         [0.7000, 0.9851]],

        [[0.5909, 0.1017],
         [0.1552, 0.1388]],

        [[0.4153, 0.0951],
         [0.5502, 0.1841]],

        [[0.4103, 0.0978],
         [0.0770, 0.8927]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
Global Adjusted Rand Index: 0.9061149819003964
Average Adjusted Rand Index: 0.9064097165458399
Iteration 0: Loss = -11284.895241884691
Iteration 10: Loss = -11270.43755632749
Iteration 20: Loss = -11266.455145354048
Iteration 30: Loss = -11263.906792815285
Iteration 40: Loss = -11249.71197761616
Iteration 50: Loss = -10997.635577831019
Iteration 60: Loss = -10996.612920755966
Iteration 70: Loss = -10996.61101595165
Iteration 80: Loss = -10996.611302751371
1
Iteration 90: Loss = -10996.611386570341
2
Iteration 100: Loss = -10996.611404930349
3
Stopping early at iteration 100 due to no improvement.
pi: tensor([[0.7709, 0.2291],
        [0.2986, 0.7014]], dtype=torch.float64)
alpha: tensor([0.5402, 0.4598])
beta: tensor([[[0.1919, 0.0947],
         [0.5721, 0.2972]],

        [[0.9495, 0.1035],
         [0.5679, 0.7597]],

        [[0.9614, 0.1017],
         [0.0237, 0.4896]],

        [[0.9709, 0.0951],
         [0.7761, 0.6739]],

        [[0.3020, 0.0978],
         [0.4793, 0.0818]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448326530612245
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
Global Adjusted Rand Index: 0.9061149819003964
Average Adjusted Rand Index: 0.9064097165458399
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22983.390540641467
Iteration 100: Loss = -11276.749731690023
Iteration 200: Loss = -11276.047818497844
Iteration 300: Loss = -11275.360774333334
Iteration 400: Loss = -11269.851998196023
Iteration 500: Loss = -11257.684663413896
Iteration 600: Loss = -11046.605031430865
Iteration 700: Loss = -11007.142473179969
Iteration 800: Loss = -11003.786445350373
Iteration 900: Loss = -11002.074981954547
Iteration 1000: Loss = -11001.760528934834
Iteration 1100: Loss = -11001.70683631869
Iteration 1200: Loss = -10999.679868744863
Iteration 1300: Loss = -10999.569333155161
Iteration 1400: Loss = -10999.514109872573
Iteration 1500: Loss = -10999.486129234841
Iteration 1600: Loss = -10999.47326022091
Iteration 1700: Loss = -10999.465064337095
Iteration 1800: Loss = -10999.457330869001
Iteration 1900: Loss = -10999.450514425447
Iteration 2000: Loss = -10999.428285755883
Iteration 2100: Loss = -10994.254218365371
Iteration 2200: Loss = -10994.250118784017
Iteration 2300: Loss = -10994.247002663244
Iteration 2400: Loss = -10994.24425794734
Iteration 2500: Loss = -10994.241746401141
Iteration 2600: Loss = -10994.239479579168
Iteration 2700: Loss = -10994.237287333106
Iteration 2800: Loss = -10994.234389392936
Iteration 2900: Loss = -10994.182280232593
Iteration 3000: Loss = -10994.173236587169
Iteration 3100: Loss = -10994.099971019525
Iteration 3200: Loss = -10994.093186182634
Iteration 3300: Loss = -10994.09218963169
Iteration 3400: Loss = -10994.09132277323
Iteration 3500: Loss = -10994.090481637799
Iteration 3600: Loss = -10994.089823993618
Iteration 3700: Loss = -10994.090296441924
1
Iteration 3800: Loss = -10994.087512337053
Iteration 3900: Loss = -10992.670138131849
Iteration 4000: Loss = -10992.478256960565
Iteration 4100: Loss = -10992.477702171267
Iteration 4200: Loss = -10992.47730886103
Iteration 4300: Loss = -10992.477003595643
Iteration 4400: Loss = -10992.476639324868
Iteration 4500: Loss = -10992.476880854007
1
Iteration 4600: Loss = -10992.47592478172
Iteration 4700: Loss = -10992.475681722171
Iteration 4800: Loss = -10992.474869002586
Iteration 4900: Loss = -10992.474093873401
Iteration 5000: Loss = -10992.473689069977
Iteration 5100: Loss = -10992.473153692405
Iteration 5200: Loss = -10992.472587414128
Iteration 5300: Loss = -10992.47210181858
Iteration 5400: Loss = -10992.472040567158
Iteration 5500: Loss = -10992.47145094878
Iteration 5600: Loss = -10992.475928024236
1
Iteration 5700: Loss = -10992.47039481251
Iteration 5800: Loss = -10992.47857881489
1
Iteration 5900: Loss = -10992.46676454174
Iteration 6000: Loss = -10992.46327907386
Iteration 6100: Loss = -10992.462125463255
Iteration 6200: Loss = -10992.461964308151
Iteration 6300: Loss = -10992.46184656869
Iteration 6400: Loss = -10992.461690953696
Iteration 6500: Loss = -10992.461625660117
Iteration 6600: Loss = -10992.461303694754
Iteration 6700: Loss = -10992.461293996841
Iteration 6800: Loss = -10992.46077996652
Iteration 6900: Loss = -10992.468015374761
1
Iteration 7000: Loss = -10992.460064773199
Iteration 7100: Loss = -10992.466083574756
1
Iteration 7200: Loss = -10992.45575031267
Iteration 7300: Loss = -10992.446504301026
Iteration 7400: Loss = -10992.446140160171
Iteration 7500: Loss = -10992.447128927743
1
Iteration 7600: Loss = -10992.446694557522
2
Iteration 7700: Loss = -10992.446042773352
Iteration 7800: Loss = -10992.445860281416
Iteration 7900: Loss = -10992.446348845651
1
Iteration 8000: Loss = -10992.44622838631
2
Iteration 8100: Loss = -10992.453954550225
3
Iteration 8200: Loss = -10992.462321079003
4
Iteration 8300: Loss = -10992.445757656116
Iteration 8400: Loss = -10992.445490262344
Iteration 8500: Loss = -10992.44592557744
1
Iteration 8600: Loss = -10992.445371829086
Iteration 8700: Loss = -10992.445577357264
1
Iteration 8800: Loss = -10992.44533669879
Iteration 8900: Loss = -10992.454458338136
1
Iteration 9000: Loss = -10992.445258203094
Iteration 9100: Loss = -10992.445212349647
Iteration 9200: Loss = -10992.444928864537
Iteration 9300: Loss = -10992.444763002219
Iteration 9400: Loss = -10992.4447370728
Iteration 9500: Loss = -10992.44474483409
1
Iteration 9600: Loss = -10992.444680308427
Iteration 9700: Loss = -10992.452783810553
1
Iteration 9800: Loss = -10992.445536039346
2
Iteration 9900: Loss = -10992.451567779155
3
Iteration 10000: Loss = -10992.489626035169
4
Iteration 10100: Loss = -10992.45298756923
5
Stopping early at iteration 10100 due to no improvement.
pi: tensor([[0.7165, 0.2835],
        [0.2130, 0.7870]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4883, 0.5117], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3043, 0.0940],
         [0.6233, 0.1954]],

        [[0.6804, 0.1035],
         [0.5020, 0.6486]],

        [[0.6715, 0.1018],
         [0.7047, 0.7049]],

        [[0.5724, 0.0954],
         [0.6907, 0.7075]],

        [[0.5142, 0.0975],
         [0.5750, 0.5328]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
Global Adjusted Rand Index: 0.9291529617618343
Average Adjusted Rand Index: 0.9291197121765995
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21925.30878321258
Iteration 100: Loss = -11276.809417081513
Iteration 200: Loss = -11276.112042683715
Iteration 300: Loss = -11275.440089604952
Iteration 400: Loss = -11268.577547122934
Iteration 500: Loss = -11145.356110698891
Iteration 600: Loss = -11006.41997855947
Iteration 700: Loss = -10993.240732804892
Iteration 800: Loss = -10992.92909759149
Iteration 900: Loss = -10992.789456503824
Iteration 1000: Loss = -10992.662754979594
Iteration 1100: Loss = -10992.60683294532
Iteration 1200: Loss = -10992.574888703019
Iteration 1300: Loss = -10992.551910844311
Iteration 1400: Loss = -10992.534275984266
Iteration 1500: Loss = -10992.520497995827
Iteration 1600: Loss = -10992.509466249789
Iteration 1700: Loss = -10992.500292374694
Iteration 1800: Loss = -10992.492340756395
Iteration 1900: Loss = -10992.484668436118
Iteration 2000: Loss = -10992.475038702292
Iteration 2100: Loss = -10992.468977693268
Iteration 2200: Loss = -10992.465428607253
Iteration 2300: Loss = -10992.462433638728
Iteration 2400: Loss = -10992.459869180044
Iteration 2500: Loss = -10992.457657948908
Iteration 2600: Loss = -10992.455654908466
Iteration 2700: Loss = -10992.453924970043
Iteration 2800: Loss = -10992.452385382858
Iteration 2900: Loss = -10992.450976870416
Iteration 3000: Loss = -10992.449733606323
Iteration 3100: Loss = -10992.448898905364
Iteration 3200: Loss = -10992.447628333897
Iteration 3300: Loss = -10992.449982630691
1
Iteration 3400: Loss = -10992.445777913405
Iteration 3500: Loss = -10992.444984440534
Iteration 3600: Loss = -10992.445227420281
1
Iteration 3700: Loss = -10992.443475907407
Iteration 3800: Loss = -10992.44295237658
Iteration 3900: Loss = -10992.444410499065
1
Iteration 4000: Loss = -10992.441954164133
Iteration 4100: Loss = -10992.44193150656
Iteration 4200: Loss = -10992.441131547337
Iteration 4300: Loss = -10992.440953891828
Iteration 4400: Loss = -10992.440430004473
Iteration 4500: Loss = -10992.440131299049
Iteration 4600: Loss = -10992.439845465915
Iteration 4700: Loss = -10992.439593574365
Iteration 4800: Loss = -10992.439294403459
Iteration 4900: Loss = -10992.439071794002
Iteration 5000: Loss = -10992.441925895095
1
Iteration 5100: Loss = -10992.438615744697
Iteration 5200: Loss = -10992.441440645332
1
Iteration 5300: Loss = -10992.438239176598
Iteration 5400: Loss = -10992.438412162155
1
Iteration 5500: Loss = -10992.437911654039
Iteration 5600: Loss = -10992.44195833817
1
Iteration 5700: Loss = -10992.437599409986
Iteration 5800: Loss = -10992.437501587205
Iteration 5900: Loss = -10992.437360014972
Iteration 6000: Loss = -10992.437232568192
Iteration 6100: Loss = -10992.437120267357
Iteration 6200: Loss = -10992.437026262858
Iteration 6300: Loss = -10992.436916186685
Iteration 6400: Loss = -10992.436811999482
Iteration 6500: Loss = -10992.43677524219
Iteration 6600: Loss = -10992.436189148191
Iteration 6700: Loss = -10992.43169814583
Iteration 6800: Loss = -10992.431762422391
1
Iteration 6900: Loss = -10992.431234252455
Iteration 7000: Loss = -10992.431162234796
Iteration 7100: Loss = -10992.431113648814
Iteration 7200: Loss = -10992.431052712302
Iteration 7300: Loss = -10992.431016096047
Iteration 7400: Loss = -10992.430928120653
Iteration 7500: Loss = -10992.430935598664
1
Iteration 7600: Loss = -10992.430847128082
Iteration 7700: Loss = -10992.430881256494
1
Iteration 7800: Loss = -10992.43078487174
Iteration 7900: Loss = -10992.430794966222
1
Iteration 8000: Loss = -10992.43136128285
2
Iteration 8100: Loss = -10992.430636102516
Iteration 8200: Loss = -10992.43104535897
1
Iteration 8300: Loss = -10992.433745580367
2
Iteration 8400: Loss = -10992.430727402965
3
Iteration 8500: Loss = -10992.43077596387
4
Iteration 8600: Loss = -10992.430650780394
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7164, 0.2836],
        [0.2124, 0.7876]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4881, 0.5119], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3045, 0.0941],
         [0.7083, 0.1952]],

        [[0.5603, 0.1037],
         [0.5854, 0.5889]],

        [[0.6871, 0.1019],
         [0.5362, 0.5036]],

        [[0.7211, 0.0956],
         [0.6385, 0.7187]],

        [[0.6946, 0.0977],
         [0.7211, 0.7053]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
Global Adjusted Rand Index: 0.9291529617618343
Average Adjusted Rand Index: 0.9291197121765995
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20049.627537762193
Iteration 100: Loss = -11320.605717755598
Iteration 200: Loss = -11286.03951754565
Iteration 300: Loss = -11270.634104013758
Iteration 400: Loss = -11217.633771048842
Iteration 500: Loss = -11179.455751223626
Iteration 600: Loss = -11156.861973693123
Iteration 700: Loss = -11156.288945889986
Iteration 800: Loss = -11156.124948860399
Iteration 900: Loss = -11150.005181978095
Iteration 1000: Loss = -11149.952315420833
Iteration 1100: Loss = -11149.913277166495
Iteration 1200: Loss = -11149.887017261535
Iteration 1300: Loss = -11149.866761128924
Iteration 1400: Loss = -11149.849959324629
Iteration 1500: Loss = -11140.938900404968
Iteration 1600: Loss = -11140.920198050202
Iteration 1700: Loss = -11140.910407462785
Iteration 1800: Loss = -11140.901376298094
Iteration 1900: Loss = -11140.893055830838
Iteration 2000: Loss = -11140.884875379481
Iteration 2100: Loss = -11139.637865411532
Iteration 2200: Loss = -11139.474536504738
Iteration 2300: Loss = -11139.470009766497
Iteration 2400: Loss = -11139.465237369277
Iteration 2500: Loss = -11139.459174839281
Iteration 2600: Loss = -11139.449047731967
Iteration 2700: Loss = -11139.367402912312
Iteration 2800: Loss = -11139.36025724808
Iteration 2900: Loss = -11139.358460475598
Iteration 3000: Loss = -11139.35567448036
Iteration 3100: Loss = -11139.353028470028
Iteration 3200: Loss = -11139.225123635313
Iteration 3300: Loss = -11139.210508746006
Iteration 3400: Loss = -11139.211923322715
1
Iteration 3500: Loss = -11139.172841985073
Iteration 3600: Loss = -11139.165486880418
Iteration 3700: Loss = -11139.07470382448
Iteration 3800: Loss = -11139.018807555429
Iteration 3900: Loss = -11138.95964285555
Iteration 4000: Loss = -11137.826909281028
Iteration 4100: Loss = -11137.34018917673
Iteration 4200: Loss = -11137.061668083194
Iteration 4300: Loss = -11069.739602066944
Iteration 4400: Loss = -11065.370201779393
Iteration 4500: Loss = -11064.97040561119
Iteration 4600: Loss = -11064.921643209887
Iteration 4700: Loss = -11054.629593297874
Iteration 4800: Loss = -11048.64378501511
Iteration 4900: Loss = -11047.777618275068
Iteration 5000: Loss = -11041.293523735483
Iteration 5100: Loss = -11041.252775554774
Iteration 5200: Loss = -11041.251239310881
Iteration 5300: Loss = -11041.25866834628
1
Iteration 5400: Loss = -11041.248677298456
Iteration 5500: Loss = -11041.264809730057
1
Iteration 5600: Loss = -11041.24112466143
Iteration 5700: Loss = -11041.241528087263
1
Iteration 5800: Loss = -11041.240054217651
Iteration 5900: Loss = -11035.29779840836
Iteration 6000: Loss = -11035.29542906303
Iteration 6100: Loss = -11035.294321235504
Iteration 6200: Loss = -11035.29176792208
Iteration 6300: Loss = -11035.276389185232
Iteration 6400: Loss = -11034.357443509041
Iteration 6500: Loss = -11033.845309934342
Iteration 6600: Loss = -11033.84508048125
Iteration 6700: Loss = -11033.847984873159
1
Iteration 6800: Loss = -11030.691259893718
Iteration 6900: Loss = -11030.689682652313
Iteration 7000: Loss = -11030.690813607354
1
Iteration 7100: Loss = -11030.697515060943
2
Iteration 7200: Loss = -11030.689070109025
Iteration 7300: Loss = -11030.540930581808
Iteration 7400: Loss = -11030.533065382637
Iteration 7500: Loss = -11030.532777848977
Iteration 7600: Loss = -11030.532377363374
Iteration 7700: Loss = -11030.547044759182
1
Iteration 7800: Loss = -11030.528586545517
Iteration 7900: Loss = -11030.495314666015
Iteration 8000: Loss = -11030.493936101106
Iteration 8100: Loss = -11030.49381907755
Iteration 8200: Loss = -11030.493712990594
Iteration 8300: Loss = -11030.49246488675
Iteration 8400: Loss = -11030.490753320513
Iteration 8500: Loss = -11030.536333989605
1
Iteration 8600: Loss = -11030.490584975487
Iteration 8700: Loss = -11030.49105251951
1
Iteration 8800: Loss = -11030.551685959967
2
Iteration 8900: Loss = -11030.490470318058
Iteration 9000: Loss = -11030.4904622913
Iteration 9100: Loss = -11030.490577672903
1
Iteration 9200: Loss = -11030.490348299756
Iteration 9300: Loss = -11030.476441195495
Iteration 9400: Loss = -11030.474818997218
Iteration 9500: Loss = -11030.475280538827
1
Iteration 9600: Loss = -11030.474547894073
Iteration 9700: Loss = -11030.47227117209
Iteration 9800: Loss = -11030.479587418418
1
Iteration 9900: Loss = -11030.472266187688
Iteration 10000: Loss = -11030.486574920473
1
Iteration 10100: Loss = -11030.233692029755
Iteration 10200: Loss = -11029.965887720206
Iteration 10300: Loss = -11029.94972545282
Iteration 10400: Loss = -11029.949678920808
Iteration 10500: Loss = -11029.974692720401
1
Iteration 10600: Loss = -11029.947387992821
Iteration 10700: Loss = -11029.947108772738
Iteration 10800: Loss = -11029.957468201805
1
Iteration 10900: Loss = -11026.904944564598
Iteration 11000: Loss = -11026.90498741367
1
Iteration 11100: Loss = -11026.882435512416
Iteration 11200: Loss = -11026.881297285216
Iteration 11300: Loss = -11026.88311059486
1
Iteration 11400: Loss = -11026.90576017804
2
Iteration 11500: Loss = -11026.888946974921
3
Iteration 11600: Loss = -11026.880488689585
Iteration 11700: Loss = -11026.880112271523
Iteration 11800: Loss = -11021.720096614084
Iteration 11900: Loss = -11021.804192131714
1
Iteration 12000: Loss = -11021.717277194679
Iteration 12100: Loss = -11021.716868333457
Iteration 12200: Loss = -11022.061970631363
1
Iteration 12300: Loss = -11021.717354873232
2
Iteration 12400: Loss = -11021.717706021196
3
Iteration 12500: Loss = -11021.722382149308
4
Iteration 12600: Loss = -11021.724231384771
5
Stopping early at iteration 12600 due to no improvement.
pi: tensor([[0.8077, 0.1923],
        [0.2825, 0.7175]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5142, 0.4858], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1918, 0.0943],
         [0.5961, 0.3080]],

        [[0.7108, 0.1097],
         [0.5789, 0.5373]],

        [[0.6899, 0.1139],
         [0.6001, 0.5409]],

        [[0.6948, 0.0958],
         [0.5139, 0.5528]],

        [[0.5579, 0.0978],
         [0.6964, 0.5347]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448427857772554
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 91
Adjusted Rand Index: 0.6691414645181857
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
Global Adjusted Rand Index: 0.8683564208917639
Average Adjusted Rand Index: 0.871272284356462
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21605.8738114622
Iteration 100: Loss = -11275.914742654533
Iteration 200: Loss = -11272.216362722764
Iteration 300: Loss = -11269.719017971202
Iteration 400: Loss = -11265.941374988855
Iteration 500: Loss = -11129.699837010625
Iteration 600: Loss = -11101.52489462671
Iteration 700: Loss = -11090.861357087653
Iteration 800: Loss = -11088.903253087195
Iteration 900: Loss = -11088.739676364252
Iteration 1000: Loss = -11085.164895938506
Iteration 1100: Loss = -11081.127418575394
Iteration 1200: Loss = -11080.966869803071
Iteration 1300: Loss = -11080.896926964648
Iteration 1400: Loss = -11078.648687867199
Iteration 1500: Loss = -11034.248053404008
Iteration 1600: Loss = -11022.29961184523
Iteration 1700: Loss = -11006.060387974265
Iteration 1800: Loss = -11006.017744343964
Iteration 1900: Loss = -11005.979855665806
Iteration 2000: Loss = -10998.662605804413
Iteration 2100: Loss = -10995.740165611318
Iteration 2200: Loss = -10995.587996423928
Iteration 2300: Loss = -10992.514412543518
Iteration 2400: Loss = -10992.506156867048
Iteration 2500: Loss = -10992.499801322252
Iteration 2600: Loss = -10992.493187676308
Iteration 2700: Loss = -10992.477526099943
Iteration 2800: Loss = -10992.46342272944
Iteration 2900: Loss = -10992.461136831278
Iteration 3000: Loss = -10992.459352031852
Iteration 3100: Loss = -10992.45631553374
Iteration 3200: Loss = -10992.454046250114
Iteration 3300: Loss = -10992.452755861717
Iteration 3400: Loss = -10992.452219289129
Iteration 3500: Loss = -10992.456495294053
1
Iteration 3600: Loss = -10992.449234254198
Iteration 3700: Loss = -10992.44801372988
Iteration 3800: Loss = -10992.451717520662
1
Iteration 3900: Loss = -10992.446465775014
Iteration 4000: Loss = -10992.458283079552
1
Iteration 4100: Loss = -10992.445260426397
Iteration 4200: Loss = -10992.44467242717
Iteration 4300: Loss = -10992.444015279312
Iteration 4400: Loss = -10992.443135301372
Iteration 4500: Loss = -10992.441900214728
Iteration 4600: Loss = -10992.440991257916
Iteration 4700: Loss = -10992.446644783198
1
Iteration 4800: Loss = -10992.439532566663
Iteration 4900: Loss = -10992.438208062087
Iteration 5000: Loss = -10992.434757507486
Iteration 5100: Loss = -10992.440299121701
1
Iteration 5200: Loss = -10992.433182854142
Iteration 5300: Loss = -10992.433227384741
1
Iteration 5400: Loss = -10992.432694721992
Iteration 5500: Loss = -10992.432686042486
Iteration 5600: Loss = -10992.432276410318
Iteration 5700: Loss = -10992.433218073627
1
Iteration 5800: Loss = -10992.431926882398
Iteration 5900: Loss = -10992.460593138752
1
Iteration 6000: Loss = -10992.431644990287
Iteration 6100: Loss = -10992.431563950173
Iteration 6200: Loss = -10992.431491416084
Iteration 6300: Loss = -10992.43133529815
Iteration 6400: Loss = -10992.431496654754
1
Iteration 6500: Loss = -10992.436158621513
2
Iteration 6600: Loss = -10992.43116268073
Iteration 6700: Loss = -10992.43094283237
Iteration 6800: Loss = -10992.430571624629
Iteration 6900: Loss = -10992.433248851323
1
Iteration 7000: Loss = -10992.430396889826
Iteration 7100: Loss = -10992.430457409431
1
Iteration 7200: Loss = -10992.430966583235
2
Iteration 7300: Loss = -10992.430243643686
Iteration 7400: Loss = -10992.43024071028
Iteration 7500: Loss = -10992.43368794095
1
Iteration 7600: Loss = -10992.430068208952
Iteration 7700: Loss = -10992.431472048149
1
Iteration 7800: Loss = -10992.429992613308
Iteration 7900: Loss = -10992.429996537996
1
Iteration 8000: Loss = -10992.429946653958
Iteration 8100: Loss = -10992.430013509489
1
Iteration 8200: Loss = -10992.429899617006
Iteration 8300: Loss = -10992.429902278263
1
Iteration 8400: Loss = -10992.441429022972
2
Iteration 8500: Loss = -10992.429855614195
Iteration 8600: Loss = -10992.43083540338
1
Iteration 8700: Loss = -10992.429833999959
Iteration 8800: Loss = -10992.509249163955
1
Iteration 8900: Loss = -10992.429826489319
Iteration 9000: Loss = -10992.429795059918
Iteration 9100: Loss = -10992.430421479285
1
Iteration 9200: Loss = -10992.429735777496
Iteration 9300: Loss = -10992.429771778072
1
Iteration 9400: Loss = -10992.42979072629
2
Iteration 9500: Loss = -10992.43014971818
3
Iteration 9600: Loss = -10992.43033247436
4
Iteration 9700: Loss = -10992.437219714311
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.7159, 0.2841],
        [0.2122, 0.7878]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4865, 0.5135], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3046, 0.0941],
         [0.5060, 0.1950]],

        [[0.6494, 0.1038],
         [0.5437, 0.5675]],

        [[0.5124, 0.1022],
         [0.5661, 0.7283]],

        [[0.7019, 0.0953],
         [0.6160, 0.5580]],

        [[0.6463, 0.0972],
         [0.6588, 0.5382]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
Global Adjusted Rand Index: 0.9291529617618343
Average Adjusted Rand Index: 0.9291197121765995
11012.610165600014
[0.9291529617618343, 0.9291529617618343, 0.8683564208917639, 0.9291529617618343] [0.9291197121765995, 0.9291197121765995, 0.871272284356462, 0.9291197121765995] [10992.45298756923, 10992.430650780394, 11021.724231384771, 10992.437219714311]
-----------------------------------------------------------------------------------------
This iteration is 12
True Objective function: Loss = -10992.22515368903
Iteration 0: Loss = -11300.792736771042
Iteration 10: Loss = -11229.542900599066
Iteration 20: Loss = -11229.401428374244
Iteration 30: Loss = -11229.333361919864
Iteration 40: Loss = -11229.278462180262
Iteration 50: Loss = -11229.228489472292
Iteration 60: Loss = -11229.179809575675
Iteration 70: Loss = -11229.129442178708
Iteration 80: Loss = -11229.073241727838
Iteration 90: Loss = -11229.004011753794
Iteration 100: Loss = -11228.902591082133
Iteration 110: Loss = -11228.682340175303
Iteration 120: Loss = -11225.545478027218
Iteration 130: Loss = -11057.500029473253
Iteration 140: Loss = -10975.347449170606
Iteration 150: Loss = -10968.717544591085
Iteration 160: Loss = -10968.242984004732
Iteration 170: Loss = -10968.231577203938
Iteration 180: Loss = -10968.231232053682
Iteration 190: Loss = -10968.231247335083
1
Iteration 200: Loss = -10968.231246867585
2
Iteration 210: Loss = -10968.23122619558
Iteration 220: Loss = -10968.23124592902
1
Iteration 230: Loss = -10968.231226195561
Iteration 240: Loss = -10968.231245929022
1
Iteration 250: Loss = -10968.231226195561
2
Iteration 260: Loss = -10968.231245929022
3
Stopping early at iteration 260 due to no improvement.
pi: tensor([[0.7349, 0.2651],
        [0.3133, 0.6867]], dtype=torch.float64)
alpha: tensor([0.5362, 0.4638])
beta: tensor([[[0.1978, 0.0920],
         [0.8705, 0.2878]],

        [[0.9637, 0.0990],
         [0.1662, 0.4200]],

        [[0.0638, 0.0877],
         [0.8907, 0.1324]],

        [[0.9888, 0.1056],
         [0.0124, 0.2221]],

        [[0.2229, 0.0972],
         [0.8193, 0.0451]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.9061148884638575
Average Adjusted Rand Index: 0.9065805500846027
Iteration 0: Loss = -11364.626657756233
Iteration 10: Loss = -11230.341832523654
Iteration 20: Loss = -11229.926968911797
Iteration 30: Loss = -11229.70006486135
Iteration 40: Loss = -11229.557611684122
Iteration 50: Loss = -11229.459507615738
Iteration 60: Loss = -11229.38524745212
Iteration 70: Loss = -11229.324463473991
Iteration 80: Loss = -11229.27118879994
Iteration 90: Loss = -11229.221776808883
Iteration 100: Loss = -11229.173109540652
Iteration 110: Loss = -11229.122190378217
Iteration 120: Loss = -11229.064831695352
Iteration 130: Loss = -11228.99276028907
Iteration 140: Loss = -11228.883522411234
Iteration 150: Loss = -11228.621708552766
Iteration 160: Loss = -11220.951246137502
Iteration 170: Loss = -11025.632544221564
Iteration 180: Loss = -10973.831616747515
Iteration 190: Loss = -10968.526572861929
Iteration 200: Loss = -10968.238312583311
Iteration 210: Loss = -10968.231437742237
Iteration 220: Loss = -10968.231232782588
Iteration 230: Loss = -10968.231241215322
1
Iteration 240: Loss = -10968.231216824748
Iteration 250: Loss = -10968.23124486168
1
Iteration 260: Loss = -10968.231224343835
2
Iteration 270: Loss = -10968.23124486169
3
Stopping early at iteration 270 due to no improvement.
pi: tensor([[0.6867, 0.3133],
        [0.2651, 0.7349]], dtype=torch.float64)
alpha: tensor([0.4638, 0.5362])
beta: tensor([[[0.2878, 0.0920],
         [0.4936, 0.1978]],

        [[0.5656, 0.0990],
         [0.0281, 0.3388]],

        [[0.6472, 0.0877],
         [0.1670, 0.4809]],

        [[0.7673, 0.1056],
         [0.1687, 0.5400]],

        [[0.9259, 0.0972],
         [0.4078, 0.5625]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080702804390127
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.9061148884638575
Average Adjusted Rand Index: 0.9065805500846027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21697.024559795696
Iteration 100: Loss = -11232.810009885648
Iteration 200: Loss = -11231.93185605913
Iteration 300: Loss = -11231.640348027826
Iteration 400: Loss = -11231.45504170583
Iteration 500: Loss = -11231.143495398986
Iteration 600: Loss = -11230.82921936653
Iteration 700: Loss = -11230.59737476436
Iteration 800: Loss = -11230.394478970427
Iteration 900: Loss = -11230.210360916586
Iteration 1000: Loss = -11230.03213910196
Iteration 1100: Loss = -11229.860782125857
Iteration 1200: Loss = -11229.70359391584
Iteration 1300: Loss = -11229.564520651302
Iteration 1400: Loss = -11229.444394231698
Iteration 1500: Loss = -11229.33130379325
Iteration 1600: Loss = -11229.185633966063
Iteration 1700: Loss = -11196.421964598669
Iteration 1800: Loss = -11040.978799012042
Iteration 1900: Loss = -11028.856815415123
Iteration 2000: Loss = -11023.69622215251
Iteration 2100: Loss = -11022.85489357151
Iteration 2200: Loss = -11003.45282215794
Iteration 2300: Loss = -11001.356962807211
Iteration 2400: Loss = -11000.871338931225
Iteration 2500: Loss = -11000.808052653301
Iteration 2600: Loss = -11000.784499001444
Iteration 2700: Loss = -11000.767029027456
Iteration 2800: Loss = -11000.754771717173
Iteration 2900: Loss = -11000.742191954629
Iteration 3000: Loss = -11000.717614658739
Iteration 3100: Loss = -10999.43354579666
Iteration 3200: Loss = -10999.413493442938
Iteration 3300: Loss = -10999.4081339796
Iteration 3400: Loss = -10999.404114359737
Iteration 3500: Loss = -10999.401524000681
Iteration 3600: Loss = -10999.398894681352
Iteration 3700: Loss = -10999.396760896569
Iteration 3800: Loss = -10999.393802875224
Iteration 3900: Loss = -10999.396242990191
1
Iteration 4000: Loss = -10999.378668702622
Iteration 4100: Loss = -10999.365179270326
Iteration 4200: Loss = -10999.326180850769
Iteration 4300: Loss = -10999.320491814191
Iteration 4400: Loss = -10999.319392410805
Iteration 4500: Loss = -10999.324537486442
1
Iteration 4600: Loss = -10999.318011807372
Iteration 4700: Loss = -10999.31682322296
Iteration 4800: Loss = -10999.318139928853
1
Iteration 4900: Loss = -10999.320955917496
2
Iteration 5000: Loss = -10999.314867730027
Iteration 5100: Loss = -10999.314095315547
Iteration 5200: Loss = -10999.313761259447
Iteration 5300: Loss = -10999.320613052525
1
Iteration 5400: Loss = -10999.313061999635
Iteration 5500: Loss = -10999.311816528378
Iteration 5600: Loss = -10999.311019009076
Iteration 5700: Loss = -10999.310575282449
Iteration 5800: Loss = -10999.323169566016
1
Iteration 5900: Loss = -10999.309602568048
Iteration 6000: Loss = -10999.311087866836
1
Iteration 6100: Loss = -10999.308497616805
Iteration 6200: Loss = -10999.31008539404
1
Iteration 6300: Loss = -10999.309453041824
2
Iteration 6400: Loss = -10999.306764219013
Iteration 6500: Loss = -10999.306405450861
Iteration 6600: Loss = -10999.307979542376
1
Iteration 6700: Loss = -10999.300830725124
Iteration 6800: Loss = -10999.300422932238
Iteration 6900: Loss = -10999.29959431877
Iteration 7000: Loss = -10999.300422382128
1
Iteration 7100: Loss = -10999.299952131732
2
Iteration 7200: Loss = -10999.299779072053
3
Iteration 7300: Loss = -10999.299777246864
4
Iteration 7400: Loss = -10999.29858830337
Iteration 7500: Loss = -10999.297798003437
Iteration 7600: Loss = -10999.297391856158
Iteration 7700: Loss = -10999.30202090332
1
Iteration 7800: Loss = -10999.2968067005
Iteration 7900: Loss = -10999.298474427787
1
Iteration 8000: Loss = -10999.296255809259
Iteration 8100: Loss = -10999.497379784585
1
Iteration 8200: Loss = -10999.296069097865
Iteration 8300: Loss = -10999.29597114433
Iteration 8400: Loss = -10999.297405111747
1
Iteration 8500: Loss = -10999.295552805675
Iteration 8600: Loss = -10999.295415191647
Iteration 8700: Loss = -10999.295589246292
1
Iteration 8800: Loss = -10999.295349630456
Iteration 8900: Loss = -10999.29531348549
Iteration 9000: Loss = -10999.296207822987
1
Iteration 9100: Loss = -10999.295185266361
Iteration 9200: Loss = -10999.295139130743
Iteration 9300: Loss = -10999.299893527594
1
Iteration 9400: Loss = -10999.29503955138
Iteration 9500: Loss = -10999.295016724414
Iteration 9600: Loss = -10999.296453660207
1
Iteration 9700: Loss = -10999.294948917433
Iteration 9800: Loss = -10999.294899913179
Iteration 9900: Loss = -10999.294902797006
1
Iteration 10000: Loss = -10999.294756400952
Iteration 10100: Loss = -10999.295089982255
1
Iteration 10200: Loss = -10999.323456459775
2
Iteration 10300: Loss = -10999.29710418192
3
Iteration 10400: Loss = -10999.29426847985
Iteration 10500: Loss = -10999.30404495416
1
Iteration 10600: Loss = -10999.294151276932
Iteration 10700: Loss = -10999.294713361352
1
Iteration 10800: Loss = -10999.293003613619
Iteration 10900: Loss = -10999.294450602807
1
Iteration 11000: Loss = -10999.292923589037
Iteration 11100: Loss = -10999.619268692253
1
Iteration 11200: Loss = -10999.292849432992
Iteration 11300: Loss = -10999.296566310202
1
Iteration 11400: Loss = -10999.293263509195
2
Iteration 11500: Loss = -10999.323014011377
3
Iteration 11600: Loss = -10999.292665884406
Iteration 11700: Loss = -10999.357934970241
1
Iteration 11800: Loss = -10999.292593894386
Iteration 11900: Loss = -10999.296863811724
1
Iteration 12000: Loss = -10999.310589621218
2
Iteration 12100: Loss = -10999.291805601248
Iteration 12200: Loss = -10999.292927103448
1
Iteration 12300: Loss = -10999.293078094117
2
Iteration 12400: Loss = -10999.292018424174
3
Iteration 12500: Loss = -10999.348503698873
4
Iteration 12600: Loss = -10999.291972008261
5
Stopping early at iteration 12600 due to no improvement.
pi: tensor([[0.2669, 0.7331],
        [0.6934, 0.3066]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4894, 0.5106], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2727, 0.0917],
         [0.6760, 0.2149]],

        [[0.6073, 0.0974],
         [0.7267, 0.6494]],

        [[0.6725, 0.0876],
         [0.7109, 0.5708]],

        [[0.6870, 0.1075],
         [0.5989, 0.5316]],

        [[0.5305, 0.0966],
         [0.6091, 0.6941]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7369552685595733
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 91
Adjusted Rand Index: 0.6691881173576176
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.05380871531966369
Average Adjusted Rand Index: 0.8261949517439652
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23691.829367078146
Iteration 100: Loss = -11232.47087023428
Iteration 200: Loss = -11231.74570859969
Iteration 300: Loss = -11231.568427891005
Iteration 400: Loss = -11231.449296734514
Iteration 500: Loss = -11231.341351522613
Iteration 600: Loss = -11231.237952329133
Iteration 700: Loss = -11231.150491912369
Iteration 800: Loss = -11231.084506113115
Iteration 900: Loss = -11231.033087129495
Iteration 1000: Loss = -11230.987047018401
Iteration 1100: Loss = -11230.938073004256
Iteration 1200: Loss = -11230.87583047507
Iteration 1300: Loss = -11230.77519637076
Iteration 1400: Loss = -11230.462099380717
Iteration 1500: Loss = -11227.7837249196
Iteration 1600: Loss = -11226.878448793132
Iteration 1700: Loss = -11224.716498585314
Iteration 1800: Loss = -11178.60950818857
Iteration 1900: Loss = -11004.139401951026
Iteration 2000: Loss = -10993.15198600412
Iteration 2100: Loss = -10984.458705270039
Iteration 2200: Loss = -10983.897532669376
Iteration 2300: Loss = -10983.86727475702
Iteration 2400: Loss = -10983.81805542322
Iteration 2500: Loss = -10983.659918756272
Iteration 2600: Loss = -10983.650787721484
Iteration 2700: Loss = -10983.6441391321
Iteration 2800: Loss = -10983.63895586279
Iteration 2900: Loss = -10983.643195763867
1
Iteration 3000: Loss = -10983.609903929117
Iteration 3100: Loss = -10983.534036015011
Iteration 3200: Loss = -10983.534111354002
1
Iteration 3300: Loss = -10983.528058446596
Iteration 3400: Loss = -10983.525081641188
Iteration 3500: Loss = -10983.521380815351
Iteration 3600: Loss = -10983.51958652899
Iteration 3700: Loss = -10983.519327281732
Iteration 3800: Loss = -10983.513547466828
Iteration 3900: Loss = -10983.502920334287
Iteration 4000: Loss = -10983.501044072693
Iteration 4100: Loss = -10983.500466419013
Iteration 4200: Loss = -10983.499805494597
Iteration 4300: Loss = -10983.496062987699
Iteration 4400: Loss = -10979.275509433308
Iteration 4500: Loss = -10979.238073854733
Iteration 4600: Loss = -10979.237649543475
Iteration 4700: Loss = -10979.236595921479
Iteration 4800: Loss = -10979.236310462606
Iteration 4900: Loss = -10979.23615649482
Iteration 5000: Loss = -10979.23554209142
Iteration 5100: Loss = -10979.236315529355
1
Iteration 5200: Loss = -10979.23477644167
Iteration 5300: Loss = -10979.234601782173
Iteration 5400: Loss = -10979.23587363407
1
Iteration 5500: Loss = -10979.238497442191
2
Iteration 5600: Loss = -10979.228309281485
Iteration 5700: Loss = -10975.52125683542
Iteration 5800: Loss = -10975.515247135487
Iteration 5900: Loss = -10975.514241221477
Iteration 6000: Loss = -10975.51540866989
1
Iteration 6100: Loss = -10975.478236858507
Iteration 6200: Loss = -10975.477716638105
Iteration 6300: Loss = -10975.378331508235
Iteration 6400: Loss = -10975.353016782134
Iteration 6500: Loss = -10975.351807581004
Iteration 6600: Loss = -10975.35138059508
Iteration 6700: Loss = -10975.350962803994
Iteration 6800: Loss = -10975.356439711
1
Iteration 6900: Loss = -10975.138832225102
Iteration 7000: Loss = -10975.13459345031
Iteration 7100: Loss = -10975.134382289496
Iteration 7200: Loss = -10975.133372822242
Iteration 7300: Loss = -10975.131210430984
Iteration 7400: Loss = -10975.130862590999
Iteration 7500: Loss = -10975.130308558972
Iteration 7600: Loss = -10975.125252993543
Iteration 7700: Loss = -10975.084561680496
Iteration 7800: Loss = -10975.084233090793
Iteration 7900: Loss = -10975.081658393598
Iteration 8000: Loss = -10975.08215661432
1
Iteration 8100: Loss = -10975.086795365025
2
Iteration 8200: Loss = -10975.080392254245
Iteration 8300: Loss = -10975.08210530901
1
Iteration 8400: Loss = -10975.082328358574
2
Iteration 8500: Loss = -10975.07688324418
Iteration 8600: Loss = -10975.076070064017
Iteration 8700: Loss = -10975.076552713046
1
Iteration 8800: Loss = -10975.075836624332
Iteration 8900: Loss = -10975.075826165632
Iteration 9000: Loss = -10975.075736131355
Iteration 9100: Loss = -10975.075721118052
Iteration 9200: Loss = -10970.317392871055
Iteration 9300: Loss = -10970.31922459282
1
Iteration 9400: Loss = -10970.315172178152
Iteration 9500: Loss = -10970.30916853763
Iteration 9600: Loss = -10970.30922684556
1
Iteration 9700: Loss = -10970.308249169839
Iteration 9800: Loss = -10970.335201066582
1
Iteration 9900: Loss = -10970.3071866193
Iteration 10000: Loss = -10970.308760547015
1
Iteration 10100: Loss = -10970.29667852671
Iteration 10200: Loss = -10970.299184139214
1
Iteration 10300: Loss = -10970.328364715207
2
Iteration 10400: Loss = -10970.304241581098
3
Iteration 10500: Loss = -10968.00146245017
Iteration 10600: Loss = -10967.982448174993
Iteration 10700: Loss = -10968.031433614766
1
Iteration 10800: Loss = -10967.975774326105
Iteration 10900: Loss = -10967.96422184941
Iteration 11000: Loss = -10967.964363844045
1
Iteration 11100: Loss = -10967.965152141076
2
Iteration 11200: Loss = -10968.1230828323
3
Iteration 11300: Loss = -10967.966419921484
4
Iteration 11400: Loss = -10967.965916029578
5
Stopping early at iteration 11400 due to no improvement.
pi: tensor([[0.7440, 0.2560],
        [0.2947, 0.7053]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5297, 0.4703], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2014, 0.0925],
         [0.5357, 0.2949]],

        [[0.7220, 0.0992],
         [0.5532, 0.5069]],

        [[0.6969, 0.0877],
         [0.5941, 0.6045]],

        [[0.6342, 0.1043],
         [0.7232, 0.5636]],

        [[0.6468, 0.0971],
         [0.5078, 0.5666]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.772104805341358
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.9137624860793706
Average Adjusted Rand Index: 0.9150656035704625
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24811.12952622011
Iteration 100: Loss = -11176.90091671181
Iteration 200: Loss = -11175.997078325483
Iteration 300: Loss = -11175.876073714948
Iteration 400: Loss = -11175.820535396953
Iteration 500: Loss = -11175.789212097801
Iteration 600: Loss = -11175.769446412543
Iteration 700: Loss = -11175.756065616184
Iteration 800: Loss = -11175.746494395496
Iteration 900: Loss = -11175.739381110836
Iteration 1000: Loss = -11175.733874705262
Iteration 1100: Loss = -11175.72958078668
Iteration 1200: Loss = -11175.726058927034
Iteration 1300: Loss = -11175.723105181996
Iteration 1400: Loss = -11175.720599400254
Iteration 1500: Loss = -11175.718392212746
Iteration 1600: Loss = -11175.716370476375
Iteration 1700: Loss = -11175.714467567332
Iteration 1800: Loss = -11175.712536641844
Iteration 1900: Loss = -11175.71059021086
Iteration 2000: Loss = -11175.708334616333
Iteration 2100: Loss = -11175.705555706922
Iteration 2200: Loss = -11175.701702901282
Iteration 2300: Loss = -11175.695548333892
Iteration 2400: Loss = -11175.68331588402
Iteration 2500: Loss = -11175.64868675565
Iteration 2600: Loss = -11175.483099947973
Iteration 2700: Loss = -11175.247015816738
Iteration 2800: Loss = -11175.111885886334
Iteration 2900: Loss = -11175.034925910617
Iteration 3000: Loss = -11175.004411201238
Iteration 3100: Loss = -11174.983446014914
Iteration 3200: Loss = -11174.961884464918
Iteration 3300: Loss = -11174.939650935097
Iteration 3400: Loss = -11174.90761790904
Iteration 3500: Loss = -11174.876751769078
Iteration 3600: Loss = -11174.80225575673
Iteration 3700: Loss = -11174.744645276678
Iteration 3800: Loss = -11174.740838571634
Iteration 3900: Loss = -11174.739642172335
Iteration 4000: Loss = -11174.738907617273
Iteration 4100: Loss = -11174.738448767052
Iteration 4200: Loss = -11174.737472961167
Iteration 4300: Loss = -11174.735914897716
Iteration 4400: Loss = -11174.72318404886
Iteration 4500: Loss = -11174.503682068234
Iteration 4600: Loss = -11174.497746168618
Iteration 4700: Loss = -11174.497508375964
Iteration 4800: Loss = -11174.497122750861
Iteration 4900: Loss = -11174.496935401052
Iteration 5000: Loss = -11174.497628740202
1
Iteration 5100: Loss = -11174.496660837316
Iteration 5200: Loss = -11174.496520883707
Iteration 5300: Loss = -11174.49643363097
Iteration 5400: Loss = -11174.496789580711
1
Iteration 5500: Loss = -11174.49622458868
Iteration 5600: Loss = -11174.507067974215
1
Iteration 5700: Loss = -11174.496072646769
Iteration 5800: Loss = -11174.496042047898
Iteration 5900: Loss = -11174.49599444675
Iteration 6000: Loss = -11174.495918187244
Iteration 6100: Loss = -11174.497433824972
1
Iteration 6200: Loss = -11174.49583887683
Iteration 6300: Loss = -11174.49626392908
1
Iteration 6400: Loss = -11174.495784669423
Iteration 6500: Loss = -11174.49570987396
Iteration 6600: Loss = -11174.495715544526
1
Iteration 6700: Loss = -11174.495601477685
Iteration 6800: Loss = -11174.497262483157
1
Iteration 6900: Loss = -11174.495564025505
Iteration 7000: Loss = -11174.501752869455
1
Iteration 7100: Loss = -11174.495493044333
Iteration 7200: Loss = -11174.495492721764
Iteration 7300: Loss = -11174.495621292454
1
Iteration 7400: Loss = -11174.495465559798
Iteration 7500: Loss = -11174.497418450233
1
Iteration 7600: Loss = -11174.495420417272
Iteration 7700: Loss = -11174.500211434424
1
Iteration 7800: Loss = -11174.502224627668
2
Iteration 7900: Loss = -11174.495359323297
Iteration 8000: Loss = -11174.49557367427
1
Iteration 8100: Loss = -11174.49534899786
Iteration 8200: Loss = -11174.508609617871
1
Iteration 8300: Loss = -11174.496445516801
2
Iteration 8400: Loss = -11174.497544042852
3
Iteration 8500: Loss = -11174.532063164052
4
Iteration 8600: Loss = -11174.585337614379
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[3.0964e-06, 1.0000e+00],
        [2.8931e-02, 9.7107e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4809, 0.5191], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3009, 0.0925],
         [0.6725, 0.1746]],

        [[0.7199, 0.1154],
         [0.6461, 0.6200]],

        [[0.5064, 0.1266],
         [0.5042, 0.6998]],

        [[0.5980, 0.0884],
         [0.5368, 0.6633]],

        [[0.6498, 0.0946],
         [0.6020, 0.6350]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: -0.004226893610597426
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.05939758540011772
Average Adjusted Rand Index: 0.18229138171116924
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21067.245392809742
Iteration 100: Loss = -11233.081115963452
Iteration 200: Loss = -11231.96022280258
Iteration 300: Loss = -11231.405739978927
Iteration 400: Loss = -11231.077743838461
Iteration 500: Loss = -11230.881042431292
Iteration 600: Loss = -11230.744257872217
Iteration 700: Loss = -11230.625984865874
Iteration 800: Loss = -11230.506315861221
Iteration 900: Loss = -11230.365136705534
Iteration 1000: Loss = -11230.15485236284
Iteration 1100: Loss = -11229.869611110296
Iteration 1200: Loss = -11229.688483324999
Iteration 1300: Loss = -11229.58705074547
Iteration 1400: Loss = -11229.50316885347
Iteration 1500: Loss = -11229.389629728066
Iteration 1600: Loss = -11229.239893831878
Iteration 1700: Loss = -11211.492942773853
Iteration 1800: Loss = -11074.312308178116
Iteration 1900: Loss = -11061.24088709688
Iteration 2000: Loss = -11054.319196900165
Iteration 2100: Loss = -11054.155414228284
Iteration 2200: Loss = -11049.719427384742
Iteration 2300: Loss = -11049.522515567922
Iteration 2400: Loss = -11049.447169249313
Iteration 2500: Loss = -11048.150449480909
Iteration 2600: Loss = -11047.507739980514
Iteration 2700: Loss = -11045.974017331278
Iteration 2800: Loss = -11032.683392443612
Iteration 2900: Loss = -11023.870602942681
Iteration 3000: Loss = -11023.796442072906
Iteration 3100: Loss = -11023.778508627935
Iteration 3200: Loss = -11023.768534885528
Iteration 3300: Loss = -11023.747178794285
Iteration 3400: Loss = -11022.203048707373
Iteration 3500: Loss = -11022.182440946652
Iteration 3600: Loss = -11022.176919955537
Iteration 3700: Loss = -11022.17292801042
Iteration 3800: Loss = -11022.17604451444
1
Iteration 3900: Loss = -11022.165898396135
Iteration 4000: Loss = -11022.162879786685
Iteration 4100: Loss = -11022.147691934444
Iteration 4200: Loss = -11022.143573219451
Iteration 4300: Loss = -11022.142704731372
Iteration 4400: Loss = -11022.142297455393
Iteration 4500: Loss = -11022.141200760878
Iteration 4600: Loss = -11022.140825235845
Iteration 4700: Loss = -11022.140603065054
Iteration 4800: Loss = -11022.145488104206
1
Iteration 4900: Loss = -11022.13943703693
Iteration 5000: Loss = -11022.13428963624
Iteration 5100: Loss = -11022.133699070828
Iteration 5200: Loss = -11022.133570975457
Iteration 5300: Loss = -11022.148032153395
1
Iteration 5400: Loss = -11022.13218310872
Iteration 5500: Loss = -11022.131477590436
Iteration 5600: Loss = -11022.131042922401
Iteration 5700: Loss = -11022.132009502331
1
Iteration 5800: Loss = -11022.133469485092
2
Iteration 5900: Loss = -11022.136778914908
3
Iteration 6000: Loss = -11022.13030850763
Iteration 6100: Loss = -11022.130173174124
Iteration 6200: Loss = -11022.130486899756
1
Iteration 6300: Loss = -11022.138107754916
2
Iteration 6400: Loss = -11022.130573579556
3
Iteration 6500: Loss = -11022.13539576244
4
Iteration 6600: Loss = -11022.136786788697
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.6615, 0.3385],
        [0.3711, 0.6289]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6669, 0.3331], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2092, 0.0944],
         [0.5674, 0.2867]],

        [[0.7014, 0.0976],
         [0.5829, 0.5757]],

        [[0.6330, 0.0876],
         [0.6216, 0.5999]],

        [[0.5348, 0.1041],
         [0.7054, 0.5219]],

        [[0.7303, 0.0968],
         [0.6026, 0.7059]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 19
Adjusted Rand Index: 0.37874408616040434
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.4135311449953944
Average Adjusted Rand Index: 0.8138468925910864
10992.22515368903
[0.05380871531966369, 0.9137624860793706, 0.05939758540011772, 0.4135311449953944] [0.8261949517439652, 0.9150656035704625, 0.18229138171116924, 0.8138468925910864] [10999.291972008261, 10967.965916029578, 11174.585337614379, 11022.136786788697]
-----------------------------------------------------------------------------------------
This iteration is 13
True Objective function: Loss = -11303.95387455235
pi: tensor([[4.8700e-26, 1.0000e+00],
        [3.2904e-24, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([2.6323e-24, 1.0000e+00])
beta: tensor([[[0.0000, 0.2843],
         [0.1330, 0.1764]],

        [[0.6477, 0.2792],
         [0.9500, 0.7524]],

        [[0.4046, 0.3088],
         [0.2085, 0.2524]],

        [[0.3930, 0.3000],
         [0.3141, 0.2592]],

        [[0.4045, 0.2971],
         [0.5948, 0.5939]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -11604.16349199279
Iteration 10: Loss = -11587.082342697531
Iteration 20: Loss = -11585.540243683383
Iteration 30: Loss = -11586.773936673007
1
Iteration 40: Loss = -11556.389154440392
Iteration 50: Loss = -11291.658519347056
Iteration 60: Loss = -11291.639113574358
Iteration 70: Loss = -11291.639205722498
1
Iteration 80: Loss = -11291.639186554676
2
Iteration 90: Loss = -11291.639186554079
3
Stopping early at iteration 90 due to no improvement.
pi: tensor([[0.7199, 0.2801],
        [0.2305, 0.7695]], dtype=torch.float64)
alpha: tensor([0.4680, 0.5320])
beta: tensor([[[0.1969, 0.0915],
         [0.7880, 0.2901]],

        [[0.2866, 0.0965],
         [0.9539, 0.9155]],

        [[0.5431, 0.1064],
         [0.1679, 0.6492]],

        [[0.4699, 0.1137],
         [0.5617, 0.2865]],

        [[0.0067, 0.1025],
         [0.9902, 0.6684]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9446731811099929
Average Adjusted Rand Index: 0.9446436586584147
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20327.30940849542
Iteration 100: Loss = -11598.638459540332
Iteration 200: Loss = -11597.424021655515
Iteration 300: Loss = -11593.522376002504
Iteration 400: Loss = -11589.14472674414
Iteration 500: Loss = -11581.244670732714
Iteration 600: Loss = -11458.007115358865
Iteration 700: Loss = -11399.051374248016
Iteration 800: Loss = -11386.149683783897
Iteration 900: Loss = -11383.7809867846
Iteration 1000: Loss = -11383.34832567404
Iteration 1100: Loss = -11382.506858311132
Iteration 1200: Loss = -11382.306603977608
Iteration 1300: Loss = -11382.053977844753
Iteration 1400: Loss = -11326.371678476298
Iteration 1500: Loss = -11302.548293327209
Iteration 1600: Loss = -11302.429342540665
Iteration 1700: Loss = -11290.718238240179
Iteration 1800: Loss = -11290.674433490227
Iteration 1900: Loss = -11290.641262730598
Iteration 2000: Loss = -11288.477456440316
Iteration 2100: Loss = -11288.44849627498
Iteration 2200: Loss = -11288.365461399106
Iteration 2300: Loss = -11288.356070992024
Iteration 2400: Loss = -11288.315713508124
Iteration 2500: Loss = -11287.892443820674
Iteration 2600: Loss = -11287.887354194763
Iteration 2700: Loss = -11287.883228846813
Iteration 2800: Loss = -11287.878991135114
Iteration 2900: Loss = -11287.862756662247
Iteration 3000: Loss = -11287.694064863814
Iteration 3100: Loss = -11287.690382271538
Iteration 3200: Loss = -11287.688446699638
Iteration 3300: Loss = -11287.686708753812
Iteration 3400: Loss = -11287.684357016924
Iteration 3500: Loss = -11287.658006010357
Iteration 3600: Loss = -11287.648463067886
Iteration 3700: Loss = -11287.647383909038
Iteration 3800: Loss = -11287.646377241676
Iteration 3900: Loss = -11287.64549360042
Iteration 4000: Loss = -11287.644440233675
Iteration 4100: Loss = -11287.643465518408
Iteration 4200: Loss = -11287.641755414948
Iteration 4300: Loss = -11287.636148542484
Iteration 4400: Loss = -11287.635740389105
Iteration 4500: Loss = -11287.634849390799
Iteration 4600: Loss = -11287.633901445726
Iteration 4700: Loss = -11287.642040190363
1
Iteration 4800: Loss = -11287.631881356054
Iteration 4900: Loss = -11287.631051705477
Iteration 5000: Loss = -11287.63036293707
Iteration 5100: Loss = -11287.629872770827
Iteration 5200: Loss = -11287.629450877805
Iteration 5300: Loss = -11287.629761699562
1
Iteration 5400: Loss = -11287.628929722316
Iteration 5500: Loss = -11287.630306880435
1
Iteration 5600: Loss = -11287.628995754912
2
Iteration 5700: Loss = -11287.628191863188
Iteration 5800: Loss = -11287.627680405518
Iteration 5900: Loss = -11287.621257380022
Iteration 6000: Loss = -11287.618746319658
Iteration 6100: Loss = -11287.65122900684
1
Iteration 6200: Loss = -11287.618338834061
Iteration 6300: Loss = -11287.620610464382
1
Iteration 6400: Loss = -11287.597825133069
Iteration 6500: Loss = -11287.621529992315
1
Iteration 6600: Loss = -11287.597376582384
Iteration 6700: Loss = -11287.597272330668
Iteration 6800: Loss = -11287.604850792351
1
Iteration 6900: Loss = -11287.597140974522
Iteration 7000: Loss = -11287.597055384153
Iteration 7100: Loss = -11287.597325183131
1
Iteration 7200: Loss = -11287.596951568594
Iteration 7300: Loss = -11287.596885156605
Iteration 7400: Loss = -11287.59760340741
1
Iteration 7500: Loss = -11287.596760559467
Iteration 7600: Loss = -11287.596714645877
Iteration 7700: Loss = -11287.596699321124
Iteration 7800: Loss = -11287.596538294387
Iteration 7900: Loss = -11287.595955052651
Iteration 8000: Loss = -11287.636822319626
1
Iteration 8100: Loss = -11287.594855800144
Iteration 8200: Loss = -11287.595758763493
1
Iteration 8300: Loss = -11287.589608830725
Iteration 8400: Loss = -11287.5899571431
1
Iteration 8500: Loss = -11287.589458458577
Iteration 8600: Loss = -11287.592983458531
1
Iteration 8700: Loss = -11287.589340030774
Iteration 8800: Loss = -11287.589992961739
1
Iteration 8900: Loss = -11287.58906835946
Iteration 9000: Loss = -11287.588963489361
Iteration 9100: Loss = -11287.584210341167
Iteration 9200: Loss = -11287.585106064309
1
Iteration 9300: Loss = -11287.584068182578
Iteration 9400: Loss = -11287.60306188476
1
Iteration 9500: Loss = -11287.58399557196
Iteration 9600: Loss = -11287.646888596078
1
Iteration 9700: Loss = -11287.583115753945
Iteration 9800: Loss = -11287.637804207083
1
Iteration 9900: Loss = -11287.583450598746
2
Iteration 10000: Loss = -11287.584538388932
3
Iteration 10100: Loss = -11287.591456203032
4
Iteration 10200: Loss = -11287.587802831822
5
Stopping early at iteration 10200 due to no improvement.
pi: tensor([[0.7775, 0.2225],
        [0.2685, 0.7315]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4896, 0.5104], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2965, 0.0918],
         [0.7209, 0.2011]],

        [[0.6587, 0.0976],
         [0.6622, 0.6679]],

        [[0.5732, 0.1061],
         [0.6852, 0.7174]],

        [[0.6590, 0.1126],
         [0.6063, 0.5267]],

        [[0.7302, 0.1024],
         [0.5829, 0.5032]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9603204221731351
Average Adjusted Rand Index: 0.9603199753100565
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24183.499668636727
Iteration 100: Loss = -11596.668152631235
Iteration 200: Loss = -11593.50887717188
Iteration 300: Loss = -11590.280189940002
Iteration 400: Loss = -11588.829999719725
Iteration 500: Loss = -11585.63868210698
Iteration 600: Loss = -11367.105727619168
Iteration 700: Loss = -11353.285001947672
Iteration 800: Loss = -11348.144376702005
Iteration 900: Loss = -11347.292270872269
Iteration 1000: Loss = -11347.23508015126
Iteration 1100: Loss = -11347.21226686938
Iteration 1200: Loss = -11347.177578778312
Iteration 1300: Loss = -11347.14058776587
Iteration 1400: Loss = -11347.130718071947
Iteration 1500: Loss = -11347.124627094032
Iteration 1600: Loss = -11347.119746961074
Iteration 1700: Loss = -11347.115913221256
Iteration 1800: Loss = -11347.112706602482
Iteration 1900: Loss = -11347.109629535971
Iteration 2000: Loss = -11347.106401774654
Iteration 2100: Loss = -11347.101526178565
Iteration 2200: Loss = -11347.097429370484
Iteration 2300: Loss = -11347.094878651835
Iteration 2400: Loss = -11347.102477441915
1
Iteration 2500: Loss = -11347.092284093138
Iteration 2600: Loss = -11347.091333726577
Iteration 2700: Loss = -11347.090367457324
Iteration 2800: Loss = -11347.08961331896
Iteration 2900: Loss = -11347.088874938618
Iteration 3000: Loss = -11347.088158572837
Iteration 3100: Loss = -11347.087560266273
Iteration 3200: Loss = -11347.087143155572
Iteration 3300: Loss = -11347.086665727888
Iteration 3400: Loss = -11347.089396808284
1
Iteration 3500: Loss = -11347.085973523826
Iteration 3600: Loss = -11347.085669169044
Iteration 3700: Loss = -11347.085378050539
Iteration 3800: Loss = -11347.085078661614
Iteration 3900: Loss = -11347.085027019075
Iteration 4000: Loss = -11347.084510093202
Iteration 4100: Loss = -11347.088254312795
1
Iteration 4200: Loss = -11347.083907165994
Iteration 4300: Loss = -11347.083448242225
Iteration 4400: Loss = -11347.082948009225
Iteration 4500: Loss = -11347.082188944634
Iteration 4600: Loss = -11347.082004555692
Iteration 4700: Loss = -11347.081887964205
Iteration 4800: Loss = -11347.081747099157
Iteration 4900: Loss = -11347.082647810577
1
Iteration 5000: Loss = -11347.081485319182
Iteration 5100: Loss = -11347.08276647073
1
Iteration 5200: Loss = -11347.08116193225
Iteration 5300: Loss = -11347.08103605108
Iteration 5400: Loss = -11347.089527593054
1
Iteration 5500: Loss = -11347.08013294523
Iteration 5600: Loss = -11347.083504152239
1
Iteration 5700: Loss = -11347.079676460608
Iteration 5800: Loss = -11347.081167180477
1
Iteration 5900: Loss = -11347.07867027183
Iteration 6000: Loss = -11347.079233338523
1
Iteration 6100: Loss = -11347.078615919709
Iteration 6200: Loss = -11347.077971737906
Iteration 6300: Loss = -11347.116362366804
1
Iteration 6400: Loss = -11347.07784407643
Iteration 6500: Loss = -11347.08376423856
1
Iteration 6600: Loss = -11347.077732203841
Iteration 6700: Loss = -11347.078647748356
1
Iteration 6800: Loss = -11347.087410447084
2
Iteration 6900: Loss = -11347.076677260138
Iteration 7000: Loss = -11347.077453775963
1
Iteration 7100: Loss = -11347.07649406843
Iteration 7200: Loss = -11347.076481902322
Iteration 7300: Loss = -11347.077169536446
1
Iteration 7400: Loss = -11347.07706023893
2
Iteration 7500: Loss = -11347.077746310055
3
Iteration 7600: Loss = -11347.077454836426
4
Iteration 7700: Loss = -11347.07621540667
Iteration 7800: Loss = -11347.07823332896
1
Iteration 7900: Loss = -11347.077014730326
2
Iteration 8000: Loss = -11347.075976601905
Iteration 8100: Loss = -11347.076199612393
1
Iteration 8200: Loss = -11347.076053073886
2
Iteration 8300: Loss = -11347.075904846135
Iteration 8400: Loss = -11347.109168034107
1
Iteration 8500: Loss = -11347.075912028595
2
Iteration 8600: Loss = -11347.075865237015
Iteration 8700: Loss = -11347.075839776007
Iteration 8800: Loss = -11347.075727656224
Iteration 8900: Loss = -11347.076202686392
1
Iteration 9000: Loss = -11347.101059182485
2
Iteration 9100: Loss = -11347.075707535405
Iteration 9200: Loss = -11347.093025375294
1
Iteration 9300: Loss = -11347.073998489152
Iteration 9400: Loss = -11347.07597802619
1
Iteration 9500: Loss = -11347.073957646466
Iteration 9600: Loss = -11347.074676333095
1
Iteration 9700: Loss = -11347.073956522816
Iteration 9800: Loss = -11347.075332669316
1
Iteration 9900: Loss = -11347.0739297089
Iteration 10000: Loss = -11347.087753728052
1
Iteration 10100: Loss = -11347.073925505427
Iteration 10200: Loss = -11347.07409022297
1
Iteration 10300: Loss = -11347.076331465509
2
Iteration 10400: Loss = -11347.07390462359
Iteration 10500: Loss = -11347.076852267663
1
Iteration 10600: Loss = -11347.083841708258
2
Iteration 10700: Loss = -11347.073921873809
3
Iteration 10800: Loss = -11347.073999408274
4
Iteration 10900: Loss = -11347.073941929708
5
Stopping early at iteration 10900 due to no improvement.
pi: tensor([[0.6341, 0.3659],
        [0.3623, 0.6377]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5104, 0.4896], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2133, 0.0911],
         [0.6219, 0.2912]],

        [[0.7040, 0.0964],
         [0.6729, 0.5612]],

        [[0.6873, 0.1050],
         [0.5487, 0.5602]],

        [[0.6299, 0.1136],
         [0.5029, 0.6420]],

        [[0.6972, 0.1000],
         [0.6078, 0.5488]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 10
Adjusted Rand Index: 0.6365003576249006
Global Adjusted Rand Index: 0.388155296335971
Average Adjusted Rand Index: 0.8954592618433403
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19589.568476048044
Iteration 100: Loss = -11597.438614197648
Iteration 200: Loss = -11595.493808462827
Iteration 300: Loss = -11594.843106087106
Iteration 400: Loss = -11592.542799489234
Iteration 500: Loss = -11573.96439118093
Iteration 600: Loss = -11567.158262797697
Iteration 700: Loss = -11532.735271321952
Iteration 800: Loss = -11437.508748473912
Iteration 900: Loss = -11389.208602886803
Iteration 1000: Loss = -11381.29216787478
Iteration 1100: Loss = -11381.036227173936
Iteration 1200: Loss = -11380.875807647206
Iteration 1300: Loss = -11380.837326894005
Iteration 1400: Loss = -11380.816950581573
Iteration 1500: Loss = -11380.800706709862
Iteration 1600: Loss = -11380.78759250491
Iteration 1700: Loss = -11380.769822392012
Iteration 1800: Loss = -11380.76216290699
Iteration 1900: Loss = -11380.754503468035
Iteration 2000: Loss = -11380.749537813843
Iteration 2100: Loss = -11380.745023858637
Iteration 2200: Loss = -11380.739884296487
Iteration 2300: Loss = -11380.730892723821
Iteration 2400: Loss = -11380.712661317251
Iteration 2500: Loss = -11380.704861854567
Iteration 2600: Loss = -11380.7010001656
Iteration 2700: Loss = -11380.694586971502
Iteration 2800: Loss = -11380.690466221982
Iteration 2900: Loss = -11380.688922948324
Iteration 3000: Loss = -11380.686997109926
Iteration 3100: Loss = -11380.685194083966
Iteration 3200: Loss = -11380.682589159193
Iteration 3300: Loss = -11380.681289851324
Iteration 3400: Loss = -11380.679777212332
Iteration 3500: Loss = -11380.65769533919
Iteration 3600: Loss = -11380.656549345205
Iteration 3700: Loss = -11380.637603441166
Iteration 3800: Loss = -11380.595262356957
Iteration 3900: Loss = -11327.079079814994
Iteration 4000: Loss = -11327.017871261687
Iteration 4100: Loss = -11327.000008737728
Iteration 4200: Loss = -11326.99639328023
Iteration 4300: Loss = -11321.4014717518
Iteration 4400: Loss = -11321.364268891977
Iteration 4500: Loss = -11321.357082157683
Iteration 4600: Loss = -11312.97715220948
Iteration 4700: Loss = -11312.953752318303
Iteration 4800: Loss = -11312.95254230372
Iteration 4900: Loss = -11312.952566815024
1
Iteration 5000: Loss = -11312.95781919594
2
Iteration 5100: Loss = -11312.95491531066
3
Iteration 5200: Loss = -11312.947956067035
Iteration 5300: Loss = -11312.941861744159
Iteration 5400: Loss = -11301.799463215188
Iteration 5500: Loss = -11301.700396897164
Iteration 5600: Loss = -11287.877947214858
Iteration 5700: Loss = -11287.835474999785
Iteration 5800: Loss = -11287.835041528679
Iteration 5900: Loss = -11287.834687486607
Iteration 6000: Loss = -11287.838834858081
1
Iteration 6100: Loss = -11287.821192916643
Iteration 6200: Loss = -11287.821287341205
1
Iteration 6300: Loss = -11287.820083532513
Iteration 6400: Loss = -11287.819531944708
Iteration 6500: Loss = -11287.816147617226
Iteration 6600: Loss = -11287.811490538967
Iteration 6700: Loss = -11287.811364660469
Iteration 6800: Loss = -11287.810990874907
Iteration 6900: Loss = -11287.814016098107
1
Iteration 7000: Loss = -11287.801789314204
Iteration 7100: Loss = -11287.801744979777
Iteration 7200: Loss = -11287.805303375424
1
Iteration 7300: Loss = -11287.801444593004
Iteration 7400: Loss = -11287.801326779998
Iteration 7500: Loss = -11287.801323624273
Iteration 7600: Loss = -11287.806659893353
1
Iteration 7700: Loss = -11287.804724186346
2
Iteration 7800: Loss = -11287.800343284287
Iteration 7900: Loss = -11287.799875851737
Iteration 8000: Loss = -11287.817115872855
1
Iteration 8100: Loss = -11287.79756016295
Iteration 8200: Loss = -11287.799296075786
1
Iteration 8300: Loss = -11287.678230720712
Iteration 8400: Loss = -11287.678166761403
Iteration 8500: Loss = -11287.678011296328
Iteration 8600: Loss = -11287.677618281994
Iteration 8700: Loss = -11287.677542208801
Iteration 8800: Loss = -11287.677674437005
1
Iteration 8900: Loss = -11287.677606284937
2
Iteration 9000: Loss = -11287.676834768408
Iteration 9100: Loss = -11287.694290861826
1
Iteration 9200: Loss = -11287.676717856013
Iteration 9300: Loss = -11287.676549710168
Iteration 9400: Loss = -11287.676549579188
Iteration 9500: Loss = -11287.676393944259
Iteration 9600: Loss = -11287.742775749337
1
Iteration 9700: Loss = -11287.676112998028
Iteration 9800: Loss = -11287.675680518943
Iteration 9900: Loss = -11287.698278531716
1
Iteration 10000: Loss = -11287.672195038322
Iteration 10100: Loss = -11287.74222675275
1
Iteration 10200: Loss = -11287.683955390603
2
Iteration 10300: Loss = -11287.671278177644
Iteration 10400: Loss = -11287.671631295783
1
Iteration 10500: Loss = -11287.679095820824
2
Iteration 10600: Loss = -11287.671263429404
Iteration 10700: Loss = -11287.670292948598
Iteration 10800: Loss = -11287.670129622353
Iteration 10900: Loss = -11287.673236857056
1
Iteration 11000: Loss = -11287.674625597761
2
Iteration 11100: Loss = -11287.669806386628
Iteration 11200: Loss = -11287.669592287457
Iteration 11300: Loss = -11287.696504010722
1
Iteration 11400: Loss = -11287.670380961014
2
Iteration 11500: Loss = -11287.66956372982
Iteration 11600: Loss = -11287.66966032988
1
Iteration 11700: Loss = -11287.719814191612
2
Iteration 11800: Loss = -11287.669505562317
Iteration 11900: Loss = -11287.67151735889
1
Iteration 12000: Loss = -11287.684077294094
2
Iteration 12100: Loss = -11287.666952188601
Iteration 12200: Loss = -11287.66668524463
Iteration 12300: Loss = -11287.675289144585
1
Iteration 12400: Loss = -11287.666744539163
2
Iteration 12500: Loss = -11287.688789276284
3
Iteration 12600: Loss = -11287.666944862638
4
Iteration 12700: Loss = -11287.666347486022
Iteration 12800: Loss = -11287.666394968932
1
Iteration 12900: Loss = -11287.665260037293
Iteration 13000: Loss = -11287.664787103166
Iteration 13100: Loss = -11287.664968584602
1
Iteration 13200: Loss = -11287.665050825792
2
Iteration 13300: Loss = -11287.664682222792
Iteration 13400: Loss = -11287.672079256234
1
Iteration 13500: Loss = -11287.664701648571
2
Iteration 13600: Loss = -11287.666972748739
3
Iteration 13700: Loss = -11287.696296521806
4
Iteration 13800: Loss = -11287.65821866857
Iteration 13900: Loss = -11287.65826767509
1
Iteration 14000: Loss = -11287.781057792708
2
Iteration 14100: Loss = -11287.657927035594
Iteration 14200: Loss = -11287.673121069902
1
Iteration 14300: Loss = -11287.657974893391
2
Iteration 14400: Loss = -11287.676667022142
3
Iteration 14500: Loss = -11287.66668384146
4
Iteration 14600: Loss = -11287.657981082733
5
Stopping early at iteration 14600 due to no improvement.
pi: tensor([[0.7778, 0.2222],
        [0.2679, 0.7321]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4900, 0.5100], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2965, 0.0911],
         [0.6579, 0.2011]],

        [[0.6144, 0.0976],
         [0.5080, 0.5851]],

        [[0.6507, 0.1061],
         [0.6066, 0.7034]],

        [[0.6349, 0.1126],
         [0.5578, 0.6490]],

        [[0.5629, 0.1024],
         [0.6502, 0.7195]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9603204221731351
Average Adjusted Rand Index: 0.9603199753100565
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24425.44311823846
Iteration 100: Loss = -11596.428664108855
Iteration 200: Loss = -11563.267669627105
Iteration 300: Loss = -11292.848624651871
Iteration 400: Loss = -11288.244345935329
Iteration 500: Loss = -11288.031533536187
Iteration 600: Loss = -11287.940664980568
Iteration 700: Loss = -11287.901748327693
Iteration 800: Loss = -11287.861342184584
Iteration 900: Loss = -11287.844060897385
Iteration 1000: Loss = -11287.832295715622
Iteration 1100: Loss = -11287.823334353205
Iteration 1200: Loss = -11287.813907120744
Iteration 1300: Loss = -11287.808580875684
Iteration 1400: Loss = -11287.805207261765
Iteration 1500: Loss = -11287.802568408953
Iteration 1600: Loss = -11287.800416516791
Iteration 1700: Loss = -11287.79859063756
Iteration 1800: Loss = -11287.796899721889
Iteration 1900: Loss = -11287.794834450018
Iteration 2000: Loss = -11287.783657114509
Iteration 2100: Loss = -11287.77984033928
Iteration 2200: Loss = -11287.77840522477
Iteration 2300: Loss = -11287.777413243804
Iteration 2400: Loss = -11287.776467574668
Iteration 2500: Loss = -11287.774482243492
Iteration 2600: Loss = -11287.77068560165
Iteration 2700: Loss = -11287.769060784347
Iteration 2800: Loss = -11287.768190715511
Iteration 2900: Loss = -11287.767673676943
Iteration 3000: Loss = -11287.767108195743
Iteration 3100: Loss = -11287.766358141174
Iteration 3200: Loss = -11287.764635805352
Iteration 3300: Loss = -11287.647233412546
Iteration 3400: Loss = -11287.646730067787
Iteration 3500: Loss = -11287.646466238491
Iteration 3600: Loss = -11287.6462177698
Iteration 3700: Loss = -11287.645991621512
Iteration 3800: Loss = -11287.645798212547
Iteration 3900: Loss = -11287.645625943542
Iteration 4000: Loss = -11287.645437342971
Iteration 4100: Loss = -11287.645676875076
1
Iteration 4200: Loss = -11287.646862060414
2
Iteration 4300: Loss = -11287.644828811552
Iteration 4400: Loss = -11287.643490188215
Iteration 4500: Loss = -11287.642106814126
Iteration 4600: Loss = -11287.642673964456
1
Iteration 4700: Loss = -11287.664744672855
2
Iteration 4800: Loss = -11287.641561828474
Iteration 4900: Loss = -11287.64205158782
1
Iteration 5000: Loss = -11287.641271910306
Iteration 5100: Loss = -11287.64441815062
1
Iteration 5200: Loss = -11287.641065683902
Iteration 5300: Loss = -11287.640930463662
Iteration 5400: Loss = -11287.640373384116
Iteration 5500: Loss = -11287.639626372675
Iteration 5600: Loss = -11287.639911154281
1
Iteration 5700: Loss = -11287.641584382167
2
Iteration 5800: Loss = -11287.639300589268
Iteration 5900: Loss = -11287.639471396948
1
Iteration 6000: Loss = -11287.638834342304
Iteration 6100: Loss = -11287.638664237556
Iteration 6200: Loss = -11287.638597691177
Iteration 6300: Loss = -11287.638608242618
1
Iteration 6400: Loss = -11287.63870828718
2
Iteration 6500: Loss = -11287.638568098637
Iteration 6600: Loss = -11287.638524258142
Iteration 6700: Loss = -11287.638550617812
1
Iteration 6800: Loss = -11287.639336105562
2
Iteration 6900: Loss = -11287.638446293167
Iteration 7000: Loss = -11287.638081077477
Iteration 7100: Loss = -11287.63812790728
1
Iteration 7200: Loss = -11287.64157474294
2
Iteration 7300: Loss = -11287.63829862541
3
Iteration 7400: Loss = -11287.637610461541
Iteration 7500: Loss = -11287.637844474626
1
Iteration 7600: Loss = -11287.637792136
2
Iteration 7700: Loss = -11287.63773313522
3
Iteration 7800: Loss = -11287.637478017232
Iteration 7900: Loss = -11287.63885800077
1
Iteration 8000: Loss = -11287.64279186583
2
Iteration 8100: Loss = -11287.637462503786
Iteration 8200: Loss = -11287.637672289788
1
Iteration 8300: Loss = -11287.639576660911
2
Iteration 8400: Loss = -11287.637869485252
3
Iteration 8500: Loss = -11287.637226042694
Iteration 8600: Loss = -11287.637650656307
1
Iteration 8700: Loss = -11287.637325357462
2
Iteration 8800: Loss = -11287.637172091658
Iteration 8900: Loss = -11287.637397286133
1
Iteration 9000: Loss = -11287.637161776056
Iteration 9100: Loss = -11287.637106139828
Iteration 9200: Loss = -11287.636926499703
Iteration 9300: Loss = -11287.633143463552
Iteration 9400: Loss = -11287.633851782768
1
Iteration 9500: Loss = -11287.633140230359
Iteration 9600: Loss = -11287.633126220622
Iteration 9700: Loss = -11287.633383912
1
Iteration 9800: Loss = -11287.633037634503
Iteration 9900: Loss = -11287.657983144138
1
Iteration 10000: Loss = -11287.632930514053
Iteration 10100: Loss = -11287.635420455244
1
Iteration 10200: Loss = -11287.633494535808
2
Iteration 10300: Loss = -11287.632280983682
Iteration 10400: Loss = -11287.632068162817
Iteration 10500: Loss = -11287.632907970168
1
Iteration 10600: Loss = -11287.63200761817
Iteration 10700: Loss = -11287.632065015929
1
Iteration 10800: Loss = -11287.64172589205
2
Iteration 10900: Loss = -11287.631886339714
Iteration 11000: Loss = -11287.647462066372
1
Iteration 11100: Loss = -11287.623132825214
Iteration 11200: Loss = -11287.622934978062
Iteration 11300: Loss = -11287.622722302132
Iteration 11400: Loss = -11287.579200642187
Iteration 11500: Loss = -11287.579265526518
1
Iteration 11600: Loss = -11287.650676181585
2
Iteration 11700: Loss = -11287.583363597685
3
Iteration 11800: Loss = -11287.588002689894
4
Iteration 11900: Loss = -11287.58013077873
5
Stopping early at iteration 11900 due to no improvement.
pi: tensor([[0.7316, 0.2684],
        [0.2225, 0.7775]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2011, 0.0914],
         [0.5320, 0.2965]],

        [[0.5252, 0.0976],
         [0.7150, 0.6527]],

        [[0.5946, 0.1061],
         [0.5944, 0.5100]],

        [[0.6189, 0.1126],
         [0.6061, 0.5750]],

        [[0.5861, 0.1024],
         [0.5872, 0.5474]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9603204221731351
Average Adjusted Rand Index: 0.9603199753100565
11303.95387455235
[0.9603204221731351, 0.388155296335971, 0.9603204221731351, 0.9603204221731351] [0.9603199753100565, 0.8954592618433403, 0.9603199753100565, 0.9603199753100565] [11287.587802831822, 11347.073941929708, 11287.657981082733, 11287.58013077873]
-----------------------------------------------------------------------------------------
This iteration is 14
True Objective function: Loss = -11217.62163806403
Iteration 0: Loss = -11623.393059101389
Iteration 10: Loss = -11430.701539477555
Iteration 20: Loss = -11198.84162001374
Iteration 30: Loss = -11194.699561534591
Iteration 40: Loss = -11194.413105284597
Iteration 50: Loss = -11194.289403999102
Iteration 60: Loss = -11194.2774064838
Iteration 70: Loss = -11194.278487839649
1
Iteration 80: Loss = -11194.279303089057
2
Iteration 90: Loss = -11194.279658763322
3
Stopping early at iteration 90 due to no improvement.
pi: tensor([[0.7036, 0.2964],
        [0.2829, 0.7171]], dtype=torch.float64)
alpha: tensor([0.4732, 0.5268])
beta: tensor([[[0.1942, 0.1037],
         [0.8761, 0.2815]],

        [[0.1223, 0.1036],
         [0.2081, 0.6336]],

        [[0.7641, 0.0938],
         [0.5184, 0.4411]],

        [[0.7637, 0.0952],
         [0.9175, 0.2250]],

        [[0.7331, 0.1131],
         [0.4703, 0.8413]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
Global Adjusted Rand Index: 0.9137635824668447
Average Adjusted Rand Index: 0.913612884880598
Iteration 0: Loss = -11642.766686872103
Iteration 10: Loss = -11364.279536208516
Iteration 20: Loss = -11194.863163029308
Iteration 30: Loss = -11194.347165753226
Iteration 40: Loss = -11194.27738308544
Iteration 50: Loss = -11194.276900019862
Iteration 60: Loss = -11194.278629840448
1
Iteration 70: Loss = -11194.279408134718
2
Iteration 80: Loss = -11194.279678715671
3
Stopping early at iteration 80 due to no improvement.
pi: tensor([[0.7036, 0.2964],
        [0.2829, 0.7171]], dtype=torch.float64)
alpha: tensor([0.4732, 0.5268])
beta: tensor([[[0.1942, 0.1037],
         [0.4451, 0.2815]],

        [[0.3905, 0.1036],
         [0.2524, 0.9638]],

        [[0.9261, 0.0938],
         [0.3977, 0.4446]],

        [[0.3373, 0.0952],
         [0.5426, 0.6262]],

        [[0.5688, 0.1131],
         [0.0412, 0.8904]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
Global Adjusted Rand Index: 0.9137635824668447
Average Adjusted Rand Index: 0.913612884880598
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20515.277274927324
Iteration 100: Loss = -11448.98849501761
Iteration 200: Loss = -11443.346426923928
Iteration 300: Loss = -11439.743043453334
Iteration 400: Loss = -11387.131885379511
Iteration 500: Loss = -11386.644438206904
Iteration 600: Loss = -11386.356779224967
Iteration 700: Loss = -11386.129438800113
Iteration 800: Loss = -11385.958081930767
Iteration 900: Loss = -11385.833179759096
Iteration 1000: Loss = -11385.737808198588
Iteration 1100: Loss = -11385.6630455688
Iteration 1200: Loss = -11385.602478460203
Iteration 1300: Loss = -11385.55501164926
Iteration 1400: Loss = -11385.520288189691
Iteration 1500: Loss = -11385.495704828903
Iteration 1600: Loss = -11385.478415472833
Iteration 1700: Loss = -11385.466153059011
Iteration 1800: Loss = -11385.45710364257
Iteration 1900: Loss = -11385.45017033248
Iteration 2000: Loss = -11385.444756660027
Iteration 2100: Loss = -11385.440399634228
Iteration 2200: Loss = -11385.436812542981
Iteration 2300: Loss = -11385.433788814977
Iteration 2400: Loss = -11385.431216092133
Iteration 2500: Loss = -11385.429015435411
Iteration 2600: Loss = -11385.427107735322
Iteration 2700: Loss = -11385.425433507453
Iteration 2800: Loss = -11385.423979429854
Iteration 2900: Loss = -11385.422731372691
Iteration 3000: Loss = -11385.421583049096
Iteration 3100: Loss = -11385.420551280635
Iteration 3200: Loss = -11385.419602002063
Iteration 3300: Loss = -11385.41878357571
Iteration 3400: Loss = -11385.417996840633
Iteration 3500: Loss = -11385.417287528564
Iteration 3600: Loss = -11385.416664271188
Iteration 3700: Loss = -11385.416049641643
Iteration 3800: Loss = -11385.415552973678
Iteration 3900: Loss = -11385.415032235374
Iteration 4000: Loss = -11385.414584408445
Iteration 4100: Loss = -11385.414155712931
Iteration 4200: Loss = -11385.413758244584
Iteration 4300: Loss = -11385.413401707032
Iteration 4400: Loss = -11385.413055672985
Iteration 4500: Loss = -11385.412726859253
Iteration 4600: Loss = -11385.412434261372
Iteration 4700: Loss = -11385.412157517776
Iteration 4800: Loss = -11385.411878036295
Iteration 4900: Loss = -11385.411620704444
Iteration 5000: Loss = -11385.411502953826
Iteration 5100: Loss = -11385.411215679434
Iteration 5200: Loss = -11385.413495997793
1
Iteration 5300: Loss = -11385.410838484586
Iteration 5400: Loss = -11385.410656179634
Iteration 5500: Loss = -11385.411166745946
1
Iteration 5600: Loss = -11385.410301190788
Iteration 5700: Loss = -11385.41462373825
1
Iteration 5800: Loss = -11385.41006825162
Iteration 5900: Loss = -11385.409918932899
Iteration 6000: Loss = -11385.411182615802
1
Iteration 6100: Loss = -11385.409681160812
Iteration 6200: Loss = -11385.409618221103
Iteration 6300: Loss = -11385.411372475432
1
Iteration 6400: Loss = -11385.409386206818
Iteration 6500: Loss = -11385.409454024946
1
Iteration 6600: Loss = -11385.409214867008
Iteration 6700: Loss = -11385.409523913153
1
Iteration 6800: Loss = -11385.409483331661
2
Iteration 6900: Loss = -11385.408986892797
Iteration 7000: Loss = -11385.40891619895
Iteration 7100: Loss = -11385.409222833643
1
Iteration 7200: Loss = -11385.409326174913
2
Iteration 7300: Loss = -11385.409414405136
3
Iteration 7400: Loss = -11385.418741933563
4
Iteration 7500: Loss = -11385.4086184647
Iteration 7600: Loss = -11385.40863482139
1
Iteration 7700: Loss = -11385.408543977774
Iteration 7800: Loss = -11385.408517748956
Iteration 7900: Loss = -11385.408566325665
1
Iteration 8000: Loss = -11385.408436344745
Iteration 8100: Loss = -11385.410074203288
1
Iteration 8200: Loss = -11385.409776720713
2
Iteration 8300: Loss = -11385.408283900086
Iteration 8400: Loss = -11385.408718688217
1
Iteration 8500: Loss = -11385.412924233635
2
Iteration 8600: Loss = -11385.40824156575
Iteration 8700: Loss = -11385.409194791418
1
Iteration 8800: Loss = -11385.408183582653
Iteration 8900: Loss = -11385.408116606055
Iteration 9000: Loss = -11385.408337222476
1
Iteration 9100: Loss = -11385.408078526089
Iteration 9200: Loss = -11385.408074626406
Iteration 9300: Loss = -11385.408081143449
1
Iteration 9400: Loss = -11385.40799641928
Iteration 9500: Loss = -11385.659243810245
1
Iteration 9600: Loss = -11385.408006894115
2
Iteration 9700: Loss = -11385.40796122338
Iteration 9800: Loss = -11385.411130268933
1
Iteration 9900: Loss = -11385.407940372113
Iteration 10000: Loss = -11385.40794429943
1
Iteration 10100: Loss = -11385.408052940185
2
Iteration 10200: Loss = -11385.407929882413
Iteration 10300: Loss = -11385.459650442657
1
Iteration 10400: Loss = -11385.407897014296
Iteration 10500: Loss = -11385.407853676019
Iteration 10600: Loss = -11385.413989579452
1
Iteration 10700: Loss = -11385.407894916925
2
Iteration 10800: Loss = -11385.40870422744
3
Iteration 10900: Loss = -11385.407916415345
4
Iteration 11000: Loss = -11385.49307475344
5
Stopping early at iteration 11000 due to no improvement.
pi: tensor([[8.4817e-06, 9.9999e-01],
        [8.0055e-02, 9.1994e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5516, 0.4484], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3073, 0.1046],
         [0.6292, 0.1684]],

        [[0.6163, 0.1531],
         [0.5229, 0.5737]],

        [[0.6218, 0.2099],
         [0.6461, 0.5332]],

        [[0.6627, 0.2250],
         [0.5822, 0.6824]],

        [[0.5534, 0.2150],
         [0.5197, 0.5090]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.006467401572035518
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.041563593218086364
Average Adjusted Rand Index: 0.19784440113788554
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22634.216037793463
Iteration 100: Loss = -11448.803347921837
Iteration 200: Loss = -11442.840143712841
Iteration 300: Loss = -11440.498485405265
Iteration 400: Loss = -11389.939750680634
Iteration 500: Loss = -11387.608210130838
Iteration 600: Loss = -11387.389359578912
Iteration 700: Loss = -11387.259928475562
Iteration 800: Loss = -11386.448216754676
Iteration 900: Loss = -11304.292436560345
Iteration 1000: Loss = -11204.971025209226
Iteration 1100: Loss = -11198.149356663727
Iteration 1200: Loss = -11191.505532502968
Iteration 1300: Loss = -11191.437581718863
Iteration 1400: Loss = -11191.36709939665
Iteration 1500: Loss = -11191.162296003564
Iteration 1600: Loss = -11191.141041809562
Iteration 1700: Loss = -11191.1231767525
Iteration 1800: Loss = -11191.109293421983
Iteration 1900: Loss = -11191.096957741403
Iteration 2000: Loss = -11191.082371638946
Iteration 2100: Loss = -11191.035512356024
Iteration 2200: Loss = -11191.025840211089
Iteration 2300: Loss = -11191.006378517572
Iteration 2400: Loss = -11191.003037675715
Iteration 2500: Loss = -11191.000614264882
Iteration 2600: Loss = -11190.997943425022
Iteration 2700: Loss = -11190.994913075094
Iteration 2800: Loss = -11190.988510749805
Iteration 2900: Loss = -11190.978258956384
Iteration 3000: Loss = -11190.973528696546
Iteration 3100: Loss = -11190.97987233089
1
Iteration 3200: Loss = -11190.968568048324
Iteration 3300: Loss = -11190.967317635246
Iteration 3400: Loss = -11190.965986402123
Iteration 3500: Loss = -11190.978151934545
1
Iteration 3600: Loss = -11190.962733577364
Iteration 3700: Loss = -11190.961591308207
Iteration 3800: Loss = -11190.954449572399
Iteration 3900: Loss = -11190.952785908836
Iteration 4000: Loss = -11190.95259368788
Iteration 4100: Loss = -11190.951579288592
Iteration 4200: Loss = -11190.950984674124
Iteration 4300: Loss = -11190.950316476074
Iteration 4400: Loss = -11190.949147190448
Iteration 4500: Loss = -11190.94656423129
Iteration 4600: Loss = -11190.929689025967
Iteration 4700: Loss = -11190.924788490966
Iteration 4800: Loss = -11190.932850898364
1
Iteration 4900: Loss = -11190.92124562878
Iteration 5000: Loss = -11190.945440688502
1
Iteration 5100: Loss = -11190.919831636005
Iteration 5200: Loss = -11190.919382776008
Iteration 5300: Loss = -11190.919321103507
Iteration 5400: Loss = -11190.918605862948
Iteration 5500: Loss = -11190.919930455546
1
Iteration 5600: Loss = -11190.91785432895
Iteration 5700: Loss = -11190.917374556859
Iteration 5800: Loss = -11190.917069722203
Iteration 5900: Loss = -11190.91676398299
Iteration 6000: Loss = -11190.91635413637
Iteration 6100: Loss = -11190.91588279424
Iteration 6200: Loss = -11190.916180898636
1
Iteration 6300: Loss = -11190.914485302668
Iteration 6400: Loss = -11190.913307033356
Iteration 6500: Loss = -11190.912535552774
Iteration 6600: Loss = -11190.911390208234
Iteration 6700: Loss = -11190.911143321153
Iteration 6800: Loss = -11190.911903820832
1
Iteration 6900: Loss = -11190.905504395238
Iteration 7000: Loss = -11190.903909923947
Iteration 7100: Loss = -11190.896168200903
Iteration 7200: Loss = -11190.953199666483
1
Iteration 7300: Loss = -11190.89591111835
Iteration 7400: Loss = -11190.895707594202
Iteration 7500: Loss = -11190.89473354748
Iteration 7600: Loss = -11190.894409280856
Iteration 7700: Loss = -11190.894300649747
Iteration 7800: Loss = -11190.894370977065
1
Iteration 7900: Loss = -11190.895553428176
2
Iteration 8000: Loss = -11190.894027068623
Iteration 8100: Loss = -11190.893889492105
Iteration 8200: Loss = -11190.893810109412
Iteration 8300: Loss = -11190.894001950372
1
Iteration 8400: Loss = -11190.893759313394
Iteration 8500: Loss = -11190.898732946325
1
Iteration 8600: Loss = -11190.893646851275
Iteration 8700: Loss = -11190.89363523059
Iteration 8800: Loss = -11190.894291626144
1
Iteration 8900: Loss = -11190.893599993535
Iteration 9000: Loss = -11190.893531997535
Iteration 9100: Loss = -11190.893591235728
1
Iteration 9200: Loss = -11190.893499273418
Iteration 9300: Loss = -11191.010250305375
1
Iteration 9400: Loss = -11190.893485659552
Iteration 9500: Loss = -11190.893448931723
Iteration 9600: Loss = -11190.89445286446
1
Iteration 9700: Loss = -11190.893391088486
Iteration 9800: Loss = -11190.893353672609
Iteration 9900: Loss = -11190.89346073608
1
Iteration 10000: Loss = -11190.892761802974
Iteration 10100: Loss = -11191.083560948247
1
Iteration 10200: Loss = -11190.890136156464
Iteration 10300: Loss = -11190.889919499934
Iteration 10400: Loss = -11190.890332403138
1
Iteration 10500: Loss = -11190.88973493004
Iteration 10600: Loss = -11190.88968403987
Iteration 10700: Loss = -11190.889675285322
Iteration 10800: Loss = -11190.889635241832
Iteration 10900: Loss = -11190.959973736035
1
Iteration 11000: Loss = -11190.889623020828
Iteration 11100: Loss = -11190.889560826512
Iteration 11200: Loss = -11191.100563055385
1
Iteration 11300: Loss = -11190.889471584496
Iteration 11400: Loss = -11190.889481062888
1
Iteration 11500: Loss = -11191.115265310405
2
Iteration 11600: Loss = -11190.889455540757
Iteration 11700: Loss = -11190.889420515925
Iteration 11800: Loss = -11191.094086517829
1
Iteration 11900: Loss = -11190.888253209705
Iteration 12000: Loss = -11190.888178439045
Iteration 12100: Loss = -11190.887442578041
Iteration 12200: Loss = -11190.886358818438
Iteration 12300: Loss = -11190.886124578525
Iteration 12400: Loss = -11190.892777708537
1
Iteration 12500: Loss = -11190.886100322501
Iteration 12600: Loss = -11190.886085623695
Iteration 12700: Loss = -11190.887054639632
1
Iteration 12800: Loss = -11190.886067703299
Iteration 12900: Loss = -11190.886049722456
Iteration 13000: Loss = -11190.88632863371
1
Iteration 13100: Loss = -11190.885027041011
Iteration 13200: Loss = -11190.8845117877
Iteration 13300: Loss = -11190.884738370598
1
Iteration 13400: Loss = -11190.884444512145
Iteration 13500: Loss = -11190.88448445027
1
Iteration 13600: Loss = -11190.885382855344
2
Iteration 13700: Loss = -11190.88446909948
3
Iteration 13800: Loss = -11190.884482194768
4
Iteration 13900: Loss = -11190.946265060804
5
Stopping early at iteration 13900 due to no improvement.
pi: tensor([[0.7234, 0.2766],
        [0.2693, 0.7307]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4499, 0.5501], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1978, 0.1057],
         [0.5025, 0.2881]],

        [[0.7279, 0.1043],
         [0.6190, 0.6226]],

        [[0.6154, 0.0944],
         [0.5575, 0.5659]],

        [[0.5765, 0.0956],
         [0.5375, 0.6252]],

        [[0.5462, 0.1130],
         [0.6313, 0.5770]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
Global Adjusted Rand Index: 0.921442873151975
Average Adjusted Rand Index: 0.9216117196646859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22306.486075503537
Iteration 100: Loss = -11496.343588710357
Iteration 200: Loss = -11457.402949364026
Iteration 300: Loss = -11439.135360624356
Iteration 400: Loss = -11416.5302821009
Iteration 500: Loss = -11376.624915454637
Iteration 600: Loss = -11369.742499699754
Iteration 700: Loss = -11360.941367432226
Iteration 800: Loss = -11354.312906235593
Iteration 900: Loss = -11353.046174467636
Iteration 1000: Loss = -11352.988058307163
Iteration 1100: Loss = -11352.91815903692
Iteration 1200: Loss = -11347.4157636048
Iteration 1300: Loss = -11347.390937207154
Iteration 1400: Loss = -11346.872549896112
Iteration 1500: Loss = -11345.642772162275
Iteration 1600: Loss = -11345.629491547255
Iteration 1700: Loss = -11345.61914512437
Iteration 1800: Loss = -11345.611776313886
Iteration 1900: Loss = -11345.605088208484
Iteration 2000: Loss = -11345.598148501576
Iteration 2100: Loss = -11345.588638377094
Iteration 2200: Loss = -11345.580348743599
Iteration 2300: Loss = -11345.564217724537
Iteration 2400: Loss = -11345.500819300329
Iteration 2500: Loss = -11326.697063843922
Iteration 2600: Loss = -11310.035473183216
Iteration 2700: Loss = -11292.986076557005
Iteration 2800: Loss = -11278.248295795805
Iteration 2900: Loss = -11263.487911999722
Iteration 3000: Loss = -11259.671785041552
Iteration 3100: Loss = -11247.790226624007
Iteration 3200: Loss = -11239.125437984414
Iteration 3300: Loss = -11239.114172019128
Iteration 3400: Loss = -11239.08756291025
Iteration 3500: Loss = -11239.083704736462
Iteration 3600: Loss = -11239.081225326576
Iteration 3700: Loss = -11239.042618883828
Iteration 3800: Loss = -11239.037638240465
Iteration 3900: Loss = -11239.019906953406
Iteration 4000: Loss = -11231.030982761175
Iteration 4100: Loss = -11231.027358841227
Iteration 4200: Loss = -11231.026623400785
Iteration 4300: Loss = -11231.026193196942
Iteration 4400: Loss = -11231.026456765712
1
Iteration 4500: Loss = -11231.025154301024
Iteration 4600: Loss = -11231.021556674856
Iteration 4700: Loss = -11231.004327513485
Iteration 4800: Loss = -11231.003888962905
Iteration 4900: Loss = -11231.004335243051
1
Iteration 5000: Loss = -11231.002801376799
Iteration 5100: Loss = -11231.001772682266
Iteration 5200: Loss = -11231.000696045983
Iteration 5300: Loss = -11230.999882524371
Iteration 5400: Loss = -11230.996014162227
Iteration 5500: Loss = -11230.99005057902
Iteration 5600: Loss = -11230.988261523125
Iteration 5700: Loss = -11230.988054361002
Iteration 5800: Loss = -11230.987897897428
Iteration 5900: Loss = -11230.98780632965
Iteration 6000: Loss = -11230.987730372763
Iteration 6100: Loss = -11230.987688105339
Iteration 6200: Loss = -11230.99014012735
1
Iteration 6300: Loss = -11230.987629263751
Iteration 6400: Loss = -11230.987448340555
Iteration 6500: Loss = -11230.987314151564
Iteration 6600: Loss = -11224.518429970212
Iteration 6700: Loss = -11224.496768957033
Iteration 6800: Loss = -11217.005552682796
Iteration 6900: Loss = -11217.005149048648
Iteration 7000: Loss = -11217.006758437516
1
Iteration 7100: Loss = -11217.004502009378
Iteration 7200: Loss = -11217.004441612824
Iteration 7300: Loss = -11209.346288896551
Iteration 7400: Loss = -11209.337613948059
Iteration 7500: Loss = -11209.338312660933
1
Iteration 7600: Loss = -11209.337291025011
Iteration 7700: Loss = -11209.336928336961
Iteration 7800: Loss = -11209.337458938382
1
Iteration 7900: Loss = -11209.336710270694
Iteration 8000: Loss = -11209.337191721625
1
Iteration 8100: Loss = -11209.336620589684
Iteration 8200: Loss = -11209.336612310646
Iteration 8300: Loss = -11209.336502630435
Iteration 8400: Loss = -11209.18953145311
Iteration 8500: Loss = -11209.183282409545
Iteration 8600: Loss = -11209.183258436107
Iteration 8700: Loss = -11209.187377134427
1
Iteration 8800: Loss = -11209.183197764607
Iteration 8900: Loss = -11209.18323783639
1
Iteration 9000: Loss = -11209.183930376921
2
Iteration 9100: Loss = -11209.179956498529
Iteration 9200: Loss = -11209.179823904897
Iteration 9300: Loss = -11209.179887275304
1
Iteration 9400: Loss = -11209.179749611394
Iteration 9500: Loss = -11205.606422588016
Iteration 9600: Loss = -11205.337339545993
Iteration 9700: Loss = -11205.337314180375
Iteration 9800: Loss = -11205.336882610069
Iteration 9900: Loss = -11205.336881483061
Iteration 10000: Loss = -11205.33665309596
Iteration 10100: Loss = -11205.336638976243
Iteration 10200: Loss = -11205.33669848883
1
Iteration 10300: Loss = -11205.337119731832
2
Iteration 10400: Loss = -11205.341768870787
3
Iteration 10500: Loss = -11205.336489402735
Iteration 10600: Loss = -11205.337287169805
1
Iteration 10700: Loss = -11205.33646919126
Iteration 10800: Loss = -11205.336852739812
1
Iteration 10900: Loss = -11205.336416982374
Iteration 11000: Loss = -11205.338843461006
1
Iteration 11100: Loss = -11205.336351150085
Iteration 11200: Loss = -11205.343010961045
1
Iteration 11300: Loss = -11205.33629174937
Iteration 11400: Loss = -11205.304867026602
Iteration 11500: Loss = -11205.300881396628
Iteration 11600: Loss = -11205.308950801907
1
Iteration 11700: Loss = -11205.32140655763
2
Iteration 11800: Loss = -11205.286471023976
Iteration 11900: Loss = -11199.759096869859
Iteration 12000: Loss = -11199.86403361884
1
Iteration 12100: Loss = -11199.759087734805
Iteration 12200: Loss = -11199.758962222755
Iteration 12300: Loss = -11200.076721540067
1
Iteration 12400: Loss = -11199.723117800422
Iteration 12500: Loss = -11199.722854981123
Iteration 12600: Loss = -11199.712154792403
Iteration 12700: Loss = -11199.711609152953
Iteration 12800: Loss = -11199.71280043777
1
Iteration 12900: Loss = -11199.725436752067
2
Iteration 13000: Loss = -11199.711601694018
Iteration 13100: Loss = -11199.740681779338
1
Iteration 13200: Loss = -11199.708289328291
Iteration 13300: Loss = -11199.724603902137
1
Iteration 13400: Loss = -11199.707780455587
Iteration 13500: Loss = -11200.055459779245
1
Iteration 13600: Loss = -11199.70775894288
Iteration 13700: Loss = -11199.720338974666
1
Iteration 13800: Loss = -11199.707753139015
Iteration 13900: Loss = -11199.707564595916
Iteration 14000: Loss = -11199.707909933495
1
Iteration 14100: Loss = -11199.707542728833
Iteration 14200: Loss = -11199.78036918944
1
Iteration 14300: Loss = -11199.707482496073
Iteration 14400: Loss = -11199.707380438997
Iteration 14500: Loss = -11199.717342576525
1
Iteration 14600: Loss = -11199.705440353415
Iteration 14700: Loss = -11199.704625754553
Iteration 14800: Loss = -11199.706303416171
1
Iteration 14900: Loss = -11199.704515721072
Iteration 15000: Loss = -11199.70452962899
1
Iteration 15100: Loss = -11199.783185279279
2
Iteration 15200: Loss = -11199.704451808637
Iteration 15300: Loss = -11199.704334345712
Iteration 15400: Loss = -11199.707413191945
1
Iteration 15500: Loss = -11199.703833904427
Iteration 15600: Loss = -11199.703696612709
Iteration 15700: Loss = -11199.704076194766
1
Iteration 15800: Loss = -11199.703684575217
Iteration 15900: Loss = -11198.936727370638
Iteration 16000: Loss = -11198.934997821227
Iteration 16100: Loss = -11198.966425505685
1
Iteration 16200: Loss = -11198.93384342805
Iteration 16300: Loss = -11198.944290650023
1
Iteration 16400: Loss = -11198.933852725364
2
Iteration 16500: Loss = -11199.066030844242
3
Iteration 16600: Loss = -11198.933707154994
Iteration 16700: Loss = -11198.933632394146
Iteration 16800: Loss = -11198.932640338024
Iteration 16900: Loss = -11198.932452710063
Iteration 17000: Loss = -11198.932439368138
Iteration 17100: Loss = -11190.909609792092
Iteration 17200: Loss = -11190.905535173237
Iteration 17300: Loss = -11190.90554139788
1
Iteration 17400: Loss = -11190.93384362628
2
Iteration 17500: Loss = -11190.905524258487
Iteration 17600: Loss = -11190.925087625415
1
Iteration 17700: Loss = -11190.905372606489
Iteration 17800: Loss = -11190.905642843572
1
Iteration 17900: Loss = -11190.905355484061
Iteration 18000: Loss = -11190.905574284656
1
Iteration 18100: Loss = -11190.905277113749
Iteration 18200: Loss = -11190.91111496822
1
Iteration 18300: Loss = -11190.90529049309
2
Iteration 18400: Loss = -11190.905189341536
Iteration 18500: Loss = -11190.90541069171
1
Iteration 18600: Loss = -11190.905193215833
2
Iteration 18700: Loss = -11190.905832615055
3
Iteration 18800: Loss = -11190.905147319043
Iteration 18900: Loss = -11190.905019474825
Iteration 19000: Loss = -11190.915573764005
1
Iteration 19100: Loss = -11190.904285087861
Iteration 19200: Loss = -11190.907791126643
1
Iteration 19300: Loss = -11190.90425946556
Iteration 19400: Loss = -11190.954214206327
1
Iteration 19500: Loss = -11190.90408999182
Iteration 19600: Loss = -11190.904992743175
1
Iteration 19700: Loss = -11190.903871839297
Iteration 19800: Loss = -11190.92328492287
1
Iteration 19900: Loss = -11190.903858593689
pi: tensor([[0.7229, 0.2771],
        [0.2660, 0.7340]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4477, 0.5523], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1982, 0.1047],
         [0.6767, 0.2876]],

        [[0.5585, 0.1037],
         [0.5901, 0.6451]],

        [[0.5781, 0.0940],
         [0.7023, 0.5848]],

        [[0.6844, 0.0951],
         [0.5069, 0.6863]],

        [[0.5822, 0.1128],
         [0.6613, 0.5637]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
Global Adjusted Rand Index: 0.921442873151975
Average Adjusted Rand Index: 0.9216117196646859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20934.740214120604
Iteration 100: Loss = -11444.66496973295
Iteration 200: Loss = -11428.90392021301
Iteration 300: Loss = -11385.802173434862
Iteration 400: Loss = -11354.144359487913
Iteration 500: Loss = -11203.915457914036
Iteration 600: Loss = -11192.11348082671
Iteration 700: Loss = -11191.651996610608
Iteration 800: Loss = -11191.511871766561
Iteration 900: Loss = -11191.455822494492
Iteration 1000: Loss = -11191.248639331912
Iteration 1100: Loss = -11191.221974571718
Iteration 1200: Loss = -11191.204057850844
Iteration 1300: Loss = -11191.18340066444
Iteration 1400: Loss = -11191.174730187144
Iteration 1500: Loss = -11191.16767415427
Iteration 1600: Loss = -11191.161983390006
Iteration 1700: Loss = -11191.157054393114
Iteration 1800: Loss = -11191.156363120717
Iteration 1900: Loss = -11191.149115251796
Iteration 2000: Loss = -11191.145505820215
Iteration 2100: Loss = -11191.140697292525
Iteration 2200: Loss = -11191.117265276609
Iteration 2300: Loss = -11191.082672087008
Iteration 2400: Loss = -11191.079733245848
Iteration 2500: Loss = -11191.077317801688
Iteration 2600: Loss = -11191.074457022167
Iteration 2700: Loss = -11191.066163853342
Iteration 2800: Loss = -11191.057297206227
Iteration 2900: Loss = -11191.05569277921
Iteration 3000: Loss = -11191.054289751743
Iteration 3100: Loss = -11191.04863851411
Iteration 3200: Loss = -11191.045903538026
Iteration 3300: Loss = -11191.035107052321
Iteration 3400: Loss = -11191.034024730789
Iteration 3500: Loss = -11191.033062037623
Iteration 3600: Loss = -11191.016558165229
Iteration 3700: Loss = -11191.015114886095
Iteration 3800: Loss = -11191.01466728356
Iteration 3900: Loss = -11191.015062166747
1
Iteration 4000: Loss = -11191.013907880235
Iteration 4100: Loss = -11191.013514752638
Iteration 4200: Loss = -11191.013233796644
Iteration 4300: Loss = -11191.012705323647
Iteration 4400: Loss = -11191.024796096173
1
Iteration 4500: Loss = -11191.011260289426
Iteration 4600: Loss = -11190.998990970515
Iteration 4700: Loss = -11191.004822985115
1
Iteration 4800: Loss = -11190.997446862591
Iteration 4900: Loss = -11190.995712071437
Iteration 5000: Loss = -11190.99545167965
Iteration 5100: Loss = -11190.995064285355
Iteration 5200: Loss = -11190.994762542257
Iteration 5300: Loss = -11190.9941662826
Iteration 5400: Loss = -11190.994007097142
Iteration 5500: Loss = -11190.99390482269
Iteration 5600: Loss = -11190.993831864342
Iteration 5700: Loss = -11190.993664714419
Iteration 5800: Loss = -11190.99359441919
Iteration 5900: Loss = -11190.99343541326
Iteration 6000: Loss = -11190.993269565428
Iteration 6100: Loss = -11190.992557094436
Iteration 6200: Loss = -11190.996791578365
1
Iteration 6300: Loss = -11190.992088279963
Iteration 6400: Loss = -11190.992198428146
1
Iteration 6500: Loss = -11190.982490715223
Iteration 6600: Loss = -11190.982087864517
Iteration 6700: Loss = -11190.981928477408
Iteration 6800: Loss = -11190.984381017262
1
Iteration 6900: Loss = -11190.981740627463
Iteration 7000: Loss = -11190.981621110685
Iteration 7100: Loss = -11190.981173470767
Iteration 7200: Loss = -11190.980392861207
Iteration 7300: Loss = -11190.993956577602
1
Iteration 7400: Loss = -11190.980270499398
Iteration 7500: Loss = -11190.980220609494
Iteration 7600: Loss = -11190.991353408908
1
Iteration 7700: Loss = -11190.980143082486
Iteration 7800: Loss = -11190.980056051643
Iteration 7900: Loss = -11190.979882540458
Iteration 8000: Loss = -11190.97988635887
1
Iteration 8100: Loss = -11190.979719174173
Iteration 8200: Loss = -11190.97966992059
Iteration 8300: Loss = -11190.99103588168
1
Iteration 8400: Loss = -11190.976976279428
Iteration 8500: Loss = -11190.94403733913
Iteration 8600: Loss = -11190.931975761237
Iteration 8700: Loss = -11190.926168019705
Iteration 8800: Loss = -11190.926157285492
Iteration 8900: Loss = -11190.919643216515
Iteration 9000: Loss = -11190.919703142792
1
Iteration 9100: Loss = -11190.919622085261
Iteration 9200: Loss = -11190.919555783512
Iteration 9300: Loss = -11190.918118611951
Iteration 9400: Loss = -11190.916989291914
Iteration 9500: Loss = -11190.91188168456
Iteration 9600: Loss = -11190.911804588848
Iteration 9700: Loss = -11190.911791453207
Iteration 9800: Loss = -11190.918251533134
1
Iteration 9900: Loss = -11190.911847623802
2
Iteration 10000: Loss = -11191.046953817122
3
Iteration 10100: Loss = -11190.910885907471
Iteration 10200: Loss = -11190.9749239668
1
Iteration 10300: Loss = -11190.910865615395
Iteration 10400: Loss = -11190.910848671116
Iteration 10500: Loss = -11190.910908129472
1
Iteration 10600: Loss = -11190.910850699773
2
Iteration 10700: Loss = -11190.912309579346
3
Iteration 10800: Loss = -11190.910835793586
Iteration 10900: Loss = -11190.910816037653
Iteration 11000: Loss = -11190.910855348699
1
Iteration 11100: Loss = -11190.91083838942
2
Iteration 11200: Loss = -11190.910641990977
Iteration 11300: Loss = -11190.9115541504
1
Iteration 11400: Loss = -11190.91008474111
Iteration 11500: Loss = -11190.958263062079
1
Iteration 11600: Loss = -11190.910115445582
2
Iteration 11700: Loss = -11190.910083899465
Iteration 11800: Loss = -11190.917719305586
1
Iteration 11900: Loss = -11190.910054790489
Iteration 12000: Loss = -11190.910053745745
Iteration 12100: Loss = -11190.916229951763
1
Iteration 12200: Loss = -11190.910053249898
Iteration 12300: Loss = -11190.91004200257
Iteration 12400: Loss = -11190.91049441726
1
Iteration 12500: Loss = -11190.910063875966
2
Iteration 12600: Loss = -11190.915779205403
3
Iteration 12700: Loss = -11190.910064149453
4
Iteration 12800: Loss = -11191.01058482547
5
Stopping early at iteration 12800 due to no improvement.
pi: tensor([[0.7381, 0.2619],
        [0.2815, 0.7185]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5549, 0.4451], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2856, 0.1044],
         [0.7206, 0.1997]],

        [[0.5551, 0.1028],
         [0.5082, 0.6548]],

        [[0.6095, 0.0943],
         [0.5338, 0.5172]],

        [[0.6964, 0.0946],
         [0.5225, 0.6755]],

        [[0.5262, 0.1117],
         [0.6591, 0.6113]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448436976567872
Global Adjusted Rand Index: 0.921442873151975
Average Adjusted Rand Index: 0.9216117196646859
11217.62163806403
[0.041563593218086364, 0.921442873151975, 0.921442873151975, 0.921442873151975] [0.19784440113788554, 0.9216117196646859, 0.9216117196646859, 0.9216117196646859] [11385.49307475344, 11190.946265060804, 11190.906371653336, 11191.01058482547]
-----------------------------------------------------------------------------------------
This iteration is 15
True Objective function: Loss = -11116.566220287514
Iteration 0: Loss = -11336.103187311115
Iteration 10: Loss = -11331.713601332547
Iteration 20: Loss = -11329.601777024773
Iteration 30: Loss = -11329.462921240822
Iteration 40: Loss = -11329.422280789544
Iteration 50: Loss = -11329.406207209562
Iteration 60: Loss = -11329.39901538772
Iteration 70: Loss = -11329.395496661558
Iteration 80: Loss = -11329.393606387835
Iteration 90: Loss = -11329.392578229148
Iteration 100: Loss = -11329.391891760644
Iteration 110: Loss = -11329.390376580543
Iteration 120: Loss = -11329.380961444112
Iteration 130: Loss = -11329.372668725407
Iteration 140: Loss = -11329.375129049962
1
Iteration 150: Loss = -11329.37692869853
2
Iteration 160: Loss = -11329.37747611152
3
Stopping early at iteration 160 due to no improvement.
pi: tensor([[0.0227, 0.9773],
        [0.0419, 0.9581]], dtype=torch.float64)
alpha: tensor([0.0411, 0.9589])
beta: tensor([[[0.2709, 0.2099],
         [0.1543, 0.1651]],

        [[0.0387, 0.1876],
         [0.7295, 0.8517]],

        [[0.6484, 0.1874],
         [0.7489, 0.8629]],

        [[0.4708, 0.2472],
         [0.8248, 0.4135]],

        [[0.1213, 0.2595],
         [0.7227, 0.4013]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: 0.003991112962394287
Average Adjusted Rand Index: 0.0005080040138409307
Iteration 0: Loss = -11336.935514985806
Iteration 10: Loss = -11333.237152559377
Iteration 20: Loss = -11330.18043288623
Iteration 30: Loss = -11329.561537170908
Iteration 40: Loss = -11329.454298263265
Iteration 50: Loss = -11329.41861730674
Iteration 60: Loss = -11329.404492231257
Iteration 70: Loss = -11329.398151372725
Iteration 80: Loss = -11329.395028976223
Iteration 90: Loss = -11329.393363331039
Iteration 100: Loss = -11329.392406710751
Iteration 110: Loss = -11329.391072787132
Iteration 120: Loss = -11329.383387614347
Iteration 130: Loss = -11329.372862234624
Iteration 140: Loss = -11329.3748098474
1
Iteration 150: Loss = -11329.376762954647
2
Iteration 160: Loss = -11329.377423394533
3
Stopping early at iteration 160 due to no improvement.
pi: tensor([[0.0227, 0.9773],
        [0.0419, 0.9581]], dtype=torch.float64)
alpha: tensor([0.0411, 0.9589])
beta: tensor([[[0.2709, 0.2099],
         [0.3350, 0.1651]],

        [[0.4373, 0.1876],
         [0.3208, 0.1326]],

        [[0.1433, 0.1874],
         [0.2699, 0.5161]],

        [[0.8534, 0.2472],
         [0.1160, 0.3202]],

        [[0.2002, 0.2595],
         [0.2024, 0.5628]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: 0.003991112962394287
Average Adjusted Rand Index: 0.0005080040138409307
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22848.43214947199
Iteration 100: Loss = -11333.649043277273
Iteration 200: Loss = -11332.344055551546
Iteration 300: Loss = -11331.120223731474
Iteration 400: Loss = -11330.435480898603
Iteration 500: Loss = -11330.065234369027
Iteration 600: Loss = -11329.851239601967
Iteration 700: Loss = -11329.721395121616
Iteration 800: Loss = -11329.636835113182
Iteration 900: Loss = -11329.577746416699
Iteration 1000: Loss = -11329.53388257318
Iteration 1100: Loss = -11329.498865726291
Iteration 1200: Loss = -11329.467790617447
Iteration 1300: Loss = -11329.435321757934
Iteration 1400: Loss = -11329.390837401812
Iteration 1500: Loss = -11329.308738378464
Iteration 1600: Loss = -11329.20237602491
Iteration 1700: Loss = -11329.162961448365
Iteration 1800: Loss = -11329.147137097645
Iteration 1900: Loss = -11329.13656178019
Iteration 2000: Loss = -11329.127679491867
Iteration 2100: Loss = -11329.119426611967
Iteration 2200: Loss = -11329.111311231003
Iteration 2300: Loss = -11329.103136775948
Iteration 2400: Loss = -11329.094901115595
Iteration 2500: Loss = -11329.086596300407
Iteration 2600: Loss = -11329.078178751808
Iteration 2700: Loss = -11329.06993366008
Iteration 2800: Loss = -11329.06174816247
Iteration 2900: Loss = -11329.053807589422
Iteration 3000: Loss = -11329.046089185113
Iteration 3100: Loss = -11329.038748313686
Iteration 3200: Loss = -11329.031792244245
Iteration 3300: Loss = -11329.025220939855
Iteration 3400: Loss = -11329.019101205635
Iteration 3500: Loss = -11329.01339522718
Iteration 3600: Loss = -11329.00813416922
Iteration 3700: Loss = -11329.003299785094
Iteration 3800: Loss = -11328.998781636677
Iteration 3900: Loss = -11328.994710168896
Iteration 4000: Loss = -11328.990976694473
Iteration 4100: Loss = -11328.987502998769
Iteration 4200: Loss = -11328.984372894995
Iteration 4300: Loss = -11328.981447955348
Iteration 4400: Loss = -11328.979283258961
Iteration 4500: Loss = -11328.976391696908
Iteration 4600: Loss = -11328.974156166329
Iteration 4700: Loss = -11328.97252708159
Iteration 4800: Loss = -11328.970263630332
Iteration 4900: Loss = -11328.968484972977
Iteration 5000: Loss = -11328.96732839692
Iteration 5100: Loss = -11328.96544109226
Iteration 5200: Loss = -11328.964106970934
Iteration 5300: Loss = -11328.962852574246
Iteration 5400: Loss = -11328.961692438796
Iteration 5500: Loss = -11328.962985423543
1
Iteration 5600: Loss = -11328.959619686297
Iteration 5700: Loss = -11328.95872199208
Iteration 5800: Loss = -11328.957805347425
Iteration 5900: Loss = -11328.95704326177
Iteration 6000: Loss = -11328.956304435951
Iteration 6100: Loss = -11328.95560007199
Iteration 6200: Loss = -11328.954998262296
Iteration 6300: Loss = -11328.954589703932
Iteration 6400: Loss = -11328.953804866029
Iteration 6500: Loss = -11328.954009760786
1
Iteration 6600: Loss = -11328.952804560618
Iteration 6700: Loss = -11328.952362915139
Iteration 6800: Loss = -11328.951967712997
Iteration 6900: Loss = -11328.95146307306
Iteration 7000: Loss = -11328.951285121186
Iteration 7100: Loss = -11328.95077470643
Iteration 7200: Loss = -11328.950472469274
Iteration 7300: Loss = -11328.950145664341
Iteration 7400: Loss = -11328.951064770465
1
Iteration 7500: Loss = -11328.951658328877
2
Iteration 7600: Loss = -11328.949352942478
Iteration 7700: Loss = -11328.949257675979
Iteration 7800: Loss = -11328.953874108061
1
Iteration 7900: Loss = -11328.948627786052
Iteration 8000: Loss = -11328.948433534499
Iteration 8100: Loss = -11328.948377045763
Iteration 8200: Loss = -11328.948047893806
Iteration 8300: Loss = -11328.947883066596
Iteration 8400: Loss = -11328.948591402477
1
Iteration 8500: Loss = -11328.94757831996
Iteration 8600: Loss = -11328.947396876933
Iteration 8700: Loss = -11328.950618146031
1
Iteration 8800: Loss = -11328.947139836653
Iteration 8900: Loss = -11328.947036081985
Iteration 9000: Loss = -11328.947722841775
1
Iteration 9100: Loss = -11328.946787389867
Iteration 9200: Loss = -11328.946779373038
Iteration 9300: Loss = -11328.946611390294
Iteration 9400: Loss = -11328.946515094856
Iteration 9500: Loss = -11329.38696101664
1
Iteration 9600: Loss = -11328.946351625043
Iteration 9700: Loss = -11328.94630236931
Iteration 9800: Loss = -11329.51278104571
1
Iteration 9900: Loss = -11328.946126941963
Iteration 10000: Loss = -11328.946020386506
Iteration 10100: Loss = -11328.946098375643
1
Iteration 10200: Loss = -11328.945936789847
Iteration 10300: Loss = -11328.94587426144
Iteration 10400: Loss = -11328.946169202245
1
Iteration 10500: Loss = -11328.945820880006
Iteration 10600: Loss = -11328.945739855866
Iteration 10700: Loss = -11328.970534785713
1
Iteration 10800: Loss = -11328.945644003781
Iteration 10900: Loss = -11328.945600989837
Iteration 11000: Loss = -11328.950634345205
1
Iteration 11100: Loss = -11328.945519047127
Iteration 11200: Loss = -11328.945475891496
Iteration 11300: Loss = -11328.946927109288
1
Iteration 11400: Loss = -11328.950184195834
2
Iteration 11500: Loss = -11328.94549601033
3
Iteration 11600: Loss = -11328.945359295267
Iteration 11700: Loss = -11328.94535291664
Iteration 11800: Loss = -11328.9458749485
1
Iteration 11900: Loss = -11328.95377027097
2
Iteration 12000: Loss = -11328.946503604535
3
Iteration 12100: Loss = -11329.090877705325
4
Iteration 12200: Loss = -11328.945665388943
5
Stopping early at iteration 12200 due to no improvement.
pi: tensor([[9.4650e-01, 5.3498e-02],
        [9.9991e-01, 9.2867e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9320, 0.0680], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1653, 0.2081],
         [0.6693, 0.3251]],

        [[0.6938, 0.1986],
         [0.6321, 0.6472]],

        [[0.5874, 0.1970],
         [0.7212, 0.5237]],

        [[0.7250, 0.2366],
         [0.7155, 0.5308]],

        [[0.6390, 0.2476],
         [0.5876, 0.6402]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.016025857647765086
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: 0.006643444438033723
Average Adjusted Rand Index: 0.003713175543393948
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23009.82791495128
Iteration 100: Loss = -11332.724805405844
Iteration 200: Loss = -11330.403939480759
Iteration 300: Loss = -11329.707632364463
Iteration 400: Loss = -11329.516869450461
Iteration 500: Loss = -11329.395297844752
Iteration 600: Loss = -11329.273403235276
Iteration 700: Loss = -11329.12429716657
Iteration 800: Loss = -11328.924593982862
Iteration 900: Loss = -11328.66817661181
Iteration 1000: Loss = -11328.337780841579
Iteration 1100: Loss = -11327.933961981505
Iteration 1200: Loss = -11327.421802981926
Iteration 1300: Loss = -11326.56798495574
Iteration 1400: Loss = -11293.090322435128
Iteration 1500: Loss = -11160.971516571806
Iteration 1600: Loss = -11157.985550143165
Iteration 1700: Loss = -11157.695255122862
Iteration 1800: Loss = -11157.532680827406
Iteration 1900: Loss = -11157.49918089538
Iteration 2000: Loss = -11157.476215806842
Iteration 2100: Loss = -11157.348577151482
Iteration 2200: Loss = -11157.33133809404
Iteration 2300: Loss = -11157.310172154355
Iteration 2400: Loss = -11157.272416533327
Iteration 2500: Loss = -11157.163248116187
Iteration 2600: Loss = -11154.78021698927
Iteration 2700: Loss = -11150.208191009517
Iteration 2800: Loss = -11147.42307061813
Iteration 2900: Loss = -11145.335601929455
Iteration 3000: Loss = -11145.308787414137
Iteration 3100: Loss = -11145.302406784218
Iteration 3200: Loss = -11145.29829748889
Iteration 3300: Loss = -11145.294711173447
Iteration 3400: Loss = -11145.2921928278
Iteration 3500: Loss = -11145.290297844302
Iteration 3600: Loss = -11145.252533255136
Iteration 3700: Loss = -11145.23244771227
Iteration 3800: Loss = -11145.216123162514
Iteration 3900: Loss = -11145.215097407865
Iteration 4000: Loss = -11145.209958226907
Iteration 4100: Loss = -11145.223743137543
1
Iteration 4200: Loss = -11145.208284336815
Iteration 4300: Loss = -11145.206821643917
Iteration 4400: Loss = -11145.206271731135
Iteration 4500: Loss = -11145.205010850214
Iteration 4600: Loss = -11145.204042440851
Iteration 4700: Loss = -11145.203839066204
Iteration 4800: Loss = -11145.210980139927
1
Iteration 4900: Loss = -11145.196756497186
Iteration 5000: Loss = -11145.196470883979
Iteration 5100: Loss = -11145.196729945206
1
Iteration 5200: Loss = -11145.195926245118
Iteration 5300: Loss = -11145.200353645298
1
Iteration 5400: Loss = -11145.195453355258
Iteration 5500: Loss = -11145.195578090275
1
Iteration 5600: Loss = -11145.194662578242
Iteration 5700: Loss = -11145.197268312417
1
Iteration 5800: Loss = -11145.193894123731
Iteration 5900: Loss = -11145.193712572294
Iteration 6000: Loss = -11145.193640091567
Iteration 6100: Loss = -11145.19341376832
Iteration 6200: Loss = -11145.19466375614
1
Iteration 6300: Loss = -11145.19316466939
Iteration 6400: Loss = -11145.211699528792
1
Iteration 6500: Loss = -11145.19305577651
Iteration 6600: Loss = -11145.192986091519
Iteration 6700: Loss = -11145.193193546796
1
Iteration 6800: Loss = -11145.192856636659
Iteration 6900: Loss = -11145.201873517533
1
Iteration 7000: Loss = -11145.192718332293
Iteration 7100: Loss = -11145.192546735469
Iteration 7200: Loss = -11145.193502919123
1
Iteration 7300: Loss = -11145.192366329786
Iteration 7400: Loss = -11145.192807648871
1
Iteration 7500: Loss = -11145.192302231106
Iteration 7600: Loss = -11145.192235094672
Iteration 7700: Loss = -11145.192192726905
Iteration 7800: Loss = -11145.260117382099
1
Iteration 7900: Loss = -11145.191183885425
Iteration 8000: Loss = -11145.254615564043
1
Iteration 8100: Loss = -11145.191059778395
Iteration 8200: Loss = -11145.191029960763
Iteration 8300: Loss = -11145.19097654929
Iteration 8400: Loss = -11145.190639883927
Iteration 8500: Loss = -11145.20854419562
1
Iteration 8600: Loss = -11145.189258022147
Iteration 8700: Loss = -11145.189221652317
Iteration 8800: Loss = -11145.204024862085
1
Iteration 8900: Loss = -11145.186749367003
Iteration 9000: Loss = -11145.186728770888
Iteration 9100: Loss = -11145.190875189355
1
Iteration 9200: Loss = -11145.186672695903
Iteration 9300: Loss = -11145.186671850233
Iteration 9400: Loss = -11145.339175944246
1
Iteration 9500: Loss = -11145.186655857344
Iteration 9600: Loss = -11145.186659072722
1
Iteration 9700: Loss = -11145.187241552056
2
Iteration 9800: Loss = -11145.186639929178
Iteration 9900: Loss = -11145.231480349168
1
Iteration 10000: Loss = -11145.186652529594
2
Iteration 10100: Loss = -11145.186630661716
Iteration 10200: Loss = -11145.262723308908
1
Iteration 10300: Loss = -11145.1866039637
Iteration 10400: Loss = -11145.18658088401
Iteration 10500: Loss = -11145.186589457573
1
Iteration 10600: Loss = -11145.19028535541
2
Iteration 10700: Loss = -11145.18659589158
3
Iteration 10800: Loss = -11145.186564546955
Iteration 10900: Loss = -11145.207916162455
1
Iteration 11000: Loss = -11145.186568790508
2
Iteration 11100: Loss = -11145.186537575453
Iteration 11200: Loss = -11145.597919977992
1
Iteration 11300: Loss = -11145.186559696403
2
Iteration 11400: Loss = -11145.186573009858
3
Iteration 11500: Loss = -11145.186763191625
4
Iteration 11600: Loss = -11145.18650803464
Iteration 11700: Loss = -11145.19782256154
1
Iteration 11800: Loss = -11145.186525907386
2
Iteration 11900: Loss = -11145.187993654432
3
Iteration 12000: Loss = -11145.186597536009
4
Iteration 12100: Loss = -11145.186537955788
5
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.6989, 0.3011],
        [0.2965, 0.7035]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2051, 0.7949], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2984, 0.1003],
         [0.6117, 0.2012]],

        [[0.6052, 0.0958],
         [0.6070, 0.6440]],

        [[0.5146, 0.1017],
         [0.5956, 0.5879]],

        [[0.6205, 0.1102],
         [0.7045, 0.6795]],

        [[0.7197, 0.0989],
         [0.5619, 0.6549]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.0645706119267247
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.5113671177038825
Average Adjusted Rand Index: 0.7575453790359817
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21860.663614111272
Iteration 100: Loss = -11333.460086891066
Iteration 200: Loss = -11332.606844274329
Iteration 300: Loss = -11330.608137963307
Iteration 400: Loss = -11330.019244113537
Iteration 500: Loss = -11329.782725795818
Iteration 600: Loss = -11329.648665270548
Iteration 700: Loss = -11329.545620142906
Iteration 800: Loss = -11329.431275530498
Iteration 900: Loss = -11329.264111044517
Iteration 1000: Loss = -11329.181378738445
Iteration 1100: Loss = -11329.14719126005
Iteration 1200: Loss = -11329.125324875531
Iteration 1300: Loss = -11329.108795994274
Iteration 1400: Loss = -11329.094604327829
Iteration 1500: Loss = -11329.081535441095
Iteration 1600: Loss = -11329.069291278167
Iteration 1700: Loss = -11329.05779744331
Iteration 1800: Loss = -11329.047129948345
Iteration 1900: Loss = -11329.037254043853
Iteration 2000: Loss = -11329.02815465911
Iteration 2100: Loss = -11329.01999639292
Iteration 2200: Loss = -11329.012610348122
Iteration 2300: Loss = -11329.005953818392
Iteration 2400: Loss = -11329.000065419626
Iteration 2500: Loss = -11328.99482212618
Iteration 2600: Loss = -11328.990181254443
Iteration 2700: Loss = -11328.986013653484
Iteration 2800: Loss = -11328.982319813664
Iteration 2900: Loss = -11328.979060352809
Iteration 3000: Loss = -11328.976160226965
Iteration 3100: Loss = -11328.973505568021
Iteration 3200: Loss = -11328.971185408262
Iteration 3300: Loss = -11328.969036813742
Iteration 3400: Loss = -11328.967177133893
Iteration 3500: Loss = -11328.965460523423
Iteration 3600: Loss = -11328.963883824004
Iteration 3700: Loss = -11328.962492773504
Iteration 3800: Loss = -11328.961207205048
Iteration 3900: Loss = -11328.960024993354
Iteration 4000: Loss = -11328.958977163225
Iteration 4100: Loss = -11328.95799209939
Iteration 4200: Loss = -11328.957137067393
Iteration 4300: Loss = -11328.956257655798
Iteration 4400: Loss = -11328.955555818953
Iteration 4500: Loss = -11328.954844382572
Iteration 4600: Loss = -11328.954736246435
Iteration 4700: Loss = -11328.95358023101
Iteration 4800: Loss = -11328.953026759285
Iteration 4900: Loss = -11328.952527960892
Iteration 5000: Loss = -11328.952020530158
Iteration 5100: Loss = -11328.95162493833
Iteration 5200: Loss = -11328.951202841166
Iteration 5300: Loss = -11328.9517609113
1
Iteration 5400: Loss = -11328.95047201453
Iteration 5500: Loss = -11328.950126826972
Iteration 5600: Loss = -11328.949894197904
Iteration 5700: Loss = -11328.949541814107
Iteration 5800: Loss = -11328.949252959743
Iteration 5900: Loss = -11328.948998675614
Iteration 6000: Loss = -11328.948753806384
Iteration 6100: Loss = -11328.948802349618
1
Iteration 6200: Loss = -11328.948329325942
Iteration 6300: Loss = -11328.952339218195
1
Iteration 6400: Loss = -11328.947962884196
Iteration 6500: Loss = -11328.94781444621
Iteration 6600: Loss = -11328.947659564727
Iteration 6700: Loss = -11328.94748916983
Iteration 6800: Loss = -11328.947378378478
Iteration 6900: Loss = -11328.947203069729
Iteration 7000: Loss = -11328.947668277931
1
Iteration 7100: Loss = -11328.94693952318
Iteration 7200: Loss = -11328.948562731197
1
Iteration 7300: Loss = -11328.946699173235
Iteration 7400: Loss = -11328.94747006297
1
Iteration 7500: Loss = -11328.946547638006
Iteration 7600: Loss = -11328.967816329225
1
Iteration 7700: Loss = -11328.946385465522
Iteration 7800: Loss = -11328.946344342114
Iteration 7900: Loss = -11329.027139439493
1
Iteration 8000: Loss = -11328.94614435771
Iteration 8100: Loss = -11328.946071473902
Iteration 8200: Loss = -11328.946010720958
Iteration 8300: Loss = -11328.945919646016
Iteration 8400: Loss = -11328.962615535122
1
Iteration 8500: Loss = -11328.945830008985
Iteration 8600: Loss = -11328.945775188702
Iteration 8700: Loss = -11328.963563125093
1
Iteration 8800: Loss = -11328.945679025695
Iteration 8900: Loss = -11328.945657305876
Iteration 9000: Loss = -11328.949159164724
1
Iteration 9100: Loss = -11328.945549566391
Iteration 9200: Loss = -11328.94551799766
Iteration 9300: Loss = -11328.945992084131
1
Iteration 9400: Loss = -11328.945469290018
Iteration 9500: Loss = -11328.94540875862
Iteration 9600: Loss = -11328.97020428027
1
Iteration 9700: Loss = -11328.945355507496
Iteration 9800: Loss = -11328.94536423773
1
Iteration 9900: Loss = -11328.945646511715
2
Iteration 10000: Loss = -11328.94531565455
Iteration 10100: Loss = -11328.945277417266
Iteration 10200: Loss = -11328.946905563345
1
Iteration 10300: Loss = -11328.945241188469
Iteration 10400: Loss = -11328.945198491478
Iteration 10500: Loss = -11328.950976283711
1
Iteration 10600: Loss = -11328.945176933088
Iteration 10700: Loss = -11328.9451227348
Iteration 10800: Loss = -11329.269827570402
1
Iteration 10900: Loss = -11328.945151974543
2
Iteration 11000: Loss = -11328.945096041012
Iteration 11100: Loss = -11328.954230652233
1
Iteration 11200: Loss = -11328.945089490893
Iteration 11300: Loss = -11328.945039558368
Iteration 11400: Loss = -11329.307298088608
1
Iteration 11500: Loss = -11328.945073791609
2
Iteration 11600: Loss = -11328.945039276894
Iteration 11700: Loss = -11328.94709721896
1
Iteration 11800: Loss = -11328.945005212963
Iteration 11900: Loss = -11328.994194491564
1
Iteration 12000: Loss = -11328.94500942737
2
Iteration 12100: Loss = -11328.945349224132
3
Iteration 12200: Loss = -11328.945030379165
4
Iteration 12300: Loss = -11328.945139610427
5
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[9.4658e-01, 5.3423e-02],
        [9.9997e-01, 3.4141e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9320, 0.0680], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1653, 0.2083],
         [0.5448, 0.3252]],

        [[0.7063, 0.1986],
         [0.6399, 0.7222]],

        [[0.6643, 0.1970],
         [0.6893, 0.5637]],

        [[0.5141, 0.2368],
         [0.6191, 0.5742]],

        [[0.5820, 0.2477],
         [0.6270, 0.6520]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.016025857647765086
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.002468638218550053
time is 4
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: 0.006643444438033723
Average Adjusted Rand Index: 0.003713175543393948
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21297.20275232912
Iteration 100: Loss = -11333.585095735942
Iteration 200: Loss = -11332.31531315821
Iteration 300: Loss = -11331.625311885195
Iteration 400: Loss = -11331.270354375303
Iteration 500: Loss = -11330.69378561975
Iteration 600: Loss = -11329.915520757802
Iteration 700: Loss = -11329.716855969444
Iteration 800: Loss = -11329.603553702238
Iteration 900: Loss = -11329.473920137038
Iteration 1000: Loss = -11329.165504805525
Iteration 1100: Loss = -11328.723955542571
Iteration 1200: Loss = -11328.304074262735
Iteration 1300: Loss = -11327.9805113586
Iteration 1400: Loss = -11327.714203794292
Iteration 1500: Loss = -11327.332715538892
Iteration 1600: Loss = -11325.864633602281
Iteration 1700: Loss = -11324.84037554158
Iteration 1800: Loss = -11324.546494265916
Iteration 1900: Loss = -11323.82746226811
Iteration 2000: Loss = -11304.99202090446
Iteration 2100: Loss = -11177.422379871865
Iteration 2200: Loss = -11144.871762232882
Iteration 2300: Loss = -11135.971209145613
Iteration 2400: Loss = -11096.1962789404
Iteration 2500: Loss = -11094.960469672998
Iteration 2600: Loss = -11094.520928653214
Iteration 2700: Loss = -11087.11044886664
Iteration 2800: Loss = -11087.093319921743
Iteration 2900: Loss = -11087.07844336991
Iteration 3000: Loss = -11087.070493480807
Iteration 3100: Loss = -11087.063570827853
Iteration 3200: Loss = -11087.030500010895
Iteration 3300: Loss = -11087.02087347072
Iteration 3400: Loss = -11087.00221011214
Iteration 3500: Loss = -11086.99690960762
Iteration 3600: Loss = -11086.973533208
Iteration 3700: Loss = -11086.96586260145
Iteration 3800: Loss = -11086.964096571794
Iteration 3900: Loss = -11086.960699755839
Iteration 4000: Loss = -11086.958528591611
Iteration 4100: Loss = -11086.957689077637
Iteration 4200: Loss = -11086.960242460624
1
Iteration 4300: Loss = -11086.956246498898
Iteration 4400: Loss = -11086.955638504853
Iteration 4500: Loss = -11086.961294594097
1
Iteration 4600: Loss = -11086.954604278311
Iteration 4700: Loss = -11086.957454433885
1
Iteration 4800: Loss = -11086.953343652787
Iteration 4900: Loss = -11086.953343746916
1
Iteration 5000: Loss = -11086.95181250016
Iteration 5100: Loss = -11086.951359084314
Iteration 5200: Loss = -11086.950981885877
Iteration 5300: Loss = -11086.950664416107
Iteration 5400: Loss = -11086.953946995674
1
Iteration 5500: Loss = -11086.949852484391
Iteration 5600: Loss = -11086.949509590206
Iteration 5700: Loss = -11086.948721978595
Iteration 5800: Loss = -11086.945593612585
Iteration 5900: Loss = -11086.944713083918
Iteration 6000: Loss = -11086.945709887315
1
Iteration 6100: Loss = -11086.94465068093
Iteration 6200: Loss = -11086.940919540522
Iteration 6300: Loss = -11086.939584065203
Iteration 6400: Loss = -11086.939055612764
Iteration 6500: Loss = -11086.937793753306
Iteration 6600: Loss = -11086.928275894486
Iteration 6700: Loss = -11086.928005271331
Iteration 6800: Loss = -11086.927835630106
Iteration 6900: Loss = -11086.927660476556
Iteration 7000: Loss = -11086.927384271235
Iteration 7100: Loss = -11086.92723749421
Iteration 7200: Loss = -11086.927102748377
Iteration 7300: Loss = -11086.926856895498
Iteration 7400: Loss = -11086.925277913399
Iteration 7500: Loss = -11086.925161711313
Iteration 7600: Loss = -11086.925388612653
1
Iteration 7700: Loss = -11086.924445737826
Iteration 7800: Loss = -11086.925531986126
1
Iteration 7900: Loss = -11086.952546004528
2
Iteration 8000: Loss = -11086.923277374657
Iteration 8100: Loss = -11086.924583852158
1
Iteration 8200: Loss = -11086.922837542335
Iteration 8300: Loss = -11086.969167365718
1
Iteration 8400: Loss = -11086.92272090807
Iteration 8500: Loss = -11086.922704995983
Iteration 8600: Loss = -11086.930299421732
1
Iteration 8700: Loss = -11086.92265702731
Iteration 8800: Loss = -11086.922604538997
Iteration 8900: Loss = -11086.922937473913
1
Iteration 9000: Loss = -11086.92278872281
2
Iteration 9100: Loss = -11086.922499923921
Iteration 9200: Loss = -11086.929295124568
1
Iteration 9300: Loss = -11086.922381173445
Iteration 9400: Loss = -11086.922348495766
Iteration 9500: Loss = -11086.9294698024
1
Iteration 9600: Loss = -11086.92129398955
Iteration 9700: Loss = -11087.125654699325
1
Iteration 9800: Loss = -11086.919465107454
Iteration 9900: Loss = -11086.920546554531
1
Iteration 10000: Loss = -11086.919176147865
Iteration 10100: Loss = -11086.920690726713
1
Iteration 10200: Loss = -11086.92159199063
2
Iteration 10300: Loss = -11086.919921922945
3
Iteration 10400: Loss = -11086.918746534424
Iteration 10500: Loss = -11086.917909564729
Iteration 10600: Loss = -11086.916461327255
Iteration 10700: Loss = -11086.916648827826
1
Iteration 10800: Loss = -11086.916269664447
Iteration 10900: Loss = -11086.916514348277
1
Iteration 11000: Loss = -11086.916325160752
2
Iteration 11100: Loss = -11086.916379566843
3
Iteration 11200: Loss = -11086.937118453314
4
Iteration 11300: Loss = -11086.91599009448
Iteration 11400: Loss = -11086.925372266358
1
Iteration 11500: Loss = -11086.915939923018
Iteration 11600: Loss = -11086.905008452568
Iteration 11700: Loss = -11087.032368646833
1
Iteration 11800: Loss = -11086.904377453704
Iteration 11900: Loss = -11086.908351661816
1
Iteration 12000: Loss = -11086.904342547587
Iteration 12100: Loss = -11086.905474334242
1
Iteration 12200: Loss = -11086.905246507979
2
Iteration 12300: Loss = -11086.904343557222
3
Iteration 12400: Loss = -11086.905391601213
4
Iteration 12500: Loss = -11086.90432742048
Iteration 12600: Loss = -11086.904300002656
Iteration 12700: Loss = -11086.906503312526
1
Iteration 12800: Loss = -11086.904346452267
2
Iteration 12900: Loss = -11087.12170960382
3
Iteration 13000: Loss = -11086.904334107576
4
Iteration 13100: Loss = -11086.904343948125
5
Stopping early at iteration 13100 due to no improvement.
pi: tensor([[0.7752, 0.2248],
        [0.2569, 0.7431]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5705, 0.4295], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.0988],
         [0.5955, 0.3011]],

        [[0.6086, 0.0952],
         [0.5359, 0.6321]],

        [[0.5105, 0.1022],
         [0.6407, 0.6897]],

        [[0.7170, 0.1107],
         [0.6626, 0.6228]],

        [[0.5917, 0.0993],
         [0.6628, 0.5732]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.929140701865355
Average Adjusted Rand Index: 0.9291153647516219
11116.566220287514
[0.006643444438033723, 0.5113671177038825, 0.006643444438033723, 0.929140701865355] [0.003713175543393948, 0.7575453790359817, 0.003713175543393948, 0.9291153647516219] [11328.945665388943, 11145.186537955788, 11328.945139610427, 11086.904343948125]
-----------------------------------------------------------------------------------------
This iteration is 16
True Objective function: Loss = -10978.031958504685
Iteration 0: Loss = -11310.28106715436
Iteration 10: Loss = -11249.926082193733
Iteration 20: Loss = -11208.21633794201
Iteration 30: Loss = -10969.630153474858
Iteration 40: Loss = -10969.255315674101
Iteration 50: Loss = -10969.252767451826
Iteration 60: Loss = -10969.253013735728
1
Iteration 70: Loss = -10969.253057570011
2
Iteration 80: Loss = -10969.253063413158
3
Stopping early at iteration 80 due to no improvement.
pi: tensor([[0.7377, 0.2623],
        [0.2742, 0.7258]], dtype=torch.float64)
alpha: tensor([0.5176, 0.4824])
beta: tensor([[[0.1934, 0.0944],
         [0.6189, 0.2908]],

        [[0.4982, 0.0975],
         [0.6365, 0.3280]],

        [[0.7687, 0.0959],
         [0.4877, 0.9860]],

        [[0.4112, 0.0956],
         [0.8996, 0.4199]],

        [[0.0170, 0.0972],
         [0.9474, 0.7876]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603206139093168
Average Adjusted Rand Index: 0.9603200653711088
Iteration 0: Loss = -11421.902259075623
Iteration 10: Loss = -11257.527675692989
Iteration 20: Loss = -11257.329580211093
Iteration 30: Loss = -11257.309056723478
Iteration 40: Loss = -11257.30797043254
Iteration 50: Loss = -11257.308240979964
1
Iteration 60: Loss = -11257.308338380668
2
Iteration 70: Loss = -11257.308387902347
3
Stopping early at iteration 70 due to no improvement.
pi: tensor([[0.0012, 0.9988],
        [0.0288, 0.9712]], dtype=torch.float64)
alpha: tensor([0.0272, 0.9728])
beta: tensor([[[0.2694, 0.1315],
         [0.4105, 0.1675]],

        [[0.2024, 0.1080],
         [0.9682, 0.9678]],

        [[0.1429, 0.2639],
         [0.6009, 0.6082]],

        [[0.7817, 0.2545],
         [0.0203, 0.3177]],

        [[0.2976, 0.0859],
         [0.7037, 0.5406]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: 0.0009813649942087015
Average Adjusted Rand Index: 0.0016495376110217531
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22914.868433177067
Iteration 100: Loss = -11261.24985698643
Iteration 200: Loss = -11254.748939978663
Iteration 300: Loss = -11245.72162740209
Iteration 400: Loss = -11041.201170936061
Iteration 500: Loss = -11038.002090371858
Iteration 600: Loss = -11037.377681695569
Iteration 700: Loss = -11037.229645149775
Iteration 800: Loss = -11037.146463876492
Iteration 900: Loss = -11037.089102610691
Iteration 1000: Loss = -11037.039778612268
Iteration 1100: Loss = -11036.98931253289
Iteration 1200: Loss = -11036.704259890306
Iteration 1300: Loss = -11036.599907055643
Iteration 1400: Loss = -11036.476456569537
Iteration 1500: Loss = -11036.212449862192
Iteration 1600: Loss = -11035.862914978954
Iteration 1700: Loss = -11035.807121075613
Iteration 1800: Loss = -11035.788274557975
Iteration 1900: Loss = -11035.772868075677
Iteration 2000: Loss = -11035.765082786824
Iteration 2100: Loss = -11035.760745414811
Iteration 2200: Loss = -11035.757542532683
Iteration 2300: Loss = -11035.75508345268
Iteration 2400: Loss = -11035.752983048018
Iteration 2500: Loss = -11035.75114786656
Iteration 2600: Loss = -11035.74965287145
Iteration 2700: Loss = -11035.747396659584
Iteration 2800: Loss = -11035.74175086313
Iteration 2900: Loss = -11035.737112071098
Iteration 3000: Loss = -11035.736042662958
Iteration 3100: Loss = -11035.735137634361
Iteration 3200: Loss = -11035.738645017866
1
Iteration 3300: Loss = -11035.733771020206
Iteration 3400: Loss = -11035.733975776488
1
Iteration 3500: Loss = -11035.732728637186
Iteration 3600: Loss = -11035.732358975332
Iteration 3700: Loss = -11035.731882454382
Iteration 3800: Loss = -11035.734097932573
1
Iteration 3900: Loss = -11035.731181857192
Iteration 4000: Loss = -11035.73090913493
Iteration 4100: Loss = -11035.730587177959
Iteration 4200: Loss = -11035.730370060588
Iteration 4300: Loss = -11035.730056435581
Iteration 4400: Loss = -11035.72994417751
Iteration 4500: Loss = -11035.729572530174
Iteration 4600: Loss = -11035.729433193075
Iteration 4700: Loss = -11035.7291644034
Iteration 4800: Loss = -11035.729174474058
1
Iteration 4900: Loss = -11035.734145997321
2
Iteration 5000: Loss = -11035.728965595803
Iteration 5100: Loss = -11035.743147883346
1
Iteration 5200: Loss = -11035.728371558193
Iteration 5300: Loss = -11035.728244911501
Iteration 5400: Loss = -11035.728543602807
1
Iteration 5500: Loss = -11035.728850773106
2
Iteration 5600: Loss = -11035.727819454401
Iteration 5700: Loss = -11035.74160501412
1
Iteration 5800: Loss = -11035.72759831684
Iteration 5900: Loss = -11035.727626891565
1
Iteration 6000: Loss = -11035.727335893487
Iteration 6100: Loss = -11035.727066188161
Iteration 6200: Loss = -11035.66447552928
Iteration 6300: Loss = -11035.663445961562
Iteration 6400: Loss = -11035.663273942806
Iteration 6500: Loss = -11035.667422691386
1
Iteration 6600: Loss = -11035.66313579701
Iteration 6700: Loss = -11035.666556656859
1
Iteration 6800: Loss = -11035.66300447439
Iteration 6900: Loss = -11035.680286013443
1
Iteration 7000: Loss = -11035.662905490994
Iteration 7100: Loss = -11035.662883008978
Iteration 7200: Loss = -11035.66282922324
Iteration 7300: Loss = -11035.662789913133
Iteration 7400: Loss = -11035.66314530666
1
Iteration 7500: Loss = -11035.662730398955
Iteration 7600: Loss = -11035.663311327564
1
Iteration 7700: Loss = -11035.662688982591
Iteration 7800: Loss = -11035.662888099047
1
Iteration 7900: Loss = -11035.662658575111
Iteration 8000: Loss = -11035.662624566796
Iteration 8100: Loss = -11035.666435856
1
Iteration 8200: Loss = -11035.675261073733
2
Iteration 8300: Loss = -11035.668913195583
3
Iteration 8400: Loss = -11035.662550283292
Iteration 8500: Loss = -11035.662670827498
1
Iteration 8600: Loss = -11035.739121334856
2
Iteration 8700: Loss = -11035.662461343773
Iteration 8800: Loss = -11035.66644729019
1
Iteration 8900: Loss = -11035.662444792397
Iteration 9000: Loss = -11035.66751304065
1
Iteration 9100: Loss = -11035.662395435817
Iteration 9200: Loss = -11035.66238868338
Iteration 9300: Loss = -11035.662369923115
Iteration 9400: Loss = -11035.6622240139
Iteration 9500: Loss = -11035.662637619407
1
Iteration 9600: Loss = -11035.662113782564
Iteration 9700: Loss = -11035.671105607651
1
Iteration 9800: Loss = -11035.662026941995
Iteration 9900: Loss = -11035.668306268028
1
Iteration 10000: Loss = -11035.711714973855
2
Iteration 10100: Loss = -11035.674465564898
3
Iteration 10200: Loss = -11035.662286180037
4
Iteration 10300: Loss = -11035.66207780447
5
Stopping early at iteration 10300 due to no improvement.
pi: tensor([[0.6748, 0.3252],
        [0.2583, 0.7417]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9467, 0.0533], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1853, 0.1138],
         [0.5069, 0.3017]],

        [[0.5951, 0.0985],
         [0.6289, 0.7251]],

        [[0.7223, 0.0964],
         [0.7092, 0.5502]],

        [[0.5418, 0.0961],
         [0.6897, 0.5619]],

        [[0.6327, 0.0974],
         [0.5870, 0.5381]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.6201982810534646
Average Adjusted Rand Index: 0.7838471166244191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22004.720250862567
Iteration 100: Loss = -11261.52302583775
Iteration 200: Loss = -11260.21048906373
Iteration 300: Loss = -11258.455696451265
Iteration 400: Loss = -11257.56424382886
Iteration 500: Loss = -11257.259656395247
Iteration 600: Loss = -11257.096839480806
Iteration 700: Loss = -11256.992166561779
Iteration 800: Loss = -11256.907450646462
Iteration 900: Loss = -11256.827245409388
Iteration 1000: Loss = -11256.751376226235
Iteration 1100: Loss = -11256.684705533522
Iteration 1200: Loss = -11256.623509536586
Iteration 1300: Loss = -11256.569097435158
Iteration 1400: Loss = -11256.522669059552
Iteration 1500: Loss = -11256.484637705846
Iteration 1600: Loss = -11256.454840155522
Iteration 1700: Loss = -11256.432307562656
Iteration 1800: Loss = -11256.41588987512
Iteration 1900: Loss = -11256.404209472623
Iteration 2000: Loss = -11256.396181071159
Iteration 2100: Loss = -11256.390684333532
Iteration 2200: Loss = -11256.387092974031
Iteration 2300: Loss = -11256.384620999495
Iteration 2400: Loss = -11256.382963071872
Iteration 2500: Loss = -11256.38181123531
Iteration 2600: Loss = -11256.380982265302
Iteration 2700: Loss = -11256.380376920433
Iteration 2800: Loss = -11256.379909660967
Iteration 2900: Loss = -11256.379483100165
Iteration 3000: Loss = -11256.379144542256
Iteration 3100: Loss = -11256.378828132785
Iteration 3200: Loss = -11256.378560748046
Iteration 3300: Loss = -11256.37830014375
Iteration 3400: Loss = -11256.378099027175
Iteration 3500: Loss = -11256.377845796773
Iteration 3600: Loss = -11256.377668212826
Iteration 3700: Loss = -11256.377480085815
Iteration 3800: Loss = -11256.377308563402
Iteration 3900: Loss = -11256.377139787279
Iteration 4000: Loss = -11256.376986490253
Iteration 4100: Loss = -11256.376900208466
Iteration 4200: Loss = -11256.376680318965
Iteration 4300: Loss = -11256.37654331983
Iteration 4400: Loss = -11256.376434526132
Iteration 4500: Loss = -11256.376301834482
Iteration 4600: Loss = -11256.376230206777
Iteration 4700: Loss = -11256.376126352461
Iteration 4800: Loss = -11256.375990141942
Iteration 4900: Loss = -11256.375880692955
Iteration 5000: Loss = -11256.376538135128
1
Iteration 5100: Loss = -11256.376388484854
2
Iteration 5200: Loss = -11256.376620561789
3
Iteration 5300: Loss = -11256.377185176549
4
Iteration 5400: Loss = -11256.375493648864
Iteration 5500: Loss = -11256.37542520901
Iteration 5600: Loss = -11256.375339479468
Iteration 5700: Loss = -11256.375296388467
Iteration 5800: Loss = -11256.375261906935
Iteration 5900: Loss = -11256.375137938123
Iteration 6000: Loss = -11256.37508594296
Iteration 6100: Loss = -11256.375275154782
1
Iteration 6200: Loss = -11256.374995222432
Iteration 6300: Loss = -11256.374957013628
Iteration 6400: Loss = -11256.374876758151
Iteration 6500: Loss = -11256.374829754302
Iteration 6600: Loss = -11256.37480312092
Iteration 6700: Loss = -11256.374769453892
Iteration 6800: Loss = -11256.375352096227
1
Iteration 6900: Loss = -11256.374677181859
Iteration 7000: Loss = -11256.374614944125
Iteration 7100: Loss = -11256.37460992172
Iteration 7200: Loss = -11256.374586405342
Iteration 7300: Loss = -11256.374543214566
Iteration 7400: Loss = -11256.376244186553
1
Iteration 7500: Loss = -11256.387169066236
2
Iteration 7600: Loss = -11256.374508522144
Iteration 7700: Loss = -11256.374516920097
1
Iteration 7800: Loss = -11256.374410525676
Iteration 7900: Loss = -11256.3744213867
1
Iteration 8000: Loss = -11256.374359313897
Iteration 8100: Loss = -11256.408891652281
1
Iteration 8200: Loss = -11256.37432906993
Iteration 8300: Loss = -11256.374282353687
Iteration 8400: Loss = -11256.374288364528
1
Iteration 8500: Loss = -11256.374253885264
Iteration 8600: Loss = -11256.374250339697
Iteration 8700: Loss = -11256.378656926148
1
Iteration 8800: Loss = -11256.374154535442
Iteration 8900: Loss = -11256.374207534353
1
Iteration 9000: Loss = -11256.374252770245
2
Iteration 9100: Loss = -11256.374179151097
3
Iteration 9200: Loss = -11256.374150067784
Iteration 9300: Loss = -11256.374199588168
1
Iteration 9400: Loss = -11256.374120550434
Iteration 9500: Loss = -11256.374100865554
Iteration 9600: Loss = -11256.374892375487
1
Iteration 9700: Loss = -11256.37410673277
2
Iteration 9800: Loss = -11256.374106089872
3
Iteration 9900: Loss = -11256.378306615774
4
Iteration 10000: Loss = -11256.37406116676
Iteration 10100: Loss = -11256.374067638184
1
Iteration 10200: Loss = -11256.504149709754
2
Iteration 10300: Loss = -11256.374043622229
Iteration 10400: Loss = -11256.37403917594
Iteration 10500: Loss = -11256.374029029217
Iteration 10600: Loss = -11256.374143173673
1
Iteration 10700: Loss = -11256.374011522892
Iteration 10800: Loss = -11256.374040735907
1
Iteration 10900: Loss = -11256.374361440849
2
Iteration 11000: Loss = -11256.374008177578
Iteration 11100: Loss = -11256.373989998083
Iteration 11200: Loss = -11256.375535640285
1
Iteration 11300: Loss = -11256.374007208398
2
Iteration 11400: Loss = -11256.373964232707
Iteration 11500: Loss = -11256.374444415478
1
Iteration 11600: Loss = -11256.373964854272
2
Iteration 11700: Loss = -11256.373980745617
3
Iteration 11800: Loss = -11256.394658018238
4
Iteration 11900: Loss = -11256.373937131006
Iteration 12000: Loss = -11256.37396249318
1
Iteration 12100: Loss = -11256.37439488043
2
Iteration 12200: Loss = -11256.37392020672
Iteration 12300: Loss = -11256.373968750931
1
Iteration 12400: Loss = -11256.388372013838
2
Iteration 12500: Loss = -11256.373948793731
3
Iteration 12600: Loss = -11256.373950801533
4
Iteration 12700: Loss = -11256.398320920556
5
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[1.7008e-06, 1.0000e+00],
        [3.7438e-02, 9.6256e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0055, 0.9945], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3989, 0.1161],
         [0.6392, 0.1692]],

        [[0.6886, 0.2221],
         [0.5992, 0.6015]],

        [[0.6246, 0.0661],
         [0.5960, 0.6118]],

        [[0.6176, 0.0801],
         [0.6070, 0.5131]],

        [[0.5465, 0.2294],
         [0.7140, 0.6627]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.012298440577296121
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0010693500176705288
Average Adjusted Rand Index: -0.0019711698006338292
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23755.346494607187
Iteration 100: Loss = -11261.426602042911
Iteration 200: Loss = -11258.707302861303
Iteration 300: Loss = -11257.744041950578
Iteration 400: Loss = -11257.284450309418
Iteration 500: Loss = -11256.355221601107
Iteration 600: Loss = -11255.628661405464
Iteration 700: Loss = -11255.24900442671
Iteration 800: Loss = -11254.555462159738
Iteration 900: Loss = -11123.16711962959
Iteration 1000: Loss = -11050.301501482176
Iteration 1100: Loss = -11038.424629218247
Iteration 1200: Loss = -11033.2288640264
Iteration 1300: Loss = -11033.138163479729
Iteration 1400: Loss = -11033.064021541355
Iteration 1500: Loss = -11033.028070070739
Iteration 1600: Loss = -11033.00627560536
Iteration 1700: Loss = -11032.955823382066
Iteration 1800: Loss = -11032.923988988683
Iteration 1900: Loss = -11032.916965332079
Iteration 2000: Loss = -11032.913367222938
Iteration 2100: Loss = -11032.907064208513
Iteration 2200: Loss = -11032.902862055564
Iteration 2300: Loss = -11032.906043152681
1
Iteration 2400: Loss = -11032.894794408841
Iteration 2500: Loss = -11032.891994399277
Iteration 2600: Loss = -11032.888824376088
Iteration 2700: Loss = -11032.880184556765
Iteration 2800: Loss = -11032.785751767362
Iteration 2900: Loss = -11032.782600323722
Iteration 3000: Loss = -11032.780754242058
Iteration 3100: Loss = -11032.779331629938
Iteration 3200: Loss = -11032.783254668964
1
Iteration 3300: Loss = -11032.77681923227
Iteration 3400: Loss = -11032.775630739845
Iteration 3500: Loss = -11032.780080213726
1
Iteration 3600: Loss = -11032.775815437113
2
Iteration 3700: Loss = -11032.772592889318
Iteration 3800: Loss = -11032.77146315775
Iteration 3900: Loss = -11032.770403331742
Iteration 4000: Loss = -11032.767925689499
Iteration 4100: Loss = -11032.767345706252
Iteration 4200: Loss = -11032.766466806686
Iteration 4300: Loss = -11032.765639097375
Iteration 4400: Loss = -11032.76477330253
Iteration 4500: Loss = -11032.763329982747
Iteration 4600: Loss = -11032.783345779339
1
Iteration 4700: Loss = -11032.75869150813
Iteration 4800: Loss = -11032.757679001585
Iteration 4900: Loss = -11032.757480240814
Iteration 5000: Loss = -11032.757279557782
Iteration 5100: Loss = -11032.75747357655
1
Iteration 5200: Loss = -11032.760557049294
2
Iteration 5300: Loss = -11032.760600867858
3
Iteration 5400: Loss = -11032.75650995701
Iteration 5500: Loss = -11032.754963077145
Iteration 5600: Loss = -11032.748947581784
Iteration 5700: Loss = -11032.748647910355
Iteration 5800: Loss = -11032.749664964484
1
Iteration 5900: Loss = -11032.746428615486
Iteration 6000: Loss = -11032.745192105216
Iteration 6100: Loss = -11032.746055110567
1
Iteration 6200: Loss = -11032.744840972071
Iteration 6300: Loss = -11032.744727247844
Iteration 6400: Loss = -11032.745938016333
1
Iteration 6500: Loss = -11032.744532728728
Iteration 6600: Loss = -11032.744577862828
1
Iteration 6700: Loss = -11032.74442945026
Iteration 6800: Loss = -11032.745909816324
1
Iteration 6900: Loss = -11032.744314452768
Iteration 7000: Loss = -11032.744235341792
Iteration 7100: Loss = -11032.744196096515
Iteration 7200: Loss = -11032.753627498663
1
Iteration 7300: Loss = -11032.74407672883
Iteration 7400: Loss = -11032.744375416372
1
Iteration 7500: Loss = -11032.744524226742
2
Iteration 7600: Loss = -11032.744328043684
3
Iteration 7700: Loss = -11032.74388395335
Iteration 7800: Loss = -11032.748273391506
1
Iteration 7900: Loss = -11032.746051231761
2
Iteration 8000: Loss = -11032.744885515069
3
Iteration 8100: Loss = -11032.777112394238
4
Iteration 8200: Loss = -11032.74682034648
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.7039, 0.2961],
        [0.3853, 0.6147]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5050, 0.4950], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2131, 0.0937],
         [0.6970, 0.2828]],

        [[0.6246, 0.0971],
         [0.5432, 0.6547]],

        [[0.6326, 0.0953],
         [0.5429, 0.6954]],

        [[0.5316, 0.0952],
         [0.6099, 0.5918]],

        [[0.5842, 0.0955],
         [0.5136, 0.6360]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 11
Adjusted Rand Index: 0.6046396195964233
Global Adjusted Rand Index: 0.3732931255920837
Average Adjusted Rand Index: 0.8582159569854152
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24549.048135049383
Iteration 100: Loss = -11259.607718114929
Iteration 200: Loss = -11255.800445515217
Iteration 300: Loss = -11254.37283911856
Iteration 400: Loss = -11251.251350635997
Iteration 500: Loss = -11174.437179984769
Iteration 600: Loss = -11057.167603064354
Iteration 700: Loss = -11037.786330793251
Iteration 800: Loss = -11037.415853304408
Iteration 900: Loss = -11037.362965461358
Iteration 1000: Loss = -11037.33367261076
Iteration 1100: Loss = -11037.30918994849
Iteration 1200: Loss = -11037.228925406269
Iteration 1300: Loss = -11037.00237448188
Iteration 1400: Loss = -11036.993769402343
Iteration 1500: Loss = -11036.988572134413
Iteration 1600: Loss = -11036.984352880641
Iteration 1700: Loss = -11036.980611037241
Iteration 1800: Loss = -11036.977173901487
Iteration 1900: Loss = -11036.974331576153
Iteration 2000: Loss = -11036.972079888299
Iteration 2100: Loss = -11036.970116842669
Iteration 2200: Loss = -11036.968443627587
Iteration 2300: Loss = -11036.966937842064
Iteration 2400: Loss = -11036.966124212231
Iteration 2500: Loss = -11036.965078898756
Iteration 2600: Loss = -11036.962487608635
Iteration 2700: Loss = -11036.962837743667
1
Iteration 2800: Loss = -11036.959698087403
Iteration 2900: Loss = -11036.957566304196
Iteration 3000: Loss = -11036.955617765649
Iteration 3100: Loss = -11036.950737695088
Iteration 3200: Loss = -11036.943898216534
Iteration 3300: Loss = -11036.925370698931
Iteration 3400: Loss = -11036.678592730028
Iteration 3500: Loss = -11035.976568796887
Iteration 3600: Loss = -11035.967325010131
Iteration 3700: Loss = -11035.946290703538
Iteration 3800: Loss = -11035.942106771345
Iteration 3900: Loss = -11035.942026419461
Iteration 4000: Loss = -11035.946839047598
1
Iteration 4100: Loss = -11035.940651746589
Iteration 4200: Loss = -11035.945162544112
1
Iteration 4300: Loss = -11035.940964477624
2
Iteration 4400: Loss = -11035.940084205955
Iteration 4500: Loss = -11035.939889818705
Iteration 4600: Loss = -11035.94062286988
1
Iteration 4700: Loss = -11035.939327453201
Iteration 4800: Loss = -11035.939127221314
Iteration 4900: Loss = -11035.938859925367
Iteration 5000: Loss = -11035.938409636095
Iteration 5100: Loss = -11035.937574372008
Iteration 5200: Loss = -11035.942194652212
1
Iteration 5300: Loss = -11035.937239586518
Iteration 5400: Loss = -11035.937051438623
Iteration 5500: Loss = -11035.93672277262
Iteration 5600: Loss = -11035.936050456488
Iteration 5700: Loss = -11035.942364165721
1
Iteration 5800: Loss = -11035.671480864265
Iteration 5900: Loss = -11035.671332080045
Iteration 6000: Loss = -11035.684807449836
1
Iteration 6100: Loss = -11035.67109644654
Iteration 6200: Loss = -11035.67049593999
Iteration 6300: Loss = -11035.66485422282
Iteration 6400: Loss = -11035.66414626503
Iteration 6500: Loss = -11035.692266019618
1
Iteration 6600: Loss = -11035.663980657127
Iteration 6700: Loss = -11035.663928077473
Iteration 6800: Loss = -11035.663693676704
Iteration 6900: Loss = -11035.667512709148
1
Iteration 7000: Loss = -11035.665581530628
2
Iteration 7100: Loss = -11035.663185797432
Iteration 7200: Loss = -11035.662545236457
Iteration 7300: Loss = -11035.662403125769
Iteration 7400: Loss = -11035.662182121378
Iteration 7500: Loss = -11035.662153053257
Iteration 7600: Loss = -11035.668789814761
1
Iteration 7700: Loss = -11035.668809379036
2
Iteration 7800: Loss = -11035.681234513755
3
Iteration 7900: Loss = -11035.662414466735
4
Iteration 8000: Loss = -11035.662123582648
Iteration 8100: Loss = -11035.662058550657
Iteration 8200: Loss = -11035.662517547127
1
Iteration 8300: Loss = -11035.663769462251
2
Iteration 8400: Loss = -11035.661957653583
Iteration 8500: Loss = -11035.665158854803
1
Iteration 8600: Loss = -11035.686925002974
2
Iteration 8700: Loss = -11035.66178740931
Iteration 8800: Loss = -11035.661860028575
1
Iteration 8900: Loss = -11035.676113068223
2
Iteration 9000: Loss = -11035.664519789709
3
Iteration 9100: Loss = -11035.663727179292
4
Iteration 9200: Loss = -11035.663801108309
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.7399, 0.2601],
        [0.3260, 0.6740]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0537, 0.9463], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3018, 0.1138],
         [0.6250, 0.1853]],

        [[0.5228, 0.0985],
         [0.5903, 0.5107]],

        [[0.6657, 0.0964],
         [0.6267, 0.5530]],

        [[0.6188, 0.0961],
         [0.6226, 0.6952]],

        [[0.5563, 0.0974],
         [0.7242, 0.7217]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.6201982810534646
Average Adjusted Rand Index: 0.7838471166244191
10978.031958504685
[0.6201982810534646, -0.0010693500176705288, 0.3732931255920837, 0.6201982810534646] [0.7838471166244191, -0.0019711698006338292, 0.8582159569854152, 0.7838471166244191] [11035.66207780447, 11256.398320920556, 11032.74682034648, 11035.663801108309]
-----------------------------------------------------------------------------------------
This iteration is 17
True Objective function: Loss = -11279.387529873846
Iteration 0: Loss = -11620.121016008366
Iteration 10: Loss = -11620.12101600882
1
Iteration 20: Loss = -11620.121061292375
2
Iteration 30: Loss = -11617.266704721389
Iteration 40: Loss = -11610.975867720252
Iteration 50: Loss = -11609.566154083488
Iteration 60: Loss = -11608.73835050996
Iteration 70: Loss = -11606.32919212184
Iteration 80: Loss = -11580.536168728971
Iteration 90: Loss = -11331.080522498096
Iteration 100: Loss = -11309.767043630181
Iteration 110: Loss = -11308.477017069386
Iteration 120: Loss = -11307.79447194401
Iteration 130: Loss = -11307.318832955301
Iteration 140: Loss = -11307.22720237821
Iteration 150: Loss = -11307.216453713234
Iteration 160: Loss = -11307.214838727125
Iteration 170: Loss = -11307.214343656156
Iteration 180: Loss = -11307.214121930732
Iteration 190: Loss = -11307.21393019041
Iteration 200: Loss = -11307.213782766808
Iteration 210: Loss = -11307.213634961947
Iteration 220: Loss = -11307.213523560742
Iteration 230: Loss = -11307.21341353428
Iteration 240: Loss = -11307.213364578554
Iteration 250: Loss = -11307.21325099046
Iteration 260: Loss = -11307.213228638315
Iteration 270: Loss = -11307.213168108308
Iteration 280: Loss = -11307.213124459186
Iteration 290: Loss = -11307.213088685967
Iteration 300: Loss = -11307.213028702668
Iteration 310: Loss = -11307.21299753731
Iteration 320: Loss = -11307.212970519868
Iteration 330: Loss = -11307.212948211924
Iteration 340: Loss = -11307.21292084314
Iteration 350: Loss = -11307.212902269172
Iteration 360: Loss = -11307.212870058644
Iteration 370: Loss = -11307.212850374253
Iteration 380: Loss = -11307.212843692452
Iteration 390: Loss = -11307.21280983003
Iteration 400: Loss = -11307.212830999302
1
Iteration 410: Loss = -11307.212838586704
2
Iteration 420: Loss = -11307.212811025098
3
Stopping early at iteration 420 due to no improvement.
pi: tensor([[0.6629, 0.3371],
        [0.3619, 0.6381]], dtype=torch.float64)
alpha: tensor([0.4976, 0.5024])
beta: tensor([[[0.2136, 0.1079],
         [0.0671, 0.2914]],

        [[0.2523, 0.0879],
         [0.1815, 0.0874]],

        [[0.0016, 0.0892],
         [0.9070, 0.7663]],

        [[0.2965, 0.0985],
         [0.3841, 0.5722]],

        [[0.0689, 0.1042],
         [0.1091, 0.2093]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 14
Adjusted Rand Index: 0.5138096559663675
Global Adjusted Rand Index: 0.388154879754242
Average Adjusted Rand Index: 0.8395590078951981
pi: tensor([[1.0000e+00, 4.2593e-53],
        [       nan,        nan]], dtype=torch.float64)
alpha: tensor([1., 0.])
beta: tensor([[[0.1770,    nan],
         [0.6751,    nan]],

        [[0.7535,    nan],
         [0.1406, 0.9610]],

        [[0.0092,    nan],
         [0.3860, 0.0368]],

        [[0.5171,    nan],
         [0.5998, 0.0643]],

        [[0.8958,    nan],
         [0.0248, 0.5884]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21711.875128879317
Iteration 100: Loss = -11620.344899614805
Iteration 200: Loss = -11619.55926724208
Iteration 300: Loss = -11614.851145376158
Iteration 400: Loss = -11607.814403554097
Iteration 500: Loss = -11525.52134175977
Iteration 600: Loss = -11300.655590441656
Iteration 700: Loss = -11265.89145807956
Iteration 800: Loss = -11262.969094849874
Iteration 900: Loss = -11262.733073928659
Iteration 1000: Loss = -11262.617598261237
Iteration 1100: Loss = -11256.365449243413
Iteration 1200: Loss = -11256.320442384815
Iteration 1300: Loss = -11256.273586369885
Iteration 1400: Loss = -11256.246432358359
Iteration 1500: Loss = -11256.2248768212
Iteration 1600: Loss = -11256.183136335854
Iteration 1700: Loss = -11255.287073993604
Iteration 1800: Loss = -11255.11784006453
Iteration 1900: Loss = -11254.981024268376
Iteration 2000: Loss = -11253.368795442972
Iteration 2100: Loss = -11253.35738656499
Iteration 2200: Loss = -11253.351438146468
Iteration 2300: Loss = -11253.345696991959
Iteration 2400: Loss = -11253.339202566027
Iteration 2500: Loss = -11253.332436286983
Iteration 2600: Loss = -11253.328024793018
Iteration 2700: Loss = -11253.323562570282
Iteration 2800: Loss = -11253.32115212337
Iteration 2900: Loss = -11253.318476736917
Iteration 3000: Loss = -11253.316798627078
Iteration 3100: Loss = -11253.31888076601
1
Iteration 3200: Loss = -11253.31393466097
Iteration 3300: Loss = -11253.312702391675
Iteration 3400: Loss = -11253.31335079389
1
Iteration 3500: Loss = -11253.310534823884
Iteration 3600: Loss = -11253.309564773906
Iteration 3700: Loss = -11253.308761796354
Iteration 3800: Loss = -11253.307775012296
Iteration 3900: Loss = -11253.307772462456
Iteration 4000: Loss = -11253.305805093501
Iteration 4100: Loss = -11253.304631012306
Iteration 4200: Loss = -11253.30522724869
1
Iteration 4300: Loss = -11253.30424061427
Iteration 4400: Loss = -11253.301770335953
Iteration 4500: Loss = -11253.300922750166
Iteration 4600: Loss = -11253.300427741973
Iteration 4700: Loss = -11253.2930746755
Iteration 4800: Loss = -11253.289627917162
Iteration 4900: Loss = -11253.28913475596
Iteration 5000: Loss = -11253.288834939323
Iteration 5100: Loss = -11253.28838010346
Iteration 5200: Loss = -11253.288636155079
1
Iteration 5300: Loss = -11253.28807742718
Iteration 5400: Loss = -11253.288658759311
1
Iteration 5500: Loss = -11253.28760552675
Iteration 5600: Loss = -11253.289259193569
1
Iteration 5700: Loss = -11253.286869449897
Iteration 5800: Loss = -11253.28666942061
Iteration 5900: Loss = -11253.295934289074
1
Iteration 6000: Loss = -11253.286245056306
Iteration 6100: Loss = -11253.286317850601
1
Iteration 6200: Loss = -11253.286645461007
2
Iteration 6300: Loss = -11253.284436382593
Iteration 6400: Loss = -11253.288335379073
1
Iteration 6500: Loss = -11253.28327528085
Iteration 6600: Loss = -11253.283355255506
1
Iteration 6700: Loss = -11253.283059302297
Iteration 6800: Loss = -11253.283020078248
Iteration 6900: Loss = -11253.28286308254
Iteration 7000: Loss = -11253.282711000937
Iteration 7100: Loss = -11253.282846169732
1
Iteration 7200: Loss = -11253.285840316852
2
Iteration 7300: Loss = -11253.282232258538
Iteration 7400: Loss = -11253.282201396245
Iteration 7500: Loss = -11253.282000993551
Iteration 7600: Loss = -11253.282912134917
1
Iteration 7700: Loss = -11253.281823223604
Iteration 7800: Loss = -11253.28446451427
1
Iteration 7900: Loss = -11253.281406581706
Iteration 8000: Loss = -11253.281290663195
Iteration 8100: Loss = -11253.286774247961
1
Iteration 8200: Loss = -11253.284652702814
2
Iteration 8300: Loss = -11253.409555697763
3
Iteration 8400: Loss = -11253.280855141507
Iteration 8500: Loss = -11253.283735938712
1
Iteration 8600: Loss = -11253.283611719336
2
Iteration 8700: Loss = -11253.28080932088
Iteration 8800: Loss = -11253.30996263389
1
Iteration 8900: Loss = -11253.280546112563
Iteration 9000: Loss = -11253.280533504787
Iteration 9100: Loss = -11253.28046953366
Iteration 9200: Loss = -11253.280678652098
1
Iteration 9300: Loss = -11253.280369346292
Iteration 9400: Loss = -11253.280351308696
Iteration 9500: Loss = -11253.280304095737
Iteration 9600: Loss = -11253.280178328403
Iteration 9700: Loss = -11253.279982469543
Iteration 9800: Loss = -11253.279806857
Iteration 9900: Loss = -11253.27968668394
Iteration 10000: Loss = -11253.291394375025
1
Iteration 10100: Loss = -11253.27966791699
Iteration 10200: Loss = -11253.279673914923
1
Iteration 10300: Loss = -11253.279908857578
2
Iteration 10400: Loss = -11253.27961131278
Iteration 10500: Loss = -11253.283488171668
1
Iteration 10600: Loss = -11253.279581618428
Iteration 10700: Loss = -11253.31882003149
1
Iteration 10800: Loss = -11253.279487509466
Iteration 10900: Loss = -11253.279434364535
Iteration 11000: Loss = -11253.279495234638
1
Iteration 11100: Loss = -11253.279392785978
Iteration 11200: Loss = -11253.279554772336
1
Iteration 11300: Loss = -11253.357282483248
2
Iteration 11400: Loss = -11253.279405507983
3
Iteration 11500: Loss = -11253.336954432272
4
Iteration 11600: Loss = -11253.279313806574
Iteration 11700: Loss = -11253.279297010477
Iteration 11800: Loss = -11253.279438358717
1
Iteration 11900: Loss = -11253.279292885785
Iteration 12000: Loss = -11253.351556201647
1
Iteration 12100: Loss = -11253.27919737694
Iteration 12200: Loss = -11253.279154346026
Iteration 12300: Loss = -11253.28410453371
1
Iteration 12400: Loss = -11253.27912086686
Iteration 12500: Loss = -11253.27910409278
Iteration 12600: Loss = -11253.279861132254
1
Iteration 12700: Loss = -11253.279102060449
Iteration 12800: Loss = -11253.279087890967
Iteration 12900: Loss = -11253.28658117585
1
Iteration 13000: Loss = -11253.279400009415
2
Iteration 13100: Loss = -11253.337642655628
3
Iteration 13200: Loss = -11253.286215822096
4
Iteration 13300: Loss = -11253.284436017762
5
Stopping early at iteration 13300 due to no improvement.
pi: tensor([[0.7368, 0.2632],
        [0.2284, 0.7716]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4801, 0.5199], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2078, 0.1069],
         [0.7067, 0.2993]],

        [[0.5925, 0.0881],
         [0.6474, 0.5979]],

        [[0.6216, 0.0884],
         [0.6592, 0.6294]],

        [[0.6103, 0.0990],
         [0.6579, 0.5795]],

        [[0.5024, 0.1066],
         [0.7216, 0.6542]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080620079101718
Global Adjusted Rand Index: 0.9061158183566643
Average Adjusted Rand Index: 0.9062497648781696
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22550.659884219032
Iteration 100: Loss = -11620.726853643984
Iteration 200: Loss = -11617.84957811585
Iteration 300: Loss = -11612.82805825187
Iteration 400: Loss = -11609.027593689943
Iteration 500: Loss = -11585.716210307066
Iteration 600: Loss = -11364.869514878621
Iteration 700: Loss = -11293.108042731663
Iteration 800: Loss = -11280.96566192722
Iteration 900: Loss = -11263.879342147311
Iteration 1000: Loss = -11263.656281845113
Iteration 1100: Loss = -11263.526244395549
Iteration 1200: Loss = -11261.952632693756
Iteration 1300: Loss = -11261.698099256286
Iteration 1400: Loss = -11261.66680882475
Iteration 1500: Loss = -11261.638937142245
Iteration 1600: Loss = -11261.607083124361
Iteration 1700: Loss = -11261.590400841318
Iteration 1800: Loss = -11261.572964497971
Iteration 1900: Loss = -11260.485940139002
Iteration 2000: Loss = -11253.79285521856
Iteration 2100: Loss = -11253.766735212685
Iteration 2200: Loss = -11253.760167106633
Iteration 2300: Loss = -11253.754753488496
Iteration 2400: Loss = -11253.749440887277
Iteration 2500: Loss = -11253.734387501696
Iteration 2600: Loss = -11253.388007601447
Iteration 2700: Loss = -11253.38396933333
Iteration 2800: Loss = -11253.38160043833
Iteration 2900: Loss = -11253.375215153801
Iteration 3000: Loss = -11253.371325379605
Iteration 3100: Loss = -11253.368723535235
Iteration 3200: Loss = -11253.366149763735
Iteration 3300: Loss = -11253.36397119545
Iteration 3400: Loss = -11253.348181189478
Iteration 3500: Loss = -11253.324437250627
Iteration 3600: Loss = -11253.322357976822
Iteration 3700: Loss = -11253.319631767927
Iteration 3800: Loss = -11253.318059803001
Iteration 3900: Loss = -11253.32483407649
1
Iteration 4000: Loss = -11253.316514322416
Iteration 4100: Loss = -11253.325503552915
1
Iteration 4200: Loss = -11253.315019922375
Iteration 4300: Loss = -11253.314472747456
Iteration 4400: Loss = -11253.313536977803
Iteration 4500: Loss = -11253.31249752298
Iteration 4600: Loss = -11253.30977519028
Iteration 4700: Loss = -11253.305826419999
Iteration 4800: Loss = -11253.305486473719
Iteration 4900: Loss = -11253.304820201101
Iteration 5000: Loss = -11253.30239074321
Iteration 5100: Loss = -11253.291874065146
Iteration 5200: Loss = -11253.288784149774
Iteration 5300: Loss = -11253.289092532388
1
Iteration 5400: Loss = -11253.2879581411
Iteration 5500: Loss = -11253.287672141607
Iteration 5600: Loss = -11253.28757649319
Iteration 5700: Loss = -11253.304494544336
1
Iteration 5800: Loss = -11253.287075255012
Iteration 5900: Loss = -11253.287115689829
1
Iteration 6000: Loss = -11253.287069659576
Iteration 6100: Loss = -11253.287501581965
1
Iteration 6200: Loss = -11253.289976346397
2
Iteration 6300: Loss = -11253.287650031096
3
Iteration 6400: Loss = -11253.286200380515
Iteration 6500: Loss = -11253.286093591456
Iteration 6600: Loss = -11253.292092414133
1
Iteration 6700: Loss = -11253.285804627112
Iteration 6800: Loss = -11253.285721798904
Iteration 6900: Loss = -11253.285706261195
Iteration 7000: Loss = -11253.291139845478
1
Iteration 7100: Loss = -11253.288907821689
2
Iteration 7200: Loss = -11253.285444057821
Iteration 7300: Loss = -11253.285086617549
Iteration 7400: Loss = -11253.303572280061
1
Iteration 7500: Loss = -11253.284671443369
Iteration 7600: Loss = -11253.284506971533
Iteration 7700: Loss = -11253.284161629266
Iteration 7800: Loss = -11253.283441770473
Iteration 7900: Loss = -11253.336330525515
1
Iteration 8000: Loss = -11253.282878474793
Iteration 8100: Loss = -11253.282617432125
Iteration 8200: Loss = -11253.572372380135
1
Iteration 8300: Loss = -11253.282381718172
Iteration 8400: Loss = -11253.282355769335
Iteration 8500: Loss = -11253.282290583953
Iteration 8600: Loss = -11253.282317082436
1
Iteration 8700: Loss = -11253.282252269328
Iteration 8800: Loss = -11253.282171135585
Iteration 8900: Loss = -11253.2834711119
1
Iteration 9000: Loss = -11253.282063449966
Iteration 9100: Loss = -11253.281992730848
Iteration 9200: Loss = -11253.28690385
1
Iteration 9300: Loss = -11253.281832208346
Iteration 9400: Loss = -11253.281809273913
Iteration 9500: Loss = -11253.295291203274
1
Iteration 9600: Loss = -11253.280736377366
Iteration 9700: Loss = -11253.280119845083
Iteration 9800: Loss = -11253.279985649857
Iteration 9900: Loss = -11253.279596026598
Iteration 10000: Loss = -11253.279348083237
Iteration 10100: Loss = -11253.283597634732
1
Iteration 10200: Loss = -11253.27923847452
Iteration 10300: Loss = -11253.287450245356
1
Iteration 10400: Loss = -11253.279113868954
Iteration 10500: Loss = -11253.30687540061
1
Iteration 10600: Loss = -11253.279067624746
Iteration 10700: Loss = -11253.279047293288
Iteration 10800: Loss = -11253.279053932982
1
Iteration 10900: Loss = -11253.279018451285
Iteration 11000: Loss = -11253.303424438514
1
Iteration 11100: Loss = -11253.278955366051
Iteration 11200: Loss = -11253.278961454944
1
Iteration 11300: Loss = -11253.279509730717
2
Iteration 11400: Loss = -11253.278881779883
Iteration 11500: Loss = -11253.278614242037
Iteration 11600: Loss = -11253.278407667402
Iteration 11700: Loss = -11253.278200570561
Iteration 11800: Loss = -11253.277848028762
Iteration 11900: Loss = -11253.277851117158
1
Iteration 12000: Loss = -11253.277748207458
Iteration 12100: Loss = -11253.401577757586
1
Iteration 12200: Loss = -11253.27773951668
Iteration 12300: Loss = -11253.277727252365
Iteration 12400: Loss = -11253.280169586347
1
Iteration 12500: Loss = -11253.277718837067
Iteration 12600: Loss = -11253.294396275114
1
Iteration 12700: Loss = -11253.277743331115
2
Iteration 12800: Loss = -11253.277749359411
3
Iteration 12900: Loss = -11253.287642576624
4
Iteration 13000: Loss = -11253.277745169169
5
Stopping early at iteration 13000 due to no improvement.
pi: tensor([[0.7710, 0.2290],
        [0.2625, 0.7375]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5191, 0.4809], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2996, 0.1071],
         [0.5801, 0.2075]],

        [[0.7166, 0.0882],
         [0.6429, 0.6526]],

        [[0.6789, 0.0886],
         [0.5135, 0.5189]],

        [[0.6665, 0.0992],
         [0.5182, 0.6938]],

        [[0.7231, 0.1068],
         [0.5642, 0.7202]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 2
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080620079101718
Global Adjusted Rand Index: 0.9061158183566643
Average Adjusted Rand Index: 0.9062497648781696
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22259.58983377826
Iteration 100: Loss = -11620.094484218855
Iteration 200: Loss = -11618.791680639568
Iteration 300: Loss = -11614.993542323968
Iteration 400: Loss = -11609.462068095103
Iteration 500: Loss = -11592.144765958565
Iteration 600: Loss = -11402.567709732868
Iteration 700: Loss = -11297.87808659917
Iteration 800: Loss = -11271.84493825855
Iteration 900: Loss = -11257.96719984996
Iteration 1000: Loss = -11256.839200331033
Iteration 1100: Loss = -11256.640228558877
Iteration 1200: Loss = -11256.497480114822
Iteration 1300: Loss = -11256.275355208612
Iteration 1400: Loss = -11255.410323744103
Iteration 1500: Loss = -11255.16060094126
Iteration 1600: Loss = -11253.557244910467
Iteration 1700: Loss = -11253.477855535886
Iteration 1800: Loss = -11253.452790508274
Iteration 1900: Loss = -11253.432915455756
Iteration 2000: Loss = -11253.416414179164
Iteration 2100: Loss = -11253.40227783238
Iteration 2200: Loss = -11253.389747849622
Iteration 2300: Loss = -11253.378259217781
Iteration 2400: Loss = -11253.36758302152
Iteration 2500: Loss = -11253.358485516905
Iteration 2600: Loss = -11253.350888491448
Iteration 2700: Loss = -11253.344682614059
Iteration 2800: Loss = -11253.339465072686
Iteration 2900: Loss = -11253.334871391107
Iteration 3000: Loss = -11253.3338707972
Iteration 3100: Loss = -11253.326637694421
Iteration 3200: Loss = -11253.322395708201
Iteration 3300: Loss = -11253.316999468314
Iteration 3400: Loss = -11253.31263951488
Iteration 3500: Loss = -11253.308259268428
Iteration 3600: Loss = -11253.305959894946
Iteration 3700: Loss = -11253.303859509206
Iteration 3800: Loss = -11253.301906898247
Iteration 3900: Loss = -11253.3011510923
Iteration 4000: Loss = -11253.29863908006
Iteration 4100: Loss = -11253.297781195246
Iteration 4200: Loss = -11253.295976006768
Iteration 4300: Loss = -11253.3006212102
1
Iteration 4400: Loss = -11253.293728953337
Iteration 4500: Loss = -11253.292751458874
Iteration 4600: Loss = -11253.293404754497
1
Iteration 4700: Loss = -11253.291046702452
Iteration 4800: Loss = -11253.290367407879
Iteration 4900: Loss = -11253.289513552478
Iteration 5000: Loss = -11253.28888551793
Iteration 5100: Loss = -11253.292928693012
1
Iteration 5200: Loss = -11253.287635735116
Iteration 5300: Loss = -11253.288879550182
1
Iteration 5400: Loss = -11253.286951618473
Iteration 5500: Loss = -11253.286393561813
Iteration 5600: Loss = -11253.296045860989
1
Iteration 5700: Loss = -11253.288895577913
2
Iteration 5800: Loss = -11253.28440152424
Iteration 5900: Loss = -11253.286555465816
1
Iteration 6000: Loss = -11253.291131372045
2
Iteration 6100: Loss = -11253.28290819184
Iteration 6200: Loss = -11253.286129290918
1
Iteration 6300: Loss = -11253.282342799705
Iteration 6400: Loss = -11253.282500289875
1
Iteration 6500: Loss = -11253.28177049708
Iteration 6600: Loss = -11253.28108707098
Iteration 6700: Loss = -11253.280675202055
Iteration 6800: Loss = -11253.280354313401
Iteration 6900: Loss = -11253.2850507026
1
Iteration 7000: Loss = -11253.27979371194
Iteration 7100: Loss = -11253.27966058028
Iteration 7200: Loss = -11253.279731308434
1
Iteration 7300: Loss = -11253.279136826292
Iteration 7400: Loss = -11253.283467331416
1
Iteration 7500: Loss = -11253.27981396237
2
Iteration 7600: Loss = -11253.279871177281
3
Iteration 7700: Loss = -11253.278628528424
Iteration 7800: Loss = -11253.278411433757
Iteration 7900: Loss = -11253.27819965556
Iteration 8000: Loss = -11253.278068968557
Iteration 8100: Loss = -11253.277515765023
Iteration 8200: Loss = -11253.28032972705
1
Iteration 8300: Loss = -11253.276670536778
Iteration 8400: Loss = -11253.276506309405
Iteration 8500: Loss = -11253.296023290912
1
Iteration 8600: Loss = -11253.27633250003
Iteration 8700: Loss = -11253.276225268102
Iteration 8800: Loss = -11253.276171862355
Iteration 8900: Loss = -11253.276227802351
1
Iteration 9000: Loss = -11253.276069651638
Iteration 9100: Loss = -11253.275969609675
Iteration 9200: Loss = -11253.276400307897
1
Iteration 9300: Loss = -11253.275888602613
Iteration 9400: Loss = -11253.275807716273
Iteration 9500: Loss = -11253.276051684848
1
Iteration 9600: Loss = -11253.27569605273
Iteration 9700: Loss = -11253.279342543354
1
Iteration 9800: Loss = -11253.2756629253
Iteration 9900: Loss = -11253.275568214303
Iteration 10000: Loss = -11253.276178581995
1
Iteration 10100: Loss = -11253.280083481244
2
Iteration 10200: Loss = -11253.275374574121
Iteration 10300: Loss = -11253.381379454493
1
Iteration 10400: Loss = -11253.275244286582
Iteration 10500: Loss = -11253.275188672053
Iteration 10600: Loss = -11253.276206595852
1
Iteration 10700: Loss = -11253.275202541343
2
Iteration 10800: Loss = -11253.279357075784
3
Iteration 10900: Loss = -11253.276458788741
4
Iteration 11000: Loss = -11253.27548533218
5
Stopping early at iteration 11000 due to no improvement.
pi: tensor([[0.7378, 0.2622],
        [0.2288, 0.7712]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4808, 0.5192], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2075, 0.1070],
         [0.5723, 0.2996]],

        [[0.6858, 0.0883],
         [0.5457, 0.5194]],

        [[0.7262, 0.0886],
         [0.5106, 0.6708]],

        [[0.6300, 0.0992],
         [0.5778, 0.5680]],

        [[0.6166, 0.1068],
         [0.5759, 0.7277]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080620079101718
Global Adjusted Rand Index: 0.9061158183566643
Average Adjusted Rand Index: 0.9062497648781696
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22739.996694702273
Iteration 100: Loss = -11620.76354048858
Iteration 200: Loss = -11619.370641116697
Iteration 300: Loss = -11615.13711893876
Iteration 400: Loss = -11608.963835622095
Iteration 500: Loss = -11584.22402806217
Iteration 600: Loss = -11387.504391161292
Iteration 700: Loss = -11304.276146174585
Iteration 800: Loss = -11281.377151415696
Iteration 900: Loss = -11279.967821377366
Iteration 1000: Loss = -11276.850019435391
Iteration 1100: Loss = -11274.686619265589
Iteration 1200: Loss = -11274.51403793142
Iteration 1300: Loss = -11274.303813370212
Iteration 1400: Loss = -11264.320256642952
Iteration 1500: Loss = -11262.874445047122
Iteration 1600: Loss = -11262.300011442854
Iteration 1700: Loss = -11262.272068153738
Iteration 1800: Loss = -11262.248446201134
Iteration 1900: Loss = -11262.227443245774
Iteration 2000: Loss = -11262.210633578832
Iteration 2100: Loss = -11262.195613739339
Iteration 2200: Loss = -11262.170514020785
Iteration 2300: Loss = -11261.642402512842
Iteration 2400: Loss = -11261.610959858313
Iteration 2500: Loss = -11261.606404889812
Iteration 2600: Loss = -11261.598902887472
Iteration 2700: Loss = -11261.594362362395
Iteration 2800: Loss = -11261.590434313159
Iteration 2900: Loss = -11261.586922465036
Iteration 3000: Loss = -11261.583702018037
Iteration 3100: Loss = -11261.580836844887
Iteration 3200: Loss = -11261.578055717646
Iteration 3300: Loss = -11261.57580520598
Iteration 3400: Loss = -11261.57590624232
1
Iteration 3500: Loss = -11261.571460163392
Iteration 3600: Loss = -11261.567961448327
Iteration 3700: Loss = -11261.560625370317
Iteration 3800: Loss = -11261.557331196242
Iteration 3900: Loss = -11261.558310873808
1
Iteration 4000: Loss = -11261.554371796257
Iteration 4100: Loss = -11261.554033392285
Iteration 4200: Loss = -11261.552361288874
Iteration 4300: Loss = -11261.551574042556
Iteration 4400: Loss = -11261.549598174352
Iteration 4500: Loss = -11261.549009663586
Iteration 4600: Loss = -11261.551051132345
1
Iteration 4700: Loss = -11261.54638335726
Iteration 4800: Loss = -11261.544453650695
Iteration 4900: Loss = -11261.534889963335
Iteration 5000: Loss = -11261.533957274098
Iteration 5100: Loss = -11261.535686241048
1
Iteration 5200: Loss = -11261.53230932129
Iteration 5300: Loss = -11261.531759937572
Iteration 5400: Loss = -11261.531570690528
Iteration 5500: Loss = -11261.53154420998
Iteration 5600: Loss = -11261.529962949518
Iteration 5700: Loss = -11261.520724476226
Iteration 5800: Loss = -11253.290577611087
Iteration 5900: Loss = -11253.288123862705
Iteration 6000: Loss = -11253.28831619204
1
Iteration 6100: Loss = -11253.287331407939
Iteration 6200: Loss = -11253.286389030936
Iteration 6300: Loss = -11253.286072827945
Iteration 6400: Loss = -11253.285109716953
Iteration 6500: Loss = -11253.28484399851
Iteration 6600: Loss = -11253.285356691204
1
Iteration 6700: Loss = -11253.284619988372
Iteration 6800: Loss = -11253.283996137958
Iteration 6900: Loss = -11253.283766264545
Iteration 7000: Loss = -11253.283518242763
Iteration 7100: Loss = -11253.289814104906
1
Iteration 7200: Loss = -11253.282926231897
Iteration 7300: Loss = -11253.292582024296
1
Iteration 7400: Loss = -11253.282130673993
Iteration 7500: Loss = -11253.281202820182
Iteration 7600: Loss = -11253.280461548993
Iteration 7700: Loss = -11253.280203309328
Iteration 7800: Loss = -11253.297059060887
1
Iteration 7900: Loss = -11253.279854573246
Iteration 8000: Loss = -11253.279651365401
Iteration 8100: Loss = -11253.326478088446
1
Iteration 8200: Loss = -11253.27940189003
Iteration 8300: Loss = -11253.279289285245
Iteration 8400: Loss = -11253.279235317354
Iteration 8500: Loss = -11253.279123546496
Iteration 8600: Loss = -11253.279041598518
Iteration 8700: Loss = -11253.279227714482
1
Iteration 8800: Loss = -11253.278833132468
Iteration 8900: Loss = -11253.278766301039
Iteration 9000: Loss = -11253.278668623472
Iteration 9100: Loss = -11253.278761340354
1
Iteration 9200: Loss = -11253.278564969676
Iteration 9300: Loss = -11253.278556193882
Iteration 9400: Loss = -11253.278977324873
1
Iteration 9500: Loss = -11253.278477526157
Iteration 9600: Loss = -11253.296467782498
1
Iteration 9700: Loss = -11253.278417724228
Iteration 9800: Loss = -11253.27944734968
1
Iteration 9900: Loss = -11253.278366934122
Iteration 10000: Loss = -11253.2788713588
1
Iteration 10100: Loss = -11253.29232414917
2
Iteration 10200: Loss = -11253.278308452822
Iteration 10300: Loss = -11253.279066060803
1
Iteration 10400: Loss = -11253.27823294951
Iteration 10500: Loss = -11253.296010475433
1
Iteration 10600: Loss = -11253.278213958863
Iteration 10700: Loss = -11253.278233243469
1
Iteration 10800: Loss = -11253.279351401226
2
Iteration 10900: Loss = -11253.278152109413
Iteration 11000: Loss = -11253.27812969657
Iteration 11100: Loss = -11253.278827235219
1
Iteration 11200: Loss = -11253.278119147093
Iteration 11300: Loss = -11253.28942856378
1
Iteration 11400: Loss = -11253.278110063866
Iteration 11500: Loss = -11253.278091620436
Iteration 11600: Loss = -11253.278248601142
1
Iteration 11700: Loss = -11253.278057663323
Iteration 11800: Loss = -11253.292037215224
1
Iteration 11900: Loss = -11253.277936235698
Iteration 12000: Loss = -11253.469152386124
1
Iteration 12100: Loss = -11253.27785859987
Iteration 12200: Loss = -11253.396456132283
1
Iteration 12300: Loss = -11253.277861590754
2
Iteration 12400: Loss = -11253.30196196402
3
Iteration 12500: Loss = -11253.277840246528
Iteration 12600: Loss = -11253.408015781833
1
Iteration 12700: Loss = -11253.277865996599
2
Iteration 12800: Loss = -11253.304676183341
3
Iteration 12900: Loss = -11253.277929490398
4
Iteration 13000: Loss = -11253.277875365886
5
Stopping early at iteration 13000 due to no improvement.
pi: tensor([[0.7711, 0.2289],
        [0.2623, 0.7377]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5191, 0.4809], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2996, 0.1071],
         [0.5632, 0.2075]],

        [[0.5934, 0.0883],
         [0.6698, 0.6428]],

        [[0.6743, 0.0886],
         [0.5468, 0.5188]],

        [[0.6097, 0.0992],
         [0.6355, 0.5269]],

        [[0.7131, 0.1068],
         [0.6082, 0.6386]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 2
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080620079101718
Global Adjusted Rand Index: 0.9061158183566643
Average Adjusted Rand Index: 0.9062497648781696
11279.387529873846
[0.9061158183566643, 0.9061158183566643, 0.9061158183566643, 0.9061158183566643] [0.9062497648781696, 0.9062497648781696, 0.9062497648781696, 0.9062497648781696] [11253.284436017762, 11253.277745169169, 11253.27548533218, 11253.277875365886]
-----------------------------------------------------------------------------------------
This iteration is 18
True Objective function: Loss = -11267.733048284359
Iteration 0: Loss = -11549.682202521062
Iteration 10: Loss = -11549.355025748806
Iteration 20: Loss = -11523.338510092593
Iteration 30: Loss = -11519.32663784295
Iteration 40: Loss = -11456.312968392362
Iteration 50: Loss = -11255.384528325212
Iteration 60: Loss = -11255.349771420912
Iteration 70: Loss = -11255.349341502153
Iteration 80: Loss = -11255.349353035108
1
Iteration 90: Loss = -11255.349337581481
Iteration 100: Loss = -11255.349336760399
Iteration 110: Loss = -11255.349341126603
1
Iteration 120: Loss = -11255.349333557378
Iteration 130: Loss = -11255.3493385228
1
Iteration 140: Loss = -11255.349334600473
2
Iteration 150: Loss = -11255.349344234077
3
Stopping early at iteration 150 due to no improvement.
pi: tensor([[0.7248, 0.2752],
        [0.3036, 0.6964]], dtype=torch.float64)
alpha: tensor([0.5180, 0.4820])
beta: tensor([[[0.2950, 0.1025],
         [0.3865, 0.1915]],

        [[0.8540, 0.0992],
         [0.9162, 0.7096]],

        [[0.4043, 0.1017],
         [0.5226, 0.1936]],

        [[0.3988, 0.1125],
         [0.8404, 0.9384]],

        [[0.9888, 0.0959],
         [0.9507, 0.1398]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9446733111422435
Average Adjusted Rand Index: 0.9446436833840768
Iteration 0: Loss = -11779.652878019331
Iteration 10: Loss = -11522.164103242594
Iteration 20: Loss = -11263.444229925586
Iteration 30: Loss = -11255.351285178258
Iteration 40: Loss = -11255.349368477559
Iteration 50: Loss = -11255.349336533773
Iteration 60: Loss = -11255.34934273382
1
Iteration 70: Loss = -11255.349342733818
2
Iteration 80: Loss = -11255.349342733818
3
Stopping early at iteration 80 due to no improvement.
pi: tensor([[0.6964, 0.3036],
        [0.2752, 0.7248]], dtype=torch.float64)
alpha: tensor([0.4820, 0.5180])
beta: tensor([[[0.1915, 0.1025],
         [0.1318, 0.2950]],

        [[0.7749, 0.0992],
         [0.2234, 0.6374]],

        [[0.9484, 0.1017],
         [0.4056, 0.0155]],

        [[0.3710, 0.1125],
         [0.9050, 0.9401]],

        [[0.6657, 0.0959],
         [0.2947, 0.9483]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9446733111422435
Average Adjusted Rand Index: 0.9446436833840768
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19764.4370675515
Iteration 100: Loss = -11549.115292893932
Iteration 200: Loss = -11547.141420384834
Iteration 300: Loss = -11528.721716925005
Iteration 400: Loss = -11367.63296137996
Iteration 500: Loss = -11317.043924610727
Iteration 600: Loss = -11274.026318441815
Iteration 700: Loss = -11263.317808129466
Iteration 800: Loss = -11261.709710423725
Iteration 900: Loss = -11255.458679671066
Iteration 1000: Loss = -11255.40975237678
Iteration 1100: Loss = -11255.347661894402
Iteration 1200: Loss = -11255.332739607378
Iteration 1300: Loss = -11255.319324042915
Iteration 1400: Loss = -11255.31254248213
Iteration 1500: Loss = -11255.307711259793
Iteration 1600: Loss = -11252.37325348382
Iteration 1700: Loss = -11252.368664965497
Iteration 1800: Loss = -11252.366299341764
Iteration 1900: Loss = -11252.364378139764
Iteration 2000: Loss = -11252.362801282598
Iteration 2100: Loss = -11252.361274176445
Iteration 2200: Loss = -11252.359572364581
Iteration 2300: Loss = -11252.360260293482
1
Iteration 2400: Loss = -11252.014455797642
Iteration 2500: Loss = -11252.012926018266
Iteration 2600: Loss = -11252.011954105421
Iteration 2700: Loss = -11252.01113874531
Iteration 2800: Loss = -11252.010428274674
Iteration 2900: Loss = -11252.009753312293
Iteration 3000: Loss = -11252.008454291592
Iteration 3100: Loss = -11251.965523489916
Iteration 3200: Loss = -11251.96505745415
Iteration 3300: Loss = -11251.97066684904
1
Iteration 3400: Loss = -11251.966921033267
2
Iteration 3500: Loss = -11251.964218776562
Iteration 3600: Loss = -11251.963847179373
Iteration 3700: Loss = -11251.958586195891
Iteration 3800: Loss = -11251.951424087172
Iteration 3900: Loss = -11251.951256422097
Iteration 4000: Loss = -11251.964026710155
1
Iteration 4100: Loss = -11251.950868763362
Iteration 4200: Loss = -11251.950707678418
Iteration 4300: Loss = -11251.950702170152
Iteration 4400: Loss = -11251.950414685252
Iteration 4500: Loss = -11251.95026851548
Iteration 4600: Loss = -11251.950551079834
1
Iteration 4700: Loss = -11251.969133635244
2
Iteration 4800: Loss = -11251.949960097994
Iteration 4900: Loss = -11251.950193985385
1
Iteration 5000: Loss = -11251.949860223536
Iteration 5100: Loss = -11251.910406833473
Iteration 5200: Loss = -11251.910132812205
Iteration 5300: Loss = -11251.910114206548
Iteration 5400: Loss = -11251.901138974854
Iteration 5500: Loss = -11251.900386306415
Iteration 5600: Loss = -11251.90636295174
1
Iteration 5700: Loss = -11251.901656044052
2
Iteration 5800: Loss = -11251.901072945337
3
Iteration 5900: Loss = -11251.904240599019
4
Iteration 6000: Loss = -11251.899575122501
Iteration 6100: Loss = -11251.899933266244
1
Iteration 6200: Loss = -11251.899492239849
Iteration 6300: Loss = -11251.900190121582
1
Iteration 6400: Loss = -11251.900148212932
2
Iteration 6500: Loss = -11251.902302682367
3
Iteration 6600: Loss = -11251.899869882924
4
Iteration 6700: Loss = -11251.916634843155
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.7381, 0.2619],
        [0.2847, 0.7153]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5017, 0.4983], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3024, 0.1032],
         [0.7152, 0.1951]],

        [[0.5700, 0.1005],
         [0.6306, 0.6661]],

        [[0.6768, 0.1015],
         [0.6871, 0.6659]],

        [[0.5817, 0.1114],
         [0.5752, 0.5508]],

        [[0.5080, 0.0958],
         [0.7219, 0.5416]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9603205790705823
Average Adjusted Rand Index: 0.9603212164969289
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24973.77038225034
Iteration 100: Loss = -11549.298404633822
Iteration 200: Loss = -11536.65700658531
Iteration 300: Loss = -11527.070932248835
Iteration 400: Loss = -11478.911761306344
Iteration 500: Loss = -11396.636894800371
Iteration 600: Loss = -11324.400066430106
Iteration 700: Loss = -11304.957384240592
Iteration 800: Loss = -11287.36937405269
Iteration 900: Loss = -11268.604162227015
Iteration 1000: Loss = -11264.989272093779
Iteration 1100: Loss = -11264.897531275308
Iteration 1200: Loss = -11264.829388353006
Iteration 1300: Loss = -11259.779230338647
Iteration 1400: Loss = -11257.345189411391
Iteration 1500: Loss = -11257.316499087992
Iteration 1600: Loss = -11257.290492200138
Iteration 1700: Loss = -11257.178313501896
Iteration 1800: Loss = -11256.990990615663
Iteration 1900: Loss = -11256.980512523152
Iteration 2000: Loss = -11256.969939846987
Iteration 2100: Loss = -11256.957459569985
Iteration 2200: Loss = -11256.927345733218
Iteration 2300: Loss = -11256.723039004639
Iteration 2400: Loss = -11255.171298493082
Iteration 2500: Loss = -11254.996759628826
Iteration 2600: Loss = -11254.966186330119
Iteration 2700: Loss = -11254.944007784958
Iteration 2800: Loss = -11254.936673905475
Iteration 2900: Loss = -11254.934431816027
Iteration 3000: Loss = -11254.932849477598
Iteration 3100: Loss = -11254.931488383916
Iteration 3200: Loss = -11254.930231381015
Iteration 3300: Loss = -11254.940743868778
1
Iteration 3400: Loss = -11254.930067649788
Iteration 3500: Loss = -11254.92721945912
Iteration 3600: Loss = -11254.926051691375
Iteration 3700: Loss = -11254.94174305567
1
Iteration 3800: Loss = -11254.923645478753
Iteration 3900: Loss = -11254.92227008689
Iteration 4000: Loss = -11254.920650388238
Iteration 4100: Loss = -11254.917257830075
Iteration 4200: Loss = -11254.917292455302
1
Iteration 4300: Loss = -11254.910137935844
Iteration 4400: Loss = -11254.90707206523
Iteration 4500: Loss = -11254.906233175589
Iteration 4600: Loss = -11254.90510072735
Iteration 4700: Loss = -11254.903738259438
Iteration 4800: Loss = -11254.902655980612
Iteration 4900: Loss = -11254.902197244846
Iteration 5000: Loss = -11254.901838140562
Iteration 5100: Loss = -11254.903410907182
1
Iteration 5200: Loss = -11254.901131991732
Iteration 5300: Loss = -11254.900816543226
Iteration 5400: Loss = -11254.899195297477
Iteration 5500: Loss = -11251.981337610887
Iteration 5600: Loss = -11251.980533930535
Iteration 5700: Loss = -11251.98000140694
Iteration 5800: Loss = -11251.977577378722
Iteration 5900: Loss = -11251.96411074335
Iteration 6000: Loss = -11251.963452887225
Iteration 6100: Loss = -11251.963107284882
Iteration 6200: Loss = -11251.966562434554
1
Iteration 6300: Loss = -11251.962778234263
Iteration 6400: Loss = -11251.963600074525
1
Iteration 6500: Loss = -11251.962543225472
Iteration 6600: Loss = -11251.962438582288
Iteration 6700: Loss = -11251.962692432493
1
Iteration 6800: Loss = -11251.961510656567
Iteration 6900: Loss = -11251.95855459257
Iteration 7000: Loss = -11251.95883661169
1
Iteration 7100: Loss = -11251.958411587499
Iteration 7200: Loss = -11251.958331157262
Iteration 7300: Loss = -11251.958412160018
1
Iteration 7400: Loss = -11251.957983276996
Iteration 7500: Loss = -11251.957679443904
Iteration 7600: Loss = -11251.957396727505
Iteration 7700: Loss = -11251.956745502977
Iteration 7800: Loss = -11251.956768493425
1
Iteration 7900: Loss = -11251.98285517302
2
Iteration 8000: Loss = -11251.95655689399
Iteration 8100: Loss = -11251.961622591289
1
Iteration 8200: Loss = -11251.973828477923
2
Iteration 8300: Loss = -11251.956035062025
Iteration 8400: Loss = -11251.955329372338
Iteration 8500: Loss = -11251.91259784781
Iteration 8600: Loss = -11251.911392513704
Iteration 8700: Loss = -11251.911361478415
Iteration 8800: Loss = -11251.911400677558
1
Iteration 8900: Loss = -11251.91126780857
Iteration 9000: Loss = -11251.911773623466
1
Iteration 9100: Loss = -11251.911227853463
Iteration 9200: Loss = -11251.949246045388
1
Iteration 9300: Loss = -11251.911227275987
Iteration 9400: Loss = -11251.912303761312
1
Iteration 9500: Loss = -11251.91114908714
Iteration 9600: Loss = -11251.928149360323
1
Iteration 9700: Loss = -11251.911184674611
2
Iteration 9800: Loss = -11251.91668147199
3
Iteration 9900: Loss = -11251.923131677357
4
Iteration 10000: Loss = -11251.912281370796
5
Stopping early at iteration 10000 due to no improvement.
pi: tensor([[0.7122, 0.2878],
        [0.2615, 0.7385]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4987, 0.5013], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1951, 0.1032],
         [0.5826, 0.3017]],

        [[0.5374, 0.1004],
         [0.6555, 0.5734]],

        [[0.7150, 0.1016],
         [0.7079, 0.6459]],

        [[0.5151, 0.1112],
         [0.5302, 0.5651]],

        [[0.6289, 0.0958],
         [0.7085, 0.5602]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9603205790705823
Average Adjusted Rand Index: 0.9603212164969289
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20127.762666700524
Iteration 100: Loss = -11548.876294211052
Iteration 200: Loss = -11539.56358109103
Iteration 300: Loss = -11500.979938803057
Iteration 400: Loss = -11262.830342228639
Iteration 500: Loss = -11261.904768049086
Iteration 600: Loss = -11255.930754953988
Iteration 700: Loss = -11255.234974614747
Iteration 800: Loss = -11255.195475825194
Iteration 900: Loss = -11255.157975641243
Iteration 1000: Loss = -11255.116821691649
Iteration 1100: Loss = -11255.095753025804
Iteration 1200: Loss = -11255.083438497935
Iteration 1300: Loss = -11255.071702870673
Iteration 1400: Loss = -11254.965836256968
Iteration 1500: Loss = -11252.085984477693
Iteration 1600: Loss = -11252.082136755951
Iteration 1700: Loss = -11252.079243997387
Iteration 1800: Loss = -11252.076908553398
Iteration 1900: Loss = -11252.075054927558
Iteration 2000: Loss = -11252.073019748575
Iteration 2100: Loss = -11252.070471620425
Iteration 2200: Loss = -11252.069310384875
Iteration 2300: Loss = -11252.068425812782
Iteration 2400: Loss = -11252.067537167028
Iteration 2500: Loss = -11252.066640730267
Iteration 2600: Loss = -11252.07982064748
1
Iteration 2700: Loss = -11252.063698986327
Iteration 2800: Loss = -11252.01087562716
Iteration 2900: Loss = -11252.009762393898
Iteration 3000: Loss = -11252.005134550922
Iteration 3100: Loss = -11251.962327559138
Iteration 3200: Loss = -11251.938149921849
Iteration 3300: Loss = -11251.937025102661
Iteration 3400: Loss = -11251.93671433992
Iteration 3500: Loss = -11251.93685675972
1
Iteration 3600: Loss = -11251.935683848813
Iteration 3700: Loss = -11251.934495293623
Iteration 3800: Loss = -11251.945957458265
1
Iteration 3900: Loss = -11251.933336924896
Iteration 4000: Loss = -11251.921971879374
Iteration 4100: Loss = -11251.92185132338
Iteration 4200: Loss = -11251.921714170134
Iteration 4300: Loss = -11251.925051274424
1
Iteration 4400: Loss = -11251.92147584146
Iteration 4500: Loss = -11251.921357484476
Iteration 4600: Loss = -11251.92123399956
Iteration 4700: Loss = -11251.921172618951
Iteration 4800: Loss = -11251.921165625032
Iteration 4900: Loss = -11251.92099734604
Iteration 5000: Loss = -11251.920919645085
Iteration 5100: Loss = -11251.920798243957
Iteration 5200: Loss = -11251.920614109276
Iteration 5300: Loss = -11251.9221372274
1
Iteration 5400: Loss = -11251.92040125128
Iteration 5500: Loss = -11251.928176774409
1
Iteration 5600: Loss = -11251.92051406593
2
Iteration 5700: Loss = -11251.920258760874
Iteration 5800: Loss = -11251.920735124224
1
Iteration 5900: Loss = -11251.920343611704
2
Iteration 6000: Loss = -11251.920456295778
3
Iteration 6100: Loss = -11251.92015359386
Iteration 6200: Loss = -11251.920125532552
Iteration 6300: Loss = -11251.92009380713
Iteration 6400: Loss = -11251.9201086396
1
Iteration 6500: Loss = -11251.92009554031
2
Iteration 6600: Loss = -11251.922933972251
3
Iteration 6700: Loss = -11251.928110339857
4
Iteration 6800: Loss = -11251.919980359211
Iteration 6900: Loss = -11251.9211654742
1
Iteration 7000: Loss = -11251.91994956081
Iteration 7100: Loss = -11251.919867255074
Iteration 7200: Loss = -11251.921392987737
1
Iteration 7300: Loss = -11251.917489457695
Iteration 7400: Loss = -11251.904540377529
Iteration 7500: Loss = -11251.906968379171
1
Iteration 7600: Loss = -11251.90128506709
Iteration 7700: Loss = -11251.910610422306
1
Iteration 7800: Loss = -11251.901963946382
2
Iteration 7900: Loss = -11251.901180719455
Iteration 8000: Loss = -11251.901609599025
1
Iteration 8100: Loss = -11251.902420850307
2
Iteration 8200: Loss = -11251.901157858694
Iteration 8300: Loss = -11251.908804959405
1
Iteration 8400: Loss = -11251.901495359349
2
Iteration 8500: Loss = -11251.90120545556
3
Iteration 8600: Loss = -11251.90565233568
4
Iteration 8700: Loss = -11251.900948907187
Iteration 8800: Loss = -11251.900944404613
Iteration 8900: Loss = -11251.901145838867
1
Iteration 9000: Loss = -11251.900898180926
Iteration 9100: Loss = -11251.900837528636
Iteration 9200: Loss = -11251.903324506162
1
Iteration 9300: Loss = -11251.907401748627
2
Iteration 9400: Loss = -11251.900820758741
Iteration 9500: Loss = -11251.901467349235
1
Iteration 9600: Loss = -11251.900294214029
Iteration 9700: Loss = -11251.900190548342
Iteration 9800: Loss = -11251.900190614402
1
Iteration 9900: Loss = -11251.90007581829
Iteration 10000: Loss = -11251.901482927291
1
Iteration 10100: Loss = -11251.89988784551
Iteration 10200: Loss = -11251.902868322404
1
Iteration 10300: Loss = -11251.936886922183
2
Iteration 10400: Loss = -11251.914139176872
3
Iteration 10500: Loss = -11251.894998681346
Iteration 10600: Loss = -11251.895962249817
1
Iteration 10700: Loss = -11251.894968873703
Iteration 10800: Loss = -11251.898191034006
1
Iteration 10900: Loss = -11251.897756131972
2
Iteration 11000: Loss = -11251.895005783772
3
Iteration 11100: Loss = -11251.894762530497
Iteration 11200: Loss = -11251.902377099073
1
Iteration 11300: Loss = -11251.894755443853
Iteration 11400: Loss = -11251.948419431224
1
Iteration 11500: Loss = -11251.895001353467
2
Iteration 11600: Loss = -11251.89476984924
3
Iteration 11700: Loss = -11251.896379553964
4
Iteration 11800: Loss = -11251.89586546489
5
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[0.7127, 0.2873],
        [0.2620, 0.7380]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4982, 0.5018], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1951, 0.1032],
         [0.7103, 0.3016]],

        [[0.6358, 0.1004],
         [0.6886, 0.5099]],

        [[0.6867, 0.1014],
         [0.7092, 0.6342]],

        [[0.6720, 0.1113],
         [0.7253, 0.6833]],

        [[0.5887, 0.0958],
         [0.6986, 0.6896]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9603205790705823
Average Adjusted Rand Index: 0.9603212164969289
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20161.61104325623
Iteration 100: Loss = -11549.670032028675
Iteration 200: Loss = -11548.673233195716
Iteration 300: Loss = -11539.383693409989
Iteration 400: Loss = -11534.851336985987
Iteration 500: Loss = -11513.7570841266
Iteration 600: Loss = -11426.210874581999
Iteration 700: Loss = -11373.505152149582
Iteration 800: Loss = -11365.524248494385
Iteration 900: Loss = -11362.973510431893
Iteration 1000: Loss = -11355.772040788255
Iteration 1100: Loss = -11351.20634906177
Iteration 1200: Loss = -11351.14584267424
Iteration 1300: Loss = -11351.099885758325
Iteration 1400: Loss = -11349.937995130482
Iteration 1500: Loss = -11342.993089697047
Iteration 1600: Loss = -11342.685941941425
Iteration 1700: Loss = -11342.568957484158
Iteration 1800: Loss = -11341.35088510233
Iteration 1900: Loss = -11339.788139471924
Iteration 2000: Loss = -11337.566791485478
Iteration 2100: Loss = -11337.554213920619
Iteration 2200: Loss = -11337.5382483624
Iteration 2300: Loss = -11337.43979555251
Iteration 2400: Loss = -11337.426849392992
Iteration 2500: Loss = -11324.836812235519
Iteration 2600: Loss = -11324.75026400396
Iteration 2700: Loss = -11324.707218428142
Iteration 2800: Loss = -11324.700726638941
Iteration 2900: Loss = -11324.685296442189
Iteration 3000: Loss = -11324.680160661383
Iteration 3100: Loss = -11324.677392086976
Iteration 3200: Loss = -11324.674384156773
Iteration 3300: Loss = -11324.673439320635
Iteration 3400: Loss = -11324.669163775792
Iteration 3500: Loss = -11324.660652245831
Iteration 3600: Loss = -11322.548515228078
Iteration 3700: Loss = -11320.904597821529
Iteration 3800: Loss = -11320.136606420614
Iteration 3900: Loss = -11319.583259963916
Iteration 4000: Loss = -11319.493595083964
Iteration 4100: Loss = -11319.441315391585
Iteration 4200: Loss = -11319.033888026539
Iteration 4300: Loss = -11319.02239763519
Iteration 4400: Loss = -11318.911859879843
Iteration 4500: Loss = -11318.907517050844
Iteration 4600: Loss = -11318.905752079518
Iteration 4700: Loss = -11318.906774894529
1
Iteration 4800: Loss = -11318.902770236016
Iteration 4900: Loss = -11318.67300447109
Iteration 5000: Loss = -11318.670568971373
Iteration 5100: Loss = -11318.670007509014
Iteration 5200: Loss = -11318.669206282866
Iteration 5300: Loss = -11318.68649611205
1
Iteration 5400: Loss = -11318.643975924777
Iteration 5500: Loss = -11318.644103439337
1
Iteration 5600: Loss = -11318.641454143992
Iteration 5700: Loss = -11318.637667029621
Iteration 5800: Loss = -11318.635396072377
Iteration 5900: Loss = -11318.634042247088
Iteration 6000: Loss = -11318.629655442262
Iteration 6100: Loss = -11318.584208609738
Iteration 6200: Loss = -11318.579405420254
Iteration 6300: Loss = -11318.579008004883
Iteration 6400: Loss = -11318.579140918027
1
Iteration 6500: Loss = -11318.57848937566
Iteration 6600: Loss = -11318.58416623094
1
Iteration 6700: Loss = -11318.5778641579
Iteration 6800: Loss = -11318.577541474302
Iteration 6900: Loss = -11318.57684834613
Iteration 7000: Loss = -11318.574455575046
Iteration 7100: Loss = -11318.574367797075
Iteration 7200: Loss = -11318.573084884605
Iteration 7300: Loss = -11318.560852576411
Iteration 7400: Loss = -11318.55321278392
Iteration 7500: Loss = -11318.552988253556
Iteration 7600: Loss = -11318.563412569476
1
Iteration 7700: Loss = -11318.552839287813
Iteration 7800: Loss = -11318.553227553612
1
Iteration 7900: Loss = -11318.554500969753
2
Iteration 8000: Loss = -11318.552548054777
Iteration 8100: Loss = -11318.552562276749
1
Iteration 8200: Loss = -11318.552319455765
Iteration 8300: Loss = -11318.55241783888
1
Iteration 8400: Loss = -11318.565304237414
2
Iteration 8500: Loss = -11318.549118104365
Iteration 8600: Loss = -11318.548897280883
Iteration 8700: Loss = -11318.549591096702
1
Iteration 8800: Loss = -11318.58116325269
2
Iteration 8900: Loss = -11318.550379181348
3
Iteration 9000: Loss = -11318.547684718342
Iteration 9100: Loss = -11318.550489868878
1
Iteration 9200: Loss = -11318.547069399432
Iteration 9300: Loss = -11318.54714466338
1
Iteration 9400: Loss = -11318.56984536951
2
Iteration 9500: Loss = -11318.54728976527
3
Iteration 9600: Loss = -11318.609836513444
4
Iteration 9700: Loss = -11318.546551097605
Iteration 9800: Loss = -11318.713109254988
1
Iteration 9900: Loss = -11318.546420442523
Iteration 10000: Loss = -11318.545501543937
Iteration 10100: Loss = -11318.551405399887
1
Iteration 10200: Loss = -11318.548158676902
2
Iteration 10300: Loss = -11318.545971083642
3
Iteration 10400: Loss = -11318.545677148708
4
Iteration 10500: Loss = -11318.54700119706
5
Stopping early at iteration 10500 due to no improvement.
pi: tensor([[0.6108, 0.3892],
        [0.2870, 0.7130]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9370, 0.0630], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1905, 0.0959],
         [0.5429, 0.3013]],

        [[0.5280, 0.1001],
         [0.6123, 0.6851]],

        [[0.6136, 0.1014],
         [0.5224, 0.5354]],

        [[0.5571, 0.1118],
         [0.6752, 0.7086]],

        [[0.5866, 0.0957],
         [0.6762, 0.7097]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.5951865327209496
Average Adjusted Rand Index: 0.7602225886451348
11267.733048284359
[0.9603205790705823, 0.9603205790705823, 0.9603205790705823, 0.5951865327209496] [0.9603212164969289, 0.9603212164969289, 0.9603212164969289, 0.7602225886451348] [11251.916634843155, 11251.912281370796, 11251.89586546489, 11318.54700119706]
-----------------------------------------------------------------------------------------
This iteration is 19
True Objective function: Loss = -11180.363177382842
Iteration 0: Loss = -11551.551863174223
Iteration 10: Loss = -11347.117548028766
Iteration 20: Loss = -11187.422889363643
Iteration 30: Loss = -11149.43454542655
Iteration 40: Loss = -11149.391499791664
Iteration 50: Loss = -11149.390211698556
Iteration 60: Loss = -11149.390201325014
Iteration 70: Loss = -11149.390215302477
1
Iteration 80: Loss = -11149.390226496635
2
Iteration 90: Loss = -11149.39021853679
3
Stopping early at iteration 90 due to no improvement.
pi: tensor([[0.7443, 0.2557],
        [0.2358, 0.7642]], dtype=torch.float64)
alpha: tensor([0.4979, 0.5021])
beta: tensor([[[0.2836, 0.1040],
         [0.2890, 0.1881]],

        [[0.8997, 0.1081],
         [0.9461, 0.2684]],

        [[0.0815, 0.1037],
         [0.8041, 0.4872]],

        [[0.6307, 0.1021],
         [0.0612, 0.0586]],

        [[0.1987, 0.1075],
         [0.0132, 0.1648]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9137631834935523
Average Adjusted Rand Index: 0.9139360987847136
Iteration 0: Loss = -11547.270360906194
Iteration 10: Loss = -11357.766298769347
Iteration 20: Loss = -11297.40491240974
Iteration 30: Loss = -11152.317855298283
Iteration 40: Loss = -11149.403779021337
Iteration 50: Loss = -11149.390627724059
Iteration 60: Loss = -11149.39020883644
Iteration 70: Loss = -11149.390207774808
Iteration 80: Loss = -11149.390213866243
1
Iteration 90: Loss = -11149.390211262653
2
Iteration 100: Loss = -11149.390213793271
3
Stopping early at iteration 100 due to no improvement.
pi: tensor([[0.7443, 0.2557],
        [0.2358, 0.7642]], dtype=torch.float64)
alpha: tensor([0.4979, 0.5021])
beta: tensor([[[0.2836, 0.1040],
         [0.0495, 0.1881]],

        [[0.1076, 0.1081],
         [0.2519, 0.2007]],

        [[0.3576, 0.1037],
         [0.4537, 0.8547]],

        [[0.1475, 0.1021],
         [0.9976, 0.8723]],

        [[0.5045, 0.1075],
         [0.4618, 0.5600]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9137631834935523
Average Adjusted Rand Index: 0.9139360987847136
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24676.968697576198
Iteration 100: Loss = -11370.698319683392
Iteration 200: Loss = -11321.286624529874
Iteration 300: Loss = -11302.434151181837
Iteration 400: Loss = -11230.265559621277
Iteration 500: Loss = -11160.254704153409
Iteration 600: Loss = -11158.24513419471
Iteration 700: Loss = -11153.306402033515
Iteration 800: Loss = -11153.272165891498
Iteration 900: Loss = -11153.250998125724
Iteration 1000: Loss = -11151.20218164192
Iteration 1100: Loss = -11145.610741774462
Iteration 1200: Loss = -11145.563513069226
Iteration 1300: Loss = -11145.54853855463
Iteration 1400: Loss = -11143.689377931545
Iteration 1500: Loss = -11143.664409623587
Iteration 1600: Loss = -11143.661749296536
Iteration 1700: Loss = -11143.659955945313
Iteration 1800: Loss = -11143.604490550933
Iteration 1900: Loss = -11143.6027484594
Iteration 2000: Loss = -11143.602670737446
Iteration 2100: Loss = -11143.600997308991
Iteration 2200: Loss = -11143.600162944145
Iteration 2300: Loss = -11143.599160376722
Iteration 2400: Loss = -11143.596814976692
Iteration 2500: Loss = -11142.993347719157
Iteration 2600: Loss = -11142.992785277589
Iteration 2700: Loss = -11142.990720377366
Iteration 2800: Loss = -11142.977770612188
Iteration 2900: Loss = -11142.934177139705
Iteration 3000: Loss = -11142.745677585048
Iteration 3100: Loss = -11142.74558686376
Iteration 3200: Loss = -11142.743091155127
Iteration 3300: Loss = -11142.742841825533
Iteration 3400: Loss = -11142.742544183504
Iteration 3500: Loss = -11142.742538385648
Iteration 3600: Loss = -11142.74636469947
1
Iteration 3700: Loss = -11142.735038353852
Iteration 3800: Loss = -11142.680446282073
Iteration 3900: Loss = -11142.680234137993
Iteration 4000: Loss = -11142.683618153344
1
Iteration 4100: Loss = -11142.678195017264
Iteration 4200: Loss = -11142.697412600215
1
Iteration 4300: Loss = -11142.6746829158
Iteration 4400: Loss = -11142.54084909592
Iteration 4500: Loss = -11142.540510434203
Iteration 4600: Loss = -11142.540274178209
Iteration 4700: Loss = -11142.539979738427
Iteration 4800: Loss = -11142.539045397174
Iteration 4900: Loss = -11142.538236390903
Iteration 5000: Loss = -11142.537986512012
Iteration 5100: Loss = -11142.543266642257
1
Iteration 5200: Loss = -11142.537280228622
Iteration 5300: Loss = -11142.520039718882
Iteration 5400: Loss = -11142.510574461789
Iteration 5500: Loss = -11142.510249418456
Iteration 5600: Loss = -11142.510596938148
1
Iteration 5700: Loss = -11142.510172037022
Iteration 5800: Loss = -11142.5101180743
Iteration 5900: Loss = -11142.510034563866
Iteration 6000: Loss = -11142.509931011024
Iteration 6100: Loss = -11142.509470363695
Iteration 6200: Loss = -11142.309490061733
Iteration 6300: Loss = -11142.309356848444
Iteration 6400: Loss = -11142.30941471313
1
Iteration 6500: Loss = -11142.309174039116
Iteration 6600: Loss = -11142.275896295832
Iteration 6700: Loss = -11142.275843207957
Iteration 6800: Loss = -11142.274083681521
Iteration 6900: Loss = -11142.273890928258
Iteration 7000: Loss = -11142.273921736354
1
Iteration 7100: Loss = -11142.27588900083
2
Iteration 7200: Loss = -11142.27379309554
Iteration 7300: Loss = -11142.284927252136
1
Iteration 7400: Loss = -11142.273779904144
Iteration 7500: Loss = -11142.273790126163
1
Iteration 7600: Loss = -11142.273732110154
Iteration 7700: Loss = -11142.273613581208
Iteration 7800: Loss = -11142.271730605367
Iteration 7900: Loss = -11142.271728730944
Iteration 8000: Loss = -11142.271687800132
Iteration 8100: Loss = -11142.28527746383
1
Iteration 8200: Loss = -11142.2715996505
Iteration 8300: Loss = -11142.271693891636
1
Iteration 8400: Loss = -11142.271005565783
Iteration 8500: Loss = -11142.271039750027
1
Iteration 8600: Loss = -11142.361292425838
2
Iteration 8700: Loss = -11142.276574374813
3
Iteration 8800: Loss = -11142.273253558093
4
Iteration 8900: Loss = -11142.270885764763
Iteration 9000: Loss = -11142.274384678349
1
Iteration 9100: Loss = -11142.270628429202
Iteration 9200: Loss = -11142.270440698816
Iteration 9300: Loss = -11142.26799059683
Iteration 9400: Loss = -11142.268428410765
1
Iteration 9500: Loss = -11142.269850020117
2
Iteration 9600: Loss = -11142.276696576719
3
Iteration 9700: Loss = -11142.270765250583
4
Iteration 9800: Loss = -11142.269870276717
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.7932, 0.2068],
        [0.2264, 0.7736]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4609, 0.5391], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1928, 0.1031],
         [0.6191, 0.2866]],

        [[0.5621, 0.1082],
         [0.7304, 0.5623]],

        [[0.6694, 0.1037],
         [0.5438, 0.6494]],

        [[0.5840, 0.1011],
         [0.5807, 0.6290]],

        [[0.5005, 0.1071],
         [0.7172, 0.6884]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9291540354044258
Average Adjusted Rand Index: 0.9294516150356007
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23432.85257993286
Iteration 100: Loss = -11373.312179418397
Iteration 200: Loss = -11364.103798544571
Iteration 300: Loss = -11337.926064219704
Iteration 400: Loss = -11273.927657430382
Iteration 500: Loss = -11169.417237862028
Iteration 600: Loss = -11157.528370807728
Iteration 700: Loss = -11149.987809210204
Iteration 800: Loss = -11145.732442607372
Iteration 900: Loss = -11145.655943928337
Iteration 1000: Loss = -11145.61534015928
Iteration 1100: Loss = -11145.372114432497
Iteration 1200: Loss = -11143.074338278988
Iteration 1300: Loss = -11142.939027004379
Iteration 1400: Loss = -11142.766803550834
Iteration 1500: Loss = -11142.729186760716
Iteration 1600: Loss = -11142.715577253695
Iteration 1700: Loss = -11142.594128662115
Iteration 1800: Loss = -11142.585308704156
Iteration 1900: Loss = -11142.58001577607
Iteration 2000: Loss = -11142.57451668186
Iteration 2100: Loss = -11142.569016167954
Iteration 2200: Loss = -11142.5653842475
Iteration 2300: Loss = -11142.56222441608
Iteration 2400: Loss = -11142.424743298201
Iteration 2500: Loss = -11142.419458711222
Iteration 2600: Loss = -11142.416528154003
Iteration 2700: Loss = -11142.406366442117
Iteration 2800: Loss = -11142.402224164907
Iteration 2900: Loss = -11142.400109979251
Iteration 3000: Loss = -11142.36557705075
Iteration 3100: Loss = -11142.373957850004
1
Iteration 3200: Loss = -11142.361795778816
Iteration 3300: Loss = -11142.33202687543
Iteration 3400: Loss = -11142.32809679933
Iteration 3500: Loss = -11142.297170355163
Iteration 3600: Loss = -11142.295412172485
Iteration 3700: Loss = -11142.294437277895
Iteration 3800: Loss = -11142.293136946682
Iteration 3900: Loss = -11142.291472692927
Iteration 4000: Loss = -11142.290056703352
Iteration 4100: Loss = -11142.288798989011
Iteration 4200: Loss = -11142.288383185643
Iteration 4300: Loss = -11142.288087429404
Iteration 4400: Loss = -11142.287851225945
Iteration 4500: Loss = -11142.287570883505
Iteration 4600: Loss = -11142.28732275543
Iteration 4700: Loss = -11142.287422005216
1
Iteration 4800: Loss = -11142.294274992173
2
Iteration 4900: Loss = -11142.30336191169
3
Iteration 5000: Loss = -11142.284159974395
Iteration 5100: Loss = -11142.283729645516
Iteration 5200: Loss = -11142.296636419043
1
Iteration 5300: Loss = -11142.271489490946
Iteration 5400: Loss = -11142.270140361128
Iteration 5500: Loss = -11142.271703812989
1
Iteration 5600: Loss = -11142.270101825496
Iteration 5700: Loss = -11142.27286719323
1
Iteration 5800: Loss = -11142.269775062667
Iteration 5900: Loss = -11142.270124512064
1
Iteration 6000: Loss = -11142.2723581591
2
Iteration 6100: Loss = -11142.269519350819
Iteration 6200: Loss = -11142.269568254898
1
Iteration 6300: Loss = -11142.269426689078
Iteration 6400: Loss = -11142.269826705211
1
Iteration 6500: Loss = -11142.26954249277
2
Iteration 6600: Loss = -11142.269637201613
3
Iteration 6700: Loss = -11142.26916903729
Iteration 6800: Loss = -11142.272981728367
1
Iteration 6900: Loss = -11142.282432108908
2
Iteration 7000: Loss = -11142.269049551038
Iteration 7100: Loss = -11142.26896074358
Iteration 7200: Loss = -11142.272005849612
1
Iteration 7300: Loss = -11142.268569951013
Iteration 7400: Loss = -11142.271277266873
1
Iteration 7500: Loss = -11142.268417792717
Iteration 7600: Loss = -11142.268338843383
Iteration 7700: Loss = -11142.268487878842
1
Iteration 7800: Loss = -11142.268542863016
2
Iteration 7900: Loss = -11142.26874710753
3
Iteration 8000: Loss = -11142.268364930289
4
Iteration 8100: Loss = -11142.268244972172
Iteration 8200: Loss = -11142.269427033467
1
Iteration 8300: Loss = -11142.238890112272
Iteration 8400: Loss = -11142.39966644082
1
Iteration 8500: Loss = -11142.23834412658
Iteration 8600: Loss = -11142.238265984337
Iteration 8700: Loss = -11142.238255200666
Iteration 8800: Loss = -11142.31384098832
1
Iteration 8900: Loss = -11142.2382786184
2
Iteration 9000: Loss = -11142.2845341494
3
Iteration 9100: Loss = -11142.23816080103
Iteration 9200: Loss = -11142.23984547506
1
Iteration 9300: Loss = -11142.240849805321
2
Iteration 9400: Loss = -11142.334512145626
3
Iteration 9500: Loss = -11142.238094335846
Iteration 9600: Loss = -11142.238264908627
1
Iteration 9700: Loss = -11142.238017856831
Iteration 9800: Loss = -11142.235722157131
Iteration 9900: Loss = -11142.235592481098
Iteration 10000: Loss = -11142.235520976692
Iteration 10100: Loss = -11142.235488062339
Iteration 10200: Loss = -11142.23847110213
1
Iteration 10300: Loss = -11142.235319557074
Iteration 10400: Loss = -11142.235445672388
1
Iteration 10500: Loss = -11142.23520576848
Iteration 10600: Loss = -11142.235187182252
Iteration 10700: Loss = -11142.289022088935
1
Iteration 10800: Loss = -11142.2351899797
2
Iteration 10900: Loss = -11142.235137132695
Iteration 11000: Loss = -11142.238711674276
1
Iteration 11100: Loss = -11142.235134769651
Iteration 11200: Loss = -11142.235132454158
Iteration 11300: Loss = -11142.269927674757
1
Iteration 11400: Loss = -11142.235118792345
Iteration 11500: Loss = -11142.235101107215
Iteration 11600: Loss = -11142.235341802707
1
Iteration 11700: Loss = -11142.235178422055
2
Iteration 11800: Loss = -11142.235072502546
Iteration 11900: Loss = -11142.244334532057
1
Iteration 12000: Loss = -11142.235031676097
Iteration 12100: Loss = -11142.238124372505
1
Iteration 12200: Loss = -11142.234978796096
Iteration 12300: Loss = -11142.23962663249
1
Iteration 12400: Loss = -11142.235083328816
2
Iteration 12500: Loss = -11142.235098738092
3
Iteration 12600: Loss = -11142.239722207934
4
Iteration 12700: Loss = -11142.232916313611
Iteration 12800: Loss = -11142.233232463599
1
Iteration 12900: Loss = -11142.236676916662
2
Iteration 13000: Loss = -11142.232158432522
Iteration 13100: Loss = -11142.276742392909
1
Iteration 13200: Loss = -11142.241114813085
2
Iteration 13300: Loss = -11142.273143944647
3
Iteration 13400: Loss = -11142.23202317684
Iteration 13500: Loss = -11142.234852291844
1
Iteration 13600: Loss = -11142.243595646258
2
Iteration 13700: Loss = -11142.232028941515
3
Iteration 13800: Loss = -11142.247909681748
4
Iteration 13900: Loss = -11142.231996121624
Iteration 14000: Loss = -11142.23292064951
1
Iteration 14100: Loss = -11142.232023165672
2
Iteration 14200: Loss = -11142.518371614746
3
Iteration 14300: Loss = -11142.231978879934
Iteration 14400: Loss = -11142.303168742439
1
Iteration 14500: Loss = -11142.231369083027
Iteration 14600: Loss = -11142.231343898067
Iteration 14700: Loss = -11142.234438892747
1
Iteration 14800: Loss = -11142.231182832995
Iteration 14900: Loss = -11142.232443897296
1
Iteration 15000: Loss = -11142.230544135266
Iteration 15100: Loss = -11142.230526153424
Iteration 15200: Loss = -11142.230678408278
1
Iteration 15300: Loss = -11142.252189564195
2
Iteration 15400: Loss = -11142.230521810552
Iteration 15500: Loss = -11142.25491125731
1
Iteration 15600: Loss = -11142.230537595926
2
Iteration 15700: Loss = -11142.244973747496
3
Iteration 15800: Loss = -11142.220415155489
Iteration 15900: Loss = -11142.220216138709
Iteration 16000: Loss = -11142.220240584644
1
Iteration 16100: Loss = -11142.29662894044
2
Iteration 16200: Loss = -11142.220214382318
Iteration 16300: Loss = -11142.220403405374
1
Iteration 16400: Loss = -11142.227614221827
2
Iteration 16500: Loss = -11142.220196033933
Iteration 16600: Loss = -11142.221542063786
1
Iteration 16700: Loss = -11142.22022943379
2
Iteration 16800: Loss = -11142.220329359265
3
Iteration 16900: Loss = -11142.226233466206
4
Iteration 17000: Loss = -11142.220822021432
5
Stopping early at iteration 17000 due to no improvement.
pi: tensor([[0.7732, 0.2268],
        [0.2063, 0.7937]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5389, 0.4611], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2869, 0.1031],
         [0.7293, 0.1928]],

        [[0.5883, 0.1082],
         [0.6510, 0.7012]],

        [[0.6862, 0.1037],
         [0.5445, 0.5346]],

        [[0.5226, 0.1012],
         [0.5108, 0.7205]],

        [[0.6868, 0.1071],
         [0.7169, 0.6324]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9291540354044258
Average Adjusted Rand Index: 0.9294516150356007
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24259.474719416827
Iteration 100: Loss = -11374.518627980451
Iteration 200: Loss = -11369.680590907592
Iteration 300: Loss = -11360.393851472258
Iteration 400: Loss = -11298.883073829109
Iteration 500: Loss = -11168.68197955796
Iteration 600: Loss = -11150.330624118682
Iteration 700: Loss = -11149.515524300632
Iteration 800: Loss = -11149.057586395273
Iteration 900: Loss = -11149.00365766504
Iteration 1000: Loss = -11142.654316065526
Iteration 1100: Loss = -11142.530504136448
Iteration 1200: Loss = -11142.516123308187
Iteration 1300: Loss = -11142.505049222744
Iteration 1400: Loss = -11142.477524825328
Iteration 1500: Loss = -11142.449707743175
Iteration 1600: Loss = -11142.44484797964
Iteration 1700: Loss = -11142.441337422932
Iteration 1800: Loss = -11142.438534889818
Iteration 1900: Loss = -11142.436175463912
Iteration 2000: Loss = -11142.43401770453
Iteration 2100: Loss = -11142.432468543706
Iteration 2200: Loss = -11142.431113285887
Iteration 2300: Loss = -11142.429975468176
Iteration 2400: Loss = -11142.429004028538
Iteration 2500: Loss = -11142.438833238566
1
Iteration 2600: Loss = -11142.427338735002
Iteration 2700: Loss = -11142.426642481889
Iteration 2800: Loss = -11142.426528189055
Iteration 2900: Loss = -11142.425484112175
Iteration 3000: Loss = -11142.424979128058
Iteration 3100: Loss = -11142.433074835246
1
Iteration 3200: Loss = -11142.424095700417
Iteration 3300: Loss = -11142.423692612463
Iteration 3400: Loss = -11142.423860201607
1
Iteration 3500: Loss = -11142.422887538922
Iteration 3600: Loss = -11142.422281826159
Iteration 3700: Loss = -11142.420154411951
Iteration 3800: Loss = -11142.409140348842
Iteration 3900: Loss = -11142.408005610396
Iteration 4000: Loss = -11142.413442596777
1
Iteration 4100: Loss = -11142.407340632264
Iteration 4200: Loss = -11142.407116433415
Iteration 4300: Loss = -11142.40722596226
1
Iteration 4400: Loss = -11142.406652605607
Iteration 4500: Loss = -11142.406496628475
Iteration 4600: Loss = -11142.406229877208
Iteration 4700: Loss = -11142.406016038667
Iteration 4800: Loss = -11142.406230465416
1
Iteration 4900: Loss = -11142.405334856472
Iteration 5000: Loss = -11142.409868064913
1
Iteration 5100: Loss = -11142.40486409858
Iteration 5200: Loss = -11142.404748381688
Iteration 5300: Loss = -11142.404639373837
Iteration 5400: Loss = -11142.40454457217
Iteration 5500: Loss = -11142.404415024508
Iteration 5600: Loss = -11142.405118273775
1
Iteration 5700: Loss = -11142.374388701579
Iteration 5800: Loss = -11142.37431539487
Iteration 5900: Loss = -11142.374214731008
Iteration 6000: Loss = -11142.37414651805
Iteration 6100: Loss = -11142.374059172593
Iteration 6200: Loss = -11142.37594103966
1
Iteration 6300: Loss = -11142.373972750709
Iteration 6400: Loss = -11142.373912854235
Iteration 6500: Loss = -11142.373901081335
Iteration 6600: Loss = -11142.373838915622
Iteration 6700: Loss = -11142.37385628698
1
Iteration 6800: Loss = -11142.373787992208
Iteration 6900: Loss = -11142.375594364075
1
Iteration 7000: Loss = -11142.373841433824
2
Iteration 7100: Loss = -11142.373517810489
Iteration 7200: Loss = -11142.372359068742
Iteration 7300: Loss = -11142.36243313147
Iteration 7400: Loss = -11142.391222828184
1
Iteration 7500: Loss = -11142.362297755564
Iteration 7600: Loss = -11142.384615256955
1
Iteration 7700: Loss = -11142.362253453528
Iteration 7800: Loss = -11142.361608319616
Iteration 7900: Loss = -11142.363231266814
1
Iteration 8000: Loss = -11142.454810797988
2
Iteration 8100: Loss = -11142.361323984167
Iteration 8200: Loss = -11142.361720088536
1
Iteration 8300: Loss = -11142.36120679671
Iteration 8400: Loss = -11142.368602056586
1
Iteration 8500: Loss = -11142.375956524254
2
Iteration 8600: Loss = -11142.37086958199
3
Iteration 8700: Loss = -11142.369978802375
4
Iteration 8800: Loss = -11142.36062588446
Iteration 8900: Loss = -11142.362490461446
1
Iteration 9000: Loss = -11142.360879795866
2
Iteration 9100: Loss = -11142.360782457215
3
Iteration 9200: Loss = -11142.3624901737
4
Iteration 9300: Loss = -11142.372489558318
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.7744, 0.2256],
        [0.2056, 0.7944]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5389, 0.4611], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2869, 0.1031],
         [0.6097, 0.1927]],

        [[0.6952, 0.1083],
         [0.6689, 0.5870]],

        [[0.6977, 0.1038],
         [0.6342, 0.6666]],

        [[0.5881, 0.1012],
         [0.5784, 0.6830]],

        [[0.5552, 0.1071],
         [0.5260, 0.6798]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9291540354044258
Average Adjusted Rand Index: 0.9294516150356007
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24236.134311565376
Iteration 100: Loss = -11375.709909071362
Iteration 200: Loss = -11373.541871396244
Iteration 300: Loss = -11365.364396895657
Iteration 400: Loss = -11345.735611063132
Iteration 500: Loss = -11307.974119462951
Iteration 600: Loss = -11242.305399434588
Iteration 700: Loss = -11153.053824876759
Iteration 800: Loss = -11146.876691887142
Iteration 900: Loss = -11145.825280660425
Iteration 1000: Loss = -11143.271835945105
Iteration 1100: Loss = -11143.027825131036
Iteration 1200: Loss = -11142.946854320915
Iteration 1300: Loss = -11142.868190318826
Iteration 1400: Loss = -11142.812879329307
Iteration 1500: Loss = -11142.764605011522
Iteration 1600: Loss = -11142.746875853723
Iteration 1700: Loss = -11142.729261866476
Iteration 1800: Loss = -11142.707297321682
Iteration 1900: Loss = -11142.69861249232
Iteration 2000: Loss = -11142.69176786974
Iteration 2100: Loss = -11142.685823825037
Iteration 2200: Loss = -11142.680585986274
Iteration 2300: Loss = -11142.676322767342
Iteration 2400: Loss = -11142.674434141034
Iteration 2500: Loss = -11142.667730802039
Iteration 2600: Loss = -11142.638416472006
Iteration 2700: Loss = -11142.632291168251
Iteration 2800: Loss = -11142.629385646265
Iteration 2900: Loss = -11142.628255665713
Iteration 3000: Loss = -11142.625678076736
Iteration 3100: Loss = -11142.62379125725
Iteration 3200: Loss = -11142.621256718578
Iteration 3300: Loss = -11142.61623993755
Iteration 3400: Loss = -11142.614714514899
Iteration 3500: Loss = -11142.612850904621
Iteration 3600: Loss = -11142.611633136135
Iteration 3700: Loss = -11142.609465617395
Iteration 3800: Loss = -11142.588016787226
Iteration 3900: Loss = -11142.530206164023
Iteration 4000: Loss = -11142.52665964779
Iteration 4100: Loss = -11142.5260615802
Iteration 4200: Loss = -11142.524819328462
Iteration 4300: Loss = -11142.527083422729
1
Iteration 4400: Loss = -11142.418774343492
Iteration 4500: Loss = -11142.409930302565
Iteration 4600: Loss = -11142.403361802604
Iteration 4700: Loss = -11142.295638607317
Iteration 4800: Loss = -11142.303274503362
1
Iteration 4900: Loss = -11142.29184926795
Iteration 5000: Loss = -11142.252768460416
Iteration 5100: Loss = -11142.250950435038
Iteration 5200: Loss = -11142.250535424646
Iteration 5300: Loss = -11142.250306276363
Iteration 5400: Loss = -11142.250179778835
Iteration 5500: Loss = -11142.249930133636
Iteration 5600: Loss = -11142.25103937538
1
Iteration 5700: Loss = -11142.249421457045
Iteration 5800: Loss = -11142.250050535504
1
Iteration 5900: Loss = -11142.248938934217
Iteration 6000: Loss = -11142.250487386846
1
Iteration 6100: Loss = -11142.249688969088
2
Iteration 6200: Loss = -11142.248496124212
Iteration 6300: Loss = -11142.2483009859
Iteration 6400: Loss = -11142.248115950699
Iteration 6500: Loss = -11142.248021315076
Iteration 6600: Loss = -11142.25258714326
1
Iteration 6700: Loss = -11142.247732821757
Iteration 6800: Loss = -11142.247806566236
1
Iteration 6900: Loss = -11142.262476081136
2
Iteration 7000: Loss = -11142.247477315805
Iteration 7100: Loss = -11142.252036371256
1
Iteration 7200: Loss = -11142.248985322056
2
Iteration 7300: Loss = -11142.290362611267
3
Iteration 7400: Loss = -11142.247151885069
Iteration 7500: Loss = -11142.247403796338
1
Iteration 7600: Loss = -11142.25245125845
2
Iteration 7700: Loss = -11142.246943983027
Iteration 7800: Loss = -11142.282455210545
1
Iteration 7900: Loss = -11142.246795832976
Iteration 8000: Loss = -11142.24672739029
Iteration 8100: Loss = -11142.246641898715
Iteration 8200: Loss = -11142.246192301931
Iteration 8300: Loss = -11142.252054255681
1
Iteration 8400: Loss = -11142.245752873583
Iteration 8500: Loss = -11142.24568679043
Iteration 8600: Loss = -11142.245618032124
Iteration 8700: Loss = -11142.390286789036
1
Iteration 8800: Loss = -11142.245474463174
Iteration 8900: Loss = -11142.266336127494
1
Iteration 9000: Loss = -11142.245511666079
2
Iteration 9100: Loss = -11142.24544978893
Iteration 9200: Loss = -11142.25044260205
1
Iteration 9300: Loss = -11142.24531794354
Iteration 9400: Loss = -11142.248046252153
1
Iteration 9500: Loss = -11142.245227449053
Iteration 9600: Loss = -11142.247849957343
1
Iteration 9700: Loss = -11142.24517532919
Iteration 9800: Loss = -11142.245257850456
1
Iteration 9900: Loss = -11142.307722160112
2
Iteration 10000: Loss = -11142.245089933163
Iteration 10100: Loss = -11142.246426097221
1
Iteration 10200: Loss = -11142.2440985545
Iteration 10300: Loss = -11142.244133016577
1
Iteration 10400: Loss = -11142.390312297744
2
Iteration 10500: Loss = -11142.244005164948
Iteration 10600: Loss = -11142.244519220421
1
Iteration 10700: Loss = -11142.24599880623
2
Iteration 10800: Loss = -11142.240230444633
Iteration 10900: Loss = -11142.240419692538
1
Iteration 11000: Loss = -11142.240142484252
Iteration 11100: Loss = -11142.240287089327
1
Iteration 11200: Loss = -11142.240095481746
Iteration 11300: Loss = -11142.245980021778
1
Iteration 11400: Loss = -11142.236580357086
Iteration 11500: Loss = -11142.235591465074
Iteration 11600: Loss = -11142.235456622302
Iteration 11700: Loss = -11142.23536639212
Iteration 11800: Loss = -11142.25513299405
1
Iteration 11900: Loss = -11142.23524734683
Iteration 12000: Loss = -11142.234754629835
Iteration 12100: Loss = -11142.233167722357
Iteration 12200: Loss = -11142.232687548838
Iteration 12300: Loss = -11142.232727252293
1
Iteration 12400: Loss = -11142.232884979207
2
Iteration 12500: Loss = -11142.30059877648
3
Iteration 12600: Loss = -11142.232681351894
Iteration 12700: Loss = -11142.366227584222
1
Iteration 12800: Loss = -11142.23281586122
2
Iteration 12900: Loss = -11142.231823013313
Iteration 13000: Loss = -11142.23363508274
1
Iteration 13100: Loss = -11142.231792813935
Iteration 13200: Loss = -11142.232444312514
1
Iteration 13300: Loss = -11142.231780118567
Iteration 13400: Loss = -11142.25012616621
1
Iteration 13500: Loss = -11142.231813263164
2
Iteration 13600: Loss = -11142.23179726302
3
Iteration 13700: Loss = -11142.234863604286
4
Iteration 13800: Loss = -11142.231176349318
Iteration 13900: Loss = -11142.24293431268
1
Iteration 14000: Loss = -11142.231187319963
2
Iteration 14100: Loss = -11142.235941928058
3
Iteration 14200: Loss = -11142.22889766841
Iteration 14300: Loss = -11142.221432147446
Iteration 14400: Loss = -11142.23136903514
1
Iteration 14500: Loss = -11142.221426099382
Iteration 14600: Loss = -11142.221406719187
Iteration 14700: Loss = -11142.222040088918
1
Iteration 14800: Loss = -11142.224245758396
2
Iteration 14900: Loss = -11142.22132669417
Iteration 15000: Loss = -11142.224398783843
1
Iteration 15100: Loss = -11142.221338901649
2
Iteration 15200: Loss = -11142.228850814712
3
Iteration 15300: Loss = -11142.221255836315
Iteration 15400: Loss = -11142.2458304701
1
Iteration 15500: Loss = -11142.221634535565
2
Iteration 15600: Loss = -11142.22125054503
Iteration 15700: Loss = -11142.22198316014
1
Iteration 15800: Loss = -11142.222030341856
2
Iteration 15900: Loss = -11142.220750207853
Iteration 16000: Loss = -11142.232205744329
1
Iteration 16100: Loss = -11142.22066389056
Iteration 16200: Loss = -11142.226464537016
1
Iteration 16300: Loss = -11142.220389812033
Iteration 16400: Loss = -11142.27825516704
1
Iteration 16500: Loss = -11142.220150508023
Iteration 16600: Loss = -11142.219526595733
Iteration 16700: Loss = -11142.220734305252
1
Iteration 16800: Loss = -11142.21935478608
Iteration 16900: Loss = -11142.21936133933
1
Iteration 17000: Loss = -11142.219859989065
2
Iteration 17100: Loss = -11142.219353576984
Iteration 17200: Loss = -11142.321494125143
1
Iteration 17300: Loss = -11142.219424983701
2
Iteration 17400: Loss = -11142.232522512004
3
Iteration 17500: Loss = -11142.224366265622
4
Iteration 17600: Loss = -11142.219353830462
5
Stopping early at iteration 17600 due to no improvement.
pi: tensor([[0.7943, 0.2057],
        [0.2268, 0.7732]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4612, 0.5388], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1927, 0.1031],
         [0.6991, 0.2870]],

        [[0.6475, 0.1082],
         [0.5992, 0.6993]],

        [[0.7163, 0.1038],
         [0.5722, 0.5872]],

        [[0.6095, 0.1013],
         [0.5513, 0.5950]],

        [[0.5017, 0.1071],
         [0.6568, 0.6671]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9291540354044258
Average Adjusted Rand Index: 0.9294516150356007
11180.363177382842
[0.9291540354044258, 0.9291540354044258, 0.9291540354044258, 0.9291540354044258] [0.9294516150356007, 0.9294516150356007, 0.9294516150356007, 0.9294516150356007] [11142.269870276717, 11142.220822021432, 11142.372489558318, 11142.219353830462]
-----------------------------------------------------------------------------------------
This iteration is 20
True Objective function: Loss = -11220.496986969172
Iteration 0: Loss = -11552.756933820105
Iteration 10: Loss = -11552.756947644895
1
Iteration 20: Loss = -11552.744900025857
Iteration 30: Loss = -11547.992205532708
Iteration 40: Loss = -11543.858536794998
Iteration 50: Loss = -11542.449637549751
Iteration 60: Loss = -11542.08744390403
Iteration 70: Loss = -11541.987565413343
Iteration 80: Loss = -11541.899134781861
Iteration 90: Loss = -11532.433160279992
Iteration 100: Loss = -11271.388929360404
Iteration 110: Loss = -11211.94517816369
Iteration 120: Loss = -11211.945755136538
1
Iteration 130: Loss = -11211.945753481714
2
Iteration 140: Loss = -11211.945763611278
3
Stopping early at iteration 140 due to no improvement.
pi: tensor([[0.7653, 0.2347],
        [0.2443, 0.7557]], dtype=torch.float64)
alpha: tensor([0.5204, 0.4796])
beta: tensor([[[0.2915, 0.0980],
         [0.5474, 0.2042]],

        [[0.6519, 0.0979],
         [0.1711, 0.5875]],

        [[0.8322, 0.0986],
         [0.9656, 0.1210]],

        [[0.9031, 0.0924],
         [0.8970, 0.9995]],

        [[0.5802, 0.1037],
         [0.7862, 0.7803]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.9446733085256775
Average Adjusted Rand Index: 0.9446442946851044
pi: tensor([[1.0000e+00, 4.3344e-26],
        [1.0000e+00, 9.3441e-64]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 3.4675e-26])
beta: tensor([[[0.1752,    nan],
         [0.1800,    nan]],

        [[0.8416,    nan],
         [0.6717, 0.8682]],

        [[0.6334, 0.2832],
         [0.8196, 0.6325]],

        [[0.3067,    nan],
         [0.3428, 0.2455]],

        [[0.4389,    nan],
         [0.5476, 0.7854]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23203.035734353572
Iteration 100: Loss = -11553.691239628268
Iteration 200: Loss = -11552.886174239546
Iteration 300: Loss = -11552.489065017187
Iteration 400: Loss = -11550.978429153365
Iteration 500: Loss = -11548.924730814339
Iteration 600: Loss = -11505.401564083182
Iteration 700: Loss = -11423.260235221393
Iteration 800: Loss = -11220.68197407879
Iteration 900: Loss = -11216.262448226847
Iteration 1000: Loss = -11209.659524797238
Iteration 1100: Loss = -11209.329618877806
Iteration 1200: Loss = -11209.129434394126
Iteration 1300: Loss = -11209.086503239081
Iteration 1400: Loss = -11209.060432667588
Iteration 1500: Loss = -11209.0389106984
Iteration 1600: Loss = -11209.010344884162
Iteration 1700: Loss = -11208.997355961852
Iteration 1800: Loss = -11208.987232836509
Iteration 1900: Loss = -11208.977697388105
Iteration 2000: Loss = -11208.965910775849
Iteration 2100: Loss = -11208.958652635794
Iteration 2200: Loss = -11208.950771282292
Iteration 2300: Loss = -11208.943185519262
Iteration 2400: Loss = -11208.938815383113
Iteration 2500: Loss = -11208.935053509642
Iteration 2600: Loss = -11208.931760260915
Iteration 2700: Loss = -11208.925276756861
Iteration 2800: Loss = -11208.92257120476
Iteration 2900: Loss = -11208.920587003404
Iteration 3000: Loss = -11208.916002949294
Iteration 3100: Loss = -11208.914543581373
Iteration 3200: Loss = -11208.936718675575
1
Iteration 3300: Loss = -11208.91214029635
Iteration 3400: Loss = -11208.911127237901
Iteration 3500: Loss = -11208.910221476193
Iteration 3600: Loss = -11208.909287150438
Iteration 3700: Loss = -11208.908453350214
Iteration 3800: Loss = -11208.907933116294
Iteration 3900: Loss = -11208.906781150909
Iteration 4000: Loss = -11208.913153440151
1
Iteration 4100: Loss = -11208.904923670701
Iteration 4200: Loss = -11208.904077910103
Iteration 4300: Loss = -11208.903662854806
Iteration 4400: Loss = -11208.904074080272
1
Iteration 4500: Loss = -11208.903664858708
2
Iteration 4600: Loss = -11208.901944082078
Iteration 4700: Loss = -11208.9014826514
Iteration 4800: Loss = -11208.900622063871
Iteration 4900: Loss = -11208.896800646105
Iteration 5000: Loss = -11208.892970906918
Iteration 5100: Loss = -11208.8987962448
1
Iteration 5200: Loss = -11208.892259456461
Iteration 5300: Loss = -11208.891744579367
Iteration 5400: Loss = -11208.891039081062
Iteration 5500: Loss = -11208.890550969196
Iteration 5600: Loss = -11208.889911318944
Iteration 5700: Loss = -11208.887713846168
Iteration 5800: Loss = -11208.883850307378
Iteration 5900: Loss = -11208.883438296965
Iteration 6000: Loss = -11208.883147857214
Iteration 6100: Loss = -11208.883297142864
1
Iteration 6200: Loss = -11208.882825276265
Iteration 6300: Loss = -11208.882548848356
Iteration 6400: Loss = -11208.883075460728
1
Iteration 6500: Loss = -11208.882597696416
2
Iteration 6600: Loss = -11208.88374027492
3
Iteration 6700: Loss = -11208.888540642982
4
Iteration 6800: Loss = -11208.881887343026
Iteration 6900: Loss = -11208.88213674079
1
Iteration 7000: Loss = -11208.882022246255
2
Iteration 7100: Loss = -11208.8904704094
3
Iteration 7200: Loss = -11208.881283568555
Iteration 7300: Loss = -11208.881275296015
Iteration 7400: Loss = -11208.881792726266
1
Iteration 7500: Loss = -11208.888094590407
2
Iteration 7600: Loss = -11208.88104483584
Iteration 7700: Loss = -11208.881001616865
Iteration 7800: Loss = -11208.882198737056
1
Iteration 7900: Loss = -11208.88140245599
2
Iteration 8000: Loss = -11208.88095264793
Iteration 8100: Loss = -11208.880817424722
Iteration 8200: Loss = -11208.882846702725
1
Iteration 8300: Loss = -11208.880728163705
Iteration 8400: Loss = -11208.889951572233
1
Iteration 8500: Loss = -11208.880631658678
Iteration 8600: Loss = -11208.880576236234
Iteration 8700: Loss = -11208.880542484638
Iteration 8800: Loss = -11208.880448590553
Iteration 8900: Loss = -11208.882301563428
1
Iteration 9000: Loss = -11208.880379348106
Iteration 9100: Loss = -11208.880321620974
Iteration 9200: Loss = -11208.880319075402
Iteration 9300: Loss = -11208.880515238954
1
Iteration 9400: Loss = -11208.888211222991
2
Iteration 9500: Loss = -11208.880667562862
3
Iteration 9600: Loss = -11208.887218933354
4
Iteration 9700: Loss = -11208.880692062698
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.7651, 0.2349],
        [0.2284, 0.7716]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4905, 0.5095], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2078, 0.0982],
         [0.6325, 0.2985]],

        [[0.6792, 0.0984],
         [0.6352, 0.6074]],

        [[0.6705, 0.0981],
         [0.6451, 0.6599]],

        [[0.6418, 0.0924],
         [0.5155, 0.7268]],

        [[0.6612, 0.1036],
         [0.6154, 0.7255]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.9681923543487778
Average Adjusted Rand Index: 0.9684838568458123
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21547.240214393638
Iteration 100: Loss = -11552.453127015835
Iteration 200: Loss = -11550.298032434854
Iteration 300: Loss = -11548.16956088626
Iteration 400: Loss = -11547.306557785394
Iteration 500: Loss = -11547.031135896666
Iteration 600: Loss = -11546.817784476374
Iteration 700: Loss = -11546.579223124314
Iteration 800: Loss = -11546.327607083962
Iteration 900: Loss = -11546.10573324083
Iteration 1000: Loss = -11545.935736981113
Iteration 1100: Loss = -11544.770512578998
Iteration 1200: Loss = -11421.915344612167
Iteration 1300: Loss = -11391.627256612042
Iteration 1400: Loss = -11391.335127191662
Iteration 1500: Loss = -11391.28937811877
Iteration 1600: Loss = -11391.261820913041
Iteration 1700: Loss = -11390.841595055988
Iteration 1800: Loss = -11390.814938279165
Iteration 1900: Loss = -11390.789683354691
Iteration 2000: Loss = -11390.75591524199
Iteration 2100: Loss = -11390.739106866577
Iteration 2200: Loss = -11390.730222129063
Iteration 2300: Loss = -11390.488161236213
Iteration 2400: Loss = -11390.443877121052
Iteration 2500: Loss = -11390.434184620264
Iteration 2600: Loss = -11386.116900370107
Iteration 2700: Loss = -11386.085260008296
Iteration 2800: Loss = -11383.555245712316
Iteration 2900: Loss = -11381.241303941948
Iteration 3000: Loss = -11376.573278892272
Iteration 3100: Loss = -11357.679061398878
Iteration 3200: Loss = -11315.595635993126
Iteration 3300: Loss = -11313.403153819074
Iteration 3400: Loss = -11313.09974468673
Iteration 3500: Loss = -11313.08955841584
Iteration 3600: Loss = -11313.058218082735
Iteration 3700: Loss = -11309.849210832814
Iteration 3800: Loss = -11305.2204271062
Iteration 3900: Loss = -11304.825353118336
Iteration 4000: Loss = -11303.51174439239
Iteration 4100: Loss = -11301.040777222946
Iteration 4200: Loss = -11297.275447135105
Iteration 4300: Loss = -11297.272499607836
Iteration 4400: Loss = -11297.270972416602
Iteration 4500: Loss = -11297.269284999435
Iteration 4600: Loss = -11297.268667196238
Iteration 4700: Loss = -11297.255194473455
Iteration 4800: Loss = -11297.260898571803
1
Iteration 4900: Loss = -11297.257427134578
2
Iteration 5000: Loss = -11297.250584839214
Iteration 5100: Loss = -11297.250179581048
Iteration 5200: Loss = -11297.250069040714
Iteration 5300: Loss = -11297.254505278761
1
Iteration 5400: Loss = -11297.249537246618
Iteration 5500: Loss = -11297.249335414372
Iteration 5600: Loss = -11297.26442555945
1
Iteration 5700: Loss = -11297.24850370743
Iteration 5800: Loss = -11297.250078790825
1
Iteration 5900: Loss = -11297.21325304696
Iteration 6000: Loss = -11297.21299989894
Iteration 6100: Loss = -11297.214715766531
1
Iteration 6200: Loss = -11297.212102615567
Iteration 6300: Loss = -11297.21529144446
1
Iteration 6400: Loss = -11297.211474488851
Iteration 6500: Loss = -11297.214722620132
1
Iteration 6600: Loss = -11297.216763836672
2
Iteration 6700: Loss = -11297.214348550027
3
Iteration 6800: Loss = -11297.204481474055
Iteration 6900: Loss = -11297.205659113939
1
Iteration 7000: Loss = -11297.20340415545
Iteration 7100: Loss = -11297.203068492632
Iteration 7200: Loss = -11297.201770076863
Iteration 7300: Loss = -11297.20128271604
Iteration 7400: Loss = -11297.201120892245
Iteration 7500: Loss = -11297.20095497745
Iteration 7600: Loss = -11297.200681978411
Iteration 7700: Loss = -11297.200513105037
Iteration 7800: Loss = -11297.200330933083
Iteration 7900: Loss = -11297.211315276056
1
Iteration 8000: Loss = -11297.199757593353
Iteration 8100: Loss = -11297.199642472277
Iteration 8200: Loss = -11297.213836606885
1
Iteration 8300: Loss = -11297.181873829579
Iteration 8400: Loss = -11297.181813135801
Iteration 8500: Loss = -11297.183248829142
1
Iteration 8600: Loss = -11297.181770818666
Iteration 8700: Loss = -11297.181743933295
Iteration 8800: Loss = -11297.181909630583
1
Iteration 8900: Loss = -11297.180844556344
Iteration 9000: Loss = -11297.179070083097
Iteration 9100: Loss = -11296.803019731511
Iteration 9200: Loss = -11296.799636567506
Iteration 9300: Loss = -11296.809925210886
1
Iteration 9400: Loss = -11296.79959164715
Iteration 9500: Loss = -11296.80016341024
1
Iteration 9600: Loss = -11296.804583767189
2
Iteration 9700: Loss = -11296.799506700947
Iteration 9800: Loss = -11296.8144425961
1
Iteration 9900: Loss = -11296.799381507122
Iteration 10000: Loss = -11296.801873208744
1
Iteration 10100: Loss = -11296.805896256608
2
Iteration 10200: Loss = -11297.020365455832
3
Iteration 10300: Loss = -11296.800117506378
4
Iteration 10400: Loss = -11296.799487859304
5
Stopping early at iteration 10400 due to no improvement.
pi: tensor([[0.6633, 0.3367],
        [0.3553, 0.6447]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4509, 0.5491], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2643, 0.0962],
         [0.5261, 0.2482]],

        [[0.6201, 0.0969],
         [0.6861, 0.5653]],

        [[0.5186, 0.0989],
         [0.6764, 0.6533]],

        [[0.5577, 0.0911],
         [0.6337, 0.7122]],

        [[0.5657, 0.1031],
         [0.7175, 0.5518]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080965973782139
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
Global Adjusted Rand Index: 0.04302819962080202
Average Adjusted Rand Index: 0.8832331119763606
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22892.44654646128
Iteration 100: Loss = -11553.587533159873
Iteration 200: Loss = -11552.254016849185
Iteration 300: Loss = -11551.231078398749
Iteration 400: Loss = -11548.59591324704
Iteration 500: Loss = -11545.528533354009
Iteration 600: Loss = -11532.171750438345
Iteration 700: Loss = -11462.838206428549
Iteration 800: Loss = -11256.34135487649
Iteration 900: Loss = -11234.237767333765
Iteration 1000: Loss = -11228.447931927825
Iteration 1100: Loss = -11220.987210149495
Iteration 1200: Loss = -11212.278974276578
Iteration 1300: Loss = -11209.632595526571
Iteration 1400: Loss = -11209.417815748853
Iteration 1500: Loss = -11209.331378837038
Iteration 1600: Loss = -11209.276371714377
Iteration 1700: Loss = -11209.235373823241
Iteration 1800: Loss = -11209.20233236781
Iteration 1900: Loss = -11209.17295493184
Iteration 2000: Loss = -11209.138000282808
Iteration 2100: Loss = -11209.077392070856
Iteration 2200: Loss = -11209.050009766104
Iteration 2300: Loss = -11208.933875730008
Iteration 2400: Loss = -11208.909795575604
Iteration 2500: Loss = -11208.900095663059
Iteration 2600: Loss = -11208.892187862339
Iteration 2700: Loss = -11208.885435906142
Iteration 2800: Loss = -11208.879849760973
Iteration 2900: Loss = -11208.874467630703
Iteration 3000: Loss = -11208.869877202307
Iteration 3100: Loss = -11208.865630721026
Iteration 3200: Loss = -11208.8614588931
Iteration 3300: Loss = -11208.86203866517
1
Iteration 3400: Loss = -11208.851469280324
Iteration 3500: Loss = -11208.847988472215
Iteration 3600: Loss = -11208.845332189177
Iteration 3700: Loss = -11208.84605880441
1
Iteration 3800: Loss = -11208.841048602051
Iteration 3900: Loss = -11208.839804464184
Iteration 4000: Loss = -11208.837576196522
Iteration 4100: Loss = -11208.837592558035
1
Iteration 4200: Loss = -11208.834638440638
Iteration 4300: Loss = -11208.83336860247
Iteration 4400: Loss = -11208.832193920245
Iteration 4500: Loss = -11208.839155186688
1
Iteration 4600: Loss = -11208.8300460463
Iteration 4700: Loss = -11208.82917970458
Iteration 4800: Loss = -11208.82819779791
Iteration 4900: Loss = -11208.827498258624
Iteration 5000: Loss = -11208.826575349882
Iteration 5100: Loss = -11208.826445057673
Iteration 5200: Loss = -11208.825136392526
Iteration 5300: Loss = -11208.824448790549
Iteration 5400: Loss = -11208.823740482563
Iteration 5500: Loss = -11208.823016550607
Iteration 5600: Loss = -11208.82556521819
1
Iteration 5700: Loss = -11208.821462290814
Iteration 5800: Loss = -11208.819833357651
Iteration 5900: Loss = -11208.820571661057
1
Iteration 6000: Loss = -11208.813930471082
Iteration 6100: Loss = -11208.81338011025
Iteration 6200: Loss = -11208.812834294802
Iteration 6300: Loss = -11208.82000516689
1
Iteration 6400: Loss = -11208.810950740586
Iteration 6500: Loss = -11208.811259802553
1
Iteration 6600: Loss = -11208.80962030075
Iteration 6700: Loss = -11208.808620999798
Iteration 6800: Loss = -11208.808179938247
Iteration 6900: Loss = -11208.807861755538
Iteration 7000: Loss = -11208.807653994116
Iteration 7100: Loss = -11208.80727455035
Iteration 7200: Loss = -11208.80703614442
Iteration 7300: Loss = -11208.807053569646
1
Iteration 7400: Loss = -11208.806616787484
Iteration 7500: Loss = -11208.80724413763
1
Iteration 7600: Loss = -11208.807515627765
2
Iteration 7700: Loss = -11208.807706248688
3
Iteration 7800: Loss = -11208.806653109878
4
Iteration 7900: Loss = -11208.834062511207
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7642, 0.2358],
        [0.2313, 0.7687]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4928, 0.5072], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2078, 0.0983],
         [0.6980, 0.2997]],

        [[0.5011, 0.0984],
         [0.6868, 0.7133]],

        [[0.5556, 0.0983],
         [0.7086, 0.6937]],

        [[0.7114, 0.0924],
         [0.6874, 0.6479]],

        [[0.6756, 0.1036],
         [0.6463, 0.5983]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.9681923543487778
Average Adjusted Rand Index: 0.9684838568458123
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21364.43245237795
Iteration 100: Loss = -11552.651261032399
Iteration 200: Loss = -11551.922674094463
Iteration 300: Loss = -11551.09559122675
Iteration 400: Loss = -11550.325586657194
Iteration 500: Loss = -11549.808462418023
Iteration 600: Loss = -11547.95504023791
Iteration 700: Loss = -11546.854857875009
Iteration 800: Loss = -11504.232510855463
Iteration 900: Loss = -11468.023349404151
Iteration 1000: Loss = -11226.546888171977
Iteration 1100: Loss = -11220.03584435836
Iteration 1200: Loss = -11219.566042361574
Iteration 1300: Loss = -11215.695610344155
Iteration 1400: Loss = -11213.417534180502
Iteration 1500: Loss = -11209.506873536871
Iteration 1600: Loss = -11209.301953528744
Iteration 1700: Loss = -11209.256045002827
Iteration 1800: Loss = -11209.149168457858
Iteration 1900: Loss = -11209.130168282254
Iteration 2000: Loss = -11209.12357258435
Iteration 2100: Loss = -11209.118232600553
Iteration 2200: Loss = -11209.113806072552
Iteration 2300: Loss = -11209.109983187049
Iteration 2400: Loss = -11209.10673309878
Iteration 2500: Loss = -11209.103924785279
Iteration 2600: Loss = -11209.101488366998
Iteration 2700: Loss = -11209.099279345985
Iteration 2800: Loss = -11209.097310859328
Iteration 2900: Loss = -11209.095387523319
Iteration 3000: Loss = -11209.09327489915
Iteration 3100: Loss = -11209.090456985206
Iteration 3200: Loss = -11209.077576705024
Iteration 3300: Loss = -11209.035117013493
Iteration 3400: Loss = -11209.032902979452
Iteration 3500: Loss = -11209.031073079203
Iteration 3600: Loss = -11209.029035310872
Iteration 3700: Loss = -11209.025651867321
Iteration 3800: Loss = -11209.028299146565
1
Iteration 3900: Loss = -11209.025584994926
Iteration 4000: Loss = -11209.021793995735
Iteration 4100: Loss = -11209.007243374326
Iteration 4200: Loss = -11208.993274008453
Iteration 4300: Loss = -11208.992497892681
Iteration 4400: Loss = -11208.991208682917
Iteration 4500: Loss = -11208.982957071166
Iteration 4600: Loss = -11208.982510918659
Iteration 4700: Loss = -11208.982199743714
Iteration 4800: Loss = -11208.981892875772
Iteration 4900: Loss = -11208.981534946735
Iteration 5000: Loss = -11208.981324401066
Iteration 5100: Loss = -11208.983364560545
1
Iteration 5200: Loss = -11208.987581424992
2
Iteration 5300: Loss = -11208.981801410608
3
Iteration 5400: Loss = -11208.979088261105
Iteration 5500: Loss = -11208.978551283919
Iteration 5600: Loss = -11208.978430667517
Iteration 5700: Loss = -11208.978195392086
Iteration 5800: Loss = -11208.978323668061
1
Iteration 5900: Loss = -11208.981120229046
2
Iteration 6000: Loss = -11208.982692132078
3
Iteration 6100: Loss = -11208.981534122933
4
Iteration 6200: Loss = -11208.977501270581
Iteration 6300: Loss = -11208.977379970433
Iteration 6400: Loss = -11208.97690526059
Iteration 6500: Loss = -11208.974291651823
Iteration 6600: Loss = -11208.821235419757
Iteration 6700: Loss = -11208.821306900973
1
Iteration 6800: Loss = -11208.821181738584
Iteration 6900: Loss = -11208.82058450691
Iteration 7000: Loss = -11208.820628898462
1
Iteration 7100: Loss = -11208.825221835836
2
Iteration 7200: Loss = -11208.821693990176
3
Iteration 7300: Loss = -11208.821599788767
4
Iteration 7400: Loss = -11208.827061724021
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.7630, 0.2370],
        [0.2296, 0.7704]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4936, 0.5064], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2078, 0.0982],
         [0.6159, 0.2988]],

        [[0.6255, 0.0984],
         [0.5952, 0.5806]],

        [[0.6348, 0.0983],
         [0.7071, 0.6771]],

        [[0.7289, 0.0924],
         [0.5076, 0.5842]],

        [[0.6967, 0.1036],
         [0.5372, 0.5308]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.9681923543487778
Average Adjusted Rand Index: 0.9684838568458123
11220.496986969172
[0.9681923543487778, 0.04302819962080202, 0.9681923543487778, 0.9681923543487778] [0.9684838568458123, 0.8832331119763606, 0.9684838568458123, 0.9684838568458123] [11208.880692062698, 11296.799487859304, 11208.834062511207, 11208.827061724021]
-----------------------------------------------------------------------------------------
This iteration is 21
True Objective function: Loss = -11164.936214332021
Iteration 0: Loss = -11433.338275960257
Iteration 10: Loss = -11419.283448352098
Iteration 20: Loss = -11418.143620619823
Iteration 30: Loss = -11416.997806880068
Iteration 40: Loss = -11371.02521233813
Iteration 50: Loss = -11144.390305029783
Iteration 60: Loss = -11144.401081085258
1
Iteration 70: Loss = -11144.403679118
2
Iteration 80: Loss = -11144.404029279256
3
Stopping early at iteration 80 due to no improvement.
pi: tensor([[0.7197, 0.2803],
        [0.2585, 0.7415]], dtype=torch.float64)
alpha: tensor([0.5173, 0.4827])
beta: tensor([[[0.1958, 0.0921],
         [0.6183, 0.2947]],

        [[0.8437, 0.0953],
         [0.1358, 0.7711]],

        [[0.5875, 0.1055],
         [0.3336, 0.1908]],

        [[0.9572, 0.1093],
         [0.6705, 0.5300]],

        [[0.7376, 0.1008],
         [0.8036, 0.3897]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9368977708572352
Average Adjusted Rand Index: 0.9366437526971272
Iteration 0: Loss = -11563.597786221897
Iteration 10: Loss = -11212.607772453219
Iteration 20: Loss = -11144.398386682778
Iteration 30: Loss = -11144.403626355801
1
Iteration 40: Loss = -11144.404033495726
2
Iteration 50: Loss = -11144.404064306957
3
Stopping early at iteration 50 due to no improvement.
pi: tensor([[0.7415, 0.2585],
        [0.2803, 0.7197]], dtype=torch.float64)
alpha: tensor([0.4827, 0.5173])
beta: tensor([[[0.2947, 0.0921],
         [0.5990, 0.1958]],

        [[0.3198, 0.0953],
         [0.1810, 0.2294]],

        [[0.9720, 0.1055],
         [0.7201, 0.7675]],

        [[0.1611, 0.1093],
         [0.6855, 0.8871]],

        [[0.6310, 0.1008],
         [0.1340, 0.4690]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9368977708572352
Average Adjusted Rand Index: 0.9366437526971272
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20338.408466712466
Iteration 100: Loss = -11423.870297256712
Iteration 200: Loss = -11421.006761727358
Iteration 300: Loss = -11328.79943058811
Iteration 400: Loss = -11225.44368888145
Iteration 500: Loss = -11216.288160149139
Iteration 600: Loss = -11216.039982903489
Iteration 700: Loss = -11215.929815508614
Iteration 800: Loss = -11215.874248305228
Iteration 900: Loss = -11215.837426885168
Iteration 1000: Loss = -11215.807136335412
Iteration 1100: Loss = -11215.778417968113
Iteration 1200: Loss = -11215.749315107783
Iteration 1300: Loss = -11215.714675615694
Iteration 1400: Loss = -11215.664249091818
Iteration 1500: Loss = -11215.568651866672
Iteration 1600: Loss = -11215.266928578123
Iteration 1700: Loss = -11214.589384706718
Iteration 1800: Loss = -11214.42924700731
Iteration 1900: Loss = -11214.368968600393
Iteration 2000: Loss = -11214.340590658965
Iteration 2100: Loss = -11214.325370654602
Iteration 2200: Loss = -11214.316473278404
Iteration 2300: Loss = -11214.310817671332
Iteration 2400: Loss = -11214.307125477128
Iteration 2500: Loss = -11214.304378425442
Iteration 2600: Loss = -11214.302916531326
Iteration 2700: Loss = -11214.300951539235
Iteration 2800: Loss = -11214.30083643595
Iteration 2900: Loss = -11214.298967203831
Iteration 3000: Loss = -11214.29809838911
Iteration 3100: Loss = -11214.297844616138
Iteration 3200: Loss = -11214.2971550004
Iteration 3300: Loss = -11214.296658140529
Iteration 3400: Loss = -11214.296296369188
Iteration 3500: Loss = -11214.297408585302
1
Iteration 3600: Loss = -11214.295591358592
Iteration 3700: Loss = -11214.295283123709
Iteration 3800: Loss = -11214.295059084934
Iteration 3900: Loss = -11214.29501742886
Iteration 4000: Loss = -11214.303154689114
1
Iteration 4100: Loss = -11214.294481772618
Iteration 4200: Loss = -11214.294944481893
1
Iteration 4300: Loss = -11214.294242440108
Iteration 4400: Loss = -11214.294099766752
Iteration 4500: Loss = -11214.294066512155
Iteration 4600: Loss = -11214.294453891429
1
Iteration 4700: Loss = -11214.293779785145
Iteration 4800: Loss = -11214.293745423049
Iteration 4900: Loss = -11214.29415514345
1
Iteration 5000: Loss = -11214.293556059789
Iteration 5100: Loss = -11214.293545750686
Iteration 5200: Loss = -11214.293410346467
Iteration 5300: Loss = -11214.293375124074
Iteration 5400: Loss = -11214.293294190147
Iteration 5500: Loss = -11214.293358878636
1
Iteration 5600: Loss = -11214.29318552261
Iteration 5700: Loss = -11214.299096086157
1
Iteration 5800: Loss = -11214.29310924473
Iteration 5900: Loss = -11214.293052183126
Iteration 6000: Loss = -11214.293025012705
Iteration 6100: Loss = -11214.292977852836
Iteration 6200: Loss = -11214.29293985378
Iteration 6300: Loss = -11214.292971995324
1
Iteration 6400: Loss = -11214.292888209931
Iteration 6500: Loss = -11214.292897415857
1
Iteration 6600: Loss = -11214.293117398882
2
Iteration 6700: Loss = -11214.292812942876
Iteration 6800: Loss = -11214.298522529105
1
Iteration 6900: Loss = -11214.292803935494
Iteration 7000: Loss = -11214.298162875295
1
Iteration 7100: Loss = -11214.315702316722
2
Iteration 7200: Loss = -11214.349610467183
3
Iteration 7300: Loss = -11214.292730441428
Iteration 7400: Loss = -11214.292758995765
1
Iteration 7500: Loss = -11214.293459800972
2
Iteration 7600: Loss = -11214.292671553787
Iteration 7700: Loss = -11214.296809435918
1
Iteration 7800: Loss = -11214.292657433944
Iteration 7900: Loss = -11214.334134399867
1
Iteration 8000: Loss = -11214.292643858296
Iteration 8100: Loss = -11214.292653477332
1
Iteration 8200: Loss = -11214.293421848985
2
Iteration 8300: Loss = -11214.292600485667
Iteration 8400: Loss = -11214.295792013578
1
Iteration 8500: Loss = -11214.292946224412
2
Iteration 8600: Loss = -11214.293936021026
3
Iteration 8700: Loss = -11214.292597124928
Iteration 8800: Loss = -11214.292613111245
1
Iteration 8900: Loss = -11214.305579038386
2
Iteration 9000: Loss = -11214.292599855704
3
Iteration 9100: Loss = -11214.292883264403
4
Iteration 9200: Loss = -11214.292589392331
Iteration 9300: Loss = -11214.294444014022
1
Iteration 9400: Loss = -11214.304036716569
2
Iteration 9500: Loss = -11214.292560879117
Iteration 9600: Loss = -11214.315922584443
1
Iteration 9700: Loss = -11214.292670751935
2
Iteration 9800: Loss = -11214.411709874836
3
Iteration 9900: Loss = -11214.292549186273
Iteration 10000: Loss = -11214.410192971429
1
Iteration 10100: Loss = -11214.292524487862
Iteration 10200: Loss = -11214.296098342997
1
Iteration 10300: Loss = -11214.316436170537
2
Iteration 10400: Loss = -11214.292801109
3
Iteration 10500: Loss = -11214.292553379542
4
Iteration 10600: Loss = -11214.411056547813
5
Stopping early at iteration 10600 due to no improvement.
pi: tensor([[0.7467, 0.2533],
        [0.3480, 0.6520]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0255, 0.9745], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3049, 0.0943],
         [0.6688, 0.1837]],

        [[0.5733, 0.0984],
         [0.6858, 0.5310]],

        [[0.6977, 0.1047],
         [0.6765, 0.6194]],

        [[0.5950, 0.1086],
         [0.5845, 0.6008]],

        [[0.6880, 0.1001],
         [0.5471, 0.5764]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.6202016374437914
Average Adjusted Rand Index: 0.7284700897989642
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22788.441872482777
Iteration 100: Loss = -11425.453100015739
Iteration 200: Loss = -11424.265908916914
Iteration 300: Loss = -11423.216537235303
Iteration 400: Loss = -11419.879257119344
Iteration 500: Loss = -11406.845846330962
Iteration 600: Loss = -11365.16810333454
Iteration 700: Loss = -11325.594032122825
Iteration 800: Loss = -11288.538314685697
Iteration 900: Loss = -11250.810171402807
Iteration 1000: Loss = -11245.86640072624
Iteration 1100: Loss = -11245.180789749269
Iteration 1200: Loss = -11242.55173129394
Iteration 1300: Loss = -11239.301561637989
Iteration 1400: Loss = -11233.411301713148
Iteration 1500: Loss = -11227.76138716215
Iteration 1600: Loss = -11222.541445100984
Iteration 1700: Loss = -11222.50083800739
Iteration 1800: Loss = -11222.47048291047
Iteration 1900: Loss = -11222.436802090799
Iteration 2000: Loss = -11219.84997929042
Iteration 2100: Loss = -11217.126579315669
Iteration 2200: Loss = -11217.111534521422
Iteration 2300: Loss = -11217.100015682014
Iteration 2400: Loss = -11217.0901514343
Iteration 2500: Loss = -11217.081989732797
Iteration 2600: Loss = -11217.074851086543
Iteration 2700: Loss = -11217.06840437538
Iteration 2800: Loss = -11217.062557496833
Iteration 2900: Loss = -11217.057369941502
Iteration 3000: Loss = -11217.052878430497
Iteration 3100: Loss = -11217.048855755076
Iteration 3200: Loss = -11217.045335744275
Iteration 3300: Loss = -11217.079335574104
1
Iteration 3400: Loss = -11217.039108921488
Iteration 3500: Loss = -11217.036272249343
Iteration 3600: Loss = -11217.039571635833
1
Iteration 3700: Loss = -11217.03059577366
Iteration 3800: Loss = -11217.02768055961
Iteration 3900: Loss = -11217.02529332363
Iteration 4000: Loss = -11217.022722745158
Iteration 4100: Loss = -11217.005815569337
Iteration 4200: Loss = -11215.888954193297
Iteration 4300: Loss = -11215.886868702479
Iteration 4400: Loss = -11215.886176923874
Iteration 4500: Loss = -11215.882945895812
Iteration 4600: Loss = -11215.881471967097
Iteration 4700: Loss = -11215.880142396552
Iteration 4800: Loss = -11215.878508887841
Iteration 4900: Loss = -11215.876512182205
Iteration 5000: Loss = -11215.875026539448
Iteration 5100: Loss = -11215.874152189783
Iteration 5200: Loss = -11215.873279219104
Iteration 5300: Loss = -11215.872486653276
Iteration 5400: Loss = -11215.87145463365
Iteration 5500: Loss = -11215.885071910456
1
Iteration 5600: Loss = -11215.867647629488
Iteration 5700: Loss = -11215.8671345867
Iteration 5800: Loss = -11215.868076340623
1
Iteration 5900: Loss = -11215.866192234163
Iteration 6000: Loss = -11215.865777730633
Iteration 6100: Loss = -11215.865409233018
Iteration 6200: Loss = -11215.864981120129
Iteration 6300: Loss = -11215.864637338169
Iteration 6400: Loss = -11215.864354104515
Iteration 6500: Loss = -11215.863915383534
Iteration 6600: Loss = -11215.863590159965
Iteration 6700: Loss = -11215.864259074264
1
Iteration 6800: Loss = -11215.849189466604
Iteration 6900: Loss = -11215.844533182468
Iteration 7000: Loss = -11215.845777686513
1
Iteration 7100: Loss = -11215.844738472848
2
Iteration 7200: Loss = -11215.843495644584
Iteration 7300: Loss = -11215.843168134943
Iteration 7400: Loss = -11215.842673306997
Iteration 7500: Loss = -11215.842113527999
Iteration 7600: Loss = -11215.841457174945
Iteration 7700: Loss = -11215.84152967955
1
Iteration 7800: Loss = -11215.837678804106
Iteration 7900: Loss = -11215.274679892509
Iteration 8000: Loss = -11214.768986431503
Iteration 8100: Loss = -11214.685329684911
Iteration 8200: Loss = -11214.515260865279
Iteration 8300: Loss = -11214.460852620508
Iteration 8400: Loss = -11214.451393776497
Iteration 8500: Loss = -11214.446161746406
Iteration 8600: Loss = -11214.453166321191
1
Iteration 8700: Loss = -11214.458759881898
2
Iteration 8800: Loss = -11214.441114583538
Iteration 8900: Loss = -11214.43774107356
Iteration 9000: Loss = -11214.386063394704
Iteration 9100: Loss = -11214.385957935427
Iteration 9200: Loss = -11214.383035299501
Iteration 9300: Loss = -11214.382401660901
Iteration 9400: Loss = -11214.381959546747
Iteration 9500: Loss = -11214.371720798725
Iteration 9600: Loss = -11214.45787502898
1
Iteration 9700: Loss = -11214.361115683943
Iteration 9800: Loss = -11214.439263021775
1
Iteration 9900: Loss = -11214.360814975242
Iteration 10000: Loss = -11214.359415239463
Iteration 10100: Loss = -11214.354829773763
Iteration 10200: Loss = -11214.354720911524
Iteration 10300: Loss = -11214.355054030339
1
Iteration 10400: Loss = -11214.35465213169
Iteration 10500: Loss = -11214.3564389995
1
Iteration 10600: Loss = -11214.354556678723
Iteration 10700: Loss = -11214.353928423025
Iteration 10800: Loss = -11214.373658588687
1
Iteration 10900: Loss = -11214.358042205962
2
Iteration 11000: Loss = -11214.35319606621
Iteration 11100: Loss = -11214.3472273971
Iteration 11200: Loss = -11214.339412577896
Iteration 11300: Loss = -11214.336332837122
Iteration 11400: Loss = -11214.325880700599
Iteration 11500: Loss = -11214.3361249784
1
Iteration 11600: Loss = -11214.325805710658
Iteration 11700: Loss = -11214.451241457098
1
Iteration 11800: Loss = -11214.322870211061
Iteration 11900: Loss = -11214.341633634638
1
Iteration 12000: Loss = -11214.323097201921
2
Iteration 12100: Loss = -11214.319917010047
Iteration 12200: Loss = -11214.321970162277
1
Iteration 12300: Loss = -11214.353534853937
2
Iteration 12400: Loss = -11214.313218732657
Iteration 12500: Loss = -11214.314064454391
1
Iteration 12600: Loss = -11214.330065110686
2
Iteration 12700: Loss = -11214.31841704366
3
Iteration 12800: Loss = -11214.314476145113
4
Iteration 12900: Loss = -11214.313496908255
5
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[0.6549, 0.3451],
        [0.2573, 0.7427]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9747, 0.0253], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1833, 0.0940],
         [0.5954, 0.3059]],

        [[0.5796, 0.0987],
         [0.5642, 0.6695]],

        [[0.6199, 0.1051],
         [0.6232, 0.7265]],

        [[0.7191, 0.1096],
         [0.5428, 0.6161]],

        [[0.6175, 0.1010],
         [0.7251, 0.6346]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.6202016374437914
Average Adjusted Rand Index: 0.7284700897989642
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21187.596804926932
Iteration 100: Loss = -11424.942879624774
Iteration 200: Loss = -11424.009412518762
Iteration 300: Loss = -11422.805869584916
Iteration 400: Loss = -11415.790760282287
Iteration 500: Loss = -11326.225278784137
Iteration 600: Loss = -11269.732495074184
Iteration 700: Loss = -11237.041467614847
Iteration 800: Loss = -11217.561937475173
Iteration 900: Loss = -11216.241441827884
Iteration 1000: Loss = -11216.08061083236
Iteration 1100: Loss = -11216.01423940432
Iteration 1200: Loss = -11215.96968285627
Iteration 1300: Loss = -11215.93684469619
Iteration 1400: Loss = -11215.908895120394
Iteration 1500: Loss = -11215.889947482068
Iteration 1600: Loss = -11215.875722130324
Iteration 1700: Loss = -11215.863466538507
Iteration 1800: Loss = -11215.852193439481
Iteration 1900: Loss = -11215.840364629845
Iteration 2000: Loss = -11215.825157616855
Iteration 2100: Loss = -11215.789821398013
Iteration 2200: Loss = -11215.032474667236
Iteration 2300: Loss = -11214.450020944296
Iteration 2400: Loss = -11214.387069546558
Iteration 2500: Loss = -11214.36727333731
Iteration 2600: Loss = -11214.358758312903
Iteration 2700: Loss = -11214.351231476006
Iteration 2800: Loss = -11214.347866752938
Iteration 2900: Loss = -11214.342748718469
Iteration 3000: Loss = -11214.339118216549
Iteration 3100: Loss = -11214.33332633424
Iteration 3200: Loss = -11214.317072279744
Iteration 3300: Loss = -11214.311677083817
Iteration 3400: Loss = -11214.310531211035
Iteration 3500: Loss = -11214.30959851317
Iteration 3600: Loss = -11214.308837596807
Iteration 3700: Loss = -11214.308194634392
Iteration 3800: Loss = -11214.30755776152
Iteration 3900: Loss = -11214.306993390028
Iteration 4000: Loss = -11214.306349840033
Iteration 4100: Loss = -11214.305576171471
Iteration 4200: Loss = -11214.304700838919
Iteration 4300: Loss = -11214.303418328689
Iteration 4400: Loss = -11214.303508729387
1
Iteration 4500: Loss = -11214.302645868223
Iteration 4600: Loss = -11214.302519808474
Iteration 4700: Loss = -11214.302123523665
Iteration 4800: Loss = -11214.302031175353
Iteration 4900: Loss = -11214.301717407081
Iteration 5000: Loss = -11214.345916074966
1
Iteration 5100: Loss = -11214.30136412832
Iteration 5200: Loss = -11214.301208551771
Iteration 5300: Loss = -11214.301848449617
1
Iteration 5400: Loss = -11214.300920527041
Iteration 5500: Loss = -11214.30078228155
Iteration 5600: Loss = -11214.301041842453
1
Iteration 5700: Loss = -11214.30052305576
Iteration 5800: Loss = -11214.300428575594
Iteration 5900: Loss = -11214.300774715506
1
Iteration 6000: Loss = -11214.300172047913
Iteration 6100: Loss = -11214.300091319084
Iteration 6200: Loss = -11214.299986758053
Iteration 6300: Loss = -11214.299884919548
Iteration 6400: Loss = -11214.300588123155
1
Iteration 6500: Loss = -11214.299881257759
Iteration 6600: Loss = -11214.299662692101
Iteration 6700: Loss = -11214.300545623077
1
Iteration 6800: Loss = -11214.29950988144
Iteration 6900: Loss = -11214.29944861347
Iteration 7000: Loss = -11214.299300675213
Iteration 7100: Loss = -11214.29911536637
Iteration 7200: Loss = -11214.298735249964
Iteration 7300: Loss = -11214.298405416394
Iteration 7400: Loss = -11214.294274753142
Iteration 7500: Loss = -11214.294179293047
Iteration 7600: Loss = -11214.294031354559
Iteration 7700: Loss = -11214.293926282451
Iteration 7800: Loss = -11214.293783468316
Iteration 7900: Loss = -11214.29414727122
1
Iteration 8000: Loss = -11214.293330669301
Iteration 8100: Loss = -11214.293680669818
1
Iteration 8200: Loss = -11214.292918022522
Iteration 8300: Loss = -11214.292862905859
Iteration 8400: Loss = -11214.29915475546
1
Iteration 8500: Loss = -11214.292795499692
Iteration 8600: Loss = -11214.292762111767
Iteration 8700: Loss = -11214.293444075633
1
Iteration 8800: Loss = -11214.292726765574
Iteration 8900: Loss = -11214.292753105914
1
Iteration 9000: Loss = -11214.295002662615
2
Iteration 9100: Loss = -11214.29277336215
3
Iteration 9200: Loss = -11214.293004442914
4
Iteration 9300: Loss = -11214.304341753945
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.7427, 0.2573],
        [0.3477, 0.6523]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0257, 0.9743], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3049, 0.0943],
         [0.6792, 0.1837]],

        [[0.5243, 0.0985],
         [0.5075, 0.7283]],

        [[0.7083, 0.1050],
         [0.6595, 0.7249]],

        [[0.7183, 0.1092],
         [0.7067, 0.6918]],

        [[0.5453, 0.1006],
         [0.5980, 0.6507]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.6202016374437914
Average Adjusted Rand Index: 0.7284700897989642
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20550.35869681873
Iteration 100: Loss = -11423.404051893312
Iteration 200: Loss = -11421.431761038075
Iteration 300: Loss = -11418.303822725642
Iteration 400: Loss = -11415.93545737287
Iteration 500: Loss = -11412.265394567832
Iteration 600: Loss = -11279.633207909314
Iteration 700: Loss = -11263.350225806158
Iteration 800: Loss = -11261.538909654153
Iteration 900: Loss = -11260.649102132411
Iteration 1000: Loss = -11260.590154648951
Iteration 1100: Loss = -11260.551577505163
Iteration 1200: Loss = -11260.535652773686
Iteration 1300: Loss = -11260.523918095325
Iteration 1400: Loss = -11260.514961976301
Iteration 1500: Loss = -11260.488408259824
Iteration 1600: Loss = -11260.474611469761
Iteration 1700: Loss = -11260.467051197315
Iteration 1800: Loss = -11260.461383658443
Iteration 1900: Loss = -11260.458320552234
Iteration 2000: Loss = -11260.455905330173
Iteration 2100: Loss = -11260.453687013764
Iteration 2200: Loss = -11260.451594991173
Iteration 2300: Loss = -11260.45187641335
1
Iteration 2400: Loss = -11260.447147851011
Iteration 2500: Loss = -11260.442820026337
Iteration 2600: Loss = -11260.41452530962
Iteration 2700: Loss = -11260.410769663033
Iteration 2800: Loss = -11260.407547192675
Iteration 2900: Loss = -11260.405610837865
Iteration 3000: Loss = -11260.404004330148
Iteration 3100: Loss = -11260.400284578172
Iteration 3200: Loss = -11260.398806542373
Iteration 3300: Loss = -11260.39846774999
Iteration 3400: Loss = -11260.397489987825
Iteration 3500: Loss = -11260.396701253037
Iteration 3600: Loss = -11260.396236697921
Iteration 3700: Loss = -11260.395248857036
Iteration 3800: Loss = -11260.39431557529
Iteration 3900: Loss = -11260.392251337265
Iteration 4000: Loss = -11260.391861798305
Iteration 4100: Loss = -11260.3916818838
Iteration 4200: Loss = -11260.391446886477
Iteration 4300: Loss = -11260.391244147528
Iteration 4400: Loss = -11260.39105500659
Iteration 4500: Loss = -11260.390820674807
Iteration 4600: Loss = -11260.381992763407
Iteration 4700: Loss = -11260.381277769387
Iteration 4800: Loss = -11260.381072001826
Iteration 4900: Loss = -11260.38076234198
Iteration 5000: Loss = -11260.380689858208
Iteration 5100: Loss = -11260.38035513906
Iteration 5200: Loss = -11260.380233825474
Iteration 5300: Loss = -11260.380196072272
Iteration 5400: Loss = -11260.379947572881
Iteration 5500: Loss = -11260.37972986022
Iteration 5600: Loss = -11260.393649378888
1
Iteration 5700: Loss = -11260.379115790176
Iteration 5800: Loss = -11260.378841736507
Iteration 5900: Loss = -11260.378716948611
Iteration 6000: Loss = -11260.378453206642
Iteration 6100: Loss = -11260.378289623102
Iteration 6200: Loss = -11260.384402387535
1
Iteration 6300: Loss = -11260.378067620237
Iteration 6400: Loss = -11260.377992991207
Iteration 6500: Loss = -11260.378598631603
1
Iteration 6600: Loss = -11260.37789201268
Iteration 6700: Loss = -11260.377894652727
1
Iteration 6800: Loss = -11260.377725188348
Iteration 6900: Loss = -11260.375459127028
Iteration 7000: Loss = -11259.679050019098
Iteration 7100: Loss = -11259.676024799404
Iteration 7200: Loss = -11259.67774069784
1
Iteration 7300: Loss = -11259.675859709083
Iteration 7400: Loss = -11259.675830761462
Iteration 7500: Loss = -11259.700454452995
1
Iteration 7600: Loss = -11259.675232510366
Iteration 7700: Loss = -11259.67518215387
Iteration 7800: Loss = -11259.676263440298
1
Iteration 7900: Loss = -11259.675143865556
Iteration 8000: Loss = -11259.675415318676
1
Iteration 8100: Loss = -11259.67511432057
Iteration 8200: Loss = -11259.675530674975
1
Iteration 8300: Loss = -11259.67562079284
2
Iteration 8400: Loss = -11259.67394072237
Iteration 8500: Loss = -11259.673521644501
Iteration 8600: Loss = -11259.672484266064
Iteration 8700: Loss = -11259.672351757483
Iteration 8800: Loss = -11259.672340308847
Iteration 8900: Loss = -11259.672299260406
Iteration 9000: Loss = -11259.673026209282
1
Iteration 9100: Loss = -11259.672228467081
Iteration 9200: Loss = -11259.672256723492
1
Iteration 9300: Loss = -11259.67219758205
Iteration 9400: Loss = -11259.672112785373
Iteration 9500: Loss = -11259.734699235534
1
Iteration 9600: Loss = -11259.672124704872
2
Iteration 9700: Loss = -11259.672123907403
3
Iteration 9800: Loss = -11259.67478514544
4
Iteration 9900: Loss = -11259.67207878929
Iteration 10000: Loss = -11259.716015583017
1
Iteration 10100: Loss = -11259.672073091979
Iteration 10200: Loss = -11259.690199042168
1
Iteration 10300: Loss = -11259.672046137688
Iteration 10400: Loss = -11259.680666545948
1
Iteration 10500: Loss = -11259.689789423443
2
Iteration 10600: Loss = -11259.67494683322
3
Iteration 10700: Loss = -11259.673105736798
4
Iteration 10800: Loss = -11259.672002239511
Iteration 10900: Loss = -11259.672514231954
1
Iteration 11000: Loss = -11259.67385474316
2
Iteration 11100: Loss = -11259.676845374892
3
Iteration 11200: Loss = -11259.676614097507
4
Iteration 11300: Loss = -11259.67207713404
5
Stopping early at iteration 11300 due to no improvement.
pi: tensor([[0.7334, 0.2666],
        [0.2384, 0.7616]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9087, 0.0913], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1910, 0.1034],
         [0.6089, 0.3049]],

        [[0.5270, 0.1006],
         [0.6420, 0.6621]],

        [[0.5545, 0.1047],
         [0.5111, 0.5380]],

        [[0.6315, 0.1097],
         [0.6650, 0.6038]],

        [[0.6953, 0.1010],
         [0.7010, 0.5647]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.01737152632365585
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 32
Adjusted Rand Index: 0.12402048831758941
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.24856904917899184
Average Adjusted Rand Index: 0.5669455151785113
11164.936214332021
[0.6202016374437914, 0.6202016374437914, 0.6202016374437914, 0.24856904917899184] [0.7284700897989642, 0.7284700897989642, 0.7284700897989642, 0.5669455151785113] [11214.411056547813, 11214.313496908255, 11214.304341753945, 11259.67207713404]
-----------------------------------------------------------------------------------------
This iteration is 22
True Objective function: Loss = -11260.965531730853
Iteration 0: Loss = -11571.1820388989
Iteration 10: Loss = -11571.1820388989
1
Iteration 20: Loss = -11571.182038898913
2
Iteration 30: Loss = -11571.18203890577
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[1.0000e+00, 2.6263e-13],
        [1.0000e+00, 2.4379e-20]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 2.4018e-13])
beta: tensor([[[0.1757, 0.1342],
         [0.1994, 0.2793]],

        [[0.7627, 0.2506],
         [0.9737, 0.5096]],

        [[0.7857, 0.2799],
         [0.3158, 0.6793]],

        [[0.4143, 0.2808],
         [0.9239, 0.8676]],

        [[0.8668, 0.2496],
         [0.3239, 0.1703]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [9:27:19<31:03:08, 1451.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [9:47:03<28:57:10, 1371.46s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [10:14:24<30:15:28, 1452.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [10:39:03<30:01:03, 1460.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [11:05:06<30:14:25, 1491.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [11:36:46<32:16:42, 1613.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [11:57:19<29:34:17, 1499.40s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|███       | 30/100 [12:26:26<30:35:59, 1573.71s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
